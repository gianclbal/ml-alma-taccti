{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch DistilBERT Fine-tuning Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "import logging\n",
    "from sklearn.metrics import classification_report\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n",
      "Training and test sets loaded.\n"
     ]
    }
   ],
   "source": [
    "# Collab\n",
    "collab = False\n",
    "\n",
    "if collab:\n",
    "  pat = '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'\n",
    "  !git clone https://{pat}@github.com/gianclbal/ALMA-TACIT.git\n",
    "  print(\"Running in collab\")\n",
    "  training_df = pd.read_csv('/content/ALMA-TACIT/data-analysis/new_data/attainment/attainment_fall_2019_fall_2023_full_sentence_training_data.csv',encoding='utf-8')\n",
    "  print(training_df.shape)\n",
    "  test_df = pd.read_csv('/content/ALMA-TACIT/data-analysis/new_data/attainment/attainment_fall_2019_fall_2023_full_sentence_test_data.csv',encoding='utf-8')\n",
    "  print(test_df.shape)\n",
    "  augmented_data = pd.read_csv(\"/content/ALMA-TACIT/data-analysis/new_data/attainment/augmented_dataset/atn_augmented_dataset_1155.csv\")\n",
    "  \n",
    "  print(\"Training and test sets loaded.\")\n",
    "else:\n",
    "  print(\"Running locally\")\n",
    "  # training_df = pd.read_csv('../../new_data/attainment/attainment_fall_2019_fall_2023_full_sentence_training_data.csv',encoding='utf-8')\n",
    "  # print(training_df.shape)\n",
    "  # test_df = pd.read_csv('../../new_data/attainment/attainment_fall_2019_fall_2023_full_sentence_test_data.csv',encoding='utf-8')\n",
    "  # print(test_df.shape)\n",
    "  # augmented_data = pd.read_csv(\"../../new_data/attainment/augmented_dataset/atn_augmented_dataset_1155.csv\")\n",
    "  \n",
    "  merged_aspirational_df = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/merged_Aspirational_sentence_level_batch_1_jaccard.csv\", encoding='utf-8')\n",
    "  merged_aspirational_df\n",
    "  training_df, test_df = train_test_split(merged_aspirational_df, test_size=0.2, random_state=18, stratify=merged_aspirational_df['label'])\n",
    "  print\n",
    "  print(\"Training and test sets loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>one is obviously this class is a requirement f...</td>\n",
       "      <td>0</td>\n",
       "      <td>['I am here so that I can enhance my education...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>to get a better and broader perspective of life.</td>\n",
       "      <td>1</td>\n",
       "      <td>['I am here because I wanted a significant cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>that is what i am in school for but it is not ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['I would like to be a cardiothoracic surgeon.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>i should have dropped the class, it was a very...</td>\n",
       "      <td>0</td>\n",
       "      <td>['This semester I want to prove to myself that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>i was apart of a sci course last semester and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['I am here because I am dedicated to my succe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label  \\\n",
       "1341  one is obviously this class is a requirement f...      0   \n",
       "3597   to get a better and broader perspective of life.      1   \n",
       "389   that is what i am in school for but it is not ...      0   \n",
       "1798  i should have dropped the class, it was a very...      0   \n",
       "1827  i was apart of a sci course last semester and ...      0   \n",
       "\n",
       "                                                 phrase  \n",
       "1341  ['I am here so that I can enhance my education...  \n",
       "3597  ['I am here because I wanted a significant cha...  \n",
       "389    ['I would like to be a cardiothoracic surgeon.']  \n",
       "1798  ['This semester I want to prove to myself that...  \n",
       "1827  ['I am here because I am dedicated to my succe...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>also i want to be an optometrist and this clas...</td>\n",
       "      <td>1</td>\n",
       "      <td>['Also I want to be an optometrist and this cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>unfortunately it is a requirement for biology ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"I am here because this class is one of many ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2681</th>\n",
       "      <td>i am here to learn beyond my prior knowledge.</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"I'm here because I need this class to gradua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>i have no specific direction in my life other ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['I want to try to get into veterinary school ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2807</th>\n",
       "      <td>im here because i want to pursue a career in t...</td>\n",
       "      <td>0</td>\n",
       "      <td>['More specifically, this class will get me on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label  \\\n",
       "447   also i want to be an optometrist and this clas...      1   \n",
       "727   unfortunately it is a requirement for biology ...      0   \n",
       "2681      i am here to learn beyond my prior knowledge.      0   \n",
       "1538  i have no specific direction in my life other ...      0   \n",
       "2807  im here because i want to pursue a career in t...      0   \n",
       "\n",
       "                                                 phrase  \n",
       "447   ['Also I want to be an optometrist and this cl...  \n",
       "727   [\"I am here because this class is one of many ...  \n",
       "2681  [\"I'm here because I need this class to gradua...  \n",
       "1538  ['I want to try to get into veterinary school ...  \n",
       "2807  ['More specifically, this class will get me on...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 150\n",
    "BATCH_SIZE = 6\n",
    "# WEIGHT_DECAY = 0.01\n",
    "# EPOCHS = 1\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# model names\n",
    "roberta_name = \"roberta-base\"\n",
    "bert_name = \"bert-base-uncased\"\n",
    "distilbert_name = \"distilbert-base-uncased\"\n",
    "\n",
    "list_of_model_names =[roberta_name, bert_name, distilbert_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceData(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len, model):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe[\"sentence\"]\n",
    "        self.targets = self.data[\"label\"]\n",
    "        self.max_len = max_len\n",
    "        self.model = model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        if self.model == \"roberta-base\" or self.model == \"bert-base-uncased\":\n",
    "            token_type_ids = inputs[\"token_type_ids\"]\n",
    "        else:\n",
    "            token_type_ids = None\n",
    "\n",
    "        return_dict = {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "        if token_type_ids is not None:\n",
    "            return_dict['token_type_ids'] = torch.tensor(token_type_ids, dtype=torch.float)\n",
    "\n",
    "        return return_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(train_df, test_df, max_len, list_of_model_names):\n",
    "\n",
    "    datasets = {}\n",
    "\n",
    "    X = train_df['sentence']\n",
    "    y = train_df['label']\n",
    "\n",
    "    # Split the data\n",
    "    train_dataset, validation_dataset = train_test_split(train_df, test_size=0.1, random_state=18, stratify=training_df.label)\n",
    "\n",
    "    train_dataset.reset_index(drop=True, inplace=True)\n",
    "    validation_dataset.reset_index(drop=True, inplace=True)\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "    print(\"VALIDATION Dataset: {}\".format(validation_dataset.shape))\n",
    "    print(\"TEST Dataset: {}\".format(test_df.shape))\n",
    "\n",
    "    # data loader parameters\n",
    "    train_params = {'batch_size': BATCH_SIZE,\n",
    "                # 'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "    validate_params = {'batch_size': BATCH_SIZE,\n",
    "                    # 'shuffle': True,\n",
    "                    'num_workers': 0\n",
    "                    }\n",
    "    test_params = {'batch_size': BATCH_SIZE,\n",
    "                    # 'shuffle': True,\n",
    "                    'num_workers': 0\n",
    "                    }\n",
    "\n",
    "    for model_name in list_of_model_names:\n",
    "        training_set = SentenceData(train_dataset, AutoTokenizer.from_pretrained(model_name), max_len, model_name)\n",
    "        validate_set = SentenceData(validation_dataset, AutoTokenizer.from_pretrained(model_name), max_len, model_name)\n",
    "        testing_set = SentenceData(test_df, AutoTokenizer.from_pretrained(model_name), max_len, model_name)\n",
    "\n",
    "        training_loader = DataLoader(training_set, **train_params)\n",
    "        validate_loader = DataLoader(validate_set, **validate_params)\n",
    "        testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "        datasets[model_name] = {'train': training_loader, 'test': testing_loader, 'validate': validate_loader}\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Dataset: (3386, 3)\n",
      "VALIDATION Dataset: (377, 3)\n",
      "TEST Dataset: (941, 3)\n"
     ]
    }
   ],
   "source": [
    "exp_4_datasets = data_loader(train_df=training_df,\n",
    "            test_df=test_df,\n",
    "            max_len=MAX_LEN,\n",
    "            list_of_model_names=list_of_model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am majoring in biology and i am planning on ...</td>\n",
       "      <td>1</td>\n",
       "      <td>['planning on declaring a concentration in zoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it helps in understanding the mechanics of the...</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"I'm in this class because it's a step to ach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i knew i was going to major in cinema and sfsu...</td>\n",
       "      <td>0</td>\n",
       "      <td>['I am here at SFSU due to its cinema program']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i know it takes time and patience to get to wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>['In the future, Im hoping to be in the medica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i just hope i can pull it off.</td>\n",
       "      <td>0</td>\n",
       "      <td>['Taking this course is a first step to unders...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  \\\n",
       "0  i am majoring in biology and i am planning on ...      1   \n",
       "1  it helps in understanding the mechanics of the...      0   \n",
       "2  i knew i was going to major in cinema and sfsu...      0   \n",
       "3  i know it takes time and patience to get to wh...      0   \n",
       "4                     i just hope i can pull it off.      0   \n",
       "\n",
       "                                              phrase  \n",
       "0  ['planning on declaring a concentration in zoo...  \n",
       "1  [\"I'm in this class because it's a step to ach...  \n",
       "2    ['I am here at SFSU due to its cinema program']  \n",
       "3  ['In the future, Im hoping to be in the medica...  \n",
       "4  ['Taking this course is a first step to unders...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_4_datasets[\"distilbert-base-uncased\"][\"train\"].dataset.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer and loss function\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_loader = exp_4_datasets[model_name][\"train\"]\n",
    "val_loader = exp_4_datasets[model_name][\"validate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/gbaldonado/miniforge3/envs/ml_env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss_train = 0\n",
    "    correct_predictions_train = 0\n",
    "\n",
    "    for _, batch in enumerate(train_loader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['ids'].to(device)\n",
    "        attention_mask = batch['mask'].to(device)\n",
    "        labels = batch['targets'].to(device)\n",
    "\n",
    "        print(\"here\")\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        total_loss_train += loss.item()\n",
    "        correct_predictions_train += torch.sum(torch.argmax(logits, dim=1) == labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    total_loss_val = 0\n",
    "    correct_predictions_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(val_loader, 0):\n",
    "            input_ids = batch['ids'].to(device)\n",
    "            attention_mask = batch['mask'].to(device)\n",
    "            labels = batch['targets'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss_val += loss.item()\n",
    "            correct_predictions_val += torch.sum(torch.argmax(logits, dim=1) == labels)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print(f'Train Loss: {total_loss_train / len(train_loader)}, Train Accuracy: {correct_predictions_train.double() / len(train_loader.dataset)}')\n",
    "    print(f'Validation Loss: {total_loss_val / len(val_loader)}, Validation Accuracy: {correct_predictions_val.double() / len(val_loader.dataset)}')\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
