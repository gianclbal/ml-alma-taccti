{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ktrain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ktrain import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>phrase</th>\n",
       "      <th>batch</th>\n",
       "      <th>original_label</th>\n",
       "      <th>changed?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>why am i here?</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"ever since i was little i wanted to be a doc...</td>\n",
       "      <td>batch_1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>well why does anyone pursue a higher education?</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"ever since i was little i wanted to be a doc...</td>\n",
       "      <td>batch_1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to better one self and be able to succeed late...</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"ever since i was little i wanted to be a doc...</td>\n",
       "      <td>batch_1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ever since i was little i wanted to be a docto...</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"ever since i was little i wanted to be a doc...</td>\n",
       "      <td>batch_1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i always wanted to be able to help people and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"ever since i was little i wanted to be a doc...</td>\n",
       "      <td>batch_1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8715</th>\n",
       "      <td>anyways, the path im referring to is basically...</td>\n",
       "      <td>0</td>\n",
       "      <td>['one of these goals is being in a career i en...</td>\n",
       "      <td>batch_2</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8716</th>\n",
       "      <td>i know that i want a career in which ill help ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['one of these goals is being in a career i en...</td>\n",
       "      <td>batch_2</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8717</th>\n",
       "      <td>in any case, i intend to keep moving forward i...</td>\n",
       "      <td>0</td>\n",
       "      <td>['one of these goals is being in a career i en...</td>\n",
       "      <td>batch_2</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8718</th>\n",
       "      <td>i am here to fulfill a covenant with myself an...</td>\n",
       "      <td>1</td>\n",
       "      <td>['i am here to fulfill a covenant with myself ...</td>\n",
       "      <td>batch_2</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8719</th>\n",
       "      <td>i am here to be a learned person.i am here to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>['i am here to fulfill a covenant with myself ...</td>\n",
       "      <td>batch_2</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8720 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label  \\\n",
       "0                                        why am i here?      0   \n",
       "1       well why does anyone pursue a higher education?      0   \n",
       "2     to better one self and be able to succeed late...      1   \n",
       "3     ever since i was little i wanted to be a docto...      1   \n",
       "4     i always wanted to be able to help people and ...      1   \n",
       "...                                                 ...    ...   \n",
       "8715  anyways, the path im referring to is basically...      0   \n",
       "8716  i know that i want a career in which ill help ...      0   \n",
       "8717  in any case, i intend to keep moving forward i...      0   \n",
       "8718  i am here to fulfill a covenant with myself an...      1   \n",
       "8719  i am here to be a learned person.i am here to ...      1   \n",
       "\n",
       "                                                 phrase    batch  \\\n",
       "0     [\"ever since i was little i wanted to be a doc...  batch_1   \n",
       "1     [\"ever since i was little i wanted to be a doc...  batch_1   \n",
       "2     [\"ever since i was little i wanted to be a doc...  batch_1   \n",
       "3     [\"ever since i was little i wanted to be a doc...  batch_1   \n",
       "4     [\"ever since i was little i wanted to be a doc...  batch_1   \n",
       "...                                                 ...      ...   \n",
       "8715  ['one of these goals is being in a career i en...  batch_2   \n",
       "8716  ['one of these goals is being in a career i en...  batch_2   \n",
       "8717  ['one of these goals is being in a career i en...  batch_2   \n",
       "8718  ['i am here to fulfill a covenant with myself ...  batch_2   \n",
       "8719  ['i am here to fulfill a covenant with myself ...  batch_2   \n",
       "\n",
       "      original_label  changed?  \n",
       "0                  0     False  \n",
       "1                  0     False  \n",
       "2                  1     False  \n",
       "3                  1     False  \n",
       "4                  1     False  \n",
       "...              ...       ...  \n",
       "8715               0     False  \n",
       "8716               0     False  \n",
       "8717               0     False  \n",
       "8718               1     False  \n",
       "8719               1     False  \n",
       "\n",
       "[8720 rows x 6 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspirational_df = pd.read_excel('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/src/data_preprocessing/prepare_sentence_level/aspirational_plus_batch_1_batch_2_merged.xlsx')\n",
    "aspirational_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 Train: 2727, Test: 770, Total: 3497\n",
      "Batch 1 + Batch 2 Train: 2893, Test: 909, Total: 8720\n",
      "‚úÖ Train and test sets successfully created and saved!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/src/data_preprocessing/prepare_sentence_level/aspirational_plus_batch_1_batch_2_merged.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Ensure 'batch' column is a string\n",
    "df[\"batch\"] = df[\"batch\"].astype(str)\n",
    "\n",
    "# Split Batch 1 only\n",
    "batch1_df = df[df[\"batch\"] == \"batch_1\"]\n",
    "\n",
    "# Perform train-test split for Batch 1 (keeping stratification)\n",
    "train_batch1, test_batch1 = train_test_split(batch1_df, test_size=0.22, random_state=42, stratify=batch1_df[\"label\"])\n",
    "\n",
    "# Verify counts\n",
    "print(f\"Batch 1 Train: {len(train_batch1)}, Test: {len(test_batch1)}, Total: {len(batch1_df)}\")\n",
    "\n",
    "# Ensure all sentences in Batch 1 train/test are found in Batch 1 + Batch 2\n",
    "batch1_2_df = df  # Full dataset (Batch 1 + Batch 2)\n",
    "\n",
    "# Use the same sentences from Batch 1 train in Batch 1 + Batch 2 train\n",
    "train_batch1_2 = batch1_2_df[batch1_2_df[\"sentence\"].isin(train_batch1[\"sentence\"])]\n",
    "\n",
    "# Use the same sentences from Batch 1 test i\"n Batch 1 + Batch 2 test\n",
    "test_batch1_2 = batch1_2_df[batch1_2_df[\"sentence\"].isin(test_batch1[\"sentence\"])]\n",
    "\n",
    "# Verify counts match\n",
    "print(f\"Batch 1 + Batch 2 Train: {len(train_batch1_2)}, Test: {len(test_batch1_2)}, Total: {len(batch1_2_df)}\")\n",
    "\n",
    "# Save final DataFrames\n",
    "train_batch1.to_excel(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/src/data_preprocessing/prepare_sentence_level/train_batch1.xlsx\", index=False)\n",
    "test_batch1.to_excel(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/src/data_preprocessing/prepare_sentence_level/test_batch1.xlsx\", index=False)\n",
    "train_batch1_2.to_excel(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/src/data_preprocessing/prepare_sentence_level/train_batch1_2.xlsx\", index=False)\n",
    "test_batch1_2.to_excel(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/src/data_preprocessing/prepare_sentence_level/test_batch1_2.xlsx\", index=False)\n",
    "\n",
    "print(\"‚úÖ Train and test sets successfully created and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 Train: 2727, Test: 770, Total: 3497\n",
      "Batch 1 + Batch 2 Train: 8116, Test: 909, Total: 8720\n",
      "‚úÖ Train and test sets successfully corrected and saved!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/src/data_preprocessing/prepare_sentence_level/aspirational_plus_batch_1_batch_2_merged.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Ensure 'batch' column is a string\n",
    "df[\"batch\"] = df[\"batch\"].astype(str)\n",
    "\n",
    "# Split Batch 1 only\n",
    "batch1_df = df[df[\"batch\"] == \"batch_1\"]\n",
    "\n",
    "# Perform train-test split for Batch 1 (keeping stratification)\n",
    "train_batch1, test_batch1 = train_test_split(batch1_df, test_size=0.22, random_state=42, stratify=batch1_df[\"label\"])\n",
    "\n",
    "# Verify counts\n",
    "print(f\"Batch 1 Train: {len(train_batch1)}, Test: {len(test_batch1)}, Total: {len(batch1_df)}\")\n",
    "\n",
    "# Ensure all sentences in Batch 1 train/test are also in Batch 1 + Batch 2\n",
    "batch1_2_df = df  # Full dataset (Batch 1 + Batch 2)\n",
    "\n",
    "# --- üõ†Ô∏è **Fix for incorrect filtering** ---\n",
    "# Step 1: Retrieve all Batch 1 sentences from Batch 1+2\n",
    "train_batch1_2 = batch1_2_df[batch1_2_df[\"sentence\"].isin(train_batch1[\"sentence\"])]\n",
    "test_batch1_2 = batch1_2_df[batch1_2_df[\"sentence\"].isin(test_batch1[\"sentence\"])]\n",
    "\n",
    "# Step 2: Add the remaining Batch 2 sentences (not already in train/test)\n",
    "batch2_df = df[df[\"batch\"] == \"batch_2\"]\n",
    "train_batch1_2 = pd.concat([train_batch1_2, batch2_df], ignore_index=True)\n",
    "\n",
    "# Verify counts match\n",
    "print(f\"Batch 1 + Batch 2 Train: {len(train_batch1_2)}, Test: {len(test_batch1_2)}, Total: {len(batch1_2_df)}\")\n",
    "\n",
    "# # Save final DataFrames\n",
    "train_batch1.to_excel(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/src/data_preprocessing/prepare_sentence_level/train_batch1.xlsx\", index=False)\n",
    "test_batch1.to_excel(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/src/data_preprocessing/prepare_sentence_level/test_batch1.xlsx\", index=False)\n",
    "train_batch1_2.to_excel(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/src/data_preprocessing/prepare_sentence_level/train_batch1_2.xlsx\", index=False)\n",
    "test_batch1_2.to_excel(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/src/data_preprocessing/prepare_sentence_level/test_batch1_2.xlsx\", index=False)\n",
    "\n",
    "print(\"‚úÖ Train and test sets successfully corrected and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä aspirational Dataset Train-Test Split Results:\n",
      "‚úÖ Train: 8116 samples (Positives: 1895, 23.35%)\n",
      "‚úÖ Test: 909 samples (Positives: 196, 21.56%)\n",
      "üîπ Total Dataset: 8720 samples\n"
     ]
    }
   ],
   "source": [
    "# Count total samples\n",
    "total_train = len(train_batch1_2)\n",
    "total_test = len(test_batch1_2)\n",
    "\n",
    "# Count positive (Class 1) samples\n",
    "train_positives = train_batch1_2[\"label\"].sum()\n",
    "test_positives = test_batch1_2[\"label\"].sum()\n",
    "\n",
    "# Calculate percentages\n",
    "train_positive_pct = (train_positives / total_train) * 100\n",
    "test_positive_pct = (test_positives / total_test) * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"üìä aspirational Dataset Train-Test Split Results:\")\n",
    "print(f\"‚úÖ Train: {total_train} samples (Positives: {train_positives}, {train_positive_pct:.2f}%)\")\n",
    "print(f\"‚úÖ Test: {total_test} samples (Positives: {test_positives}, {test_positive_pct:.2f}%)\")\n",
    "print(f\"üîπ Total Dataset: {len(batch1_2_df)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_batch1_2['sentence']\n",
    "y = train_batch1_2['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 18, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing train...\n",
      "language: en\n",
      "train sequence lengths:\n",
      "\tmean : 21\n",
      "\t95percentile : 40\n",
      "\t99percentile : 56\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 21\n",
      "\t95percentile : 40\n",
      "\t99percentile : 53\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distillbert_transformer = text.Transformer(model_name, maxlen=MAXLEN, class_names=[0,1])\n",
    "training_set = distillbert_transformer.preprocess_train(X_train.tolist(), y_train.tolist())\n",
    "validation_set = distillbert_transformer.preprocess_test(X_test.tolist(), y_test.tolist())\n",
    "distillbert_base_model = distillbert_transformer.get_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "distillbert_learner = ktrain.get_learner(distillbert_base_model, train_data=training_set, val_data=validation_set, batch_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using triangular learning rate policy with max lr of 2e-05...\n",
      "Epoch 1/5\n",
      "1082/1082 [==============================] - 227s 199ms/step - loss: 0.3437 - accuracy: 0.8524 - val_loss: 0.2927 - val_accuracy: 0.8824\n",
      "Epoch 2/5\n",
      "1082/1082 [==============================] - 211s 195ms/step - loss: 0.2199 - accuracy: 0.9145 - val_loss: 0.3029 - val_accuracy: 0.8793\n",
      "Epoch 3/5\n",
      "1082/1082 [==============================] - 201s 185ms/step - loss: 0.1554 - accuracy: 0.9432 - val_loss: 0.3330 - val_accuracy: 0.8824\n",
      "Epoch 4/5\n",
      "1082/1082 [==============================] - 214s 198ms/step - loss: 0.1060 - accuracy: 0.9650 - val_loss: 0.3728 - val_accuracy: 0.8793\n",
      "Epoch 5/5\n",
      "1082/1082 [==============================] - 255s 235ms/step - loss: 0.0726 - accuracy: 0.9769 - val_loss: 0.4266 - val_accuracy: 0.8762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x39cf74eb0>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distillbert_learner.autofit(2e-5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 17s 250ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92      1245\n",
      "           1       0.74      0.72      0.73       379\n",
      "\n",
      "    accuracy                           0.88      1624\n",
      "   macro avg       0.83      0.82      0.82      1624\n",
      "weighted avg       0.87      0.88      0.88      1624\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1152,   93],\n",
       "       [ 108,  271]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distillbert_learner.validate(class_names=distillbert_transformer.get_classes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 10s 194ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92      1245\n",
      "           1       0.74      0.72      0.73       379\n",
      "\n",
      "    accuracy                           0.88      1624\n",
      "   macro avg       0.83      0.82      0.82      1624\n",
      "weighted avg       0.87      0.88      0.88      1624\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1152,   93],\n",
       "       [ 108,  271]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distillbert_learner.validate(class_names=distillbert_transformer.get_classes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMa  multiple                  66362880  \n",
      " inLayer)                                                        \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      " dropout_239 (Dropout)       multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66955010 (255.41 MB)\n",
      "Trainable params: 66955010 (255.41 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "distillbert_learner.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "distillbert_predictor = ktrain.get_predictor(distillbert_learner.model, preproc=distillbert_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "distillbert_test_data = test_batch1_2['sentence'].tolist()\n",
    "distillbert_test_label = test_batch1_2['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_distillbert = distillbert_predictor.predict(distillbert_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_distillbert = [int(x) for x in y_pred_distillbert]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negative: 655, False Positive: 58, False Negative: 63, True Positive: 133\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(distillbert_test_label, y_pred_distillbert).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.92       713\n",
      "           1       0.70      0.68      0.69       196\n",
      "\n",
      "    accuracy                           0.87       909\n",
      "   macro avg       0.80      0.80      0.80       909\n",
      "weighted avg       0.87      0.87      0.87       909\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('  Classification Report:\\n',classification_report(distillbert_test_label,y_pred_distillbert),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95       117\n",
      "           1       0.72      0.92      0.81        25\n",
      "\n",
      "    accuracy                           0.92       142\n",
      "   macro avg       0.85      0.92      0.88       142\n",
      "weighted avg       0.94      0.92      0.93       142\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('  Classification Report:\\n',classification_report(distillbert_test_label,y_pred_distillbert),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distillbert_predictor.save('./model/distilbert_base_uncased_model') # 256 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC roc score for distillbert model:  0.8009703180009733\n"
     ]
    }
   ],
   "source": [
    "print(\"AUC roc score for distillbert model: \", roc_auc_score(distillbert_test_label,y_pred_distillbert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
