{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from random import shuffle\n",
    "random.seed(18)\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# from nltk import word_tokenize\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import stem\n",
    "import csv\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import learning_curve,GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a79dfa17e243518f9142b1d52325ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-12 18:17:45 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-07-12 18:17:45 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-07-12 18:17:47 INFO: File exists: /Users/gbaldonado/stanza_resources/en/default.zip\n",
      "2024-07-12 18:17:50 INFO: Finished downloading models and saved to /Users/gbaldonado/stanza_resources\n",
      "2024-07-12 18:17:50 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1cde6efdede4aa4a2c1995dd18ed594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-12 18:17:51 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-07-12 18:17:52 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-07-12 18:17:52 INFO: Using device: cpu\n",
      "2024-07-12 18:17:52 INFO: Loading: tokenize\n",
      "2024-07-12 18:17:52 INFO: Loading: mwt\n",
      "2024-07-12 18:17:52 INFO: Loading: pos\n",
      "2024-07-12 18:17:53 INFO: Loading: lemma\n",
      "2024-07-12 18:17:53 INFO: Loading: constituency\n",
      "2024-07-12 18:17:53 INFO: Loading: depparse\n",
      "2024-07-12 18:17:53 INFO: Loading: sentiment\n",
      "2024-07-12 18:17:54 INFO: Loading: ner\n",
      "2024-07-12 18:17:54 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# from pycorenlp import StanfordCoreNLP\n",
    "# nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "import stanza\n",
    "stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline('en') # initialize English neural pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_aspirational_df = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/merged_Aspirational_sentence_level_batch_1_jaccard.csv\", encoding='utf-8')\n",
    "training_df, test_df = train_test_split(merged_aspirational_df, test_size=0.2, random_state=18, stratify=merged_aspirational_df['label'])\n",
    "training_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one is obviously this class is a requirement for my general education credits and also because i thought it'd be interesting to further my education on astronomy.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I am here so that I can enhance my education and earn a bachelors degree so that I can get a job that makes a good salary and that I get to do something exciting and interesting.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to get a better and broader perspective of life.</td>\n",
       "      <td>1</td>\n",
       "      <td>['I am here because I wanted a significant change in my life.', 'To get a better and broader perspective of life.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that is what i am in school for but it is not going so well.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I would like to be a cardiothoracic surgeon.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i should have dropped the class, it was a very frustration experience, but i waited it out because i thought that if i studied enough, went to office hours enough, that i would pass.</td>\n",
       "      <td>0</td>\n",
       "      <td>['This semester I want to prove to myself that I am smart and capable of passing this class with an A. I want to actually learn the concepts and not come into class everyday confused and lost.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was apart of a sci course last semester and i loved how helpful the instructors were.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I am here because I am dedicated to my success in my STEM based courses.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3758</th>\n",
       "      <td>by the time i got here at sfsu i decided to just do it.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I am at San Francisco State University to get a bachelors degree in Civil Engineering.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3759</th>\n",
       "      <td>thiis is something that is shitty because i do not want to study cs. it is something my parents want me to study and i and doing so because they are paying for my classes.</td>\n",
       "      <td>0</td>\n",
       "      <td>['i want to get a degree in what i am studying']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3760</th>\n",
       "      <td>in addition, i want to become an engineer someday and that starts with me learning basics physics concepts that are going to be applied to my career.</td>\n",
       "      <td>1</td>\n",
       "      <td>['I want to become an engineer someday and that starts with me learning basics physics concepts that are going to be applied to my career.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3761</th>\n",
       "      <td>ever since i was little i wanted to be a doctor, so much so that i can't see myself doing anything else.</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"Ever since I was little I wanted to be a doctor, so much so that I can't see myself doing anything else.\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3762</th>\n",
       "      <td>why im here i am here because of my career goal.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I am here because of my career goal. I want to be a Doctor and have a subspecialty as a neonatologist.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3763 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                    sentence  \\\n",
       "0                         one is obviously this class is a requirement for my general education credits and also because i thought it'd be interesting to further my education on astronomy.   \n",
       "1                                                                                                                                           to get a better and broader perspective of life.   \n",
       "2                                                                                                                               that is what i am in school for but it is not going so well.   \n",
       "3     i should have dropped the class, it was a very frustration experience, but i waited it out because i thought that if i studied enough, went to office hours enough, that i would pass.   \n",
       "4                                                                                                    i was apart of a sci course last semester and i loved how helpful the instructors were.   \n",
       "...                                                                                                                                                                                      ...   \n",
       "3758                                                                                                                                 by the time i got here at sfsu i decided to just do it.   \n",
       "3759             thiis is something that is shitty because i do not want to study cs. it is something my parents want me to study and i and doing so because they are paying for my classes.   \n",
       "3760                                   in addition, i want to become an engineer someday and that starts with me learning basics physics concepts that are going to be applied to my career.   \n",
       "3761                                                                                ever since i was little i wanted to be a doctor, so much so that i can't see myself doing anything else.   \n",
       "3762                                                                                                                                        why im here i am here because of my career goal.   \n",
       "\n",
       "      label  \\\n",
       "0         0   \n",
       "1         1   \n",
       "2         0   \n",
       "3         0   \n",
       "4         0   \n",
       "...     ...   \n",
       "3758      0   \n",
       "3759      0   \n",
       "3760      1   \n",
       "3761      1   \n",
       "3762      0   \n",
       "\n",
       "                                                                                                                                                                                                  phrase  \n",
       "0                 ['I am here so that I can enhance my education and earn a bachelors degree so that I can get a job that makes a good salary and that I get to do something exciting and interesting.']  \n",
       "1                                                                                    ['I am here because I wanted a significant change in my life.', 'To get a better and broader perspective of life.']  \n",
       "2                                                                                                                                                       ['I would like to be a cardiothoracic surgeon.']  \n",
       "3     ['This semester I want to prove to myself that I am smart and capable of passing this class with an A. I want to actually learn the concepts and not come into class everyday confused and lost.']  \n",
       "4                                                                                                                           ['I am here because I am dedicated to my success in my STEM based courses.']  \n",
       "...                                                                                                                                                                                                  ...  \n",
       "3758                                                                                                          ['I am at San Francisco State University to get a bachelors degree in Civil Engineering.']  \n",
       "3759                                                                                                                                                    ['i want to get a degree in what i am studying']  \n",
       "3760                                                        ['I want to become an engineer someday and that starts with me learning basics physics concepts that are going to be applied to my career.']  \n",
       "3761                                                                                        [\"Ever since I was little I wanted to be a doctor, so much so that I can't see myself doing anything else.\"]  \n",
       "3762                                                                                          ['I am here because of my career goal. I want to be a Doctor and have a subspecialty as a neonatologist.']  \n",
       "\n",
       "[3763 rows x 3 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one is obviously this class is a requirement for my general education credits and also because i thought it'd be interesting to further my education on astronomy.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I am here so that I can enhance my education and earn a bachelors degree so that I can get a job that makes a good salary and that I get to do something exciting and interesting.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to get a better and broader perspective of life.</td>\n",
       "      <td>1</td>\n",
       "      <td>['I am here because I wanted a significant change in my life.', 'To get a better and broader perspective of life.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that is what i am in school for but it is not going so well.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I would like to be a cardiothoracic surgeon.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i should have dropped the class, it was a very frustration experience, but i waited it out because i thought that if i studied enough, went to office hours enough, that i would pass.</td>\n",
       "      <td>0</td>\n",
       "      <td>['This semester I want to prove to myself that I am smart and capable of passing this class with an A. I want to actually learn the concepts and not come into class everyday confused and lost.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was apart of a sci course last semester and i loved how helpful the instructors were.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I am here because I am dedicated to my success in my STEM based courses.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                 sentence  \\\n",
       "0                      one is obviously this class is a requirement for my general education credits and also because i thought it'd be interesting to further my education on astronomy.   \n",
       "1                                                                                                                                        to get a better and broader perspective of life.   \n",
       "2                                                                                                                            that is what i am in school for but it is not going so well.   \n",
       "3  i should have dropped the class, it was a very frustration experience, but i waited it out because i thought that if i studied enough, went to office hours enough, that i would pass.   \n",
       "4                                                                                                 i was apart of a sci course last semester and i loved how helpful the instructors were.   \n",
       "\n",
       "   label  \\\n",
       "0      0   \n",
       "1      1   \n",
       "2      0   \n",
       "3      0   \n",
       "4      0   \n",
       "\n",
       "                                                                                                                                                                                               phrase  \n",
       "0              ['I am here so that I can enhance my education and earn a bachelors degree so that I can get a job that makes a good salary and that I get to do something exciting and interesting.']  \n",
       "1                                                                                 ['I am here because I wanted a significant change in my life.', 'To get a better and broader perspective of life.']  \n",
       "2                                                                                                                                                    ['I would like to be a cardiothoracic surgeon.']  \n",
       "3  ['This semester I want to prove to myself that I am smart and capable of passing this class with an A. I want to actually learn the concepts and not come into class everyday confused and lost.']  \n",
       "4                                                                                                                        ['I am here because I am dedicated to my success in my STEM based courses.']  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive labels present in the dataset : 470  out of 3763 or 12.490034546904067%\n"
     ]
    }
   ],
   "source": [
    "pos_labels = len([n for n in training_df['label'] if n==1])\n",
    "print(\"Positive labels present in the dataset : {}  out of {} or {}%\".format(pos_labels, len(training_df['label']), (pos_labels/len(training_df['label']))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = training_df['label'].value_counts()\n",
    "sns.barplot(x)\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.title('Distribution of labels in training set')\n",
    "plt.gca().set_ylabel('Sentences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(941, 3)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>also i want to be an optometrist and this class is part of that journey.</td>\n",
       "      <td>1</td>\n",
       "      <td>['Also I want to be an optometrist and this class is part of that journey.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unfortunately it is a requirement for biology major students.</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"I am here because this class is one of many classes I need to take to complete a bachelor's degree in biology.\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i am here to learn beyond my prior knowledge.</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"I'm here because I need this class to graduate, but to zoom out, I'm in school to succeed. I am here to learn beyond my prior knowledge. I am here so that I am can become confident in speaking up and sharing my thoughts in an organized structure. I am here so I am knowledgeable enough to transform and create ideas and creations.\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i have no specific direction in my life other than the general goal i have always had since i knew how to talk.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I want to try to get into veterinary school that is why I am in OCHEM.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im here because i want to pursue a career in the medical field.</td>\n",
       "      <td>0</td>\n",
       "      <td>['More specifically, this class will get me one step closer to my career goal']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                          sentence  \\\n",
       "0                                         also i want to be an optometrist and this class is part of that journey.   \n",
       "1                                                    unfortunately it is a requirement for biology major students.   \n",
       "2                                                                    i am here to learn beyond my prior knowledge.   \n",
       "3  i have no specific direction in my life other than the general goal i have always had since i knew how to talk.   \n",
       "4                                                  im here because i want to pursue a career in the medical field.   \n",
       "\n",
       "   label  \\\n",
       "0      1   \n",
       "1      0   \n",
       "2      0   \n",
       "3      0   \n",
       "4      0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                           phrase  \n",
       "0                                                                                                                                                                                                                                                                    ['Also I want to be an optometrist and this class is part of that journey.']  \n",
       "1                                                                                                                                                                                                                              [\"I am here because this class is one of many classes I need to take to complete a bachelor's degree in biology.\"]  \n",
       "2  [\"I'm here because I need this class to graduate, but to zoom out, I'm in school to succeed. I am here to learn beyond my prior knowledge. I am here so that I am can become confident in speaking up and sharing my thoughts in an organized structure. I am here so I am knowledgeable enough to transform and create ideas and creations.\"]  \n",
       "3                                                                                                                                                                                                                                                                      ['I want to try to get into veterinary school that is why I am in OCHEM.']  \n",
       "4                                                                                                                                                                                                                                                                 ['More specifically, this class will get me one step closer to my career goal']  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive labels present in the test dataset : 118  out of 941 or 12.539851222104145%\n"
     ]
    }
   ],
   "source": [
    "pos_labels = len([n for n in test_df['label'] if n==1])\n",
    "print(\"Positive labels present in the test dataset : {}  out of {} or {}%\".format(pos_labels, len(test_df['label']), (pos_labels/len(test_df['label']))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1 = test_df['label'].value_counts()\n",
    "sns.barplot(x1)\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.title('Distribution of labels in test set')\n",
    "plt.gca().set_ylabel('Sentences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. NER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack Obama']\n"
     ]
    }
   ],
   "source": [
    "def get_ner(text):\n",
    "    ner_list = []\n",
    "    # Annotate the text using stanza\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for entity in sentence.ents:\n",
    "            if entity.type == 'PERSON':\n",
    "                ner_list.append(entity.text)\n",
    "\n",
    "    return ner_list\n",
    "\n",
    "# Example usage\n",
    "text = \"Barack Obama was the 44th doctor of the United States.\"\n",
    "print(get_ner(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a named entity is present in the sentence\n",
    "def named_entity_present(sentence):\n",
    "    ner_list = get_ner(sentence)\n",
    "    if len(ner_list) > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Similarity Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the similar words and similarity score\n",
    "# The function takes tokens of sentence as input and if its not a stop word, get its similarity with synsets of STEM.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words |= set([\"help\",\"try\", \"work\", \"process\", \"support\", \"job\"] )\n",
    "def word_similarity(tokens, syns, field):    \n",
    "    if field in ['engineering', 'technology']:\n",
    "        score_threshold = 0.5\n",
    "    else:\n",
    "        score_threshold = 0.2\n",
    "    sim_words = 0\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            try:\n",
    "                syns_word = wordnet.synsets(token) \n",
    "                score = syns_word[0].path_similarity(syns[0])\n",
    "                if score >= score_threshold:\n",
    "                    sim_words += 1\n",
    "            except: \n",
    "                score = 0\n",
    "    \n",
    "    return sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to create columns for similarity based on all STEM fields\n",
    "syns_bio = wordnet.synsets(lemmatizer.lemmatize(\"biology\"))\n",
    "syns_maths = wordnet.synsets(lemmatizer.lemmatize(\"mathematics\")) \n",
    "syns_tech = wordnet.synsets(lemmatizer.lemmatize(\"technology\"))\n",
    "syns_eng = wordnet.synsets(lemmatizer.lemmatize(\"engineering\"))\n",
    "syns_chem = wordnet.synsets(lemmatizer.lemmatize(\"chemistry\"))\n",
    "syns_phy = wordnet.synsets(lemmatizer.lemmatize(\"physics\"))\n",
    "syns_sci = wordnet.synsets(lemmatizer.lemmatize(\"science\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Medical Word Count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pelvic', 'medical', 'interventional', 'advanced', 'child', 'endocrinology', 'nephrology', 'rheumatology', 'calculi', 'strabismus', 'neuromuscular', 'internal', 'disabilities', 'neuropathology', 'genetic', 'immunopathology', 'critical', 'anesthesiology', 'oculoplastics', 'emergency', 'cardiovascular', 'internal', 'behavioral', 'neck', 'transplant', 'neuroradiology', 'transplant', 'neuro', 'imaging', 'genetic', 'allergy', 'sports', 'retardation', 'sleep', 'pediatrics', 'preventive', 'musculoskeletal', 'liaison', 'anatomical', 'cardiology', 'cardiothoracic', 'radiation', 'blood', 'pain', 'disease', 'dermatopathology', 'surgery', 'nephrology', 'ophthalmology', 'segment', 'consultation', 'abuse', 'dermatology', 'endocrinology', 'gastrointestinal', 'gynecologic', 'interventional', 'perinatal', 'psychiatry', 'reproductive', 'gastroenterology', 'sports', 'microbiology', 'adolescent', 'cardiac', 'male', 'forensic', 'metabolism', 'rehabilitation', 'orbit', 'molecular', 'neurology', 'hospice', 'neurodevelopmental', 'failure', 'electrophysiology', 'diabetes', 'neurophysiology', 'uveitis', 'pediatric', 'head', 'aerospace', 'obstetrics', 'surgery', 'transfusion', 'physical', 'cytopathology', 'rheumatology', 'toxicology', 'heart', 'pathology', 'reconstructive', 'female', 'retina', 'neurology', 'cytogenetics', 'dermatology', 'oncology', 'radiology', 'neuroradiology', 'injury', 'military', 'research', 'family', 'reconstructive', 'clinical', 'geriatric', 'pediatrics', 'vascular', 'fetal', 'abdominal', 'banking', 'neonatal', 'developmental', 'hematology', 'ophthalmology', 'immunology', 'critical', 'pulmonary', 'hepatology', 'health', 'urology', 'occupational', 'public', 'pathology', 'endocrinologists', 'cornea', 'ocular', 'breast', 'renal', 'psychosomatic', 'chest', 'gastroenterology', 'hematology', 'infectious', 'endovascular', 'psychiatric', 'diagnostic', 'adolescent', 'biochemical', 'nuclear', 'anesthesiology', 'medicine', 'care', 'infertility', 'surgical', 'chemical', 'pediatric', 'urologic', 'neurourology', 'administrative', 'infectious', 'ophthalmic', 'oncology', 'urology', 'psychiatry', 'maternal', 'gynecology', 'palliative', 'brain', 'procedural', 'diseases', 'pulmonology', 'addiction', 'glaucoma', 'plastic', 'and', 'mental', 'community', 'genitourinary', 'genetics', 'anterior']\n"
     ]
    }
   ],
   "source": [
    "# Load the medical specialization text file and create a list\n",
    "medical_list = []\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/medical_specialities.txt', 'r') as medical_fields:\n",
    "    for line in medical_fields.readlines():\n",
    "        special_field = line.rstrip('\\n')\n",
    "        special_field = re.sub(\"\\W\",\" \", special_field )\n",
    "#         print(special_field)\n",
    "        medical_list += special_field.split()\n",
    "medical_list = list(set(medical_list))  \n",
    "medical_list = [x.lower() for x in medical_list]\n",
    "print(medical_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get medical words\n",
    "def check_medical_words(tokens):\n",
    "    for token in tokens:\n",
    "        if token not in stop_words and token in [x.lower() for x in medical_list]:\n",
    "            return 1\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Sentiment Polarity and Subjectivity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get polarity and subjectivity of the sentence using TexBlob\n",
    "def get_sentiment(sentence):\n",
    "    sentiments =TextBlob(sentence).sentiment\n",
    "    polarity = sentiments.polarity\n",
    "    subjectivity = sentiments.subjectivity\n",
    "    return polarity, subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. POS Tag Count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the count of POS tags of the sentence\n",
    "def count_pos_tags(tokens):\n",
    "    token_pos = pos_tag(tokens)\n",
    "    count = Counter(tag for word,tag in token_pos)\n",
    "    interjections =  count['UH']\n",
    "    nouns = count['NN'] + count['NNS'] + count['NNP'] + count['NNPS']\n",
    "    adverb = count['RB'] + count['RBS'] + count['RBR']\n",
    "    verb = count['VB'] + count['VBD'] + count['VBG'] + count['VBN']\n",
    "    determiner = count['DT']\n",
    "    pronoun = count['PRP']\n",
    "    adjetive = count['JJ'] + count['JJR'] + count['JJS']\n",
    "    preposition = count['IN']\n",
    "    return interjections, nouns, adverb, verb, determiner, pronoun, adjetive,preposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_extraction(dataframe, field, func, column_names):\n",
    "    return pd.concat((\n",
    "        dataframe,\n",
    "        dataframe[field].apply(\n",
    "            lambda cell: pd.Series(func(cell), index=column_names))), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Word Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the w2v dict from pickle file\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/pickle/embeddings06122024.pickle', 'rb') as w2v_file:\n",
    "    w2v_dict = pickle.load(w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of word embeddings:  4762\n"
     ]
    }
   ],
   "source": [
    "print(\"length of word embeddings: \", len(w2v_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vectors for the essay\n",
    "def vectorizer(sequence):\n",
    "    vect = []\n",
    "    numw = 0\n",
    "    for w in sequence: \n",
    "        try :\n",
    "            if numw == 0:\n",
    "                vect = w2v_dict[w]\n",
    "            else:\n",
    "                vect = np.add(vect, w2v_dict[w])\n",
    "            numw += 1\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    return vect/ numw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into words\n",
    "def split_into_words(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Unigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vectorizer\n",
    "unigram_vect = CountVectorizer(ngram_range=(1, 1), min_df=2, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WRAPPER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for feature engineering\n",
    "def feature_engineering(original_dataset):\n",
    "\n",
    "    dataset = original_dataset.copy()\n",
    "    # create a new column with sentence tokens\n",
    "    dataset['tokens'] = dataset['sentence'].apply(word_tokenize)\n",
    "    # 1. Similarity features\n",
    "    # biology\n",
    "    dataset['bio_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_bio,'biology',)) \n",
    "    # chemistry\n",
    "    dataset['chem_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_chem,'chemistry',))\n",
    "    # physics\n",
    "    dataset['phy_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_phy,'physics',))\n",
    "    # mathematics\n",
    "    dataset['math_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_maths,'mathematics',))\n",
    "    # technology\n",
    "    dataset['tech_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_tech,'technology',))\n",
    "    # engineering\n",
    "    dataset['eng_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_eng,'engineering',))\n",
    "    \n",
    "    # medical terms\n",
    "    dataset['medical_terms'] = dataset['tokens'].apply(check_medical_words)\n",
    "    \n",
    "    # polarity and subjectivity\n",
    "    dataset['polarity'], dataset['subjectivity'] = zip(*dataset['sentence'].apply(get_sentiment))\n",
    "    \n",
    "    # named entity recognition\n",
    "    dataset['ner'] = dataset['sentence'].apply(named_entity_present)\n",
    "    \n",
    "    # pos tag count\n",
    "    dataset = pos_tag_extraction(dataset, 'tokens', count_pos_tags, ['interjections', 'nouns', 'adverb', 'verb', 'determiner', 'pronoun', 'adjetive','preposition'])\n",
    "    \n",
    "    # labels\n",
    "    data_labels = dataset['label']\n",
    "    # X\n",
    "    data_x = dataset.drop(columns='label')\n",
    "\n",
    "    \n",
    "    # vectorize all the essays\n",
    "    vect_arr = data_x.tokens.apply(vectorizer)\n",
    "    for index in range(0, len(vect_arr)):\n",
    "        i = 0\n",
    "        for item in vect_arr[index]:\n",
    "            column_name= \"embedding\" + str(i)\n",
    "            data_x.loc[index, column_name] = item\n",
    "            i +=1\n",
    "    \n",
    "    return data_x,data_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = feature_engineering(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 121)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>phrase</th>\n",
       "      <th>tokens</th>\n",
       "      <th>bio_sim_words</th>\n",
       "      <th>chem_sim_words</th>\n",
       "      <th>phy_sim_words</th>\n",
       "      <th>math_sim_words</th>\n",
       "      <th>tech_sim_words</th>\n",
       "      <th>eng_sim_words</th>\n",
       "      <th>medical_terms</th>\n",
       "      <th>...</th>\n",
       "      <th>embedding90</th>\n",
       "      <th>embedding91</th>\n",
       "      <th>embedding92</th>\n",
       "      <th>embedding93</th>\n",
       "      <th>embedding94</th>\n",
       "      <th>embedding95</th>\n",
       "      <th>embedding96</th>\n",
       "      <th>embedding97</th>\n",
       "      <th>embedding98</th>\n",
       "      <th>embedding99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one is obviously this class is a requirement for my general education credits and also because i thought it'd be interesting to further my education on astronomy.</td>\n",
       "      <td>['I am here so that I can enhance my education and earn a bachelors degree so that I can get a job that makes a good salary and that I get to do something exciting and interesting.']</td>\n",
       "      <td>[one, is, obviously, this, class, is, a, requirement, for, my, general, education, credits, and, also, because, i, thought, it, 'd, be, interesting, to, further, my, education, on, astronomy, .]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.487845</td>\n",
       "      <td>0.183000</td>\n",
       "      <td>-0.029280</td>\n",
       "      <td>0.080841</td>\n",
       "      <td>0.227600</td>\n",
       "      <td>-0.069685</td>\n",
       "      <td>0.336133</td>\n",
       "      <td>-0.074894</td>\n",
       "      <td>-0.010053</td>\n",
       "      <td>0.114453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to get a better and broader perspective of life.</td>\n",
       "      <td>['I am here because I wanted a significant change in my life.', 'To get a better and broader perspective of life.']</td>\n",
       "      <td>[to, get, a, better, and, broader, perspective, of, life, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.344721</td>\n",
       "      <td>0.162200</td>\n",
       "      <td>0.014252</td>\n",
       "      <td>-0.021162</td>\n",
       "      <td>0.154372</td>\n",
       "      <td>-0.072484</td>\n",
       "      <td>0.340524</td>\n",
       "      <td>-0.152142</td>\n",
       "      <td>-0.024123</td>\n",
       "      <td>0.113133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that is what i am in school for but it is not going so well.</td>\n",
       "      <td>['I would like to be a cardiothoracic surgeon.']</td>\n",
       "      <td>[that, is, what, i, am, in, school, for, but, it, is, not, going, so, well, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.488623</td>\n",
       "      <td>0.204053</td>\n",
       "      <td>-0.097133</td>\n",
       "      <td>0.085072</td>\n",
       "      <td>0.215851</td>\n",
       "      <td>-0.046570</td>\n",
       "      <td>0.238907</td>\n",
       "      <td>-0.034365</td>\n",
       "      <td>0.008272</td>\n",
       "      <td>0.054369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i should have dropped the class, it was a very frustration experience, but i waited it out because i thought that if i studied enough, went to office hours enough, that i would pass.</td>\n",
       "      <td>['This semester I want to prove to myself that I am smart and capable of passing this class with an A. I want to actually learn the concepts and not come into class everyday confused and lost.']</td>\n",
       "      <td>[i, should, have, dropped, the, class, ,, it, was, a, very, frustration, experience, ,, but, i, waited, it, out, because, i, thought, that, if, i, studied, enough, ,, went, to, office, hours, enough, ,, that, i, would, pass, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.434654</td>\n",
       "      <td>0.203142</td>\n",
       "      <td>-0.040918</td>\n",
       "      <td>0.069848</td>\n",
       "      <td>0.238936</td>\n",
       "      <td>-0.023290</td>\n",
       "      <td>0.183520</td>\n",
       "      <td>-0.074296</td>\n",
       "      <td>-0.001161</td>\n",
       "      <td>0.090196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was apart of a sci course last semester and i loved how helpful the instructors were.</td>\n",
       "      <td>['I am here because I am dedicated to my success in my STEM based courses.']</td>\n",
       "      <td>[i, was, apart, of, a, sci, course, last, semester, and, i, loved, how, helpful, the, instructors, were, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447531</td>\n",
       "      <td>0.231019</td>\n",
       "      <td>-0.051642</td>\n",
       "      <td>0.052862</td>\n",
       "      <td>0.309309</td>\n",
       "      <td>0.016724</td>\n",
       "      <td>0.275399</td>\n",
       "      <td>-0.121663</td>\n",
       "      <td>0.026029</td>\n",
       "      <td>0.079961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>firstly, i am here to challenge myself and make myself known in the stem field.</td>\n",
       "      <td>['Firstly, I am here to challenge myself and make myself known in the STEM field.', 'Lastly, I am here because I am passionate about my education and I never want to stop learning.']</td>\n",
       "      <td>[firstly, ,, i, am, here, to, challenge, myself, and, make, myself, known, in, the, stem, field, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.387175</td>\n",
       "      <td>0.225236</td>\n",
       "      <td>0.021366</td>\n",
       "      <td>-0.000490</td>\n",
       "      <td>0.140667</td>\n",
       "      <td>-0.079013</td>\n",
       "      <td>0.284686</td>\n",
       "      <td>-0.135737</td>\n",
       "      <td>-0.059571</td>\n",
       "      <td>0.147064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>though i love to write fiction, the fact that it is not the most stable option does not elude me.</td>\n",
       "      <td>['I am here to create. With an education in computer science and a long cultivated skill in narrative capability, I am in a better position to create than ever before. Its another tool in my belt, and a pillar of stability. I want to create phenomena of all sorts that provoke thought and challenge the very way people perceive the world around them.', 'I would love to be among those who challenge people to think truly on their own.']</td>\n",
       "      <td>[though, i, love, to, write, fiction, ,, the, fact, that, it, is, not, the, most, stable, option, does, not, elude, me, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410373</td>\n",
       "      <td>0.158538</td>\n",
       "      <td>-0.072777</td>\n",
       "      <td>0.063357</td>\n",
       "      <td>0.282437</td>\n",
       "      <td>-0.041377</td>\n",
       "      <td>0.245481</td>\n",
       "      <td>-0.081472</td>\n",
       "      <td>-0.004222</td>\n",
       "      <td>0.022449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i have a drive to do better in a world that constantly belittles my community.</td>\n",
       "      <td>['I want to create change and its all about finding ways to make those changes.']</td>\n",
       "      <td>[i, have, a, drive, to, do, better, in, a, world, that, constantly, belittles, my, community, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.371230</td>\n",
       "      <td>0.157696</td>\n",
       "      <td>0.016486</td>\n",
       "      <td>0.013763</td>\n",
       "      <td>0.173781</td>\n",
       "      <td>-0.083295</td>\n",
       "      <td>0.293860</td>\n",
       "      <td>-0.122086</td>\n",
       "      <td>-0.027427</td>\n",
       "      <td>0.123767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>of course i am here because it is required by my major, but i think that this class is cool.</td>\n",
       "      <td>['I hope that this lab will help me understand what is happening in the lectures.']</td>\n",
       "      <td>[of, course, i, am, here, because, it, is, required, by, my, major, ,, but, i, think, that, this, class, is, cool, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.532362</td>\n",
       "      <td>0.208799</td>\n",
       "      <td>-0.087226</td>\n",
       "      <td>0.076617</td>\n",
       "      <td>0.312229</td>\n",
       "      <td>-0.028548</td>\n",
       "      <td>0.327525</td>\n",
       "      <td>-0.066168</td>\n",
       "      <td>-0.019366</td>\n",
       "      <td>0.046247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>other times im so grateful to have made a big step and change attending sfsu, i've changed so much and so has my perspective slowly but surely developing with such rich information i have obtained since arriving here.</td>\n",
       "      <td>[\"I'm here at SFSU because I want to do something I enjoy doing and make it into a career.\", 'To learn and grow from the things that spark my interest.']</td>\n",
       "      <td>[other, times, im, so, grateful, to, have, made, a, big, step, and, change, attending, sfsu, ,, i, 've, changed, so, much, and, so, has, my, perspective, slowly, but, surely, developing, with, such, rich, information, i, have, obtained, since, arriving, here, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400875</td>\n",
       "      <td>0.173734</td>\n",
       "      <td>-0.019800</td>\n",
       "      <td>0.050819</td>\n",
       "      <td>0.195523</td>\n",
       "      <td>-0.034905</td>\n",
       "      <td>0.250485</td>\n",
       "      <td>-0.107129</td>\n",
       "      <td>-0.027802</td>\n",
       "      <td>0.098920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i would like to do well in the course and i have heard that the supplemental instruction courses are very helpful and that students who take these courses, on average, do better in the actual class in comparison to students who dont take the sci course.</td>\n",
       "      <td>['planning on declaring a concentration in zoology by the end of this semester']</td>\n",
       "      <td>[i, would, like, to, do, well, in, the, course, and, i, have, heard, that, the, supplemental, instruction, courses, are, very, helpful, and, that, students, who, take, these, courses, ,, on, average, ,, do, better, in, the, actual, class, in, comparison, to, students, who, dont, take, the, sci, course, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415625</td>\n",
       "      <td>0.154433</td>\n",
       "      <td>-0.042980</td>\n",
       "      <td>0.025510</td>\n",
       "      <td>0.323622</td>\n",
       "      <td>-0.017973</td>\n",
       "      <td>0.301982</td>\n",
       "      <td>-0.070117</td>\n",
       "      <td>-0.024235</td>\n",
       "      <td>0.061443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>i want to be able to say that i finished university and get my degree.</td>\n",
       "      <td>['I want to be able to say that I finished university and get my degree']</td>\n",
       "      <td>[i, want, to, be, able, to, say, that, i, finished, university, and, get, my, degree, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.427688</td>\n",
       "      <td>0.188796</td>\n",
       "      <td>0.047763</td>\n",
       "      <td>0.058651</td>\n",
       "      <td>0.128562</td>\n",
       "      <td>-0.091643</td>\n",
       "      <td>0.245693</td>\n",
       "      <td>-0.091650</td>\n",
       "      <td>-0.040748</td>\n",
       "      <td>0.199471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>i am here because i want to fulfill all of my ge requirements.</td>\n",
       "      <td>['I want to get my degree get a good paying job and be financially stable for my future.', 'I want to show the world my own ideas and make it into a film. I also am here to be smart and finish my degree.']</td>\n",
       "      <td>[i, am, here, because, i, want, to, fulfill, all, of, my, ge, requirements, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.487230</td>\n",
       "      <td>0.238541</td>\n",
       "      <td>0.006825</td>\n",
       "      <td>0.029006</td>\n",
       "      <td>0.267879</td>\n",
       "      <td>-0.067559</td>\n",
       "      <td>0.318811</td>\n",
       "      <td>-0.113233</td>\n",
       "      <td>-0.087736</td>\n",
       "      <td>0.157306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>it is important to be able to relate the way in which these different theories are a part of the real word and our life.</td>\n",
       "      <td>['My goal is to become a pediatrician and there are many courses which I have to take in order to work towards that goal.']</td>\n",
       "      <td>[it, is, important, to, be, able, to, relate, the, way, in, which, these, different, theories, are, a, part, of, the, real, word, and, our, life, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.383767</td>\n",
       "      <td>0.159114</td>\n",
       "      <td>-0.068921</td>\n",
       "      <td>0.008272</td>\n",
       "      <td>0.320715</td>\n",
       "      <td>-0.033519</td>\n",
       "      <td>0.319892</td>\n",
       "      <td>-0.153601</td>\n",
       "      <td>-0.028358</td>\n",
       "      <td>0.038378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ever since i was little if have always been fascinated with aircraft and me at first wanted to be a pilot.</td>\n",
       "      <td>['I am here studying Mechanical Engineering to obtain the skills and knowledge required to pursue a career in Aeronautical engineering', \"I still hope to obtain a pilot's license for recreational use but until then I will study to get a step closer to my goals\"]</td>\n",
       "      <td>[ever, since, i, was, little, if, have, always, been, fascinated, with, aircraft, and, me, at, first, wanted, to, be, a, pilot, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.413273</td>\n",
       "      <td>0.230347</td>\n",
       "      <td>-0.029829</td>\n",
       "      <td>0.102004</td>\n",
       "      <td>0.163942</td>\n",
       "      <td>-0.045857</td>\n",
       "      <td>0.170878</td>\n",
       "      <td>-0.132293</td>\n",
       "      <td>0.003270</td>\n",
       "      <td>0.098107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lastly, i am here because i am passionate about my education and i never want to stop learning.</td>\n",
       "      <td>['Firstly, I am here to challenge myself and make myself known in the STEM field.', 'Lastly, I am here because I am passionate about my education and I never want to stop learning.']</td>\n",
       "      <td>[lastly, ,, i, am, here, because, i, am, passionate, about, my, education, and, i, never, want, to, stop, learning, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469319</td>\n",
       "      <td>0.246066</td>\n",
       "      <td>-0.008304</td>\n",
       "      <td>0.020312</td>\n",
       "      <td>0.228684</td>\n",
       "      <td>-0.029125</td>\n",
       "      <td>0.245188</td>\n",
       "      <td>-0.148681</td>\n",
       "      <td>-0.066602</td>\n",
       "      <td>0.139215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>i want to better myself, improve myself in every way possible.</td>\n",
       "      <td>['I want to be able to apply my knowledge to my work.', 'I want to be successful. I want to better myself, improve myself in every way possible.']</td>\n",
       "      <td>[i, want, to, better, myself, ,, improve, myself, in, every, way, possible, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.398400</td>\n",
       "      <td>0.192166</td>\n",
       "      <td>0.006978</td>\n",
       "      <td>-0.022969</td>\n",
       "      <td>0.139345</td>\n",
       "      <td>-0.082131</td>\n",
       "      <td>0.300009</td>\n",
       "      <td>-0.124053</td>\n",
       "      <td>-0.089298</td>\n",
       "      <td>0.155135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>towards the end of high school i found out that through hard work in my studies, i would be able to reach my goals and even provide the next generation of my family the opportunity to enhance their living standards as well.</td>\n",
       "      <td>['As a kid growing up in the East Bay Area, I always aimed for a prestigious career path because I was unsatisfied with my upbringing']</td>\n",
       "      <td>[towards, the, end, of, high, school, i, found, out, that, through, hard, work, in, my, studies, ,, i, would, be, able, to, reach, my, goals, and, even, provide, the, next, generation, of, my, family, the, opportunity, to, enhance, their, living, standards, as, well, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370870</td>\n",
       "      <td>0.197650</td>\n",
       "      <td>0.005402</td>\n",
       "      <td>0.034783</td>\n",
       "      <td>0.196729</td>\n",
       "      <td>-0.071205</td>\n",
       "      <td>0.274734</td>\n",
       "      <td>-0.092001</td>\n",
       "      <td>-0.019056</td>\n",
       "      <td>0.145877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>i was excited coming into this semester to take physics because i am a very logical thinker and i feel physics really falls in line with that line of thinking.</td>\n",
       "      <td>['I am here to prepare myself for my future in teaching special education.']</td>\n",
       "      <td>[i, was, excited, coming, into, this, semester, to, take, physics, because, i, am, a, very, logical, thinker, and, i, feel, physics, really, falls, in, line, with, that, line, of, thinking, .]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.444853</td>\n",
       "      <td>0.225695</td>\n",
       "      <td>-0.031645</td>\n",
       "      <td>0.051098</td>\n",
       "      <td>0.251177</td>\n",
       "      <td>-0.003729</td>\n",
       "      <td>0.260669</td>\n",
       "      <td>-0.095449</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.103024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>im not too sure if medicine is what i want to pursue, but if it is then i have to know physics for the mcat. if that is the case then i need this class to better prepare me for the physics portion of the mcat. i am a biology major so this is the last physics course i would need for the remainder of my undergrad.</td>\n",
       "      <td>['If that is the case then I need this class to better prepare me for the physics portion of the MCAT.']</td>\n",
       "      <td>[im, not, too, sure, if, medicine, is, what, i, want, to, pursue, ,, but, if, it, is, then, i, have, to, know, physics, for, the, mcat, ., if, that, is, the, case, then, i, need, this, class, to, better, prepare, me, for, the, physics, portion, of, the, mcat, ., i, am, a, biology, major, so, this, is, the, last, physics, course, i, would, need, for, the, remainder, of, my, undergrad, .]</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.461872</td>\n",
       "      <td>0.186751</td>\n",
       "      <td>-0.055008</td>\n",
       "      <td>0.056270</td>\n",
       "      <td>0.273370</td>\n",
       "      <td>-0.044621</td>\n",
       "      <td>0.313242</td>\n",
       "      <td>-0.067993</td>\n",
       "      <td>-0.022376</td>\n",
       "      <td>0.055177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                     sentence  \\\n",
       "0                                                                                                                                                          one is obviously this class is a requirement for my general education credits and also because i thought it'd be interesting to further my education on astronomy.   \n",
       "1                                                                                                                                                                                                                                                                            to get a better and broader perspective of life.   \n",
       "2                                                                                                                                                                                                                                                                that is what i am in school for but it is not going so well.   \n",
       "3                                                                                                                                      i should have dropped the class, it was a very frustration experience, but i waited it out because i thought that if i studied enough, went to office hours enough, that i would pass.   \n",
       "4                                                                                                                                                                                                                                     i was apart of a sci course last semester and i loved how helpful the instructors were.   \n",
       "5                                                                                                                                                                                                                                             firstly, i am here to challenge myself and make myself known in the stem field.   \n",
       "6                                                                                                                                                                                                                           though i love to write fiction, the fact that it is not the most stable option does not elude me.   \n",
       "7                                                                                                                                                                                                                                              i have a drive to do better in a world that constantly belittles my community.   \n",
       "8                                                                                                                                                                                                                                of course i am here because it is required by my major, but i think that this class is cool.   \n",
       "9                                                                                                   other times im so grateful to have made a big step and change attending sfsu, i've changed so much and so has my perspective slowly but surely developing with such rich information i have obtained since arriving here.   \n",
       "10                                                              i would like to do well in the course and i have heard that the supplemental instruction courses are very helpful and that students who take these courses, on average, do better in the actual class in comparison to students who dont take the sci course.   \n",
       "11                                                                                                                                                                                                                                                     i want to be able to say that i finished university and get my degree.   \n",
       "12                                                                                                                                                                                                                                                             i am here because i want to fulfill all of my ge requirements.   \n",
       "13                                                                                                                                                                                                   it is important to be able to relate the way in which these different theories are a part of the real word and our life.   \n",
       "14                                                                                                                                                                                                                 ever since i was little if have always been fascinated with aircraft and me at first wanted to be a pilot.   \n",
       "15                                                                                                                                                                                                                            lastly, i am here because i am passionate about my education and i never want to stop learning.   \n",
       "16                                                                                                                                                                                                                                                             i want to better myself, improve myself in every way possible.   \n",
       "17                                                                                            towards the end of high school i found out that through hard work in my studies, i would be able to reach my goals and even provide the next generation of my family the opportunity to enhance their living standards as well.   \n",
       "18                                                                                                                                                            i was excited coming into this semester to take physics because i am a very logical thinker and i feel physics really falls in line with that line of thinking.   \n",
       "19  im not too sure if medicine is what i want to pursue, but if it is then i have to know physics for the mcat. if that is the case then i need this class to better prepare me for the physics portion of the mcat. i am a biology major so this is the last physics course i would need for the remainder of my undergrad.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                  phrase  \\\n",
       "0                                                                                                                                                                                                                                                                 ['I am here so that I can enhance my education and earn a bachelors degree so that I can get a job that makes a good salary and that I get to do something exciting and interesting.']   \n",
       "1                                                                                                                                                                                                                                                                                                                                    ['I am here because I wanted a significant change in my life.', 'To get a better and broader perspective of life.']   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                       ['I would like to be a cardiothoracic surgeon.']   \n",
       "3                                                                                                                                                                                                                                                     ['This semester I want to prove to myself that I am smart and capable of passing this class with an A. I want to actually learn the concepts and not come into class everyday confused and lost.']   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                           ['I am here because I am dedicated to my success in my STEM based courses.']   \n",
       "5                                                                                                                                                                                                                                                                 ['Firstly, I am here to challenge myself and make myself known in the STEM field.', 'Lastly, I am here because I am passionate about my education and I never want to stop learning.']   \n",
       "6   ['I am here to create. With an education in computer science and a long cultivated skill in narrative capability, I am in a better position to create than ever before. Its another tool in my belt, and a pillar of stability. I want to create phenomena of all sorts that provoke thought and challenge the very way people perceive the world around them.', 'I would love to be among those who challenge people to think truly on their own.']   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                      ['I want to create change and its all about finding ways to make those changes.']   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                    ['I hope that this lab will help me understand what is happening in the lectures.']   \n",
       "9                                                                                                                                                                                                                                                                                              [\"I'm here at SFSU because I want to do something I enjoy doing and make it into a career.\", 'To learn and grow from the things that spark my interest.']   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                      ['planning on declaring a concentration in zoology by the end of this semester']   \n",
       "11                                                                                                                                                                                                                                                                                                                                                                             ['I want to be able to say that I finished university and get my degree']   \n",
       "12                                                                                                                                                                                                                                         ['I want to get my degree get a good paying job and be financially stable for my future.', 'I want to show the world my own ideas and make it into a film. I also am here to be smart and finish my degree.']   \n",
       "13                                                                                                                                                                                                                                                                                                                           ['My goal is to become a pediatrician and there are many courses which I have to take in order to work towards that goal.']   \n",
       "14                                                                                                                                                                                ['I am here studying Mechanical Engineering to obtain the skills and knowledge required to pursue a career in Aeronautical engineering', \"I still hope to obtain a pilot's license for recreational use but until then I will study to get a step closer to my goals\"]   \n",
       "15                                                                                                                                                                                                                                                                ['Firstly, I am here to challenge myself and make myself known in the STEM field.', 'Lastly, I am here because I am passionate about my education and I never want to stop learning.']   \n",
       "16                                                                                                                                                                                                                                                                                                    ['I want to be able to apply my knowledge to my work.', 'I want to be successful. I want to better myself, improve myself in every way possible.']   \n",
       "17                                                                                                                                                                                                                                                                                                               ['As a kid growing up in the East Bay Area, I always aimed for a prestigious career path because I was unsatisfied with my upbringing']   \n",
       "18                                                                                                                                                                                                                                                                                                                                                                          ['I am here to prepare myself for my future in teaching special education.']   \n",
       "19                                                                                                                                                                                                                                                                                                                                              ['If that is the case then I need this class to better prepare me for the physics portion of the MCAT.']   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                   tokens  \\\n",
       "0                                                                                                                                                                                                      [one, is, obviously, this, class, is, a, requirement, for, my, general, education, credits, and, also, because, i, thought, it, 'd, be, interesting, to, further, my, education, on, astronomy, .]   \n",
       "1                                                                                                                                                                                                                                                                                                                                            [to, get, a, better, and, broader, perspective, of, life, .]   \n",
       "2                                                                                                                                                                                                                                                                                                                          [that, is, what, i, am, in, school, for, but, it, is, not, going, so, well, .]   \n",
       "3                                                                                                                                                                     [i, should, have, dropped, the, class, ,, it, was, a, very, frustration, experience, ,, but, i, waited, it, out, because, i, thought, that, if, i, studied, enough, ,, went, to, office, hours, enough, ,, that, i, would, pass, .]   \n",
       "4                                                                                                                                                                                                                                                                                             [i, was, apart, of, a, sci, course, last, semester, and, i, loved, how, helpful, the, instructors, were, .]   \n",
       "5                                                                                                                                                                                                                                                                                                     [firstly, ,, i, am, here, to, challenge, myself, and, make, myself, known, in, the, stem, field, .]   \n",
       "6                                                                                                                                                                                                                                                                              [though, i, love, to, write, fiction, ,, the, fact, that, it, is, not, the, most, stable, option, does, not, elude, me, .]   \n",
       "7                                                                                                                                                                                                                                                                                                        [i, have, a, drive, to, do, better, in, a, world, that, constantly, belittles, my, community, .]   \n",
       "8                                                                                                                                                                                                                                                                                   [of, course, i, am, here, because, it, is, required, by, my, major, ,, but, i, think, that, this, class, is, cool, .]   \n",
       "9                                                                                                                                  [other, times, im, so, grateful, to, have, made, a, big, step, and, change, attending, sfsu, ,, i, 've, changed, so, much, and, so, has, my, perspective, slowly, but, surely, developing, with, such, rich, information, i, have, obtained, since, arriving, here, .]   \n",
       "10                                                                                     [i, would, like, to, do, well, in, the, course, and, i, have, heard, that, the, supplemental, instruction, courses, are, very, helpful, and, that, students, who, take, these, courses, ,, on, average, ,, do, better, in, the, actual, class, in, comparison, to, students, who, dont, take, the, sci, course, .]   \n",
       "11                                                                                                                                                                                                                                                                                                               [i, want, to, be, able, to, say, that, i, finished, university, and, get, my, degree, .]   \n",
       "12                                                                                                                                                                                                                                                                                                                         [i, am, here, because, i, want, to, fulfill, all, of, my, ge, requirements, .]   \n",
       "13                                                                                                                                                                                                                                                   [it, is, important, to, be, able, to, relate, the, way, in, which, these, different, theories, are, a, part, of, the, real, word, and, our, life, .]   \n",
       "14                                                                                                                                                                                                                                                                     [ever, since, i, was, little, if, have, always, been, fascinated, with, aircraft, and, me, at, first, wanted, to, be, a, pilot, .]   \n",
       "15                                                                                                                                                                                                                                                                                 [lastly, ,, i, am, here, because, i, am, passionate, about, my, education, and, i, never, want, to, stop, learning, .]   \n",
       "16                                                                                                                                                                                                                                                                                                                         [i, want, to, better, myself, ,, improve, myself, in, every, way, possible, .]   \n",
       "17                                                                                                                         [towards, the, end, of, high, school, i, found, out, that, through, hard, work, in, my, studies, ,, i, would, be, able, to, reach, my, goals, and, even, provide, the, next, generation, of, my, family, the, opportunity, to, enhance, their, living, standards, as, well, .]   \n",
       "18                                                                                                                                                                                                       [i, was, excited, coming, into, this, semester, to, take, physics, because, i, am, a, very, logical, thinker, and, i, feel, physics, really, falls, in, line, with, that, line, of, thinking, .]   \n",
       "19  [im, not, too, sure, if, medicine, is, what, i, want, to, pursue, ,, but, if, it, is, then, i, have, to, know, physics, for, the, mcat, ., if, that, is, the, case, then, i, need, this, class, to, better, prepare, me, for, the, physics, portion, of, the, mcat, ., i, am, a, biology, major, so, this, is, the, last, physics, course, i, would, need, for, the, remainder, of, my, undergrad, .]   \n",
       "\n",
       "    bio_sim_words  chem_sim_words  phy_sim_words  math_sim_words  \\\n",
       "0               1               1              0               1   \n",
       "1               0               0              0               0   \n",
       "2               0               0              0               0   \n",
       "3               0               0              0               0   \n",
       "4               0               0              0               0   \n",
       "5               0               0              0               0   \n",
       "6               0               0              0               0   \n",
       "7               0               0              0               0   \n",
       "8               0               0              0               0   \n",
       "9               0               0              0               0   \n",
       "10              0               0              0               0   \n",
       "11              0               0              0               0   \n",
       "12              0               0              0               0   \n",
       "13              0               0              0               0   \n",
       "14              0               0              0               0   \n",
       "15              0               0              0               0   \n",
       "16              0               0              0               0   \n",
       "17              0               0              0               0   \n",
       "18              2               2              0               2   \n",
       "19              5               5              0               4   \n",
       "\n",
       "    tech_sim_words  eng_sim_words  medical_terms  ...  embedding90  \\\n",
       "0                0              0              0  ...     0.487845   \n",
       "1                0              0              0  ...     0.344721   \n",
       "2                0              0              0  ...     0.488623   \n",
       "3                0              0              0  ...     0.434654   \n",
       "4                0              0              0  ...     0.447531   \n",
       "5                0              0              0  ...     0.387175   \n",
       "6                0              0              0  ...     0.410373   \n",
       "7                0              0              1  ...     0.371230   \n",
       "8                0              0              0  ...     0.532362   \n",
       "9                0              0              0  ...     0.400875   \n",
       "10               0              0              0  ...     0.415625   \n",
       "11               0              0              0  ...     0.427688   \n",
       "12               0              0              0  ...     0.487230   \n",
       "13               0              0              0  ...     0.383767   \n",
       "14               0              0              0  ...     0.413273   \n",
       "15               0              0              0  ...     0.469319   \n",
       "16               0              0              0  ...     0.398400   \n",
       "17               0              0              1  ...     0.370870   \n",
       "18               0              0              0  ...     0.444853   \n",
       "19               0              0              1  ...     0.461872   \n",
       "\n",
       "    embedding91  embedding92  embedding93  embedding94  embedding95  \\\n",
       "0      0.183000    -0.029280     0.080841     0.227600    -0.069685   \n",
       "1      0.162200     0.014252    -0.021162     0.154372    -0.072484   \n",
       "2      0.204053    -0.097133     0.085072     0.215851    -0.046570   \n",
       "3      0.203142    -0.040918     0.069848     0.238936    -0.023290   \n",
       "4      0.231019    -0.051642     0.052862     0.309309     0.016724   \n",
       "5      0.225236     0.021366    -0.000490     0.140667    -0.079013   \n",
       "6      0.158538    -0.072777     0.063357     0.282437    -0.041377   \n",
       "7      0.157696     0.016486     0.013763     0.173781    -0.083295   \n",
       "8      0.208799    -0.087226     0.076617     0.312229    -0.028548   \n",
       "9      0.173734    -0.019800     0.050819     0.195523    -0.034905   \n",
       "10     0.154433    -0.042980     0.025510     0.323622    -0.017973   \n",
       "11     0.188796     0.047763     0.058651     0.128562    -0.091643   \n",
       "12     0.238541     0.006825     0.029006     0.267879    -0.067559   \n",
       "13     0.159114    -0.068921     0.008272     0.320715    -0.033519   \n",
       "14     0.230347    -0.029829     0.102004     0.163942    -0.045857   \n",
       "15     0.246066    -0.008304     0.020312     0.228684    -0.029125   \n",
       "16     0.192166     0.006978    -0.022969     0.139345    -0.082131   \n",
       "17     0.197650     0.005402     0.034783     0.196729    -0.071205   \n",
       "18     0.225695    -0.031645     0.051098     0.251177    -0.003729   \n",
       "19     0.186751    -0.055008     0.056270     0.273370    -0.044621   \n",
       "\n",
       "    embedding96  embedding97  embedding98  embedding99  \n",
       "0      0.336133    -0.074894    -0.010053     0.114453  \n",
       "1      0.340524    -0.152142    -0.024123     0.113133  \n",
       "2      0.238907    -0.034365     0.008272     0.054369  \n",
       "3      0.183520    -0.074296    -0.001161     0.090196  \n",
       "4      0.275399    -0.121663     0.026029     0.079961  \n",
       "5      0.284686    -0.135737    -0.059571     0.147064  \n",
       "6      0.245481    -0.081472    -0.004222     0.022449  \n",
       "7      0.293860    -0.122086    -0.027427     0.123767  \n",
       "8      0.327525    -0.066168    -0.019366     0.046247  \n",
       "9      0.250485    -0.107129    -0.027802     0.098920  \n",
       "10     0.301982    -0.070117    -0.024235     0.061443  \n",
       "11     0.245693    -0.091650    -0.040748     0.199471  \n",
       "12     0.318811    -0.113233    -0.087736     0.157306  \n",
       "13     0.319892    -0.153601    -0.028358     0.038378  \n",
       "14     0.170878    -0.132293     0.003270     0.098107  \n",
       "15     0.245188    -0.148681    -0.066602     0.139215  \n",
       "16     0.300009    -0.124053    -0.089298     0.155135  \n",
       "17     0.274734    -0.092001    -0.019056     0.145877  \n",
       "18     0.260669    -0.095449     0.000680     0.103024  \n",
       "19     0.313242    -0.067993    -0.022376     0.055177  \n",
       "\n",
       "[20 rows x 121 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_engineering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[123], line 28\u001b[0m, in \u001b[0;36mfeature_engineering\u001b[0;34m(original_dataset)\u001b[0m\n\u001b[1;32m     25\u001b[0m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolarity\u001b[39m\u001b[38;5;124m'\u001b[39m], dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubjectivity\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mdataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(get_sentiment))\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# named entity recognition\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnamed_entity_present\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# pos tag count\u001b[39;00m\n\u001b[1;32m     31\u001b[0m dataset \u001b[38;5;241m=\u001b[39m pos_tag_extraction(dataset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m, count_pos_tags, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterjections\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnouns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madverb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeterminer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpronoun\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madjetive\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreposition\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[46], line 3\u001b[0m, in \u001b[0;36mnamed_entity_present\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnamed_entity_present\u001b[39m(sentence):\n\u001b[0;32m----> 3\u001b[0m     ner_list \u001b[38;5;241m=\u001b[39m \u001b[43mget_ner\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ner_list) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[45], line 4\u001b[0m, in \u001b[0;36mget_ner\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m ner_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Annotate the text using stanza\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msentences:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m entity \u001b[38;5;129;01min\u001b[39;00m sentence\u001b[38;5;241m.\u001b[39ments:\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/stanza/pipeline/core.py:480\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc, processors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/stanza/pipeline/core.py:431\u001b[0m, in \u001b[0;36mPipeline.process\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors\u001b[38;5;241m.\u001b[39mget(processor_name):\n\u001b[1;32m    430\u001b[0m         process \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mbulk_process \u001b[38;5;28;01mif\u001b[39;00m bulk \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mprocess\n\u001b[0;32m--> 431\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/stanza/pipeline/constituency_processor.py:60\u001b[0m, in \u001b[0;36mConstituencyProcessor.process\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tqdm:\n\u001b[1;32m     58\u001b[0m     words \u001b[38;5;241m=\u001b[39m tqdm(words)\n\u001b[0;32m---> 60\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_tagged_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m document\u001b[38;5;241m.\u001b[39mset(CONSTITUENCY, trees, to_sentence\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m document\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/stanza/models/constituency/base_model.py:387\u001b[0m, in \u001b[0;36mBaseModel.parse_tagged_words\u001b[0;34m(self, words, batch_size)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    386\u001b[0m sentence_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(words)\n\u001b[0;32m--> 387\u001b[0m treebank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_sentences_no_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_batch_from_tagged_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_constituents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m results \u001b[38;5;241m=\u001b[39m [t\u001b[38;5;241m.\u001b[39mpredictions[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtree \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m treebank]\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/stanza/models/constituency/base_model.py:352\u001b[0m, in \u001b[0;36mBaseModel.parse_sentences_no_grad\u001b[0;34m(self, data_iterator, build_batch_fn, batch_size, transition_choice, keep_state, keep_constituents, keep_scores)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mGiven an iterator over the data and a method for building batches, returns a list of parse trees.\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03mno_grad() is so that gradients aren't kept, which makes the model\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03mrun faster and use less memory at inference time\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuild_batch_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransition_choice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_constituents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_scores\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/stanza/models/constituency/base_model.py:291\u001b[0m, in \u001b[0;36mBaseModel.parse_sentences\u001b[0;34m(self, data_iterator, build_batch_fn, batch_size, transition_choice, keep_state, keep_constituents, keep_scores)\u001b[0m\n\u001b[1;32m    289\u001b[0m treebank \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    290\u001b[0m treebank_indices \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 291\u001b[0m state_batch \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_batch_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# used to track which indices we are currently parsing\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# since the parses get finished at different times, this will let us unsort after\u001b[39;00m\n\u001b[1;32m    294\u001b[0m batch_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(state_batch)))\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/stanza/models/constituency/base_model.py:268\u001b[0m, in \u001b[0;36mBaseModel.build_batch_from_tagged_words\u001b[0;34m(self, batch_size, data_iterator)\u001b[0m\n\u001b[1;32m    265\u001b[0m     state_batch\u001b[38;5;241m.\u001b[39mappend(sentence)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(state_batch) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 268\u001b[0m     state_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitial_state_from_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state_batch\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/stanza/models/constituency/base_model.py:219\u001b[0m, in \u001b[0;36mBaseModel.initial_state_from_words\u001b[0;34m(self, word_lists)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitial_state_from_words\u001b[39m(\u001b[38;5;28mself\u001b[39m, word_lists):\n\u001b[1;32m    217\u001b[0m     preterminal_lists \u001b[38;5;241m=\u001b[39m [[Tree(tag, Tree(word)) \u001b[38;5;28;01mfor\u001b[39;00m word, tag \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m    218\u001b[0m                          \u001b[38;5;28;01mfor\u001b[39;00m words \u001b[38;5;129;01min\u001b[39;00m word_lists]\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitial_state_from_preterminals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreterminal_lists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgold_trees\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgold_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/stanza/models/constituency/base_model.py:196\u001b[0m, in \u001b[0;36mBaseModel.initial_state_from_preterminals\u001b[0;34m(self, preterminal_lists, gold_trees, gold_sequences)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitial_state_from_preterminals\u001b[39m(\u001b[38;5;28mself\u001b[39m, preterminal_lists, gold_trees, gold_sequences):\n\u001b[1;32m    193\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m    what is passed in should be a list of list of preterminals\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     word_queues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitial_word_queues\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreterminal_lists\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# this is the bottom of the TreeStack and will be the same for each State\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     transitions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_transitions()\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/stanza/models/constituency/lstm_model.py:734\u001b[0m, in \u001b[0;36mLSTMModel.initial_word_queues\u001b[0;34m(self, tagged_word_lists)\u001b[0m\n\u001b[1;32m    731\u001b[0m     all_word_inputs\u001b[38;5;241m.\u001b[39mappend(word_inputs)\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_charlm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 734\u001b[0m     all_forward_chars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_charlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_char_representation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_word_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word_inputs, forward_chars \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(all_word_inputs, all_forward_chars):\n\u001b[1;32m    736\u001b[0m         word_inputs\u001b[38;5;241m.\u001b[39mappend(forward_chars)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/stanza/models/common/char_model.py:222\u001b[0m, in \u001b[0;36mCharacterLanguageModel.build_char_representation\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    219\u001b[0m chars \u001b[38;5;241m=\u001b[39m get_long_tensor(chars, \u001b[38;5;28mlen\u001b[39m(all_data), pad_id\u001b[38;5;241m=\u001b[39mvocab\u001b[38;5;241m.\u001b[39munit2id(CHARLM_END))\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 222\u001b[0m     output, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m     res \u001b[38;5;241m=\u001b[39m [output[i, offsets] \u001b[38;5;28;01mfor\u001b[39;00m i, offsets \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(char_offsets)]\n\u001b[1;32m    224\u001b[0m     res \u001b[38;5;241m=\u001b[39m unsort(res, orig_idx)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/stanza/models/common/char_model.py:153\u001b[0m, in \u001b[0;36mCharacterLanguageModel.forward\u001b[0;34m(self, chars, charlens, hidden)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \n\u001b[1;32m    151\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharlstm_h_init\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_num_layers\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_hidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcontiguous(),\n\u001b[1;32m    152\u001b[0m               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharlstm_c_init\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_num_layers\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_hidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcontiguous())\n\u001b[0;32m--> 153\u001b[0m output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcharlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pad_packed_sequence(output, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    155\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(output)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/stanza/models/common/packed_lstm.py:22\u001b[0m, in \u001b[0;36mPackedLSTM.forward\u001b[0;34m(self, input, lengths, hx)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, PackedSequence):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m pack_padded_sequence(\u001b[38;5;28minput\u001b[39m, lengths, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[0;32m---> 22\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad:\n\u001b[1;32m     24\u001b[0m     res \u001b[38;5;241m=\u001b[39m (pad_packed_sequence(res[\u001b[38;5;241m0\u001b[39m], batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)[\u001b[38;5;241m0\u001b[39m], res[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/torch/nn/modules/rnn.py:882\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    879\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    880\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 882\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    884\u001b[0m output \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    885\u001b[0m hidden \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_test, y_test = feature_engineering(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(941, 121)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Unigram features for both train and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the unigram df for train :  (3763, 2134)\n"
     ]
    }
   ],
   "source": [
    "# Unigrams for training set\n",
    "unigram_matrix = unigram_vect.fit_transform(X_train['sentence'])\n",
    "unigrams = pd.DataFrame(unigram_matrix.toarray())\n",
    "print(\"Shape of the unigram df for train : \",unigrams.shape)\n",
    "unigrams = unigrams.reset_index()\n",
    "unigrams = unigrams.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = pd.concat([X_train, unigrams], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final.shape\n",
    "X_train_final.to_csv(\"X_train_final.csv\", index=False)\n",
    "X_test_final.to_csv(\"X_test_final.csv\", index=False)\n",
    "y_train.to_csv(\"y_train.csv\", index=False)\n",
    "y_test.to_csv(\"y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final.columns = X_train_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test unigram df shape :  (941, 2134)\n"
     ]
    }
   ],
   "source": [
    "unigram_matrix_test = unigram_vect.transform(X_test['sentence'])\n",
    "unigrams_test = pd.DataFrame(unigram_matrix_test.toarray())\n",
    "unigrams_test = unigrams_test.reset_index()\n",
    "unigrams_test = unigrams_test.iloc[:,1:]\n",
    "print(\"Test unigram df shape : \",unigrams_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final = pd.concat([X_test, unigrams_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final.columns = X_test_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(941, 2255)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ---- sentence\n",
      "1 ---- phrase\n",
      "2 ---- tokens\n",
      "3 ---- bio_sim_words\n",
      "4 ---- chem_sim_words\n",
      "5 ---- phy_sim_words\n",
      "6 ---- math_sim_words\n",
      "7 ---- tech_sim_words\n",
      "8 ---- eng_sim_words\n",
      "9 ---- medical_terms\n",
      "10 ---- polarity\n",
      "11 ---- subjectivity\n",
      "12 ---- ner\n",
      "13 ---- interjections\n",
      "14 ---- nouns\n",
      "15 ---- adverb\n",
      "16 ---- verb\n",
      "17 ---- determiner\n",
      "18 ---- pronoun\n",
      "19 ---- adjetive\n",
      "20 ---- preposition\n",
      "21 ---- embedding0\n",
      "22 ---- embedding1\n",
      "23 ---- embedding2\n",
      "24 ---- embedding3\n",
      "25 ---- embedding4\n",
      "26 ---- embedding5\n",
      "27 ---- embedding6\n",
      "28 ---- embedding7\n",
      "29 ---- embedding8\n",
      "30 ---- embedding9\n",
      "31 ---- embedding10\n",
      "32 ---- embedding11\n",
      "33 ---- embedding12\n",
      "34 ---- embedding13\n",
      "35 ---- embedding14\n",
      "36 ---- embedding15\n",
      "37 ---- embedding16\n",
      "38 ---- embedding17\n",
      "39 ---- embedding18\n",
      "40 ---- embedding19\n",
      "41 ---- embedding20\n",
      "42 ---- embedding21\n",
      "43 ---- embedding22\n",
      "44 ---- embedding23\n",
      "45 ---- embedding24\n",
      "46 ---- embedding25\n",
      "47 ---- embedding26\n",
      "48 ---- embedding27\n",
      "49 ---- embedding28\n",
      "50 ---- embedding29\n",
      "51 ---- embedding30\n",
      "52 ---- embedding31\n",
      "53 ---- embedding32\n",
      "54 ---- embedding33\n",
      "55 ---- embedding34\n",
      "56 ---- embedding35\n",
      "57 ---- embedding36\n",
      "58 ---- embedding37\n",
      "59 ---- embedding38\n",
      "60 ---- embedding39\n",
      "61 ---- embedding40\n",
      "62 ---- embedding41\n",
      "63 ---- embedding42\n",
      "64 ---- embedding43\n",
      "65 ---- embedding44\n",
      "66 ---- embedding45\n",
      "67 ---- embedding46\n",
      "68 ---- embedding47\n",
      "69 ---- embedding48\n",
      "70 ---- embedding49\n",
      "71 ---- embedding50\n",
      "72 ---- embedding51\n",
      "73 ---- embedding52\n",
      "74 ---- embedding53\n",
      "75 ---- embedding54\n",
      "76 ---- embedding55\n",
      "77 ---- embedding56\n",
      "78 ---- embedding57\n",
      "79 ---- embedding58\n",
      "80 ---- embedding59\n",
      "81 ---- embedding60\n",
      "82 ---- embedding61\n",
      "83 ---- embedding62\n",
      "84 ---- embedding63\n",
      "85 ---- embedding64\n",
      "86 ---- embedding65\n",
      "87 ---- embedding66\n",
      "88 ---- embedding67\n",
      "89 ---- embedding68\n",
      "90 ---- embedding69\n",
      "91 ---- embedding70\n",
      "92 ---- embedding71\n",
      "93 ---- embedding72\n",
      "94 ---- embedding73\n",
      "95 ---- embedding74\n",
      "96 ---- embedding75\n",
      "97 ---- embedding76\n",
      "98 ---- embedding77\n",
      "99 ---- embedding78\n",
      "100 ---- embedding79\n",
      "101 ---- embedding80\n",
      "102 ---- embedding81\n",
      "103 ---- embedding82\n",
      "104 ---- embedding83\n",
      "105 ---- embedding84\n",
      "106 ---- embedding85\n",
      "107 ---- embedding86\n",
      "108 ---- embedding87\n",
      "109 ---- embedding88\n",
      "110 ---- embedding89\n",
      "111 ---- embedding90\n",
      "112 ---- embedding91\n",
      "113 ---- embedding92\n",
      "114 ---- embedding93\n",
      "115 ---- embedding94\n",
      "116 ---- embedding95\n",
      "117 ---- embedding96\n",
      "118 ---- embedding97\n",
      "119 ---- embedding98\n",
      "120 ---- embedding99\n",
      "121 ---- 0\n",
      "122 ---- 1\n",
      "123 ---- 2\n",
      "124 ---- 3\n",
      "125 ---- 4\n",
      "126 ---- 5\n",
      "127 ---- 6\n",
      "128 ---- 7\n",
      "129 ---- 8\n",
      "130 ---- 9\n",
      "131 ---- 10\n",
      "132 ---- 11\n",
      "133 ---- 12\n",
      "134 ---- 13\n",
      "135 ---- 14\n",
      "136 ---- 15\n",
      "137 ---- 16\n",
      "138 ---- 17\n",
      "139 ---- 18\n",
      "140 ---- 19\n",
      "141 ---- 20\n",
      "142 ---- 21\n",
      "143 ---- 22\n",
      "144 ---- 23\n",
      "145 ---- 24\n",
      "146 ---- 25\n",
      "147 ---- 26\n",
      "148 ---- 27\n",
      "149 ---- 28\n",
      "150 ---- 29\n",
      "151 ---- 30\n",
      "152 ---- 31\n",
      "153 ---- 32\n",
      "154 ---- 33\n",
      "155 ---- 34\n",
      "156 ---- 35\n",
      "157 ---- 36\n",
      "158 ---- 37\n",
      "159 ---- 38\n",
      "160 ---- 39\n",
      "161 ---- 40\n",
      "162 ---- 41\n",
      "163 ---- 42\n",
      "164 ---- 43\n",
      "165 ---- 44\n",
      "166 ---- 45\n",
      "167 ---- 46\n",
      "168 ---- 47\n",
      "169 ---- 48\n",
      "170 ---- 49\n",
      "171 ---- 50\n",
      "172 ---- 51\n",
      "173 ---- 52\n",
      "174 ---- 53\n",
      "175 ---- 54\n",
      "176 ---- 55\n",
      "177 ---- 56\n",
      "178 ---- 57\n",
      "179 ---- 58\n",
      "180 ---- 59\n",
      "181 ---- 60\n",
      "182 ---- 61\n",
      "183 ---- 62\n",
      "184 ---- 63\n",
      "185 ---- 64\n",
      "186 ---- 65\n",
      "187 ---- 66\n",
      "188 ---- 67\n",
      "189 ---- 68\n",
      "190 ---- 69\n",
      "191 ---- 70\n",
      "192 ---- 71\n",
      "193 ---- 72\n",
      "194 ---- 73\n",
      "195 ---- 74\n",
      "196 ---- 75\n",
      "197 ---- 76\n",
      "198 ---- 77\n",
      "199 ---- 78\n",
      "200 ---- 79\n",
      "201 ---- 80\n",
      "202 ---- 81\n",
      "203 ---- 82\n",
      "204 ---- 83\n",
      "205 ---- 84\n",
      "206 ---- 85\n",
      "207 ---- 86\n",
      "208 ---- 87\n",
      "209 ---- 88\n",
      "210 ---- 89\n",
      "211 ---- 90\n",
      "212 ---- 91\n",
      "213 ---- 92\n",
      "214 ---- 93\n",
      "215 ---- 94\n",
      "216 ---- 95\n",
      "217 ---- 96\n",
      "218 ---- 97\n",
      "219 ---- 98\n",
      "220 ---- 99\n",
      "221 ---- 100\n",
      "222 ---- 101\n",
      "223 ---- 102\n",
      "224 ---- 103\n",
      "225 ---- 104\n",
      "226 ---- 105\n",
      "227 ---- 106\n",
      "228 ---- 107\n",
      "229 ---- 108\n",
      "230 ---- 109\n",
      "231 ---- 110\n",
      "232 ---- 111\n",
      "233 ---- 112\n",
      "234 ---- 113\n",
      "235 ---- 114\n",
      "236 ---- 115\n",
      "237 ---- 116\n",
      "238 ---- 117\n",
      "239 ---- 118\n",
      "240 ---- 119\n",
      "241 ---- 120\n",
      "242 ---- 121\n",
      "243 ---- 122\n",
      "244 ---- 123\n",
      "245 ---- 124\n",
      "246 ---- 125\n",
      "247 ---- 126\n",
      "248 ---- 127\n",
      "249 ---- 128\n",
      "250 ---- 129\n",
      "251 ---- 130\n",
      "252 ---- 131\n",
      "253 ---- 132\n",
      "254 ---- 133\n",
      "255 ---- 134\n",
      "256 ---- 135\n",
      "257 ---- 136\n",
      "258 ---- 137\n",
      "259 ---- 138\n",
      "260 ---- 139\n",
      "261 ---- 140\n",
      "262 ---- 141\n",
      "263 ---- 142\n",
      "264 ---- 143\n",
      "265 ---- 144\n",
      "266 ---- 145\n",
      "267 ---- 146\n",
      "268 ---- 147\n",
      "269 ---- 148\n",
      "270 ---- 149\n",
      "271 ---- 150\n",
      "272 ---- 151\n",
      "273 ---- 152\n",
      "274 ---- 153\n",
      "275 ---- 154\n",
      "276 ---- 155\n",
      "277 ---- 156\n",
      "278 ---- 157\n",
      "279 ---- 158\n",
      "280 ---- 159\n",
      "281 ---- 160\n",
      "282 ---- 161\n",
      "283 ---- 162\n",
      "284 ---- 163\n",
      "285 ---- 164\n",
      "286 ---- 165\n",
      "287 ---- 166\n",
      "288 ---- 167\n",
      "289 ---- 168\n",
      "290 ---- 169\n",
      "291 ---- 170\n",
      "292 ---- 171\n",
      "293 ---- 172\n",
      "294 ---- 173\n",
      "295 ---- 174\n",
      "296 ---- 175\n",
      "297 ---- 176\n",
      "298 ---- 177\n",
      "299 ---- 178\n",
      "300 ---- 179\n",
      "301 ---- 180\n",
      "302 ---- 181\n",
      "303 ---- 182\n",
      "304 ---- 183\n",
      "305 ---- 184\n",
      "306 ---- 185\n",
      "307 ---- 186\n",
      "308 ---- 187\n",
      "309 ---- 188\n",
      "310 ---- 189\n",
      "311 ---- 190\n",
      "312 ---- 191\n",
      "313 ---- 192\n",
      "314 ---- 193\n",
      "315 ---- 194\n",
      "316 ---- 195\n",
      "317 ---- 196\n",
      "318 ---- 197\n",
      "319 ---- 198\n",
      "320 ---- 199\n",
      "321 ---- 200\n",
      "322 ---- 201\n",
      "323 ---- 202\n",
      "324 ---- 203\n",
      "325 ---- 204\n",
      "326 ---- 205\n",
      "327 ---- 206\n",
      "328 ---- 207\n",
      "329 ---- 208\n",
      "330 ---- 209\n",
      "331 ---- 210\n",
      "332 ---- 211\n",
      "333 ---- 212\n",
      "334 ---- 213\n",
      "335 ---- 214\n",
      "336 ---- 215\n",
      "337 ---- 216\n",
      "338 ---- 217\n",
      "339 ---- 218\n",
      "340 ---- 219\n",
      "341 ---- 220\n",
      "342 ---- 221\n",
      "343 ---- 222\n",
      "344 ---- 223\n",
      "345 ---- 224\n",
      "346 ---- 225\n",
      "347 ---- 226\n",
      "348 ---- 227\n",
      "349 ---- 228\n",
      "350 ---- 229\n",
      "351 ---- 230\n",
      "352 ---- 231\n",
      "353 ---- 232\n",
      "354 ---- 233\n",
      "355 ---- 234\n",
      "356 ---- 235\n",
      "357 ---- 236\n",
      "358 ---- 237\n",
      "359 ---- 238\n",
      "360 ---- 239\n",
      "361 ---- 240\n",
      "362 ---- 241\n",
      "363 ---- 242\n",
      "364 ---- 243\n",
      "365 ---- 244\n",
      "366 ---- 245\n",
      "367 ---- 246\n",
      "368 ---- 247\n",
      "369 ---- 248\n",
      "370 ---- 249\n",
      "371 ---- 250\n",
      "372 ---- 251\n",
      "373 ---- 252\n",
      "374 ---- 253\n",
      "375 ---- 254\n",
      "376 ---- 255\n",
      "377 ---- 256\n",
      "378 ---- 257\n",
      "379 ---- 258\n",
      "380 ---- 259\n",
      "381 ---- 260\n",
      "382 ---- 261\n",
      "383 ---- 262\n",
      "384 ---- 263\n",
      "385 ---- 264\n",
      "386 ---- 265\n",
      "387 ---- 266\n",
      "388 ---- 267\n",
      "389 ---- 268\n",
      "390 ---- 269\n",
      "391 ---- 270\n",
      "392 ---- 271\n",
      "393 ---- 272\n",
      "394 ---- 273\n",
      "395 ---- 274\n",
      "396 ---- 275\n",
      "397 ---- 276\n",
      "398 ---- 277\n",
      "399 ---- 278\n",
      "400 ---- 279\n",
      "401 ---- 280\n",
      "402 ---- 281\n",
      "403 ---- 282\n",
      "404 ---- 283\n",
      "405 ---- 284\n",
      "406 ---- 285\n",
      "407 ---- 286\n",
      "408 ---- 287\n",
      "409 ---- 288\n",
      "410 ---- 289\n",
      "411 ---- 290\n",
      "412 ---- 291\n",
      "413 ---- 292\n",
      "414 ---- 293\n",
      "415 ---- 294\n",
      "416 ---- 295\n",
      "417 ---- 296\n",
      "418 ---- 297\n",
      "419 ---- 298\n",
      "420 ---- 299\n",
      "421 ---- 300\n",
      "422 ---- 301\n",
      "423 ---- 302\n",
      "424 ---- 303\n",
      "425 ---- 304\n",
      "426 ---- 305\n",
      "427 ---- 306\n",
      "428 ---- 307\n",
      "429 ---- 308\n",
      "430 ---- 309\n",
      "431 ---- 310\n",
      "432 ---- 311\n",
      "433 ---- 312\n",
      "434 ---- 313\n",
      "435 ---- 314\n",
      "436 ---- 315\n",
      "437 ---- 316\n",
      "438 ---- 317\n",
      "439 ---- 318\n",
      "440 ---- 319\n",
      "441 ---- 320\n",
      "442 ---- 321\n",
      "443 ---- 322\n",
      "444 ---- 323\n",
      "445 ---- 324\n",
      "446 ---- 325\n",
      "447 ---- 326\n",
      "448 ---- 327\n",
      "449 ---- 328\n",
      "450 ---- 329\n",
      "451 ---- 330\n",
      "452 ---- 331\n",
      "453 ---- 332\n",
      "454 ---- 333\n",
      "455 ---- 334\n",
      "456 ---- 335\n",
      "457 ---- 336\n",
      "458 ---- 337\n",
      "459 ---- 338\n",
      "460 ---- 339\n",
      "461 ---- 340\n",
      "462 ---- 341\n",
      "463 ---- 342\n",
      "464 ---- 343\n",
      "465 ---- 344\n",
      "466 ---- 345\n",
      "467 ---- 346\n",
      "468 ---- 347\n",
      "469 ---- 348\n",
      "470 ---- 349\n",
      "471 ---- 350\n",
      "472 ---- 351\n",
      "473 ---- 352\n",
      "474 ---- 353\n",
      "475 ---- 354\n",
      "476 ---- 355\n",
      "477 ---- 356\n",
      "478 ---- 357\n",
      "479 ---- 358\n",
      "480 ---- 359\n",
      "481 ---- 360\n",
      "482 ---- 361\n",
      "483 ---- 362\n",
      "484 ---- 363\n",
      "485 ---- 364\n",
      "486 ---- 365\n",
      "487 ---- 366\n",
      "488 ---- 367\n",
      "489 ---- 368\n",
      "490 ---- 369\n",
      "491 ---- 370\n",
      "492 ---- 371\n",
      "493 ---- 372\n",
      "494 ---- 373\n",
      "495 ---- 374\n",
      "496 ---- 375\n",
      "497 ---- 376\n",
      "498 ---- 377\n",
      "499 ---- 378\n",
      "500 ---- 379\n",
      "501 ---- 380\n",
      "502 ---- 381\n",
      "503 ---- 382\n",
      "504 ---- 383\n",
      "505 ---- 384\n",
      "506 ---- 385\n",
      "507 ---- 386\n",
      "508 ---- 387\n",
      "509 ---- 388\n",
      "510 ---- 389\n",
      "511 ---- 390\n",
      "512 ---- 391\n",
      "513 ---- 392\n",
      "514 ---- 393\n",
      "515 ---- 394\n",
      "516 ---- 395\n",
      "517 ---- 396\n",
      "518 ---- 397\n",
      "519 ---- 398\n",
      "520 ---- 399\n",
      "521 ---- 400\n",
      "522 ---- 401\n",
      "523 ---- 402\n",
      "524 ---- 403\n",
      "525 ---- 404\n",
      "526 ---- 405\n",
      "527 ---- 406\n",
      "528 ---- 407\n",
      "529 ---- 408\n",
      "530 ---- 409\n",
      "531 ---- 410\n",
      "532 ---- 411\n",
      "533 ---- 412\n",
      "534 ---- 413\n",
      "535 ---- 414\n",
      "536 ---- 415\n",
      "537 ---- 416\n",
      "538 ---- 417\n",
      "539 ---- 418\n",
      "540 ---- 419\n",
      "541 ---- 420\n",
      "542 ---- 421\n",
      "543 ---- 422\n",
      "544 ---- 423\n",
      "545 ---- 424\n",
      "546 ---- 425\n",
      "547 ---- 426\n",
      "548 ---- 427\n",
      "549 ---- 428\n",
      "550 ---- 429\n",
      "551 ---- 430\n",
      "552 ---- 431\n",
      "553 ---- 432\n",
      "554 ---- 433\n",
      "555 ---- 434\n",
      "556 ---- 435\n",
      "557 ---- 436\n",
      "558 ---- 437\n",
      "559 ---- 438\n",
      "560 ---- 439\n",
      "561 ---- 440\n",
      "562 ---- 441\n",
      "563 ---- 442\n",
      "564 ---- 443\n",
      "565 ---- 444\n",
      "566 ---- 445\n",
      "567 ---- 446\n",
      "568 ---- 447\n",
      "569 ---- 448\n",
      "570 ---- 449\n",
      "571 ---- 450\n",
      "572 ---- 451\n",
      "573 ---- 452\n",
      "574 ---- 453\n",
      "575 ---- 454\n",
      "576 ---- 455\n",
      "577 ---- 456\n",
      "578 ---- 457\n",
      "579 ---- 458\n",
      "580 ---- 459\n",
      "581 ---- 460\n",
      "582 ---- 461\n",
      "583 ---- 462\n",
      "584 ---- 463\n",
      "585 ---- 464\n",
      "586 ---- 465\n",
      "587 ---- 466\n",
      "588 ---- 467\n",
      "589 ---- 468\n",
      "590 ---- 469\n",
      "591 ---- 470\n",
      "592 ---- 471\n",
      "593 ---- 472\n",
      "594 ---- 473\n",
      "595 ---- 474\n",
      "596 ---- 475\n",
      "597 ---- 476\n",
      "598 ---- 477\n",
      "599 ---- 478\n",
      "600 ---- 479\n",
      "601 ---- 480\n",
      "602 ---- 481\n",
      "603 ---- 482\n",
      "604 ---- 483\n",
      "605 ---- 484\n",
      "606 ---- 485\n",
      "607 ---- 486\n",
      "608 ---- 487\n",
      "609 ---- 488\n",
      "610 ---- 489\n",
      "611 ---- 490\n",
      "612 ---- 491\n",
      "613 ---- 492\n",
      "614 ---- 493\n",
      "615 ---- 494\n",
      "616 ---- 495\n",
      "617 ---- 496\n",
      "618 ---- 497\n",
      "619 ---- 498\n",
      "620 ---- 499\n",
      "621 ---- 500\n",
      "622 ---- 501\n",
      "623 ---- 502\n",
      "624 ---- 503\n",
      "625 ---- 504\n",
      "626 ---- 505\n",
      "627 ---- 506\n",
      "628 ---- 507\n",
      "629 ---- 508\n",
      "630 ---- 509\n",
      "631 ---- 510\n",
      "632 ---- 511\n",
      "633 ---- 512\n",
      "634 ---- 513\n",
      "635 ---- 514\n",
      "636 ---- 515\n",
      "637 ---- 516\n",
      "638 ---- 517\n",
      "639 ---- 518\n",
      "640 ---- 519\n",
      "641 ---- 520\n",
      "642 ---- 521\n",
      "643 ---- 522\n",
      "644 ---- 523\n",
      "645 ---- 524\n",
      "646 ---- 525\n",
      "647 ---- 526\n",
      "648 ---- 527\n",
      "649 ---- 528\n",
      "650 ---- 529\n",
      "651 ---- 530\n",
      "652 ---- 531\n",
      "653 ---- 532\n",
      "654 ---- 533\n",
      "655 ---- 534\n",
      "656 ---- 535\n",
      "657 ---- 536\n",
      "658 ---- 537\n",
      "659 ---- 538\n",
      "660 ---- 539\n",
      "661 ---- 540\n",
      "662 ---- 541\n",
      "663 ---- 542\n",
      "664 ---- 543\n",
      "665 ---- 544\n",
      "666 ---- 545\n",
      "667 ---- 546\n",
      "668 ---- 547\n",
      "669 ---- 548\n",
      "670 ---- 549\n",
      "671 ---- 550\n",
      "672 ---- 551\n",
      "673 ---- 552\n",
      "674 ---- 553\n",
      "675 ---- 554\n",
      "676 ---- 555\n",
      "677 ---- 556\n",
      "678 ---- 557\n",
      "679 ---- 558\n",
      "680 ---- 559\n",
      "681 ---- 560\n",
      "682 ---- 561\n",
      "683 ---- 562\n",
      "684 ---- 563\n",
      "685 ---- 564\n",
      "686 ---- 565\n",
      "687 ---- 566\n",
      "688 ---- 567\n",
      "689 ---- 568\n",
      "690 ---- 569\n",
      "691 ---- 570\n",
      "692 ---- 571\n",
      "693 ---- 572\n",
      "694 ---- 573\n",
      "695 ---- 574\n",
      "696 ---- 575\n",
      "697 ---- 576\n",
      "698 ---- 577\n",
      "699 ---- 578\n",
      "700 ---- 579\n",
      "701 ---- 580\n",
      "702 ---- 581\n",
      "703 ---- 582\n",
      "704 ---- 583\n",
      "705 ---- 584\n",
      "706 ---- 585\n",
      "707 ---- 586\n",
      "708 ---- 587\n",
      "709 ---- 588\n",
      "710 ---- 589\n",
      "711 ---- 590\n",
      "712 ---- 591\n",
      "713 ---- 592\n",
      "714 ---- 593\n",
      "715 ---- 594\n",
      "716 ---- 595\n",
      "717 ---- 596\n",
      "718 ---- 597\n",
      "719 ---- 598\n",
      "720 ---- 599\n",
      "721 ---- 600\n",
      "722 ---- 601\n",
      "723 ---- 602\n",
      "724 ---- 603\n",
      "725 ---- 604\n",
      "726 ---- 605\n",
      "727 ---- 606\n",
      "728 ---- 607\n",
      "729 ---- 608\n",
      "730 ---- 609\n",
      "731 ---- 610\n",
      "732 ---- 611\n",
      "733 ---- 612\n",
      "734 ---- 613\n",
      "735 ---- 614\n",
      "736 ---- 615\n",
      "737 ---- 616\n",
      "738 ---- 617\n",
      "739 ---- 618\n",
      "740 ---- 619\n",
      "741 ---- 620\n",
      "742 ---- 621\n",
      "743 ---- 622\n",
      "744 ---- 623\n",
      "745 ---- 624\n",
      "746 ---- 625\n",
      "747 ---- 626\n",
      "748 ---- 627\n",
      "749 ---- 628\n",
      "750 ---- 629\n",
      "751 ---- 630\n",
      "752 ---- 631\n",
      "753 ---- 632\n",
      "754 ---- 633\n",
      "755 ---- 634\n",
      "756 ---- 635\n",
      "757 ---- 636\n",
      "758 ---- 637\n",
      "759 ---- 638\n",
      "760 ---- 639\n",
      "761 ---- 640\n",
      "762 ---- 641\n",
      "763 ---- 642\n",
      "764 ---- 643\n",
      "765 ---- 644\n",
      "766 ---- 645\n",
      "767 ---- 646\n",
      "768 ---- 647\n",
      "769 ---- 648\n",
      "770 ---- 649\n",
      "771 ---- 650\n",
      "772 ---- 651\n",
      "773 ---- 652\n",
      "774 ---- 653\n",
      "775 ---- 654\n",
      "776 ---- 655\n",
      "777 ---- 656\n",
      "778 ---- 657\n",
      "779 ---- 658\n",
      "780 ---- 659\n",
      "781 ---- 660\n",
      "782 ---- 661\n",
      "783 ---- 662\n",
      "784 ---- 663\n",
      "785 ---- 664\n",
      "786 ---- 665\n",
      "787 ---- 666\n",
      "788 ---- 667\n",
      "789 ---- 668\n",
      "790 ---- 669\n",
      "791 ---- 670\n",
      "792 ---- 671\n",
      "793 ---- 672\n",
      "794 ---- 673\n",
      "795 ---- 674\n",
      "796 ---- 675\n",
      "797 ---- 676\n",
      "798 ---- 677\n",
      "799 ---- 678\n",
      "800 ---- 679\n",
      "801 ---- 680\n",
      "802 ---- 681\n",
      "803 ---- 682\n",
      "804 ---- 683\n",
      "805 ---- 684\n",
      "806 ---- 685\n",
      "807 ---- 686\n",
      "808 ---- 687\n",
      "809 ---- 688\n",
      "810 ---- 689\n",
      "811 ---- 690\n",
      "812 ---- 691\n",
      "813 ---- 692\n",
      "814 ---- 693\n",
      "815 ---- 694\n",
      "816 ---- 695\n",
      "817 ---- 696\n",
      "818 ---- 697\n",
      "819 ---- 698\n",
      "820 ---- 699\n",
      "821 ---- 700\n",
      "822 ---- 701\n",
      "823 ---- 702\n",
      "824 ---- 703\n",
      "825 ---- 704\n",
      "826 ---- 705\n",
      "827 ---- 706\n",
      "828 ---- 707\n",
      "829 ---- 708\n",
      "830 ---- 709\n",
      "831 ---- 710\n",
      "832 ---- 711\n",
      "833 ---- 712\n",
      "834 ---- 713\n",
      "835 ---- 714\n",
      "836 ---- 715\n",
      "837 ---- 716\n",
      "838 ---- 717\n",
      "839 ---- 718\n",
      "840 ---- 719\n",
      "841 ---- 720\n",
      "842 ---- 721\n",
      "843 ---- 722\n",
      "844 ---- 723\n",
      "845 ---- 724\n",
      "846 ---- 725\n",
      "847 ---- 726\n",
      "848 ---- 727\n",
      "849 ---- 728\n",
      "850 ---- 729\n",
      "851 ---- 730\n",
      "852 ---- 731\n",
      "853 ---- 732\n",
      "854 ---- 733\n",
      "855 ---- 734\n",
      "856 ---- 735\n",
      "857 ---- 736\n",
      "858 ---- 737\n",
      "859 ---- 738\n",
      "860 ---- 739\n",
      "861 ---- 740\n",
      "862 ---- 741\n",
      "863 ---- 742\n",
      "864 ---- 743\n",
      "865 ---- 744\n",
      "866 ---- 745\n",
      "867 ---- 746\n",
      "868 ---- 747\n",
      "869 ---- 748\n",
      "870 ---- 749\n",
      "871 ---- 750\n",
      "872 ---- 751\n",
      "873 ---- 752\n",
      "874 ---- 753\n",
      "875 ---- 754\n",
      "876 ---- 755\n",
      "877 ---- 756\n",
      "878 ---- 757\n",
      "879 ---- 758\n",
      "880 ---- 759\n",
      "881 ---- 760\n",
      "882 ---- 761\n",
      "883 ---- 762\n",
      "884 ---- 763\n",
      "885 ---- 764\n",
      "886 ---- 765\n",
      "887 ---- 766\n",
      "888 ---- 767\n",
      "889 ---- 768\n",
      "890 ---- 769\n",
      "891 ---- 770\n",
      "892 ---- 771\n",
      "893 ---- 772\n",
      "894 ---- 773\n",
      "895 ---- 774\n",
      "896 ---- 775\n",
      "897 ---- 776\n",
      "898 ---- 777\n",
      "899 ---- 778\n",
      "900 ---- 779\n",
      "901 ---- 780\n",
      "902 ---- 781\n",
      "903 ---- 782\n",
      "904 ---- 783\n",
      "905 ---- 784\n",
      "906 ---- 785\n",
      "907 ---- 786\n",
      "908 ---- 787\n",
      "909 ---- 788\n",
      "910 ---- 789\n",
      "911 ---- 790\n",
      "912 ---- 791\n",
      "913 ---- 792\n",
      "914 ---- 793\n",
      "915 ---- 794\n",
      "916 ---- 795\n",
      "917 ---- 796\n",
      "918 ---- 797\n",
      "919 ---- 798\n",
      "920 ---- 799\n",
      "921 ---- 800\n",
      "922 ---- 801\n",
      "923 ---- 802\n",
      "924 ---- 803\n",
      "925 ---- 804\n",
      "926 ---- 805\n",
      "927 ---- 806\n",
      "928 ---- 807\n",
      "929 ---- 808\n",
      "930 ---- 809\n",
      "931 ---- 810\n",
      "932 ---- 811\n",
      "933 ---- 812\n",
      "934 ---- 813\n",
      "935 ---- 814\n",
      "936 ---- 815\n",
      "937 ---- 816\n",
      "938 ---- 817\n",
      "939 ---- 818\n",
      "940 ---- 819\n",
      "941 ---- 820\n",
      "942 ---- 821\n",
      "943 ---- 822\n",
      "944 ---- 823\n",
      "945 ---- 824\n",
      "946 ---- 825\n",
      "947 ---- 826\n",
      "948 ---- 827\n",
      "949 ---- 828\n",
      "950 ---- 829\n",
      "951 ---- 830\n",
      "952 ---- 831\n",
      "953 ---- 832\n",
      "954 ---- 833\n",
      "955 ---- 834\n",
      "956 ---- 835\n",
      "957 ---- 836\n",
      "958 ---- 837\n",
      "959 ---- 838\n",
      "960 ---- 839\n",
      "961 ---- 840\n",
      "962 ---- 841\n",
      "963 ---- 842\n",
      "964 ---- 843\n",
      "965 ---- 844\n",
      "966 ---- 845\n",
      "967 ---- 846\n",
      "968 ---- 847\n",
      "969 ---- 848\n",
      "970 ---- 849\n",
      "971 ---- 850\n",
      "972 ---- 851\n",
      "973 ---- 852\n",
      "974 ---- 853\n",
      "975 ---- 854\n",
      "976 ---- 855\n",
      "977 ---- 856\n",
      "978 ---- 857\n",
      "979 ---- 858\n",
      "980 ---- 859\n",
      "981 ---- 860\n",
      "982 ---- 861\n",
      "983 ---- 862\n",
      "984 ---- 863\n",
      "985 ---- 864\n",
      "986 ---- 865\n",
      "987 ---- 866\n",
      "988 ---- 867\n",
      "989 ---- 868\n",
      "990 ---- 869\n",
      "991 ---- 870\n",
      "992 ---- 871\n",
      "993 ---- 872\n",
      "994 ---- 873\n",
      "995 ---- 874\n",
      "996 ---- 875\n",
      "997 ---- 876\n",
      "998 ---- 877\n",
      "999 ---- 878\n",
      "1000 ---- 879\n",
      "1001 ---- 880\n",
      "1002 ---- 881\n",
      "1003 ---- 882\n",
      "1004 ---- 883\n",
      "1005 ---- 884\n",
      "1006 ---- 885\n",
      "1007 ---- 886\n",
      "1008 ---- 887\n",
      "1009 ---- 888\n",
      "1010 ---- 889\n",
      "1011 ---- 890\n",
      "1012 ---- 891\n",
      "1013 ---- 892\n",
      "1014 ---- 893\n",
      "1015 ---- 894\n",
      "1016 ---- 895\n",
      "1017 ---- 896\n",
      "1018 ---- 897\n",
      "1019 ---- 898\n",
      "1020 ---- 899\n",
      "1021 ---- 900\n",
      "1022 ---- 901\n",
      "1023 ---- 902\n",
      "1024 ---- 903\n",
      "1025 ---- 904\n",
      "1026 ---- 905\n",
      "1027 ---- 906\n",
      "1028 ---- 907\n",
      "1029 ---- 908\n",
      "1030 ---- 909\n",
      "1031 ---- 910\n",
      "1032 ---- 911\n",
      "1033 ---- 912\n",
      "1034 ---- 913\n",
      "1035 ---- 914\n",
      "1036 ---- 915\n",
      "1037 ---- 916\n",
      "1038 ---- 917\n",
      "1039 ---- 918\n",
      "1040 ---- 919\n",
      "1041 ---- 920\n",
      "1042 ---- 921\n",
      "1043 ---- 922\n",
      "1044 ---- 923\n",
      "1045 ---- 924\n",
      "1046 ---- 925\n",
      "1047 ---- 926\n",
      "1048 ---- 927\n",
      "1049 ---- 928\n",
      "1050 ---- 929\n",
      "1051 ---- 930\n",
      "1052 ---- 931\n",
      "1053 ---- 932\n",
      "1054 ---- 933\n",
      "1055 ---- 934\n",
      "1056 ---- 935\n",
      "1057 ---- 936\n",
      "1058 ---- 937\n",
      "1059 ---- 938\n",
      "1060 ---- 939\n",
      "1061 ---- 940\n",
      "1062 ---- 941\n",
      "1063 ---- 942\n",
      "1064 ---- 943\n",
      "1065 ---- 944\n",
      "1066 ---- 945\n",
      "1067 ---- 946\n",
      "1068 ---- 947\n",
      "1069 ---- 948\n",
      "1070 ---- 949\n",
      "1071 ---- 950\n",
      "1072 ---- 951\n",
      "1073 ---- 952\n",
      "1074 ---- 953\n",
      "1075 ---- 954\n",
      "1076 ---- 955\n",
      "1077 ---- 956\n",
      "1078 ---- 957\n",
      "1079 ---- 958\n",
      "1080 ---- 959\n",
      "1081 ---- 960\n",
      "1082 ---- 961\n",
      "1083 ---- 962\n",
      "1084 ---- 963\n",
      "1085 ---- 964\n",
      "1086 ---- 965\n",
      "1087 ---- 966\n",
      "1088 ---- 967\n",
      "1089 ---- 968\n",
      "1090 ---- 969\n",
      "1091 ---- 970\n",
      "1092 ---- 971\n",
      "1093 ---- 972\n",
      "1094 ---- 973\n",
      "1095 ---- 974\n",
      "1096 ---- 975\n",
      "1097 ---- 976\n",
      "1098 ---- 977\n",
      "1099 ---- 978\n",
      "1100 ---- 979\n",
      "1101 ---- 980\n",
      "1102 ---- 981\n",
      "1103 ---- 982\n",
      "1104 ---- 983\n",
      "1105 ---- 984\n",
      "1106 ---- 985\n",
      "1107 ---- 986\n",
      "1108 ---- 987\n",
      "1109 ---- 988\n",
      "1110 ---- 989\n",
      "1111 ---- 990\n",
      "1112 ---- 991\n",
      "1113 ---- 992\n",
      "1114 ---- 993\n",
      "1115 ---- 994\n",
      "1116 ---- 995\n",
      "1117 ---- 996\n",
      "1118 ---- 997\n",
      "1119 ---- 998\n",
      "1120 ---- 999\n",
      "1121 ---- 1000\n",
      "1122 ---- 1001\n",
      "1123 ---- 1002\n",
      "1124 ---- 1003\n",
      "1125 ---- 1004\n",
      "1126 ---- 1005\n",
      "1127 ---- 1006\n",
      "1128 ---- 1007\n",
      "1129 ---- 1008\n",
      "1130 ---- 1009\n",
      "1131 ---- 1010\n",
      "1132 ---- 1011\n",
      "1133 ---- 1012\n",
      "1134 ---- 1013\n",
      "1135 ---- 1014\n",
      "1136 ---- 1015\n",
      "1137 ---- 1016\n",
      "1138 ---- 1017\n",
      "1139 ---- 1018\n",
      "1140 ---- 1019\n",
      "1141 ---- 1020\n",
      "1142 ---- 1021\n",
      "1143 ---- 1022\n",
      "1144 ---- 1023\n",
      "1145 ---- 1024\n",
      "1146 ---- 1025\n",
      "1147 ---- 1026\n",
      "1148 ---- 1027\n",
      "1149 ---- 1028\n",
      "1150 ---- 1029\n",
      "1151 ---- 1030\n",
      "1152 ---- 1031\n",
      "1153 ---- 1032\n",
      "1154 ---- 1033\n",
      "1155 ---- 1034\n",
      "1156 ---- 1035\n",
      "1157 ---- 1036\n",
      "1158 ---- 1037\n",
      "1159 ---- 1038\n",
      "1160 ---- 1039\n",
      "1161 ---- 1040\n",
      "1162 ---- 1041\n",
      "1163 ---- 1042\n",
      "1164 ---- 1043\n",
      "1165 ---- 1044\n",
      "1166 ---- 1045\n",
      "1167 ---- 1046\n",
      "1168 ---- 1047\n",
      "1169 ---- 1048\n",
      "1170 ---- 1049\n",
      "1171 ---- 1050\n",
      "1172 ---- 1051\n",
      "1173 ---- 1052\n",
      "1174 ---- 1053\n",
      "1175 ---- 1054\n",
      "1176 ---- 1055\n",
      "1177 ---- 1056\n",
      "1178 ---- 1057\n",
      "1179 ---- 1058\n",
      "1180 ---- 1059\n",
      "1181 ---- 1060\n",
      "1182 ---- 1061\n",
      "1183 ---- 1062\n",
      "1184 ---- 1063\n",
      "1185 ---- 1064\n",
      "1186 ---- 1065\n",
      "1187 ---- 1066\n",
      "1188 ---- 1067\n",
      "1189 ---- 1068\n",
      "1190 ---- 1069\n",
      "1191 ---- 1070\n",
      "1192 ---- 1071\n",
      "1193 ---- 1072\n",
      "1194 ---- 1073\n",
      "1195 ---- 1074\n",
      "1196 ---- 1075\n",
      "1197 ---- 1076\n",
      "1198 ---- 1077\n",
      "1199 ---- 1078\n",
      "1200 ---- 1079\n",
      "1201 ---- 1080\n",
      "1202 ---- 1081\n",
      "1203 ---- 1082\n",
      "1204 ---- 1083\n",
      "1205 ---- 1084\n",
      "1206 ---- 1085\n",
      "1207 ---- 1086\n",
      "1208 ---- 1087\n",
      "1209 ---- 1088\n",
      "1210 ---- 1089\n",
      "1211 ---- 1090\n",
      "1212 ---- 1091\n",
      "1213 ---- 1092\n",
      "1214 ---- 1093\n",
      "1215 ---- 1094\n",
      "1216 ---- 1095\n",
      "1217 ---- 1096\n",
      "1218 ---- 1097\n",
      "1219 ---- 1098\n",
      "1220 ---- 1099\n",
      "1221 ---- 1100\n",
      "1222 ---- 1101\n",
      "1223 ---- 1102\n",
      "1224 ---- 1103\n",
      "1225 ---- 1104\n",
      "1226 ---- 1105\n",
      "1227 ---- 1106\n",
      "1228 ---- 1107\n",
      "1229 ---- 1108\n",
      "1230 ---- 1109\n",
      "1231 ---- 1110\n",
      "1232 ---- 1111\n",
      "1233 ---- 1112\n",
      "1234 ---- 1113\n",
      "1235 ---- 1114\n",
      "1236 ---- 1115\n",
      "1237 ---- 1116\n",
      "1238 ---- 1117\n",
      "1239 ---- 1118\n",
      "1240 ---- 1119\n",
      "1241 ---- 1120\n",
      "1242 ---- 1121\n",
      "1243 ---- 1122\n",
      "1244 ---- 1123\n",
      "1245 ---- 1124\n",
      "1246 ---- 1125\n",
      "1247 ---- 1126\n",
      "1248 ---- 1127\n",
      "1249 ---- 1128\n",
      "1250 ---- 1129\n",
      "1251 ---- 1130\n",
      "1252 ---- 1131\n",
      "1253 ---- 1132\n",
      "1254 ---- 1133\n",
      "1255 ---- 1134\n",
      "1256 ---- 1135\n",
      "1257 ---- 1136\n",
      "1258 ---- 1137\n",
      "1259 ---- 1138\n",
      "1260 ---- 1139\n",
      "1261 ---- 1140\n",
      "1262 ---- 1141\n",
      "1263 ---- 1142\n",
      "1264 ---- 1143\n",
      "1265 ---- 1144\n",
      "1266 ---- 1145\n",
      "1267 ---- 1146\n",
      "1268 ---- 1147\n",
      "1269 ---- 1148\n",
      "1270 ---- 1149\n",
      "1271 ---- 1150\n",
      "1272 ---- 1151\n",
      "1273 ---- 1152\n",
      "1274 ---- 1153\n",
      "1275 ---- 1154\n",
      "1276 ---- 1155\n",
      "1277 ---- 1156\n",
      "1278 ---- 1157\n",
      "1279 ---- 1158\n",
      "1280 ---- 1159\n",
      "1281 ---- 1160\n",
      "1282 ---- 1161\n",
      "1283 ---- 1162\n",
      "1284 ---- 1163\n",
      "1285 ---- 1164\n",
      "1286 ---- 1165\n",
      "1287 ---- 1166\n",
      "1288 ---- 1167\n",
      "1289 ---- 1168\n",
      "1290 ---- 1169\n",
      "1291 ---- 1170\n",
      "1292 ---- 1171\n",
      "1293 ---- 1172\n",
      "1294 ---- 1173\n",
      "1295 ---- 1174\n",
      "1296 ---- 1175\n",
      "1297 ---- 1176\n",
      "1298 ---- 1177\n",
      "1299 ---- 1178\n",
      "1300 ---- 1179\n",
      "1301 ---- 1180\n",
      "1302 ---- 1181\n",
      "1303 ---- 1182\n",
      "1304 ---- 1183\n",
      "1305 ---- 1184\n",
      "1306 ---- 1185\n",
      "1307 ---- 1186\n",
      "1308 ---- 1187\n",
      "1309 ---- 1188\n",
      "1310 ---- 1189\n",
      "1311 ---- 1190\n",
      "1312 ---- 1191\n",
      "1313 ---- 1192\n",
      "1314 ---- 1193\n",
      "1315 ---- 1194\n",
      "1316 ---- 1195\n",
      "1317 ---- 1196\n",
      "1318 ---- 1197\n",
      "1319 ---- 1198\n",
      "1320 ---- 1199\n",
      "1321 ---- 1200\n",
      "1322 ---- 1201\n",
      "1323 ---- 1202\n",
      "1324 ---- 1203\n",
      "1325 ---- 1204\n",
      "1326 ---- 1205\n",
      "1327 ---- 1206\n",
      "1328 ---- 1207\n",
      "1329 ---- 1208\n",
      "1330 ---- 1209\n",
      "1331 ---- 1210\n",
      "1332 ---- 1211\n",
      "1333 ---- 1212\n",
      "1334 ---- 1213\n",
      "1335 ---- 1214\n",
      "1336 ---- 1215\n",
      "1337 ---- 1216\n",
      "1338 ---- 1217\n",
      "1339 ---- 1218\n",
      "1340 ---- 1219\n",
      "1341 ---- 1220\n",
      "1342 ---- 1221\n",
      "1343 ---- 1222\n",
      "1344 ---- 1223\n",
      "1345 ---- 1224\n",
      "1346 ---- 1225\n",
      "1347 ---- 1226\n",
      "1348 ---- 1227\n",
      "1349 ---- 1228\n",
      "1350 ---- 1229\n",
      "1351 ---- 1230\n",
      "1352 ---- 1231\n",
      "1353 ---- 1232\n",
      "1354 ---- 1233\n",
      "1355 ---- 1234\n",
      "1356 ---- 1235\n",
      "1357 ---- 1236\n",
      "1358 ---- 1237\n",
      "1359 ---- 1238\n",
      "1360 ---- 1239\n",
      "1361 ---- 1240\n",
      "1362 ---- 1241\n",
      "1363 ---- 1242\n",
      "1364 ---- 1243\n",
      "1365 ---- 1244\n",
      "1366 ---- 1245\n",
      "1367 ---- 1246\n",
      "1368 ---- 1247\n",
      "1369 ---- 1248\n",
      "1370 ---- 1249\n",
      "1371 ---- 1250\n",
      "1372 ---- 1251\n",
      "1373 ---- 1252\n",
      "1374 ---- 1253\n",
      "1375 ---- 1254\n",
      "1376 ---- 1255\n",
      "1377 ---- 1256\n",
      "1378 ---- 1257\n",
      "1379 ---- 1258\n",
      "1380 ---- 1259\n",
      "1381 ---- 1260\n",
      "1382 ---- 1261\n",
      "1383 ---- 1262\n",
      "1384 ---- 1263\n",
      "1385 ---- 1264\n",
      "1386 ---- 1265\n",
      "1387 ---- 1266\n",
      "1388 ---- 1267\n",
      "1389 ---- 1268\n",
      "1390 ---- 1269\n",
      "1391 ---- 1270\n",
      "1392 ---- 1271\n",
      "1393 ---- 1272\n",
      "1394 ---- 1273\n",
      "1395 ---- 1274\n",
      "1396 ---- 1275\n",
      "1397 ---- 1276\n",
      "1398 ---- 1277\n",
      "1399 ---- 1278\n",
      "1400 ---- 1279\n",
      "1401 ---- 1280\n",
      "1402 ---- 1281\n",
      "1403 ---- 1282\n",
      "1404 ---- 1283\n",
      "1405 ---- 1284\n",
      "1406 ---- 1285\n",
      "1407 ---- 1286\n",
      "1408 ---- 1287\n",
      "1409 ---- 1288\n",
      "1410 ---- 1289\n",
      "1411 ---- 1290\n",
      "1412 ---- 1291\n",
      "1413 ---- 1292\n",
      "1414 ---- 1293\n",
      "1415 ---- 1294\n",
      "1416 ---- 1295\n",
      "1417 ---- 1296\n",
      "1418 ---- 1297\n",
      "1419 ---- 1298\n",
      "1420 ---- 1299\n",
      "1421 ---- 1300\n",
      "1422 ---- 1301\n",
      "1423 ---- 1302\n",
      "1424 ---- 1303\n",
      "1425 ---- 1304\n",
      "1426 ---- 1305\n",
      "1427 ---- 1306\n",
      "1428 ---- 1307\n",
      "1429 ---- 1308\n",
      "1430 ---- 1309\n",
      "1431 ---- 1310\n",
      "1432 ---- 1311\n",
      "1433 ---- 1312\n",
      "1434 ---- 1313\n",
      "1435 ---- 1314\n",
      "1436 ---- 1315\n",
      "1437 ---- 1316\n",
      "1438 ---- 1317\n",
      "1439 ---- 1318\n",
      "1440 ---- 1319\n",
      "1441 ---- 1320\n",
      "1442 ---- 1321\n",
      "1443 ---- 1322\n",
      "1444 ---- 1323\n",
      "1445 ---- 1324\n",
      "1446 ---- 1325\n",
      "1447 ---- 1326\n",
      "1448 ---- 1327\n",
      "1449 ---- 1328\n",
      "1450 ---- 1329\n",
      "1451 ---- 1330\n",
      "1452 ---- 1331\n",
      "1453 ---- 1332\n",
      "1454 ---- 1333\n",
      "1455 ---- 1334\n",
      "1456 ---- 1335\n",
      "1457 ---- 1336\n",
      "1458 ---- 1337\n",
      "1459 ---- 1338\n",
      "1460 ---- 1339\n",
      "1461 ---- 1340\n",
      "1462 ---- 1341\n",
      "1463 ---- 1342\n",
      "1464 ---- 1343\n",
      "1465 ---- 1344\n",
      "1466 ---- 1345\n",
      "1467 ---- 1346\n",
      "1468 ---- 1347\n",
      "1469 ---- 1348\n",
      "1470 ---- 1349\n",
      "1471 ---- 1350\n",
      "1472 ---- 1351\n",
      "1473 ---- 1352\n",
      "1474 ---- 1353\n",
      "1475 ---- 1354\n",
      "1476 ---- 1355\n",
      "1477 ---- 1356\n",
      "1478 ---- 1357\n",
      "1479 ---- 1358\n",
      "1480 ---- 1359\n",
      "1481 ---- 1360\n",
      "1482 ---- 1361\n",
      "1483 ---- 1362\n",
      "1484 ---- 1363\n",
      "1485 ---- 1364\n",
      "1486 ---- 1365\n",
      "1487 ---- 1366\n",
      "1488 ---- 1367\n",
      "1489 ---- 1368\n",
      "1490 ---- 1369\n",
      "1491 ---- 1370\n",
      "1492 ---- 1371\n",
      "1493 ---- 1372\n",
      "1494 ---- 1373\n",
      "1495 ---- 1374\n",
      "1496 ---- 1375\n",
      "1497 ---- 1376\n",
      "1498 ---- 1377\n",
      "1499 ---- 1378\n",
      "1500 ---- 1379\n",
      "1501 ---- 1380\n",
      "1502 ---- 1381\n",
      "1503 ---- 1382\n",
      "1504 ---- 1383\n",
      "1505 ---- 1384\n",
      "1506 ---- 1385\n",
      "1507 ---- 1386\n",
      "1508 ---- 1387\n",
      "1509 ---- 1388\n",
      "1510 ---- 1389\n",
      "1511 ---- 1390\n",
      "1512 ---- 1391\n",
      "1513 ---- 1392\n",
      "1514 ---- 1393\n",
      "1515 ---- 1394\n",
      "1516 ---- 1395\n",
      "1517 ---- 1396\n",
      "1518 ---- 1397\n",
      "1519 ---- 1398\n",
      "1520 ---- 1399\n",
      "1521 ---- 1400\n",
      "1522 ---- 1401\n",
      "1523 ---- 1402\n",
      "1524 ---- 1403\n",
      "1525 ---- 1404\n",
      "1526 ---- 1405\n",
      "1527 ---- 1406\n",
      "1528 ---- 1407\n",
      "1529 ---- 1408\n",
      "1530 ---- 1409\n",
      "1531 ---- 1410\n",
      "1532 ---- 1411\n",
      "1533 ---- 1412\n",
      "1534 ---- 1413\n",
      "1535 ---- 1414\n",
      "1536 ---- 1415\n",
      "1537 ---- 1416\n",
      "1538 ---- 1417\n",
      "1539 ---- 1418\n",
      "1540 ---- 1419\n",
      "1541 ---- 1420\n",
      "1542 ---- 1421\n",
      "1543 ---- 1422\n",
      "1544 ---- 1423\n",
      "1545 ---- 1424\n",
      "1546 ---- 1425\n",
      "1547 ---- 1426\n",
      "1548 ---- 1427\n",
      "1549 ---- 1428\n",
      "1550 ---- 1429\n",
      "1551 ---- 1430\n",
      "1552 ---- 1431\n",
      "1553 ---- 1432\n",
      "1554 ---- 1433\n",
      "1555 ---- 1434\n",
      "1556 ---- 1435\n",
      "1557 ---- 1436\n",
      "1558 ---- 1437\n",
      "1559 ---- 1438\n",
      "1560 ---- 1439\n",
      "1561 ---- 1440\n",
      "1562 ---- 1441\n",
      "1563 ---- 1442\n",
      "1564 ---- 1443\n",
      "1565 ---- 1444\n",
      "1566 ---- 1445\n",
      "1567 ---- 1446\n",
      "1568 ---- 1447\n",
      "1569 ---- 1448\n",
      "1570 ---- 1449\n",
      "1571 ---- 1450\n",
      "1572 ---- 1451\n",
      "1573 ---- 1452\n",
      "1574 ---- 1453\n",
      "1575 ---- 1454\n",
      "1576 ---- 1455\n",
      "1577 ---- 1456\n",
      "1578 ---- 1457\n",
      "1579 ---- 1458\n",
      "1580 ---- 1459\n",
      "1581 ---- 1460\n",
      "1582 ---- 1461\n",
      "1583 ---- 1462\n",
      "1584 ---- 1463\n",
      "1585 ---- 1464\n",
      "1586 ---- 1465\n",
      "1587 ---- 1466\n",
      "1588 ---- 1467\n",
      "1589 ---- 1468\n",
      "1590 ---- 1469\n",
      "1591 ---- 1470\n",
      "1592 ---- 1471\n",
      "1593 ---- 1472\n",
      "1594 ---- 1473\n",
      "1595 ---- 1474\n",
      "1596 ---- 1475\n",
      "1597 ---- 1476\n",
      "1598 ---- 1477\n",
      "1599 ---- 1478\n",
      "1600 ---- 1479\n",
      "1601 ---- 1480\n",
      "1602 ---- 1481\n",
      "1603 ---- 1482\n",
      "1604 ---- 1483\n",
      "1605 ---- 1484\n",
      "1606 ---- 1485\n",
      "1607 ---- 1486\n",
      "1608 ---- 1487\n",
      "1609 ---- 1488\n",
      "1610 ---- 1489\n",
      "1611 ---- 1490\n",
      "1612 ---- 1491\n",
      "1613 ---- 1492\n",
      "1614 ---- 1493\n",
      "1615 ---- 1494\n",
      "1616 ---- 1495\n",
      "1617 ---- 1496\n",
      "1618 ---- 1497\n",
      "1619 ---- 1498\n",
      "1620 ---- 1499\n",
      "1621 ---- 1500\n",
      "1622 ---- 1501\n",
      "1623 ---- 1502\n",
      "1624 ---- 1503\n",
      "1625 ---- 1504\n",
      "1626 ---- 1505\n",
      "1627 ---- 1506\n",
      "1628 ---- 1507\n",
      "1629 ---- 1508\n",
      "1630 ---- 1509\n",
      "1631 ---- 1510\n",
      "1632 ---- 1511\n",
      "1633 ---- 1512\n",
      "1634 ---- 1513\n",
      "1635 ---- 1514\n",
      "1636 ---- 1515\n",
      "1637 ---- 1516\n",
      "1638 ---- 1517\n",
      "1639 ---- 1518\n",
      "1640 ---- 1519\n",
      "1641 ---- 1520\n",
      "1642 ---- 1521\n",
      "1643 ---- 1522\n",
      "1644 ---- 1523\n",
      "1645 ---- 1524\n",
      "1646 ---- 1525\n",
      "1647 ---- 1526\n",
      "1648 ---- 1527\n",
      "1649 ---- 1528\n",
      "1650 ---- 1529\n",
      "1651 ---- 1530\n",
      "1652 ---- 1531\n",
      "1653 ---- 1532\n",
      "1654 ---- 1533\n",
      "1655 ---- 1534\n",
      "1656 ---- 1535\n",
      "1657 ---- 1536\n",
      "1658 ---- 1537\n",
      "1659 ---- 1538\n",
      "1660 ---- 1539\n",
      "1661 ---- 1540\n",
      "1662 ---- 1541\n",
      "1663 ---- 1542\n",
      "1664 ---- 1543\n",
      "1665 ---- 1544\n",
      "1666 ---- 1545\n",
      "1667 ---- 1546\n",
      "1668 ---- 1547\n",
      "1669 ---- 1548\n",
      "1670 ---- 1549\n",
      "1671 ---- 1550\n",
      "1672 ---- 1551\n",
      "1673 ---- 1552\n",
      "1674 ---- 1553\n",
      "1675 ---- 1554\n",
      "1676 ---- 1555\n",
      "1677 ---- 1556\n",
      "1678 ---- 1557\n",
      "1679 ---- 1558\n",
      "1680 ---- 1559\n",
      "1681 ---- 1560\n",
      "1682 ---- 1561\n",
      "1683 ---- 1562\n",
      "1684 ---- 1563\n",
      "1685 ---- 1564\n",
      "1686 ---- 1565\n",
      "1687 ---- 1566\n",
      "1688 ---- 1567\n",
      "1689 ---- 1568\n",
      "1690 ---- 1569\n",
      "1691 ---- 1570\n",
      "1692 ---- 1571\n",
      "1693 ---- 1572\n",
      "1694 ---- 1573\n",
      "1695 ---- 1574\n",
      "1696 ---- 1575\n",
      "1697 ---- 1576\n",
      "1698 ---- 1577\n",
      "1699 ---- 1578\n",
      "1700 ---- 1579\n",
      "1701 ---- 1580\n",
      "1702 ---- 1581\n",
      "1703 ---- 1582\n",
      "1704 ---- 1583\n",
      "1705 ---- 1584\n",
      "1706 ---- 1585\n",
      "1707 ---- 1586\n",
      "1708 ---- 1587\n",
      "1709 ---- 1588\n",
      "1710 ---- 1589\n",
      "1711 ---- 1590\n",
      "1712 ---- 1591\n",
      "1713 ---- 1592\n",
      "1714 ---- 1593\n",
      "1715 ---- 1594\n",
      "1716 ---- 1595\n",
      "1717 ---- 1596\n",
      "1718 ---- 1597\n",
      "1719 ---- 1598\n",
      "1720 ---- 1599\n",
      "1721 ---- 1600\n",
      "1722 ---- 1601\n",
      "1723 ---- 1602\n",
      "1724 ---- 1603\n",
      "1725 ---- 1604\n",
      "1726 ---- 1605\n",
      "1727 ---- 1606\n",
      "1728 ---- 1607\n",
      "1729 ---- 1608\n",
      "1730 ---- 1609\n",
      "1731 ---- 1610\n",
      "1732 ---- 1611\n",
      "1733 ---- 1612\n",
      "1734 ---- 1613\n",
      "1735 ---- 1614\n",
      "1736 ---- 1615\n",
      "1737 ---- 1616\n",
      "1738 ---- 1617\n",
      "1739 ---- 1618\n",
      "1740 ---- 1619\n",
      "1741 ---- 1620\n",
      "1742 ---- 1621\n",
      "1743 ---- 1622\n",
      "1744 ---- 1623\n",
      "1745 ---- 1624\n",
      "1746 ---- 1625\n",
      "1747 ---- 1626\n",
      "1748 ---- 1627\n",
      "1749 ---- 1628\n",
      "1750 ---- 1629\n",
      "1751 ---- 1630\n",
      "1752 ---- 1631\n",
      "1753 ---- 1632\n",
      "1754 ---- 1633\n",
      "1755 ---- 1634\n",
      "1756 ---- 1635\n",
      "1757 ---- 1636\n",
      "1758 ---- 1637\n",
      "1759 ---- 1638\n",
      "1760 ---- 1639\n",
      "1761 ---- 1640\n",
      "1762 ---- 1641\n",
      "1763 ---- 1642\n",
      "1764 ---- 1643\n",
      "1765 ---- 1644\n",
      "1766 ---- 1645\n",
      "1767 ---- 1646\n",
      "1768 ---- 1647\n",
      "1769 ---- 1648\n",
      "1770 ---- 1649\n",
      "1771 ---- 1650\n",
      "1772 ---- 1651\n",
      "1773 ---- 1652\n",
      "1774 ---- 1653\n",
      "1775 ---- 1654\n",
      "1776 ---- 1655\n",
      "1777 ---- 1656\n",
      "1778 ---- 1657\n",
      "1779 ---- 1658\n",
      "1780 ---- 1659\n",
      "1781 ---- 1660\n",
      "1782 ---- 1661\n",
      "1783 ---- 1662\n",
      "1784 ---- 1663\n",
      "1785 ---- 1664\n",
      "1786 ---- 1665\n",
      "1787 ---- 1666\n",
      "1788 ---- 1667\n",
      "1789 ---- 1668\n",
      "1790 ---- 1669\n",
      "1791 ---- 1670\n",
      "1792 ---- 1671\n",
      "1793 ---- 1672\n",
      "1794 ---- 1673\n",
      "1795 ---- 1674\n",
      "1796 ---- 1675\n",
      "1797 ---- 1676\n",
      "1798 ---- 1677\n",
      "1799 ---- 1678\n",
      "1800 ---- 1679\n",
      "1801 ---- 1680\n",
      "1802 ---- 1681\n",
      "1803 ---- 1682\n",
      "1804 ---- 1683\n",
      "1805 ---- 1684\n",
      "1806 ---- 1685\n",
      "1807 ---- 1686\n",
      "1808 ---- 1687\n",
      "1809 ---- 1688\n",
      "1810 ---- 1689\n",
      "1811 ---- 1690\n",
      "1812 ---- 1691\n",
      "1813 ---- 1692\n",
      "1814 ---- 1693\n",
      "1815 ---- 1694\n",
      "1816 ---- 1695\n",
      "1817 ---- 1696\n",
      "1818 ---- 1697\n",
      "1819 ---- 1698\n",
      "1820 ---- 1699\n",
      "1821 ---- 1700\n",
      "1822 ---- 1701\n",
      "1823 ---- 1702\n",
      "1824 ---- 1703\n",
      "1825 ---- 1704\n",
      "1826 ---- 1705\n",
      "1827 ---- 1706\n",
      "1828 ---- 1707\n",
      "1829 ---- 1708\n",
      "1830 ---- 1709\n",
      "1831 ---- 1710\n",
      "1832 ---- 1711\n",
      "1833 ---- 1712\n",
      "1834 ---- 1713\n",
      "1835 ---- 1714\n",
      "1836 ---- 1715\n",
      "1837 ---- 1716\n",
      "1838 ---- 1717\n",
      "1839 ---- 1718\n",
      "1840 ---- 1719\n",
      "1841 ---- 1720\n",
      "1842 ---- 1721\n",
      "1843 ---- 1722\n",
      "1844 ---- 1723\n",
      "1845 ---- 1724\n",
      "1846 ---- 1725\n",
      "1847 ---- 1726\n",
      "1848 ---- 1727\n",
      "1849 ---- 1728\n",
      "1850 ---- 1729\n",
      "1851 ---- 1730\n",
      "1852 ---- 1731\n",
      "1853 ---- 1732\n",
      "1854 ---- 1733\n",
      "1855 ---- 1734\n",
      "1856 ---- 1735\n",
      "1857 ---- 1736\n",
      "1858 ---- 1737\n",
      "1859 ---- 1738\n",
      "1860 ---- 1739\n",
      "1861 ---- 1740\n",
      "1862 ---- 1741\n",
      "1863 ---- 1742\n",
      "1864 ---- 1743\n",
      "1865 ---- 1744\n",
      "1866 ---- 1745\n",
      "1867 ---- 1746\n",
      "1868 ---- 1747\n",
      "1869 ---- 1748\n",
      "1870 ---- 1749\n",
      "1871 ---- 1750\n",
      "1872 ---- 1751\n",
      "1873 ---- 1752\n",
      "1874 ---- 1753\n",
      "1875 ---- 1754\n",
      "1876 ---- 1755\n",
      "1877 ---- 1756\n",
      "1878 ---- 1757\n",
      "1879 ---- 1758\n",
      "1880 ---- 1759\n",
      "1881 ---- 1760\n",
      "1882 ---- 1761\n",
      "1883 ---- 1762\n",
      "1884 ---- 1763\n",
      "1885 ---- 1764\n",
      "1886 ---- 1765\n",
      "1887 ---- 1766\n",
      "1888 ---- 1767\n",
      "1889 ---- 1768\n",
      "1890 ---- 1769\n",
      "1891 ---- 1770\n",
      "1892 ---- 1771\n",
      "1893 ---- 1772\n",
      "1894 ---- 1773\n",
      "1895 ---- 1774\n",
      "1896 ---- 1775\n",
      "1897 ---- 1776\n",
      "1898 ---- 1777\n",
      "1899 ---- 1778\n",
      "1900 ---- 1779\n",
      "1901 ---- 1780\n",
      "1902 ---- 1781\n",
      "1903 ---- 1782\n",
      "1904 ---- 1783\n",
      "1905 ---- 1784\n",
      "1906 ---- 1785\n",
      "1907 ---- 1786\n",
      "1908 ---- 1787\n",
      "1909 ---- 1788\n",
      "1910 ---- 1789\n",
      "1911 ---- 1790\n",
      "1912 ---- 1791\n",
      "1913 ---- 1792\n",
      "1914 ---- 1793\n",
      "1915 ---- 1794\n",
      "1916 ---- 1795\n",
      "1917 ---- 1796\n",
      "1918 ---- 1797\n",
      "1919 ---- 1798\n",
      "1920 ---- 1799\n",
      "1921 ---- 1800\n",
      "1922 ---- 1801\n",
      "1923 ---- 1802\n",
      "1924 ---- 1803\n",
      "1925 ---- 1804\n",
      "1926 ---- 1805\n",
      "1927 ---- 1806\n",
      "1928 ---- 1807\n",
      "1929 ---- 1808\n",
      "1930 ---- 1809\n",
      "1931 ---- 1810\n",
      "1932 ---- 1811\n",
      "1933 ---- 1812\n",
      "1934 ---- 1813\n",
      "1935 ---- 1814\n",
      "1936 ---- 1815\n",
      "1937 ---- 1816\n",
      "1938 ---- 1817\n",
      "1939 ---- 1818\n",
      "1940 ---- 1819\n",
      "1941 ---- 1820\n",
      "1942 ---- 1821\n",
      "1943 ---- 1822\n",
      "1944 ---- 1823\n",
      "1945 ---- 1824\n",
      "1946 ---- 1825\n",
      "1947 ---- 1826\n",
      "1948 ---- 1827\n",
      "1949 ---- 1828\n",
      "1950 ---- 1829\n",
      "1951 ---- 1830\n",
      "1952 ---- 1831\n",
      "1953 ---- 1832\n",
      "1954 ---- 1833\n",
      "1955 ---- 1834\n",
      "1956 ---- 1835\n",
      "1957 ---- 1836\n",
      "1958 ---- 1837\n",
      "1959 ---- 1838\n",
      "1960 ---- 1839\n",
      "1961 ---- 1840\n",
      "1962 ---- 1841\n",
      "1963 ---- 1842\n",
      "1964 ---- 1843\n",
      "1965 ---- 1844\n",
      "1966 ---- 1845\n",
      "1967 ---- 1846\n",
      "1968 ---- 1847\n",
      "1969 ---- 1848\n",
      "1970 ---- 1849\n",
      "1971 ---- 1850\n",
      "1972 ---- 1851\n",
      "1973 ---- 1852\n",
      "1974 ---- 1853\n",
      "1975 ---- 1854\n",
      "1976 ---- 1855\n",
      "1977 ---- 1856\n",
      "1978 ---- 1857\n",
      "1979 ---- 1858\n",
      "1980 ---- 1859\n",
      "1981 ---- 1860\n",
      "1982 ---- 1861\n",
      "1983 ---- 1862\n",
      "1984 ---- 1863\n",
      "1985 ---- 1864\n",
      "1986 ---- 1865\n",
      "1987 ---- 1866\n",
      "1988 ---- 1867\n",
      "1989 ---- 1868\n",
      "1990 ---- 1869\n",
      "1991 ---- 1870\n",
      "1992 ---- 1871\n",
      "1993 ---- 1872\n",
      "1994 ---- 1873\n",
      "1995 ---- 1874\n",
      "1996 ---- 1875\n",
      "1997 ---- 1876\n",
      "1998 ---- 1877\n",
      "1999 ---- 1878\n",
      "2000 ---- 1879\n",
      "2001 ---- 1880\n",
      "2002 ---- 1881\n",
      "2003 ---- 1882\n",
      "2004 ---- 1883\n",
      "2005 ---- 1884\n",
      "2006 ---- 1885\n",
      "2007 ---- 1886\n",
      "2008 ---- 1887\n",
      "2009 ---- 1888\n",
      "2010 ---- 1889\n",
      "2011 ---- 1890\n",
      "2012 ---- 1891\n",
      "2013 ---- 1892\n",
      "2014 ---- 1893\n",
      "2015 ---- 1894\n",
      "2016 ---- 1895\n",
      "2017 ---- 1896\n",
      "2018 ---- 1897\n",
      "2019 ---- 1898\n",
      "2020 ---- 1899\n",
      "2021 ---- 1900\n",
      "2022 ---- 1901\n",
      "2023 ---- 1902\n",
      "2024 ---- 1903\n",
      "2025 ---- 1904\n",
      "2026 ---- 1905\n",
      "2027 ---- 1906\n",
      "2028 ---- 1907\n",
      "2029 ---- 1908\n",
      "2030 ---- 1909\n",
      "2031 ---- 1910\n",
      "2032 ---- 1911\n",
      "2033 ---- 1912\n",
      "2034 ---- 1913\n",
      "2035 ---- 1914\n",
      "2036 ---- 1915\n",
      "2037 ---- 1916\n",
      "2038 ---- 1917\n",
      "2039 ---- 1918\n",
      "2040 ---- 1919\n",
      "2041 ---- 1920\n",
      "2042 ---- 1921\n",
      "2043 ---- 1922\n",
      "2044 ---- 1923\n",
      "2045 ---- 1924\n",
      "2046 ---- 1925\n",
      "2047 ---- 1926\n",
      "2048 ---- 1927\n",
      "2049 ---- 1928\n",
      "2050 ---- 1929\n",
      "2051 ---- 1930\n",
      "2052 ---- 1931\n",
      "2053 ---- 1932\n",
      "2054 ---- 1933\n",
      "2055 ---- 1934\n",
      "2056 ---- 1935\n",
      "2057 ---- 1936\n",
      "2058 ---- 1937\n",
      "2059 ---- 1938\n",
      "2060 ---- 1939\n",
      "2061 ---- 1940\n",
      "2062 ---- 1941\n",
      "2063 ---- 1942\n",
      "2064 ---- 1943\n",
      "2065 ---- 1944\n",
      "2066 ---- 1945\n",
      "2067 ---- 1946\n",
      "2068 ---- 1947\n",
      "2069 ---- 1948\n",
      "2070 ---- 1949\n",
      "2071 ---- 1950\n",
      "2072 ---- 1951\n",
      "2073 ---- 1952\n",
      "2074 ---- 1953\n",
      "2075 ---- 1954\n",
      "2076 ---- 1955\n",
      "2077 ---- 1956\n",
      "2078 ---- 1957\n",
      "2079 ---- 1958\n",
      "2080 ---- 1959\n",
      "2081 ---- 1960\n",
      "2082 ---- 1961\n",
      "2083 ---- 1962\n",
      "2084 ---- 1963\n",
      "2085 ---- 1964\n",
      "2086 ---- 1965\n",
      "2087 ---- 1966\n",
      "2088 ---- 1967\n",
      "2089 ---- 1968\n",
      "2090 ---- 1969\n",
      "2091 ---- 1970\n",
      "2092 ---- 1971\n",
      "2093 ---- 1972\n",
      "2094 ---- 1973\n",
      "2095 ---- 1974\n",
      "2096 ---- 1975\n",
      "2097 ---- 1976\n",
      "2098 ---- 1977\n",
      "2099 ---- 1978\n",
      "2100 ---- 1979\n",
      "2101 ---- 1980\n",
      "2102 ---- 1981\n",
      "2103 ---- 1982\n",
      "2104 ---- 1983\n",
      "2105 ---- 1984\n",
      "2106 ---- 1985\n",
      "2107 ---- 1986\n",
      "2108 ---- 1987\n",
      "2109 ---- 1988\n",
      "2110 ---- 1989\n",
      "2111 ---- 1990\n",
      "2112 ---- 1991\n",
      "2113 ---- 1992\n",
      "2114 ---- 1993\n",
      "2115 ---- 1994\n",
      "2116 ---- 1995\n",
      "2117 ---- 1996\n",
      "2118 ---- 1997\n",
      "2119 ---- 1998\n",
      "2120 ---- 1999\n",
      "2121 ---- 2000\n",
      "2122 ---- 2001\n",
      "2123 ---- 2002\n",
      "2124 ---- 2003\n",
      "2125 ---- 2004\n",
      "2126 ---- 2005\n",
      "2127 ---- 2006\n",
      "2128 ---- 2007\n",
      "2129 ---- 2008\n",
      "2130 ---- 2009\n",
      "2131 ---- 2010\n",
      "2132 ---- 2011\n",
      "2133 ---- 2012\n",
      "2134 ---- 2013\n",
      "2135 ---- 2014\n",
      "2136 ---- 2015\n",
      "2137 ---- 2016\n",
      "2138 ---- 2017\n",
      "2139 ---- 2018\n",
      "2140 ---- 2019\n",
      "2141 ---- 2020\n",
      "2142 ---- 2021\n",
      "2143 ---- 2022\n",
      "2144 ---- 2023\n",
      "2145 ---- 2024\n",
      "2146 ---- 2025\n",
      "2147 ---- 2026\n",
      "2148 ---- 2027\n",
      "2149 ---- 2028\n",
      "2150 ---- 2029\n",
      "2151 ---- 2030\n",
      "2152 ---- 2031\n",
      "2153 ---- 2032\n",
      "2154 ---- 2033\n",
      "2155 ---- 2034\n",
      "2156 ---- 2035\n",
      "2157 ---- 2036\n",
      "2158 ---- 2037\n",
      "2159 ---- 2038\n",
      "2160 ---- 2039\n",
      "2161 ---- 2040\n",
      "2162 ---- 2041\n",
      "2163 ---- 2042\n",
      "2164 ---- 2043\n",
      "2165 ---- 2044\n",
      "2166 ---- 2045\n",
      "2167 ---- 2046\n",
      "2168 ---- 2047\n",
      "2169 ---- 2048\n",
      "2170 ---- 2049\n",
      "2171 ---- 2050\n",
      "2172 ---- 2051\n",
      "2173 ---- 2052\n",
      "2174 ---- 2053\n",
      "2175 ---- 2054\n",
      "2176 ---- 2055\n",
      "2177 ---- 2056\n",
      "2178 ---- 2057\n",
      "2179 ---- 2058\n",
      "2180 ---- 2059\n",
      "2181 ---- 2060\n",
      "2182 ---- 2061\n",
      "2183 ---- 2062\n",
      "2184 ---- 2063\n",
      "2185 ---- 2064\n",
      "2186 ---- 2065\n",
      "2187 ---- 2066\n",
      "2188 ---- 2067\n",
      "2189 ---- 2068\n",
      "2190 ---- 2069\n",
      "2191 ---- 2070\n",
      "2192 ---- 2071\n",
      "2193 ---- 2072\n",
      "2194 ---- 2073\n",
      "2195 ---- 2074\n",
      "2196 ---- 2075\n",
      "2197 ---- 2076\n",
      "2198 ---- 2077\n",
      "2199 ---- 2078\n",
      "2200 ---- 2079\n",
      "2201 ---- 2080\n",
      "2202 ---- 2081\n",
      "2203 ---- 2082\n",
      "2204 ---- 2083\n",
      "2205 ---- 2084\n",
      "2206 ---- 2085\n",
      "2207 ---- 2086\n",
      "2208 ---- 2087\n",
      "2209 ---- 2088\n",
      "2210 ---- 2089\n",
      "2211 ---- 2090\n",
      "2212 ---- 2091\n",
      "2213 ---- 2092\n",
      "2214 ---- 2093\n",
      "2215 ---- 2094\n",
      "2216 ---- 2095\n",
      "2217 ---- 2096\n",
      "2218 ---- 2097\n",
      "2219 ---- 2098\n",
      "2220 ---- 2099\n",
      "2221 ---- 2100\n",
      "2222 ---- 2101\n",
      "2223 ---- 2102\n",
      "2224 ---- 2103\n",
      "2225 ---- 2104\n",
      "2226 ---- 2105\n",
      "2227 ---- 2106\n",
      "2228 ---- 2107\n",
      "2229 ---- 2108\n",
      "2230 ---- 2109\n",
      "2231 ---- 2110\n",
      "2232 ---- 2111\n",
      "2233 ---- 2112\n",
      "2234 ---- 2113\n",
      "2235 ---- 2114\n",
      "2236 ---- 2115\n",
      "2237 ---- 2116\n",
      "2238 ---- 2117\n",
      "2239 ---- 2118\n",
      "2240 ---- 2119\n",
      "2241 ---- 2120\n",
      "2242 ---- 2121\n",
      "2243 ---- 2122\n",
      "2244 ---- 2123\n",
      "2245 ---- 2124\n",
      "2246 ---- 2125\n",
      "2247 ---- 2126\n",
      "2248 ---- 2127\n",
      "2249 ---- 2128\n",
      "2250 ---- 2129\n",
      "2251 ---- 2130\n",
      "2252 ---- 2131\n",
      "2253 ---- 2132\n",
      "2254 ---- 2133\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(X_train_final.columns)):\n",
    "    print('{} ---- {}'.format(i, X_train_final.columns[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Unigrams, POS Tag Count, Sentiment Polarity, Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_1 = X_train_final.iloc[:,np.r_[10:12,13:21,121:1113]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3763, 1002)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>interjections</th>\n",
       "      <th>nouns</th>\n",
       "      <th>adverb</th>\n",
       "      <th>verb</th>\n",
       "      <th>determiner</th>\n",
       "      <th>pronoun</th>\n",
       "      <th>adjetive</th>\n",
       "      <th>preposition</th>\n",
       "      <th>...</th>\n",
       "      <th>982</th>\n",
       "      <th>983</th>\n",
       "      <th>984</th>\n",
       "      <th>985</th>\n",
       "      <th>986</th>\n",
       "      <th>987</th>\n",
       "      <th>988</th>\n",
       "      <th>989</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1375</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity  subjectivity  interjections  nouns  adverb  verb  determiner  \\\n",
       "0    0.1375           0.5              0      7       2     4           2   \n",
       "1    0.5000           0.5              0      2       1     1           1   \n",
       "\n",
       "   pronoun  adjetive  preposition  ...  982  983  984  985  986  987  988  \\\n",
       "0        1         1            3  ...    0    0    0    0    0    0    0   \n",
       "1        0         1            1  ...    0    0    0    0    0    0    0   \n",
       "\n",
       "   989  990  991  \n",
       "0    0    0    0  \n",
       "1    0    0    0  \n",
       "\n",
       "[2 rows x 1002 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_1 = X_test_final.iloc[:,np.r_[10:12,13:21,121:1113]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(941, 1002)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.385\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9274    0.7606    0.8358       823\n",
      "           1     0.2594    0.5847    0.3594       118\n",
      "\n",
      "    accuracy                         0.7386       941\n",
      "   macro avg     0.5934    0.6727    0.5976       941\n",
      "weighted avg     0.8436    0.7386    0.7760       941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_1_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_1_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_1,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_1), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 626, False Positive: 197, False Negative: 49, True Positive: 69\n",
      "--------------------------------------------------------------------------------\n",
      "[[626 197]\n",
      " [ 49  69]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.76      0.84       823\n",
      "           1       0.26      0.58      0.36       118\n",
      "\n",
      "    accuracy                           0.74       941\n",
      "   macro avg       0.59      0.67      0.60       941\n",
      "weighted avg       0.84      0.74      0.78       941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_1 = LogisticRegression(random_state=18, \n",
    "                                solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], \n",
    "                                class_weight='balanced').fit(X_train_model_1, y_train)\n",
    "y_lr = lr_model_1.predict(X_test_model_1)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | Precision | Recall | F1 | Data Points |\n",
    "| --- | --- |--- | --- | --- |\n",
    "| Class 0 | 0.93 | 0.85 | 0.89 | 117 |\n",
    "| Class 1 |0.51 | 0.72 | 0.60 | 25 |\n",
    "| Macro Average Score | 0.72 | 0.79 | 0.75 | 142|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_2 = X_train_final.iloc[:,np.r_[3:1113]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3763, 1110)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_2 = X_test_final.iloc[:,np.r_[3:1113]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(941, 1110)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.418\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9377    0.7315    0.8218       823\n",
      "           1     0.2609    0.6610    0.3741       118\n",
      "\n",
      "    accuracy                         0.7226       941\n",
      "   macro avg     0.5993    0.6962    0.5980       941\n",
      "weighted avg     0.8528    0.7226    0.7657       941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_2_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_2_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_2,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_2), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 602, False Positive: 221, False Negative: 40, True Positive: 78\n",
      "--------------------------------------------------------------------------------\n",
      "[[602 221]\n",
      " [ 40  78]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.73      0.82       823\n",
      "           1       0.26      0.66      0.37       118\n",
      "\n",
      "    accuracy                           0.72       941\n",
      "   macro avg       0.60      0.70      0.60       941\n",
      "weighted avg       0.85      0.72      0.77       941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_2 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_2, y_train)\n",
    "y_lr = lr_model_2.predict(X_test_model_2)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding87 --- Value: -0.9479933063617542\n",
      "220 --- Value: -0.6743455123271457\n",
      "941 --- Value: -0.6310531457644357\n",
      "embedding83 --- Value: -0.6296186073342195\n",
      "536 --- Value: -0.6239875705388835\n",
      "embedding94 --- Value: -0.6225832018200506\n",
      "742 --- Value: -0.6000490991544583\n",
      "embedding0 --- Value: -0.5044714418808884\n",
      "373 --- Value: -0.49319366030039435\n",
      "925 --- Value: -0.4723239460194282\n",
      "embedding61 --- Value: -0.46769548037563663\n",
      "758 --- Value: -0.466905977196651\n",
      "371 --- Value: -0.4640438426685858\n",
      "940 --- Value: -0.4613502722642347\n",
      "embedding9 --- Value: -0.4542733907569744\n",
      "embedding85 --- Value: -0.44837356367803766\n",
      "345 --- Value: -0.44619449054606103\n",
      "embedding88 --- Value: -0.4319095619640387\n",
      "915 --- Value: -0.3936171881940246\n",
      "646 --- Value: -0.3688521695899657\n",
      "823 --- Value: -0.3683499967536823\n",
      "embedding44 --- Value: -0.3662050922540079\n",
      "embedding79 --- Value: -0.3613705539872949\n",
      "embedding90 --- Value: -0.3429500473121967\n",
      "subjectivity --- Value: -0.33959222316461485\n",
      "461 --- Value: -0.3373336916418019\n",
      "embedding93 --- Value: -0.32533818892576283\n",
      "embedding42 --- Value: -0.3227977247415187\n",
      "embedding43 --- Value: -0.3178013972240496\n",
      "187 --- Value: -0.3150687802938355\n",
      "embedding74 --- Value: -0.31432534109063787\n",
      "embedding2 --- Value: -0.3132704548461373\n",
      "693 --- Value: -0.30874620641791933\n",
      "248 --- Value: -0.30481383966685\n",
      "embedding45 --- Value: -0.30093495732941555\n",
      "embedding80 --- Value: -0.29162584054358426\n",
      "950 --- Value: -0.29013147899900216\n",
      "314 --- Value: -0.27978772395198737\n",
      "745 --- Value: -0.27834812797315606\n",
      "176 --- Value: -0.26749492523496027\n",
      "242 --- Value: -0.2657465715800501\n",
      "905 --- Value: -0.26410233746860473\n",
      "537 --- Value: -0.2637350684753706\n",
      "604 --- Value: -0.25904947933634354\n",
      "538 --- Value: -0.2590182000072564\n",
      "653 --- Value: -0.25769115424297717\n",
      "660 --- Value: -0.25637476668752385\n",
      "334 --- Value: -0.25518113715177315\n",
      "810 --- Value: -0.2551177049814566\n",
      "967 --- Value: -0.24629692734447173\n",
      "embedding7 --- Value: -0.24283144662150222\n",
      "173 --- Value: -0.2415849011900351\n",
      "868 --- Value: -0.23926807597094663\n",
      "156 --- Value: -0.23580256180044187\n",
      "894 --- Value: -0.23446754304429815\n",
      "621 --- Value: -0.23159052943605152\n",
      "151 --- Value: -0.22961967008741346\n",
      "466 --- Value: -0.22954939036816646\n",
      "686 --- Value: -0.2290990232572527\n",
      "323 --- Value: -0.22468876982972688\n",
      "embedding95 --- Value: -0.22318300919334516\n",
      "embedding75 --- Value: -0.2214152129445661\n",
      "378 --- Value: -0.22069321155002208\n",
      "embedding98 --- Value: -0.21768451987088527\n",
      "446 --- Value: -0.21545439213751974\n",
      "937 --- Value: -0.21354072410261857\n",
      "288 --- Value: -0.21208439894416237\n",
      "848 --- Value: -0.21068265867840852\n",
      "867 --- Value: -0.20993242760197062\n",
      "457 --- Value: -0.2066195538072886\n",
      "819 --- Value: -0.20519234388716356\n",
      "pronoun --- Value: -0.20416021796987657\n",
      "152 --- Value: -0.2016366911586534\n",
      "474 --- Value: -0.19963979824634698\n",
      "384 --- Value: -0.19782257459892227\n",
      "793 --- Value: -0.19721231481434198\n",
      "767 --- Value: -0.1970557187477267\n",
      "470 --- Value: -0.19664781234171733\n",
      "972 --- Value: -0.19436682601784586\n",
      "916 --- Value: -0.19256134336863043\n",
      "482 --- Value: -0.19013324095026657\n",
      "embedding67 --- Value: -0.18770527945188914\n",
      "423 --- Value: -0.1871895594254559\n",
      "352 --- Value: -0.18690506306944024\n",
      "397 --- Value: -0.18257108172408532\n",
      "125 --- Value: -0.18183977412023647\n",
      "embedding33 --- Value: -0.17871781305835205\n",
      "927 --- Value: -0.17832158484232966\n",
      "608 --- Value: -0.17500270656978684\n",
      "325 --- Value: -0.17483847107076028\n",
      "5 --- Value: -0.17458224651747262\n",
      "486 --- Value: -0.17345649770614976\n",
      "115 --- Value: -0.17274554341886852\n",
      "598 --- Value: -0.1712322613901177\n",
      "embedding81 --- Value: -0.17122814708980227\n",
      "283 --- Value: -0.16920574600750465\n",
      "655 --- Value: -0.1687160096550697\n",
      "714 --- Value: -0.1673477327673282\n",
      "embedding12 --- Value: -0.1654364036106987\n",
      "embedding72 --- Value: -0.16468908833681253\n",
      "988 --- Value: -0.16346096316066833\n",
      "877 --- Value: -0.16256043301077103\n",
      "861 --- Value: -0.15937750951235288\n",
      "153 --- Value: -0.1587411507880716\n",
      "577 --- Value: -0.158536124441631\n",
      "627 --- Value: -0.15836467018544287\n",
      "616 --- Value: -0.15828447486716418\n",
      "751 --- Value: -0.15807046083820125\n",
      "744 --- Value: -0.15665673234489166\n",
      "895 --- Value: -0.15488829317637054\n",
      "271 --- Value: -0.1547337992121851\n",
      "567 --- Value: -0.15309271947234754\n",
      "890 --- Value: -0.15298789612540556\n",
      "500 --- Value: -0.150005707871752\n",
      "embedding77 --- Value: -0.14902049509637447\n",
      "469 --- Value: -0.1489766251856211\n",
      "749 --- Value: -0.14791258544955693\n",
      "747 --- Value: -0.14753041909645556\n",
      "309 --- Value: -0.14690309459219136\n",
      "292 --- Value: -0.14684454109662687\n",
      "124 --- Value: -0.1453575321934273\n",
      "683 --- Value: -0.14467959475931988\n",
      "684 --- Value: -0.14259767460253814\n",
      "362 --- Value: -0.14193715260972684\n",
      "196 --- Value: -0.1418228313197847\n",
      "763 --- Value: -0.14176388405173124\n",
      "929 --- Value: -0.13906619127945893\n",
      "909 --- Value: -0.1387229218541249\n",
      "897 --- Value: -0.1383547890780548\n",
      "873 --- Value: -0.13655034138944747\n",
      "548 --- Value: -0.13540751470671847\n",
      "502 --- Value: -0.13536166456086593\n",
      "217 --- Value: -0.13504456182252053\n",
      "215 --- Value: -0.13455572429697812\n",
      "850 --- Value: -0.13394330710300326\n",
      "756 --- Value: -0.13307878004405366\n",
      "embedding76 --- Value: -0.133068784175498\n",
      "796 --- Value: -0.13304740160887957\n",
      "614 --- Value: -0.13245220069216934\n",
      "860 --- Value: -0.1324234402335781\n",
      "29 --- Value: -0.13242183710696864\n",
      "399 --- Value: -0.13216550901140614\n",
      "573 --- Value: -0.13155332313593024\n",
      "166 --- Value: -0.13053502306718376\n",
      "892 --- Value: -0.12935424660696931\n",
      "484 --- Value: -0.12875401243045287\n",
      "132 --- Value: -0.12861532577647838\n",
      "546 --- Value: -0.12806176809860087\n",
      "embedding52 --- Value: -0.12659018812739756\n",
      "408 --- Value: -0.12649061792036598\n",
      "707 --- Value: -0.12454143569902106\n",
      "478 --- Value: -0.124448089052905\n",
      "728 --- Value: -0.12275833772873294\n",
      "534 --- Value: -0.12227328138762213\n",
      "888 --- Value: -0.12096690119548721\n",
      "639 --- Value: -0.120784868791256\n",
      "41 --- Value: -0.12038810826757519\n",
      "697 --- Value: -0.11978613623712725\n",
      "205 --- Value: -0.11917947321566259\n",
      "978 --- Value: -0.11826319672465035\n",
      "498 --- Value: -0.11802228229337773\n",
      "463 --- Value: -0.11719445232692886\n",
      "633 --- Value: -0.1167473514211376\n",
      "459 --- Value: -0.11643828559021789\n",
      "800 --- Value: -0.11599219744369617\n",
      "embedding55 --- Value: -0.1156593458667153\n",
      "350 --- Value: -0.11549710486801595\n",
      "703 --- Value: -0.11539587698708322\n",
      "489 --- Value: -0.11426785127691891\n",
      "564 --- Value: -0.11397618063583488\n",
      "527 --- Value: -0.11386648665736772\n",
      "117 --- Value: -0.11243583441278487\n",
      "255 --- Value: -0.11243256273316808\n",
      "505 --- Value: -0.11145530873682857\n",
      "233 --- Value: -0.10978492790071781\n",
      "embedding18 --- Value: -0.10954357887107455\n",
      "801 --- Value: -0.10942488813447226\n",
      "805 --- Value: -0.1091908281360615\n",
      "324 --- Value: -0.10913230094921574\n",
      "528 --- Value: -0.10851189511919991\n",
      "415 --- Value: -0.10792510990966735\n",
      "108 --- Value: -0.10770899569336667\n",
      "586 --- Value: -0.1060918160351074\n",
      "261 --- Value: -0.10575919122227184\n",
      "194 --- Value: -0.10557345786282085\n",
      "698 --- Value: -0.10525031322152023\n",
      "embedding51 --- Value: -0.10517117634445158\n",
      "embedding20 --- Value: -0.10481187699836345\n",
      "494 --- Value: -0.10388885984836305\n",
      "161 --- Value: -0.10319115040843023\n",
      "964 --- Value: -0.10170559270939213\n",
      "358 --- Value: -0.101607007102415\n",
      "embedding91 --- Value: -0.10159251282014208\n",
      "67 --- Value: -0.1014349585900478\n",
      "100 --- Value: -0.10089163299122257\n",
      "embedding3 --- Value: -0.1005419427531708\n",
      "336 --- Value: -0.09997329484524567\n",
      "394 --- Value: -0.0998667608793654\n",
      "20 --- Value: -0.09885716277791994\n",
      "65 --- Value: -0.09752403211138617\n",
      "962 --- Value: -0.09694702271575928\n",
      "150 --- Value: -0.09651818429512034\n",
      "954 --- Value: -0.09642653701693255\n",
      "987 --- Value: -0.09577425898388266\n",
      "259 --- Value: -0.09317367061891423\n",
      "42 --- Value: -0.09145308780207856\n",
      "872 --- Value: -0.09127654064354825\n",
      "910 --- Value: -0.08995057824457121\n",
      "234 --- Value: -0.08896944974149412\n",
      "32 --- Value: -0.08882857927903916\n",
      "embedding35 --- Value: -0.08858838417800122\n",
      "269 --- Value: -0.08790067934919697\n",
      "851 --- Value: -0.08699476295349585\n",
      "93 --- Value: -0.08691674264142737\n",
      "734 --- Value: -0.08648492657196542\n",
      "898 --- Value: -0.0861181426473966\n",
      "237 --- Value: -0.0860470453144724\n",
      "208 --- Value: -0.08603087237913237\n",
      "541 --- Value: -0.08530153022832973\n",
      "840 --- Value: -0.08523716000679521\n",
      "30 --- Value: -0.08499409639877929\n",
      "433 --- Value: -0.08471908911779984\n",
      "682 --- Value: -0.08459278628350056\n",
      "296 --- Value: -0.08436602217448164\n",
      "938 --- Value: -0.08424293400528242\n",
      "971 --- Value: -0.08424069915792305\n",
      "591 --- Value: -0.08368487707795254\n",
      "342 --- Value: -0.08359586783989417\n",
      "272 --- Value: -0.08330383354488378\n",
      "501 --- Value: -0.08324858356436145\n",
      "529 --- Value: -0.08301548353571565\n",
      "369 --- Value: -0.08257132175812579\n",
      "775 --- Value: -0.0815174286476201\n",
      "19 --- Value: -0.08131001612710588\n",
      "813 --- Value: -0.08087246617643709\n",
      "675 --- Value: -0.08071647853143803\n",
      "847 --- Value: -0.08055259622815555\n",
      "268 --- Value: -0.08045184947784127\n",
      "581 --- Value: -0.08042102537438345\n",
      "519 --- Value: -0.07983348555553407\n",
      "68 --- Value: -0.07982612975036413\n",
      "355 --- Value: -0.07945915547132075\n",
      "549 --- Value: -0.07918850843406379\n",
      "575 --- Value: -0.07885396190831521\n",
      "930 --- Value: -0.07880625573520601\n",
      "824 --- Value: -0.0787588203358555\n",
      "798 --- Value: -0.07866346191939448\n",
      "412 --- Value: -0.07858433334488292\n",
      "133 --- Value: -0.07855275835098975\n",
      "76 --- Value: -0.07832508735967063\n",
      "382 --- Value: -0.07804186012034438\n",
      "565 --- Value: -0.0775369149678103\n",
      "506 --- Value: -0.07752193311109838\n",
      "280 --- Value: -0.0773062003431326\n",
      "973 --- Value: -0.0771739026973908\n",
      "449 --- Value: -0.07708367477986161\n",
      "439 --- Value: -0.07684296111049142\n",
      "418 --- Value: -0.07669161892223476\n",
      "385 --- Value: -0.07613703097435869\n",
      "725 --- Value: -0.07610397664653976\n",
      "571 --- Value: -0.07585687664681437\n",
      "231 --- Value: -0.07575987631750411\n",
      "213 --- Value: -0.0756351547448534\n",
      "886 --- Value: -0.07486670020459693\n",
      "embedding5 --- Value: -0.07473149870592768\n",
      "771 --- Value: -0.07457449820832544\n",
      "embedding49 --- Value: -0.07437098198709981\n",
      "405 --- Value: -0.07413421105989175\n",
      "embedding50 --- Value: -0.07397267381936039\n",
      "444 --- Value: -0.07383083330386529\n",
      "240 --- Value: -0.07365167520177618\n",
      "254 --- Value: -0.07349889127282815\n",
      "799 --- Value: -0.07297799291068562\n",
      "666 --- Value: -0.07290743026149696\n",
      "421 --- Value: -0.07242274412099621\n",
      "392 --- Value: -0.07212229704376383\n",
      "556 --- Value: -0.07207214978004217\n",
      "107 --- Value: -0.07191066993440047\n",
      "792 --- Value: -0.07172599828152036\n",
      "110 --- Value: -0.07169958901432706\n",
      "553 --- Value: -0.07158131409050696\n",
      "584 --- Value: -0.07158131409050696\n",
      "223 --- Value: -0.0711308676494848\n",
      "250 --- Value: -0.07095541809668948\n",
      "bio_sim_words --- Value: -0.07036458365964701\n",
      "465 --- Value: -0.07031829016753366\n",
      "374 --- Value: -0.07011359193357265\n",
      "128 --- Value: -0.06995957655174614\n",
      "732 --- Value: -0.06966136814247383\n",
      "400 --- Value: -0.06947304485400373\n",
      "882 --- Value: -0.06941696592707569\n",
      "embedding59 --- Value: -0.06939852505165045\n",
      "821 --- Value: -0.06919133188722097\n",
      "762 --- Value: -0.06869755583430075\n",
      "174 --- Value: -0.06855685564439369\n",
      "740 --- Value: -0.0684699243450793\n",
      "977 --- Value: -0.06845626930116004\n",
      "629 --- Value: -0.0683541910425688\n",
      "147 --- Value: -0.0682360359325657\n",
      "62 --- Value: -0.06795059732249074\n",
      "120 --- Value: -0.06791251658770883\n",
      "122 --- Value: -0.06765249981418815\n",
      "440 --- Value: -0.06764765515671688\n",
      "935 --- Value: -0.06715620728965983\n",
      "551 --- Value: -0.0671060221004415\n",
      "721 --- Value: -0.06705140080121658\n",
      "834 --- Value: -0.0667084437432958\n",
      "521 --- Value: -0.06670585814735311\n",
      "635 --- Value: -0.06652366893081046\n",
      "83 --- Value: -0.06619722951591808\n",
      "560 --- Value: -0.06616241933060438\n",
      "830 --- Value: -0.06590138314230379\n",
      "92 --- Value: -0.06550846737769289\n",
      "610 --- Value: -0.0654960168745829\n",
      "856 --- Value: -0.0654960168745829\n",
      "118 --- Value: -0.06531658520365019\n",
      "134 --- Value: -0.06525155646391786\n",
      "222 --- Value: -0.06498786916130543\n",
      "170 --- Value: -0.06493939210985253\n",
      "623 --- Value: -0.06492222103970394\n",
      "49 --- Value: -0.0648739216079608\n",
      "22 --- Value: -0.06480223333494378\n",
      "969 --- Value: -0.06479056373408112\n",
      "923 --- Value: -0.06476349797590027\n",
      "143 --- Value: -0.0645488251178783\n",
      "752 --- Value: -0.06449343600051137\n",
      "241 --- Value: -0.06446172041790102\n",
      "411 --- Value: -0.06422142338354098\n",
      "413 --- Value: -0.06422142338354098\n",
      "496 --- Value: -0.06422142338354098\n",
      "540 --- Value: -0.06422142338354098\n",
      "258 --- Value: -0.06421736488348709\n",
      "47 --- Value: -0.06376602830123969\n",
      "671 --- Value: -0.06376602830123969\n",
      "966 --- Value: -0.06367954722802316\n",
      "906 --- Value: -0.06365257416672884\n",
      "285 --- Value: -0.06353066249400927\n",
      "583 --- Value: -0.06277990833131297\n",
      "788 --- Value: -0.06277990833131297\n",
      "730 --- Value: -0.06272403425719\n",
      "593 --- Value: -0.06269850528565535\n",
      "786 --- Value: -0.06269850528565535\n",
      "398 --- Value: -0.06244819832638006\n",
      "911 --- Value: -0.06234112218667521\n",
      "198 --- Value: -0.06190312288457841\n",
      "136 --- Value: -0.06164085863130032\n",
      "797 --- Value: -0.06155999804173528\n",
      "957 --- Value: -0.06136591798931059\n",
      "429 --- Value: -0.061162541329354086\n",
      "8 --- Value: -0.0608287847114864\n",
      "251 --- Value: -0.06081905692090627\n",
      "332 --- Value: -0.06077599327917997\n",
      "948 --- Value: -0.06040036655300766\n",
      "727 --- Value: -0.060368694849282124\n",
      "669 --- Value: -0.05998694125665216\n",
      "612 --- Value: -0.05985603729905205\n",
      "532 --- Value: -0.059784678648007475\n",
      "embedding32 --- Value: -0.05973987008574412\n",
      "781 --- Value: -0.05972871504964628\n",
      "200 --- Value: -0.05958508588078278\n",
      "555 --- Value: -0.05929589153320467\n",
      "539 --- Value: -0.05903572077568686\n",
      "410 --- Value: -0.05895454620190358\n",
      "372 --- Value: -0.058916611378114825\n",
      "75 --- Value: -0.05870222111726015\n",
      "630 --- Value: -0.05866122007113601\n",
      "178 --- Value: -0.05859722616346443\n",
      "597 --- Value: -0.05859016102139926\n",
      "462 --- Value: -0.058461468804968025\n",
      "513 --- Value: -0.05844914474464931\n",
      "1 --- Value: -0.0582791788071936\n",
      "167 --- Value: -0.05795909126312614\n",
      "918 --- Value: -0.05764858444479023\n",
      "603 --- Value: -0.057531499409782035\n",
      "716 --- Value: -0.0574494892212464\n",
      "842 --- Value: -0.057010982134970904\n",
      "402 --- Value: -0.056855484620993504\n",
      "558 --- Value: -0.0561589686828682\n",
      "45 --- Value: -0.05612679832817793\n",
      "287 --- Value: -0.056085247848406745\n",
      "654 --- Value: -0.0559279918980399\n",
      "986 --- Value: -0.05561843086127688\n",
      "980 --- Value: -0.05553628491515992\n",
      "642 --- Value: -0.05550966512258488\n",
      "587 --- Value: -0.05526580003676796\n",
      "649 --- Value: -0.05521385624368226\n",
      "201 --- Value: -0.05518683890738286\n",
      "691 --- Value: -0.05509650521765155\n",
      "481 --- Value: -0.054843290003554285\n",
      "276 --- Value: -0.05473735987130512\n",
      "619 --- Value: -0.054548987580309787\n",
      "712 --- Value: -0.05443204668425538\n",
      "112 --- Value: -0.05434560886525658\n",
      "475 --- Value: -0.0540337641483538\n",
      "175 --- Value: -0.05397840449739859\n",
      "58 --- Value: -0.05376183502645129\n",
      "599 --- Value: -0.053746271547962125\n",
      "383 --- Value: -0.05348345049468934\n",
      "145 --- Value: -0.05338110725932618\n",
      "221 --- Value: -0.05330318562783918\n",
      "431 --- Value: -0.05330318562783918\n",
      "820 --- Value: -0.05329946757254453\n",
      "89 --- Value: -0.05198438765933512\n",
      "837 --- Value: -0.051505451410256975\n",
      "622 --- Value: -0.05124569067089858\n",
      "55 --- Value: -0.051214147921401956\n",
      "974 --- Value: -0.05110923072460203\n",
      "0 --- Value: -0.050884345614481394\n",
      "73 --- Value: -0.05068003480466375\n",
      "214 --- Value: -0.050547071904365797\n",
      "782 --- Value: -0.05054686142807383\n",
      "195 --- Value: -0.05004282535686751\n",
      "53 --- Value: -0.0493825154481627\n",
      "754 --- Value: -0.049226830468258695\n",
      "471 --- Value: -0.049113735750851366\n",
      "493 --- Value: -0.04902326346346996\n",
      "545 --- Value: -0.048798488473775964\n",
      "447 --- Value: -0.048384393997822406\n",
      "77 --- Value: -0.048190366197682\n",
      "566 --- Value: -0.047812048233932196\n",
      "367 --- Value: -0.04780551849385591\n",
      "95 --- Value: -0.047777365344340836\n",
      "308 --- Value: -0.04773131513879748\n",
      "947 --- Value: -0.04741769608459202\n",
      "401 --- Value: -0.047259073184953974\n",
      "264 --- Value: -0.04723842470807403\n",
      "760 --- Value: -0.047104332987641835\n",
      "472 --- Value: -0.04705909618442412\n",
      "516 --- Value: -0.04705909618442412\n",
      "448 --- Value: -0.04679984743093629\n",
      "316 --- Value: -0.04677201725716937\n",
      "182 --- Value: -0.04647796650641724\n",
      "804 --- Value: -0.04637132896557803\n",
      "203 --- Value: -0.04633473521118252\n",
      "304 --- Value: -0.04626190396535343\n",
      "547 --- Value: -0.04621526608969254\n",
      "913 --- Value: -0.045816904011850165\n",
      "260 --- Value: -0.045639560424918194\n",
      "138 --- Value: -0.04554941402448099\n",
      "159 --- Value: -0.04541188639867856\n",
      "366 --- Value: -0.045290239405366285\n",
      "678 --- Value: -0.04497768388657794\n",
      "329 --- Value: -0.04475982328696786\n",
      "902 --- Value: -0.04474259574790803\n",
      "39 --- Value: -0.04473675031600831\n",
      "33 --- Value: -0.044689664981801325\n",
      "976 --- Value: -0.04453059341553102\n",
      "158 --- Value: -0.04452011954977247\n",
      "990 --- Value: -0.04436181495060209\n",
      "704 --- Value: -0.044313166194587035\n",
      "919 --- Value: -0.04408386201835647\n",
      "46 --- Value: -0.04401545019918508\n",
      "442 --- Value: -0.0439093722814205\n",
      "50 --- Value: -0.043627910646397566\n",
      "975 --- Value: -0.043592938815722335\n",
      "17 --- Value: -0.04349466670921851\n",
      "ner --- Value: -0.0430467949648416\n",
      "284 --- Value: -0.04296084151526202\n",
      "594 --- Value: -0.042942866138039784\n",
      "563 --- Value: -0.04293807856554959\n",
      "114 --- Value: -0.04273674405201041\n",
      "320 --- Value: -0.04266624382215505\n",
      "624 --- Value: -0.0425456612088564\n",
      "91 --- Value: -0.04232069386355903\n",
      "456 --- Value: -0.042218338403345254\n",
      "270 --- Value: -0.04207027959576887\n",
      "866 --- Value: -0.04186808499276245\n",
      "776 --- Value: -0.04177055498500377\n",
      "815 --- Value: -0.041586363438312894\n",
      "676 --- Value: -0.04130198569550935\n",
      "12 --- Value: -0.04119058148060684\n",
      "245 --- Value: -0.04119058148060684\n",
      "27 --- Value: -0.04119058148060684\n",
      "365 --- Value: -0.04119058148060684\n",
      "724 --- Value: -0.04119058148060684\n",
      "657 --- Value: -0.0411836760612069\n",
      "955 --- Value: -0.04114497685183702\n",
      "2 --- Value: -0.04107710704404761\n",
      "13 --- Value: -0.040970954215835374\n",
      "600 --- Value: -0.040682044034161126\n",
      "632 --- Value: -0.040682044034161126\n",
      "embedding31 --- Value: -0.04055293628074451\n",
      "224 --- Value: -0.040510031834067976\n",
      "225 --- Value: -0.040510031834067976\n",
      "464 --- Value: -0.040510031834067976\n",
      "123 --- Value: -0.0403002001378766\n",
      "631 --- Value: -0.04026179153305529\n",
      "414 --- Value: -0.04008016578668495\n",
      "139 --- Value: -0.03989895676922815\n",
      "218 --- Value: -0.0398747800036494\n",
      "795 --- Value: -0.03982191840362512\n",
      "673 --- Value: -0.03929117978437451\n",
      "97 --- Value: -0.039236413445465725\n",
      "230 --- Value: -0.03891210665881547\n",
      "327 --- Value: -0.038773471323635636\n",
      "142 --- Value: -0.03867223711163381\n",
      "490 --- Value: -0.03848545815863165\n",
      "52 --- Value: -0.03814957490978253\n",
      "863 --- Value: -0.03814957490978253\n",
      "347 --- Value: -0.03802161746621761\n",
      "557 --- Value: -0.03768550265098578\n",
      "778 --- Value: -0.03764913267362889\n",
      "279 --- Value: -0.03751842836296977\n",
      "928 --- Value: -0.03742799689453393\n",
      "57 --- Value: -0.037220065830468205\n",
      "893 --- Value: -0.03706607686610374\n",
      "96 --- Value: -0.03703114815479613\n",
      "636 --- Value: -0.036940637794433205\n",
      "326 --- Value: -0.0368910928508573\n",
      "127 --- Value: -0.03688879227442648\n",
      "379 --- Value: -0.03683836446290437\n",
      "111 --- Value: -0.03680514613948647\n",
      "249 --- Value: -0.03680514613948647\n",
      "785 --- Value: -0.03667972625391332\n",
      "422 --- Value: -0.03653032321709125\n",
      "435 --- Value: -0.03652684876544808\n",
      "295 --- Value: -0.03630857169660948\n",
      "180 --- Value: -0.036094634408658864\n",
      "982 --- Value: -0.03597315435332793\n",
      "722 --- Value: -0.035890050925925975\n",
      "881 --- Value: -0.03583745652744603\n",
      "281 --- Value: -0.03547094968503556\n",
      "497 --- Value: -0.03547094968503556\n",
      "965 --- Value: -0.035426867362902556\n",
      "667 --- Value: -0.035409096829226536\n",
      "700 --- Value: -0.035344222130108106\n",
      "104 --- Value: -0.03533914850174969\n",
      "179 --- Value: -0.03533914850174969\n",
      "737 --- Value: -0.03526700959100776\n",
      "357 --- Value: -0.03508990685060754\n",
      "eng_sim_words --- Value: -0.03502104834794646\n",
      "tech_sim_words --- Value: -0.03502104834794646\n",
      "953 --- Value: -0.03495598494929639\n",
      "441 --- Value: -0.034862751624994186\n",
      "72 --- Value: -0.03486228498314199\n",
      "663 --- Value: -0.03483012885264736\n",
      "736 --- Value: -0.03455815495133606\n",
      "750 --- Value: -0.03426709123406222\n",
      "605 --- Value: -0.03400365498335041\n",
      "403 --- Value: -0.03389480652457407\n",
      "351 --- Value: -0.03388324022715374\n",
      "609 --- Value: -0.033775342679520336\n",
      "172 --- Value: -0.03371119580540501\n",
      "715 --- Value: -0.03371119580540501\n",
      "243 --- Value: -0.03360813551092007\n",
      "69 --- Value: -0.03347561765222121\n",
      "391 --- Value: -0.03313295637130293\n",
      "809 --- Value: -0.03309860586213433\n",
      "256 --- Value: -0.03304395060941397\n",
      "768 --- Value: -0.032906093721006606\n",
      "705 --- Value: -0.03263312442273105\n",
      "512 --- Value: -0.032585181430863634\n",
      "719 --- Value: -0.03256349108611058\n",
      "945 --- Value: -0.032493676720617745\n",
      "244 --- Value: -0.032459260836053126\n",
      "60 --- Value: -0.032459260836053126\n",
      "846 --- Value: -0.032459260836053126\n",
      "388 --- Value: -0.03238255274546455\n",
      "769 --- Value: -0.03234668636016858\n",
      "626 --- Value: -0.03173934158732152\n",
      "780 --- Value: -0.03153975064605922\n",
      "427 --- Value: -0.03140340972111369\n",
      "825 --- Value: -0.031247847959312677\n",
      "7 --- Value: -0.03099605502581989\n",
      "453 --- Value: -0.030875048802492536\n",
      "38 --- Value: -0.030848556243737762\n",
      "843 --- Value: -0.030815252633833664\n",
      "265 --- Value: -0.030567815256287906\n",
      "148 --- Value: -0.03036509427117431\n",
      "419 --- Value: -0.030320303212338657\n",
      "933 --- Value: -0.030280558129741758\n",
      "216 --- Value: -0.030138021099363074\n",
      "912 --- Value: -0.029984731262535664\n",
      "637 --- Value: -0.029661582064229402\n",
      "585 --- Value: -0.02964364307173759\n",
      "625 --- Value: -0.02959810889327233\n",
      "51 --- Value: -0.02958646622350345\n",
      "753 --- Value: -0.029499525455565674\n",
      "381 --- Value: -0.029352752721991414\n",
      "451 --- Value: -0.029270241985762085\n",
      "879 --- Value: -0.029119058437819798\n",
      "611 --- Value: -0.029098641805649587\n",
      "290 --- Value: -0.02906785411033724\n",
      "186 --- Value: -0.028981098087294942\n",
      "84 --- Value: -0.02895389013460872\n",
      "425 --- Value: -0.02872303189745545\n",
      "101 --- Value: -0.028688186852188123\n",
      "658 --- Value: -0.02842408154656081\n",
      "491 --- Value: -0.028302614062138444\n",
      "430 --- Value: -0.028258349502303513\n",
      "885 --- Value: -0.027932675092074484\n",
      "15 --- Value: -0.027909203910149907\n",
      "24 --- Value: -0.027909203910149907\n",
      "197 --- Value: -0.027803131343877883\n",
      "adverb --- Value: -0.027791763520916803\n",
      "16 --- Value: -0.027765310185446507\n",
      "164 --- Value: -0.027729009352296125\n",
      "389 --- Value: -0.02770907195349053\n",
      "380 --- Value: -0.02769235812743122\n",
      "26 --- Value: -0.02741433751818078\n",
      "144 --- Value: -0.027360469469494088\n",
      "184 --- Value: -0.027317833164442536\n",
      "embedding60 --- Value: -0.027171132147567466\n",
      "246 --- Value: -0.027063255550131646\n",
      "199 --- Value: -0.027054536505660323\n",
      "14 --- Value: -0.026654282620654803\n",
      "86 --- Value: -0.026654282620654803\n",
      "386 --- Value: -0.026601792615142332\n",
      "436 --- Value: -0.02654296317770115\n",
      "944 --- Value: -0.026323238246718707\n",
      "247 --- Value: -0.02624510626713001\n",
      "458 --- Value: -0.02615068483922824\n",
      "640 --- Value: -0.02606182454860496\n",
      "592 --- Value: -0.026041186584905805\n",
      "426 --- Value: -0.025933381019801312\n",
      "40 --- Value: -0.02557277475112166\n",
      "188 --- Value: -0.025461151764481722\n",
      "98 --- Value: -0.02503661508776042\n",
      "634 --- Value: -0.02489031841778075\n",
      "907 --- Value: -0.024879094292697725\n",
      "681 --- Value: -0.024779634488290004\n",
      "31 --- Value: -0.024774624671662134\n",
      "154 --- Value: -0.02477385511411537\n",
      "294 --- Value: -0.024660771418758297\n",
      "508 --- Value: -0.02458542232272299\n",
      "360 --- Value: -0.024574172271824023\n",
      "985 --- Value: -0.02450355861983573\n",
      "169 --- Value: -0.024214285674399375\n",
      "488 --- Value: -0.024177237433829026\n",
      "embedding19 --- Value: -0.024040234094415247\n",
      "70 --- Value: -0.024016968302811196\n",
      "239 --- Value: -0.02400530113287956\n",
      "346 --- Value: -0.02393831921248291\n",
      "662 --- Value: -0.023877645193038883\n",
      "165 --- Value: -0.023860267910053416\n",
      "901 --- Value: -0.023860267910053416\n",
      "37 --- Value: -0.023819000627737723\n",
      "787 --- Value: -0.023819000627737723\n",
      "853 --- Value: -0.023546220876249566\n",
      "717 --- Value: -0.02340160865695028\n",
      "561 --- Value: -0.0231454588499249\n",
      "368 --- Value: -0.02290008488836963\n",
      "452 --- Value: -0.022711437429719143\n",
      "904 --- Value: -0.0226881232577943\n",
      "606 --- Value: -0.022682731298904236\n",
      "582 --- Value: -0.02248825657406549\n",
      "659 --- Value: -0.022352193742649216\n",
      "908 --- Value: -0.022317272356587984\n",
      "328 --- Value: -0.02199799526922431\n",
      "849 --- Value: -0.021981740841939566\n",
      "900 --- Value: -0.02177444240856516\n",
      "852 --- Value: -0.02142082149638663\n",
      "434 --- Value: -0.021392045730037657\n",
      "696 --- Value: -0.021392045730037657\n",
      "387 --- Value: -0.021281284741276066\n",
      "443 --- Value: -0.02112974382963374\n",
      "522 --- Value: -0.021050256920229254\n",
      "377 --- Value: -0.021027283949756306\n",
      "554 --- Value: -0.021011655425394786\n",
      "694 --- Value: -0.021011655425394786\n",
      "343 --- Value: -0.02091258472225098\n",
      "487 --- Value: -0.020909281999478956\n",
      "511 --- Value: -0.02086972996683121\n",
      "450 --- Value: -0.020830897634527168\n",
      "476 --- Value: -0.02064851941248763\n",
      "789 --- Value: -0.02061691349876168\n",
      "282 --- Value: -0.020552898652702156\n",
      "438 --- Value: -0.020444039693971034\n",
      "428 --- Value: -0.02037443519926872\n",
      "263 --- Value: -0.020141649554425132\n",
      "543 --- Value: -0.020136078134422614\n",
      "876 --- Value: -0.020103162387652217\n",
      "interjections --- Value: -0.019910704427549497\n",
      "903 --- Value: -0.01989627150490076\n",
      "257 --- Value: -0.019791630331169253\n",
      "702 --- Value: -0.019791630331169253\n",
      "891 --- Value: -0.019791630331169253\n",
      "262 --- Value: -0.019574112957457042\n",
      "432 --- Value: -0.01954266435378203\n",
      "943 --- Value: -0.019344173981136376\n",
      "530 --- Value: -0.019254386024667978\n",
      "455 --- Value: -0.01912170260496647\n",
      "802 --- Value: -0.019119153962324702\n",
      "424 --- Value: -0.019041100648249168\n",
      "370 --- Value: -0.018962258383342943\n",
      "970 --- Value: -0.018920513828050683\n",
      "25 --- Value: -0.01884264576014213\n",
      "835 --- Value: -0.018785671401940013\n",
      "103 --- Value: -0.01854340227497353\n",
      "601 --- Value: -0.01849368452363734\n",
      "958 --- Value: -0.018419859855289596\n",
      "816 --- Value: -0.018219700277537717\n",
      "579 --- Value: -0.018138549322632185\n",
      "770 --- Value: -0.01797477371429382\n",
      "361 --- Value: -0.017907555602601308\n",
      "78 --- Value: -0.017840349501114666\n",
      "931 --- Value: -0.017382891748455573\n",
      "9 --- Value: -0.01642319950042072\n",
      "651 --- Value: -0.016417952910224357\n",
      "74 --- Value: -0.016338002594634585\n",
      "99 --- Value: -0.015835005396112455\n",
      "739 --- Value: -0.01580541488802093\n",
      "determiner --- Value: -0.015710679227328873\n",
      "495 --- Value: -0.015558663298508212\n",
      "981 --- Value: -0.014987060669089206\n",
      "phy_sim_words --- Value: -0.014765093763146471\n",
      "477 --- Value: -0.014704552104296647\n",
      "562 --- Value: -0.014645683403893796\n",
      "670 --- Value: -0.014553131378310422\n",
      "340 --- Value: -0.013684350345832255\n",
      "191 --- Value: -0.013351720114192677\n",
      "305 --- Value: -0.013065032198794162\n",
      "212 --- Value: -0.01268442206510191\n",
      "315 --- Value: -0.01268442206510191\n",
      "822 --- Value: -0.012535702986494809\n",
      "473 --- Value: -0.012457667015432468\n",
      "595 --- Value: -0.012457667015432468\n",
      "699 --- Value: -0.012457667015432468\n",
      "514 --- Value: -0.012380519481860561\n",
      "922 --- Value: -0.012370130505747898\n",
      "618 --- Value: -0.01227587318964718\n",
      "348 --- Value: -0.012088617068941706\n",
      "841 --- Value: -0.011824574275168382\n",
      "363 --- Value: -0.011489025683694\n",
      "826 --- Value: -0.011402333021487495\n",
      "105 --- Value: -0.011356648637065574\n",
      "926 --- Value: -0.010910895134331343\n",
      "59 --- Value: -0.010387104780211626\n",
      "772 --- Value: -0.007911388768860697\n",
      "790 --- Value: -0.007846430970310537\n",
      "952 --- Value: -0.00781135317991124\n",
      "189 --- Value: -0.007753212161215171\n",
      "920 --- Value: -0.007752843670756835\n",
      "274 --- Value: -0.007612623138692044\n",
      "468 --- Value: -0.005466129130035764\n",
      "310 --- Value: -0.004374509851528459\n",
      "644 --- Value: -0.0030912278687279266\n",
      "664 --- Value: -0.0029364927647616886\n",
      "956 --- Value: -0.0020086569694533394\n",
      "291 --- Value: -0.0019557254583898133\n",
      "485 --- Value: 0.0007122972310974338\n",
      "968 --- Value: 0.0018685245458172415\n",
      "746 --- Value: 0.0032710962324202803\n",
      "90 --- Value: 0.004291858551520215\n",
      "141 --- Value: 0.0053930036753381295\n",
      "204 --- Value: 0.006002661823318419\n",
      "337 --- Value: 0.006128556700853957\n",
      "779 --- Value: 0.006292223047856341\n",
      "726 --- Value: 0.006367312438278156\n",
      "674 --- Value: 0.010641416945966365\n",
      "56 --- Value: 0.011057196327997334\n",
      "934 --- Value: 0.011209233424865624\n",
      "311 --- Value: 0.011319915036350835\n",
      "552 --- Value: 0.012810938817753116\n",
      "757 --- Value: 0.012994130457629404\n",
      "253 --- Value: 0.013290767029713447\n",
      "731 --- Value: 0.013700216346823273\n",
      "665 --- Value: 0.014181563989865779\n",
      "130 --- Value: 0.014721271095278127\n",
      "829 --- Value: 0.015489075179275514\n",
      "331 --- Value: 0.015593938135565127\n",
      "656 --- Value: 0.015693439560287077\n",
      "23 --- Value: 0.01648690681778301\n",
      "924 --- Value: 0.01736708514558732\n",
      "116 --- Value: 0.017435798956321146\n",
      "146 --- Value: 0.017792045825820894\n",
      "870 --- Value: 0.020971132167673913\n",
      "303 --- Value: 0.02139671337332928\n",
      "814 --- Value: 0.021459481885596127\n",
      "764 --- Value: 0.024371006488515756\n",
      "18 --- Value: 0.024948500593884496\n",
      "162 --- Value: 0.02516699749247779\n",
      "85 --- Value: 0.025476059615549256\n",
      "376 --- Value: 0.025676884252711543\n",
      "559 --- Value: 0.02587370172013536\n",
      "embedding89 --- Value: 0.026251919581898906\n",
      "335 --- Value: 0.029208899409521063\n",
      "359 --- Value: 0.029450047118113304\n",
      "874 --- Value: 0.030336382106052634\n",
      "580 --- Value: 0.03064308993525258\n",
      "embedding71 --- Value: 0.03076454707849123\n",
      "140 --- Value: 0.031212435869453405\n",
      "319 --- Value: 0.03129022595538587\n",
      "344 --- Value: 0.03131552710293717\n",
      "613 --- Value: 0.031425205656741326\n",
      "759 --- Value: 0.031588131828193505\n",
      "395 --- Value: 0.031828882248999525\n",
      "483 --- Value: 0.03214889799363484\n",
      "318 --- Value: 0.032570717799185835\n",
      "889 --- Value: 0.0330751735551981\n",
      "706 --- Value: 0.03444886338817364\n",
      "420 --- Value: 0.03521254134275247\n",
      "61 --- Value: 0.03590253321144223\n",
      "211 --- Value: 0.03684710111386743\n",
      "embedding58 --- Value: 0.03744978794657483\n",
      "859 --- Value: 0.03794181436949293\n",
      "811 --- Value: 0.03825835088401976\n",
      "82 --- Value: 0.039156125992365926\n",
      "333 --- Value: 0.03972211545020569\n",
      "510 --- Value: 0.03972211545020569\n",
      "524 --- Value: 0.03972211545020569\n",
      "228 --- Value: 0.03992158873267112\n",
      "28 --- Value: 0.03994179071958184\n",
      "921 --- Value: 0.04075835441099142\n",
      "preposition --- Value: 0.041553598878902843\n",
      "914 --- Value: 0.04207754889766143\n",
      "883 --- Value: 0.04210189636637245\n",
      "341 --- Value: 0.04245378923050586\n",
      "917 --- Value: 0.04338619694684903\n",
      "984 --- Value: 0.04380406880123833\n",
      "525 --- Value: 0.045110584894921785\n",
      "adjetive --- Value: 0.046049230496926635\n",
      "339 --- Value: 0.0460536423907406\n",
      "572 --- Value: 0.0462395680392125\n",
      "11 --- Value: 0.046266690317292344\n",
      "492 --- Value: 0.048160419851949646\n",
      "nouns --- Value: 0.0482529983287762\n",
      "738 --- Value: 0.04830515250807934\n",
      "embedding96 --- Value: 0.04864100530352074\n",
      "949 --- Value: 0.049517988638637926\n",
      "207 --- Value: 0.05087929887233966\n",
      "718 --- Value: 0.0514829293866731\n",
      "embedding97 --- Value: 0.05177258452611226\n",
      "887 --- Value: 0.05201434375836036\n",
      "685 --- Value: 0.05419297736679892\n",
      "720 --- Value: 0.05456073156189633\n",
      "131 --- Value: 0.05462600124035316\n",
      "871 --- Value: 0.054939917824514885\n",
      "437 --- Value: 0.05603234378060243\n",
      "embedding65 --- Value: 0.05640908198192016\n",
      "445 --- Value: 0.05676319656812875\n",
      "688 --- Value: 0.05688465151930558\n",
      "129 --- Value: 0.05809899748719249\n",
      "34 --- Value: 0.05887732556185256\n",
      "embedding26 --- Value: 0.05902707165029196\n",
      "300 --- Value: 0.05944871134038431\n",
      "808 --- Value: 0.059841022392069065\n",
      "896 --- Value: 0.060320005700946346\n",
      "embedding14 --- Value: 0.062394098485232447\n",
      "202 --- Value: 0.06261333800339376\n",
      "409 --- Value: 0.06358766343292112\n",
      "4 --- Value: 0.06509921419926291\n",
      "454 --- Value: 0.06595099412023862\n",
      "354 --- Value: 0.06611855346515587\n",
      "356 --- Value: 0.06646703795535061\n",
      "743 --- Value: 0.06646703795535061\n",
      "298 --- Value: 0.06792009591741224\n",
      "979 --- Value: 0.06812370082391403\n",
      "math_sim_words --- Value: 0.06891906491678369\n",
      "679 --- Value: 0.06894699874609861\n",
      "869 --- Value: 0.06922632861942773\n",
      "523 --- Value: 0.06963747352461162\n",
      "939 --- Value: 0.06969894533509884\n",
      "794 --- Value: 0.07023762566604053\n",
      "406 --- Value: 0.07047801456071942\n",
      "35 --- Value: 0.07058551293552483\n",
      "286 --- Value: 0.07198138625276668\n",
      "711 --- Value: 0.0735861464105021\n",
      "embedding40 --- Value: 0.07376070556591255\n",
      "845 --- Value: 0.07398728459486006\n",
      "177 --- Value: 0.07588600522186331\n",
      "171 --- Value: 0.07595781323268086\n",
      "embedding29 --- Value: 0.07640672838699149\n",
      "106 --- Value: 0.0765087344305902\n",
      "embedding57 --- Value: 0.07694922287763017\n",
      "293 --- Value: 0.0769793505056548\n",
      "596 --- Value: 0.07746749396409332\n",
      "766 --- Value: 0.07771425995752033\n",
      "878 --- Value: 0.0782722884325822\n",
      "672 --- Value: 0.08009800661098988\n",
      "393 --- Value: 0.08123968581018838\n",
      "535 --- Value: 0.08244573758461463\n",
      "652 --- Value: 0.08310130489208557\n",
      "570 --- Value: 0.08490757846101173\n",
      "181 --- Value: 0.08575038235053499\n",
      "266 --- Value: 0.0870676008203012\n",
      "526 --- Value: 0.0871364758566336\n",
      "833 --- Value: 0.08747362161118205\n",
      "232 --- Value: 0.08792889755242837\n",
      "embedding4 --- Value: 0.08798140746645527\n",
      "407 --- Value: 0.08878264400581454\n",
      "617 --- Value: 0.08883027483013993\n",
      "857 --- Value: 0.08972451435409497\n",
      "518 --- Value: 0.09051936162749769\n",
      "48 --- Value: 0.09099595514265792\n",
      "839 --- Value: 0.0917765358373287\n",
      "210 --- Value: 0.09178915445039437\n",
      "983 --- Value: 0.09245305794208654\n",
      "677 --- Value: 0.09283831793420406\n",
      "836 --- Value: 0.09283831793420406\n",
      "641 --- Value: 0.09323946794131521\n",
      "278 --- Value: 0.09340981631043392\n",
      "531 --- Value: 0.0945103419453999\n",
      "embedding16 --- Value: 0.09530833017586547\n",
      "480 --- Value: 0.09639163694614684\n",
      "312 --- Value: 0.09687938191969825\n",
      "774 --- Value: 0.09768085659771115\n",
      "542 --- Value: 0.09795823821430447\n",
      "647 --- Value: 0.09804646228981716\n",
      "574 --- Value: 0.09847425096891481\n",
      "504 --- Value: 0.09906542986140758\n",
      "183 --- Value: 0.09997141644469719\n",
      "21 --- Value: 0.10080163552671258\n",
      "119 --- Value: 0.10101201234784923\n",
      "embedding56 --- Value: 0.10168509241691877\n",
      "embedding69 --- Value: 0.10194792762871345\n",
      "236 --- Value: 0.10198695548676381\n",
      "121 --- Value: 0.1021465267482795\n",
      "embedding70 --- Value: 0.10227946874685295\n",
      "687 --- Value: 0.10238640816481258\n",
      "503 --- Value: 0.10239911652283536\n",
      "390 --- Value: 0.10257016036855834\n",
      "396 --- Value: 0.1028701852490508\n",
      "960 --- Value: 0.10311395395882962\n",
      "708 --- Value: 0.10381678199776417\n",
      "650 --- Value: 0.10384942629795542\n",
      "755 --- Value: 0.10384942629795542\n",
      "690 --- Value: 0.10386440875582623\n",
      "638 --- Value: 0.10467949898037897\n",
      "989 --- Value: 0.10469531431946005\n",
      "589 --- Value: 0.10496983168949654\n",
      "267 --- Value: 0.10530750053267948\n",
      "807 --- Value: 0.10564199661166539\n",
      "713 --- Value: 0.10589467434658152\n",
      "695 --- Value: 0.10622492538089469\n",
      "193 --- Value: 0.10642853015444188\n",
      "884 --- Value: 0.10675012115427777\n",
      "942 --- Value: 0.10717180339982954\n",
      "87 --- Value: 0.10845658055842936\n",
      "289 --- Value: 0.11143572554815075\n",
      "302 --- Value: 0.11335758895071729\n",
      "embedding36 --- Value: 0.11344533029004282\n",
      "838 --- Value: 0.11347212030509575\n",
      "862 --- Value: 0.11439374983375676\n",
      "54 --- Value: 0.11446269457267529\n",
      "322 --- Value: 0.11611271352936064\n",
      "576 --- Value: 0.11618827415740512\n",
      "94 --- Value: 0.1167722936253115\n",
      "192 --- Value: 0.1167814350751567\n",
      "168 --- Value: 0.1183788019743974\n",
      "936 --- Value: 0.11951385627534138\n",
      "817 --- Value: 0.12268034208805198\n",
      "951 --- Value: 0.12333066877782843\n",
      "185 --- Value: 0.12362154613721443\n",
      "313 --- Value: 0.12373336454407255\n",
      "590 --- Value: 0.12373336454407255\n",
      "828 --- Value: 0.12393065375168591\n",
      "36 --- Value: 0.12455518169775612\n",
      "113 --- Value: 0.1249770796121538\n",
      "602 --- Value: 0.1249770796121538\n",
      "44 --- Value: 0.12599679201841585\n",
      "550 --- Value: 0.12705178398418368\n",
      "81 --- Value: 0.1279184651190423\n",
      "307 --- Value: 0.12796901955865309\n",
      "43 --- Value: 0.12878484926976003\n",
      "6 --- Value: 0.12924112401710475\n",
      "160 --- Value: 0.1295274357085999\n",
      "330 --- Value: 0.12963895812656803\n",
      "991 --- Value: 0.1298140494492041\n",
      "206 --- Value: 0.1299659211536\n",
      "88 --- Value: 0.13005471310954905\n",
      "404 --- Value: 0.13054159535835316\n",
      "648 --- Value: 0.13098806168984378\n",
      "568 --- Value: 0.13413761279398195\n",
      "784 --- Value: 0.13452011024805013\n",
      "607 --- Value: 0.13538082370223026\n",
      "79 --- Value: 0.13619712218869792\n",
      "163 --- Value: 0.1370743163151113\n",
      "verb --- Value: 0.137828063885992\n",
      "692 --- Value: 0.13787861184044214\n",
      "946 --- Value: 0.13794973041012384\n",
      "embedding8 --- Value: 0.14028575936587104\n",
      "252 --- Value: 0.1406300784694977\n",
      "embedding28 --- Value: 0.14369713737484813\n",
      "137 --- Value: 0.14422375130742185\n",
      "embedding1 --- Value: 0.14580045485737447\n",
      "479 --- Value: 0.14614487794866626\n",
      "517 --- Value: 0.14670109334826623\n",
      "375 --- Value: 0.14832491473998494\n",
      "219 --- Value: 0.14990102666348\n",
      "416 --- Value: 0.1505387694352713\n",
      "364 --- Value: 0.15072458035105193\n",
      "126 --- Value: 0.15133521184519733\n",
      "832 --- Value: 0.15487195559505457\n",
      "71 --- Value: 0.15489768344719163\n",
      "155 --- Value: 0.1551788983144977\n",
      "embedding73 --- Value: 0.15611574895976757\n",
      "317 --- Value: 0.15646971693014944\n",
      "818 --- Value: 0.157224235420088\n",
      "306 --- Value: 0.15771876542721464\n",
      "899 --- Value: 0.1599781673849259\n",
      "854 --- Value: 0.1622206503523856\n",
      "520 --- Value: 0.16344133661899804\n",
      "709 --- Value: 0.16406099239554167\n",
      "844 --- Value: 0.1644998823411039\n",
      "149 --- Value: 0.16477997621603171\n",
      "932 --- Value: 0.16535297502199475\n",
      "735 --- Value: 0.16603762194143384\n",
      "507 --- Value: 0.1670605901964043\n",
      "578 --- Value: 0.1672433032189565\n",
      "102 --- Value: 0.16753823012833502\n",
      "157 --- Value: 0.16879522664572846\n",
      "80 --- Value: 0.17097584974991872\n",
      "235 --- Value: 0.17144941755772236\n",
      "773 --- Value: 0.17406103457754626\n",
      "66 --- Value: 0.17458746629092708\n",
      "689 --- Value: 0.17508620995116145\n",
      "embedding17 --- Value: 0.1751115427550994\n",
      "227 --- Value: 0.17612270800786992\n",
      "338 --- Value: 0.17667744469515606\n",
      "embedding21 --- Value: 0.17800989512669008\n",
      "238 --- Value: 0.17932030435372592\n",
      "embedding53 --- Value: 0.18155718838484514\n",
      "embedding23 --- Value: 0.1816224693062329\n",
      "277 --- Value: 0.18210594707346278\n",
      "embedding64 --- Value: 0.1856887530830857\n",
      "806 --- Value: 0.18628868825292852\n",
      "723 --- Value: 0.18794734235115493\n",
      "668 --- Value: 0.1914548714376839\n",
      "embedding84 --- Value: 0.19265125807216116\n",
      "791 --- Value: 0.19837316163710567\n",
      "509 --- Value: 0.20080328623170002\n",
      "710 --- Value: 0.20083167761468287\n",
      "875 --- Value: 0.20100807109768845\n",
      "620 --- Value: 0.20145005445085573\n",
      "353 --- Value: 0.20676974001722412\n",
      "777 --- Value: 0.209930267736576\n",
      "460 --- Value: 0.2099682347777179\n",
      "748 --- Value: 0.21385921262268137\n",
      "embedding13 --- Value: 0.2138918969573816\n",
      "349 --- Value: 0.21389927153251231\n",
      "827 --- Value: 0.21420091839321811\n",
      "858 --- Value: 0.21466627434610122\n",
      "3 --- Value: 0.2156749601110859\n",
      "209 --- Value: 0.21624598163511843\n",
      "533 --- Value: 0.21900446890550412\n",
      "embedding34 --- Value: 0.21911771802989044\n",
      "321 --- Value: 0.22059047840776086\n",
      "embedding63 --- Value: 0.22233459851852302\n",
      "63 --- Value: 0.2224070174275936\n",
      "783 --- Value: 0.22343093125069877\n",
      "831 --- Value: 0.22348968435139263\n",
      "embedding6 --- Value: 0.22527063534787176\n",
      "661 --- Value: 0.225813871908536\n",
      "190 --- Value: 0.2302255852022165\n",
      "135 --- Value: 0.2409786942018202\n",
      "chem_sim_words --- Value: 0.24516551667604683\n",
      "109 --- Value: 0.245408945480947\n",
      "embedding68 --- Value: 0.25556501715493235\n",
      "544 --- Value: 0.26224797884364615\n",
      "761 --- Value: 0.26229399729484\n",
      "273 --- Value: 0.26859824873296784\n",
      "embedding47 --- Value: 0.26934439321639536\n",
      "embedding38 --- Value: 0.2787516765866163\n",
      "765 --- Value: 0.2804216964565736\n",
      "embedding41 --- Value: 0.2814951547443037\n",
      "10 --- Value: 0.28890601515434755\n",
      "226 --- Value: 0.29106584410212105\n",
      "729 --- Value: 0.2998016067908401\n",
      "645 --- Value: 0.30233160458855524\n",
      "64 --- Value: 0.3058546200845615\n",
      "embedding54 --- Value: 0.3097538237452755\n",
      "499 --- Value: 0.31036087447800237\n",
      "embedding46 --- Value: 0.31319226739448797\n",
      "299 --- Value: 0.31341025570533376\n",
      "628 --- Value: 0.31826402808207394\n",
      "297 --- Value: 0.31872152572805773\n",
      "417 --- Value: 0.328345850868375\n",
      "803 --- Value: 0.33032974669255266\n",
      "embedding10 --- Value: 0.3310609584275581\n",
      "963 --- Value: 0.3336848577301099\n",
      "588 --- Value: 0.33816388519483276\n",
      "741 --- Value: 0.33895645680368486\n",
      "855 --- Value: 0.34091484251088183\n",
      "polarity --- Value: 0.3476363283831213\n",
      "275 --- Value: 0.3482498981544161\n",
      "embedding27 --- Value: 0.363763237662095\n",
      "embedding11 --- Value: 0.365827198732942\n",
      "embedding82 --- Value: 0.37790813212257174\n",
      "812 --- Value: 0.3779638367918022\n",
      "embedding15 --- Value: 0.3786814613612078\n",
      "embedding24 --- Value: 0.3848728740711037\n",
      "680 --- Value: 0.39354486116056075\n",
      "embedding25 --- Value: 0.39367460538585747\n",
      "embedding92 --- Value: 0.40216127410610253\n",
      "467 --- Value: 0.4123289282683277\n",
      "embedding86 --- Value: 0.42404003122548634\n",
      "615 --- Value: 0.42828716702209524\n",
      "embedding30 --- Value: 0.43715316956254485\n",
      "embedding78 --- Value: 0.45098488388186403\n",
      "643 --- Value: 0.483965544709331\n",
      "embedding37 --- Value: 0.48920388293050204\n",
      "733 --- Value: 0.5000533057897407\n",
      "embedding48 --- Value: 0.5107566169145149\n",
      "medical_terms --- Value: 0.537756391506753\n",
      "embedding99 --- Value: 0.5533915309892484\n",
      "229 --- Value: 0.5759194558300262\n",
      "701 --- Value: 0.6217009968417812\n",
      "embedding39 --- Value: 0.6252547648512049\n",
      "embedding62 --- Value: 0.6400557385844828\n",
      "embedding22 --- Value: 0.651876608119858\n",
      "880 --- Value: 0.6708599188076411\n",
      "865 --- Value: 0.6745096818010882\n",
      "embedding66 --- Value: 0.6808641467926696\n",
      "569 --- Value: 0.6836849713267162\n",
      "961 --- Value: 0.7544020579348794\n",
      "301 --- Value: 0.770488809091563\n",
      "864 --- Value: 0.8215329119271296\n",
      "515 --- Value: 0.8426379498836322\n",
      "959 --- Value: 1.0806560257499103\n"
     ]
    }
   ],
   "source": [
    "NameOfVariables = X_train_model_2.columns.values\n",
    "Var_coef = zip(lr_model_2.coef_[0,:],NameOfVariables)\n",
    "sorted_coef_val = sorted(Var_coef)\n",
    "for item in sorted_coef_val:\n",
    "    print(item[1],'--- Value:',item[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | Precision | Recall | F1 | Data Points |\n",
    "| --- | --- |--- | --- | --- |\n",
    "| Class 0 | 0.97 | 0.88 | 0.92 | 117 |\n",
    "| Class 1 |0.61 | 0.88 | 0.72 | 25 |\n",
    "| Macro Average Score | 0.79 | 0.88 | 0.82 | 142|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Without Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_3 = X_train_final.iloc[:,np.r_[3:121]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3763, 118)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_3 = X_test_final.iloc[:,np.r_[3:121]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(941, 118)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.388\n",
      "Best parameters set:\n",
      "\tclf__C: 25\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9521    0.6768    0.7912       823\n",
      "           1     0.2528    0.7627    0.3797       118\n",
      "\n",
      "    accuracy                         0.6876       941\n",
      "   macro avg     0.6025    0.7198    0.5855       941\n",
      "weighted avg     0.8644    0.6876    0.7396       941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_3_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_3_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_3,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_3), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 557, False Positive: 266, False Negative: 28, True Positive: 90\n",
      "--------------------------------------------------------------------------------\n",
      "[[557 266]\n",
      " [ 28  90]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.68      0.79       823\n",
      "           1       0.25      0.76      0.38       118\n",
      "\n",
      "    accuracy                           0.69       941\n",
      "   macro avg       0.60      0.72      0.59       941\n",
      "weighted avg       0.86      0.69      0.74       941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_3 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_3, y_train)\n",
    "y_lr = lr_model_3.predict(X_test_model_3)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | Precision | Recall | F1 | Data Points |\n",
    "| --- | --- |--- | --- | --- |\n",
    "| Class 0 | 0.99 | 0.82 | 0.90 | 117 |\n",
    "| Class 1 |0.53 | 0.96 | 0.69 | 25 |\n",
    "| Macro Average Score | 0.76 | 0.89 | 0.79 | 142|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Without Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_4 = X_train_final.iloc[:,np.r_[3:21,121:1113]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3763, 1010)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_4 = X_test_final.iloc[:,np.r_[3:21,121:1113]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(941, 1010)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.390\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9270    0.7558    0.8327       823\n",
      "           1     0.2556    0.5847    0.3557       118\n",
      "\n",
      "    accuracy                         0.7343       941\n",
      "   macro avg     0.5913    0.6703    0.5942       941\n",
      "weighted avg     0.8428    0.7343    0.7728       941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_4_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_4_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_4,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_4), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 622, False Positive: 201, False Negative: 49, True Positive: 69\n",
      "--------------------------------------------------------------------------------\n",
      "[[622 201]\n",
      " [ 49  69]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.76      0.83       823\n",
      "           1       0.26      0.58      0.36       118\n",
      "\n",
      "    accuracy                           0.73       941\n",
      "   macro avg       0.59      0.67      0.59       941\n",
      "weighted avg       0.84      0.73      0.77       941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_4 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_4, y_train)\n",
    "y_lr = lr_model_4.predict(X_test_model_4)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | Precision | Recall | F1 | Data Points |\n",
    "| --- | --- |--- | --- | --- |\n",
    "| Class 0 | 0.94 | 0.89 | 0.91 | 117 |\n",
    "| Class 1 |0.58 | 0.72 | 0.64 | 25 |\n",
    "| Macro Average Score | 0.76 | 0.80 | 0.78 | 142|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Without POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_5 = X_train_final.iloc[:,np.r_[3:13,21:1113]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3763, 1102)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_5 = X_test_final.iloc[:,np.r_[3:13,21:1113]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(941, 1102)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.402\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.74      0.83       823\n",
      "           1       0.29      0.73      0.41       118\n",
      "\n",
      "    accuracy                           0.74       941\n",
      "   macro avg       0.62      0.73      0.62       941\n",
      "weighted avg       0.87      0.74      0.78       941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_5_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_5_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_5,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_5), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 608, False Positive: 215, False Negative: 32, True Positive: 86\n",
      "--------------------------------------------------------------------------------\n",
      "[[608 215]\n",
      " [ 32  86]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.74      0.83       823\n",
      "           1       0.29      0.73      0.41       118\n",
      "\n",
      "    accuracy                           0.74       941\n",
      "   macro avg       0.62      0.73      0.62       941\n",
      "weighted avg       0.87      0.74      0.78       941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_5 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_5, y_train)\n",
    "y_lr = lr_model_5.predict(X_test_model_5)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | Precision | Recall | F1 | Data Points |\n",
    "| --- | --- |--- | --- | --- |\n",
    "| Class 0 | 0.96 | 0.86 | 0.91 | 117 |\n",
    "| Class 1 |0.57 | 0.84 | 0.68 | 25 |\n",
    "| Macro Average Score | 0.76 | 0.85 | 0.79 | 142|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding87 --- Value: -0.8182971709303518\n",
      "embedding94 --- Value: -0.6286071870410861\n",
      "embedding83 --- Value: -0.6203036533307168\n",
      "220 --- Value: -0.5839417332981749\n",
      "742 --- Value: -0.5790220769537034\n",
      "536 --- Value: -0.5299428086786621\n",
      "embedding0 --- Value: -0.5245878286796771\n",
      "embedding9 --- Value: -0.5221138408228698\n",
      "941 --- Value: -0.5035552874939756\n",
      "940 --- Value: -0.4956391496166636\n",
      "embedding85 --- Value: -0.4760989103672994\n",
      "373 --- Value: -0.46180211779019253\n",
      "758 --- Value: -0.43352217387564745\n",
      "345 --- Value: -0.42914489840674896\n",
      "embedding61 --- Value: -0.42403341467880185\n",
      "925 --- Value: -0.40066946264742426\n",
      "embedding43 --- Value: -0.3870855520864859\n",
      "646 --- Value: -0.3731299678693096\n",
      "823 --- Value: -0.3638545148457173\n",
      "embedding42 --- Value: -0.36256803608671273\n",
      "embedding88 --- Value: -0.3549277599261198\n",
      "embedding90 --- Value: -0.3526643887810724\n",
      "371 --- Value: -0.34804648082533857\n",
      "embedding44 --- Value: -0.3361042405316454\n",
      "embedding74 --- Value: -0.3336721360747008\n",
      "embedding93 --- Value: -0.3149923309029826\n",
      "915 --- Value: -0.31317171005405176\n",
      "embedding45 --- Value: -0.30927412651086905\n",
      "248 --- Value: -0.3087604569496014\n",
      "187 --- Value: -0.3060286801117915\n",
      "334 --- Value: -0.304718772370358\n",
      "745 --- Value: -0.29381676915014615\n",
      "embedding67 --- Value: -0.28955003186849887\n",
      "embedding80 --- Value: -0.28830053329232674\n",
      "693 --- Value: -0.2836975539970524\n",
      "950 --- Value: -0.280104923778206\n",
      "embedding79 --- Value: -0.2678693171255329\n",
      "905 --- Value: -0.2612879624398159\n",
      "embedding2 --- Value: -0.25562893150580873\n",
      "461 --- Value: -0.25041486414618974\n",
      "151 --- Value: -0.2497489499628013\n",
      "810 --- Value: -0.2493611915170691\n",
      "242 --- Value: -0.2491761394923283\n",
      "314 --- Value: -0.24383743982673797\n",
      "604 --- Value: -0.24299237777671995\n",
      "446 --- Value: -0.24062156850733435\n",
      "156 --- Value: -0.24005114715088302\n",
      "653 --- Value: -0.23507781331283373\n",
      "embedding52 --- Value: -0.23487202263239054\n",
      "embedding75 --- Value: -0.23087732765753227\n",
      "subjectivity --- Value: -0.2289629462127183\n",
      "894 --- Value: -0.22682853765726307\n",
      "474 --- Value: -0.2251297329517382\n",
      "embedding98 --- Value: -0.22431343651139882\n",
      "538 --- Value: -0.21566042250495646\n",
      "686 --- Value: -0.21333631453209506\n",
      "176 --- Value: -0.2116805990295878\n",
      "937 --- Value: -0.21016197234184134\n",
      "537 --- Value: -0.2033226763679093\n",
      "767 --- Value: -0.1997050535909457\n",
      "embedding81 --- Value: -0.19936895857649212\n",
      "660 --- Value: -0.19808962347913614\n",
      "972 --- Value: -0.19616480476199036\n",
      "867 --- Value: -0.1960753440244647\n",
      "embedding95 --- Value: -0.19437930074291287\n",
      "608 --- Value: -0.1922631390798612\n",
      "916 --- Value: -0.19022953851300797\n",
      "621 --- Value: -0.18862549207815035\n",
      "125 --- Value: -0.1865523840501004\n",
      "927 --- Value: -0.18183441128589534\n",
      "470 --- Value: -0.18129568449577801\n",
      "5 --- Value: -0.18006695932068073\n",
      "573 --- Value: -0.1799294202398366\n",
      "546 --- Value: -0.17926591418898616\n",
      "793 --- Value: -0.17760740121942908\n",
      "819 --- Value: -0.17680078857631423\n",
      "967 --- Value: -0.17636205874994546\n",
      "323 --- Value: -0.1760046403742408\n",
      "848 --- Value: -0.1750301370813001\n",
      "152 --- Value: -0.17458757614374584\n",
      "115 --- Value: -0.17329732018550878\n",
      "embedding7 --- Value: -0.17281594733000966\n",
      "627 --- Value: -0.17237405685091092\n",
      "325 --- Value: -0.17116788572458827\n",
      "embedding18 --- Value: -0.17106858397872401\n",
      "288 --- Value: -0.16913983633749416\n",
      "457 --- Value: -0.16889523288820635\n",
      "890 --- Value: -0.16673423283378377\n",
      "486 --- Value: -0.16665372149170746\n",
      "616 --- Value: -0.16633918644629486\n",
      "283 --- Value: -0.1653683564898458\n",
      "embedding33 --- Value: -0.1643441761853877\n",
      "embedding76 --- Value: -0.16370690040004238\n",
      "714 --- Value: -0.16366083117173968\n",
      "749 --- Value: -0.1633376869331297\n",
      "173 --- Value: -0.16328498982886205\n",
      "378 --- Value: -0.16194307262735125\n",
      "877 --- Value: -0.16043969362817873\n",
      "860 --- Value: -0.15970049337263556\n",
      "655 --- Value: -0.15940642319891218\n",
      "861 --- Value: -0.1589124774337372\n",
      "397 --- Value: -0.15454340756308457\n",
      "embedding32 --- Value: -0.15382913191417388\n",
      "744 --- Value: -0.15214719802073778\n",
      "153 --- Value: -0.15206153656843782\n",
      "895 --- Value: -0.15151783432815968\n",
      "466 --- Value: -0.1498059961752388\n",
      "384 --- Value: -0.1437216846968978\n",
      "684 --- Value: -0.1435911437808105\n",
      "751 --- Value: -0.1430878241578577\n",
      "embedding12 --- Value: -0.14022874517721026\n",
      "598 --- Value: -0.13901360194645496\n",
      "482 --- Value: -0.13844837949958394\n",
      "683 --- Value: -0.13838712276836904\n",
      "124 --- Value: -0.13686768704595959\n",
      "929 --- Value: -0.1352182970325461\n",
      "embedding51 --- Value: -0.13471629988747263\n",
      "502 --- Value: -0.13429057286619636\n",
      "469 --- Value: -0.13166731266490916\n",
      "577 --- Value: -0.13156704969616437\n",
      "423 --- Value: -0.13149357785789934\n",
      "756 --- Value: -0.13144864492932726\n",
      "embedding77 --- Value: -0.13038707672786773\n",
      "633 --- Value: -0.1282812525479535\n",
      "747 --- Value: -0.12738383842131012\n",
      "707 --- Value: -0.12720054776548603\n",
      "614 --- Value: -0.1260065588761237\n",
      "271 --- Value: -0.12527616696066526\n",
      "100 --- Value: -0.12398489031629605\n",
      "399 --- Value: -0.12359483783015543\n",
      "478 --- Value: -0.12311286651183942\n",
      "132 --- Value: -0.12275969846315557\n",
      "309 --- Value: -0.12143958068064795\n",
      "703 --- Value: -0.12014446407062032\n",
      "527 --- Value: -0.11996073935205832\n",
      "796 --- Value: -0.1190856096996939\n",
      "459 --- Value: -0.11901579875775642\n",
      "548 --- Value: -0.1187866514611943\n",
      "408 --- Value: -0.11870425428187492\n",
      "763 --- Value: -0.1181880243325483\n",
      "728 --- Value: -0.11778391768909653\n",
      "29 --- Value: -0.11712356100889251\n",
      "888 --- Value: -0.11610368371529235\n",
      "909 --- Value: -0.1150398190642944\n",
      "534 --- Value: -0.11386492663283955\n",
      "217 --- Value: -0.11351288889891145\n",
      "embedding50 --- Value: -0.11334621914251039\n",
      "215 --- Value: -0.1112559261362878\n",
      "697 --- Value: -0.10960644162774845\n",
      "255 --- Value: -0.1083295809642875\n",
      "362 --- Value: -0.1077171163601065\n",
      "910 --- Value: -0.10754421571504845\n",
      "489 --- Value: -0.10688905477078935\n",
      "292 --- Value: -0.10676468383419713\n",
      "591 --- Value: -0.10663347653355047\n",
      "564 --- Value: -0.10636842239061606\n",
      "259 --- Value: -0.10634893451680986\n",
      "484 --- Value: -0.10560262493057271\n",
      "233 --- Value: -0.105579154484212\n",
      "41 --- Value: -0.10458422570504088\n",
      "801 --- Value: -0.10331706788043653\n",
      "892 --- Value: -0.10253068494616388\n",
      "20 --- Value: -0.10250225161265893\n",
      "800 --- Value: -0.1016554086604237\n",
      "978 --- Value: -0.10161428522481257\n",
      "67 --- Value: -0.10124360067921474\n",
      "42 --- Value: -0.10079237992845426\n",
      "505 --- Value: -0.10015720222417293\n",
      "639 --- Value: -0.10005657963311702\n",
      "205 --- Value: -0.09966802470973102\n",
      "851 --- Value: -0.09949038656337114\n",
      "358 --- Value: -0.09920625414590471\n",
      "575 --- Value: -0.09598084623080315\n",
      "463 --- Value: -0.09539391033834278\n",
      "268 --- Value: -0.09479434419377211\n",
      "394 --- Value: -0.0940109180432013\n",
      "108 --- Value: -0.09390422932496408\n",
      "196 --- Value: -0.09350679086184835\n",
      "567 --- Value: -0.09162822643899116\n",
      "324 --- Value: -0.09150159185723143\n",
      "698 --- Value: -0.09115334640955215\n",
      "237 --- Value: -0.09083535963432314\n",
      "150 --- Value: -0.09044807442397525\n",
      "234 --- Value: -0.08904914134767142\n",
      "194 --- Value: -0.08904481643904272\n",
      "32 --- Value: -0.08889593393459519\n",
      "433 --- Value: -0.08869271248456773\n",
      "971 --- Value: -0.08848631674797892\n",
      "embedding72 --- Value: -0.08809678717726274\n",
      "68 --- Value: -0.0878906345133245\n",
      "269 --- Value: -0.08758838534455575\n",
      "581 --- Value: -0.08703140762951689\n",
      "964 --- Value: -0.08689663091397389\n",
      "352 --- Value: -0.0864617275327769\n",
      "439 --- Value: -0.08554072293163485\n",
      "954 --- Value: -0.08544915093935859\n",
      "565 --- Value: -0.08366565514964372\n",
      "868 --- Value: -0.0830091836054261\n",
      "418 --- Value: -0.08260972562524808\n",
      "19 --- Value: -0.08215726871829548\n",
      "847 --- Value: -0.08174047525554147\n",
      "280 --- Value: -0.0811928879242682\n",
      "834 --- Value: -0.08106944903645687\n",
      "166 --- Value: -0.08106384313548019\n",
      "49 --- Value: -0.08104112819685766\n",
      "261 --- Value: -0.08096476010424303\n",
      "824 --- Value: -0.08078139843786435\n",
      "513 --- Value: -0.08058883360901231\n",
      "65 --- Value: -0.07993482642035557\n",
      "296 --- Value: -0.07979094251082136\n",
      "840 --- Value: -0.07971471638291809\n",
      "830 --- Value: -0.07950494453189072\n",
      "161 --- Value: -0.07921678113601709\n",
      "571 --- Value: -0.07916593139463507\n",
      "771 --- Value: -0.07881599544568756\n",
      "987 --- Value: -0.07851075430387566\n",
      "93 --- Value: -0.07841535754379135\n",
      "213 --- Value: -0.07835045788877545\n",
      "930 --- Value: -0.07786073954452483\n",
      "873 --- Value: -0.07781828662061822\n",
      "134 --- Value: -0.07775098395421404\n",
      "76 --- Value: -0.07768663940077634\n",
      "821 --- Value: -0.0772927398174772\n",
      "421 --- Value: -0.07719466694407331\n",
      "415 --- Value: -0.07716216716230528\n",
      "805 --- Value: -0.07696559804286626\n",
      "872 --- Value: -0.07630823571975648\n",
      "897 --- Value: -0.07628992942423043\n",
      "272 --- Value: -0.07607404480001606\n",
      "519 --- Value: -0.07607108126816196\n",
      "385 --- Value: -0.07592115020985823\n",
      "392 --- Value: -0.07579589312254924\n",
      "973 --- Value: -0.07559769770572401\n",
      "208 --- Value: -0.0751655789575897\n",
      "682 --- Value: -0.07494074002011926\n",
      "635 --- Value: -0.07483805640359076\n",
      "412 --- Value: -0.07457037082132323\n",
      "133 --- Value: -0.07451189959397572\n",
      "embedding3 --- Value: -0.07435406485354754\n",
      "222 --- Value: -0.07433588388412057\n",
      "886 --- Value: -0.07410634325346423\n",
      "254 --- Value: -0.07298140187451946\n",
      "369 --- Value: -0.07288670573553399\n",
      "449 --- Value: -0.07256843320378586\n",
      "494 --- Value: -0.07243718201108652\n",
      "610 --- Value: -0.07227754146697768\n",
      "856 --- Value: -0.07227754146697768\n",
      "374 --- Value: -0.07224806136137642\n",
      "734 --- Value: -0.07179606926154854\n",
      "465 --- Value: -0.07130585898897322\n",
      "923 --- Value: -0.07125711127175209\n",
      "342 --- Value: -0.07120655867210476\n",
      "911 --- Value: -0.07082419683544526\n",
      "740 --- Value: -0.0702241515302133\n",
      "730 --- Value: -0.07016739670792099\n",
      "752 --- Value: -0.0701215344535153\n",
      "350 --- Value: -0.07010998850518062\n",
      "732 --- Value: -0.0698711044967372\n",
      "541 --- Value: -0.06951103979106062\n",
      "382 --- Value: -0.06945151820198937\n",
      "410 --- Value: -0.06942979799190019\n",
      "400 --- Value: -0.0692567460594163\n",
      "797 --- Value: -0.06902815259051624\n",
      "799 --- Value: -0.0687376428403451\n",
      "444 --- Value: -0.06845248164817827\n",
      "107 --- Value: -0.06835821185726602\n",
      "560 --- Value: -0.06767046900355164\n",
      "167 --- Value: -0.06765573732576045\n",
      "250 --- Value: -0.06713978283372889\n",
      "898 --- Value: -0.06705895435398473\n",
      "501 --- Value: -0.0667989633960073\n",
      "549 --- Value: -0.06668972464995439\n",
      "embedding5 --- Value: -0.0666219363882659\n",
      "174 --- Value: -0.06624769356538256\n",
      "556 --- Value: -0.06622580999059252\n",
      "612 --- Value: -0.06584460165581206\n",
      "906 --- Value: -0.06569358803197228\n",
      "475 --- Value: -0.065675316358124\n",
      "675 --- Value: -0.06555242709736836\n",
      "506 --- Value: -0.06554098580492525\n",
      "555 --- Value: -0.06552715501863683\n",
      "775 --- Value: -0.06507762950111544\n",
      "539 --- Value: -0.06507264284083235\n",
      "521 --- Value: -0.06486451285906907\n",
      "110 --- Value: -0.0647798412573651\n",
      "957 --- Value: -0.06470331163785899\n",
      "966 --- Value: -0.06376755331289981\n",
      "798 --- Value: -0.06312718977882813\n",
      "725 --- Value: -0.06307662906132507\n",
      "embedding35 --- Value: -0.06272537610739644\n",
      "850 --- Value: -0.06256978889617035\n",
      "22 --- Value: -0.06250814617951277\n",
      "440 --- Value: -0.06244748470074458\n",
      "304 --- Value: -0.06244453493508584\n",
      "398 --- Value: -0.062346816114924146\n",
      "721 --- Value: -0.062346810450975634\n",
      "629 --- Value: -0.06178239430928732\n",
      "551 --- Value: -0.061724921184380235\n",
      "241 --- Value: -0.06157423790856988\n",
      "198 --- Value: -0.06133596320272924\n",
      "980 --- Value: -0.0612591630914313\n",
      "258 --- Value: -0.06116127422735205\n",
      "988 --- Value: -0.061026114154827546\n",
      "619 --- Value: -0.06087679818795425\n",
      "462 --- Value: -0.060845168533197345\n",
      "1 --- Value: -0.06081944911309762\n",
      "762 --- Value: -0.06048238111631297\n",
      "118 --- Value: -0.06037873577406878\n",
      "223 --- Value: -0.06023717045865207\n",
      "221 --- Value: -0.05987224260727093\n",
      "431 --- Value: -0.05987224260727093\n",
      "935 --- Value: -0.05937962775241458\n",
      "231 --- Value: -0.05898946319341837\n",
      "122 --- Value: -0.058901968406230534\n",
      "666 --- Value: -0.058854000452435454\n",
      "669 --- Value: -0.05878304101064676\n",
      "962 --- Value: -0.05855402981826406\n",
      "embedding91 --- Value: -0.058500640701885646\n",
      "918 --- Value: -0.05845429079729885\n",
      "336 --- Value: -0.058102308731689804\n",
      "218 --- Value: -0.0580564992360102\n",
      "287 --- Value: -0.05797731533823883\n",
      "170 --- Value: -0.057801953367613006\n",
      "649 --- Value: -0.05761538442513676\n",
      "0 --- Value: -0.05759078319251207\n",
      "147 --- Value: -0.05745434225151262\n",
      "47 --- Value: -0.05731021667864858\n",
      "671 --- Value: -0.05731021667864858\n",
      "622 --- Value: -0.057187338937707055\n",
      "974 --- Value: -0.05686300337417733\n",
      "969 --- Value: -0.056785528192715845\n",
      "429 --- Value: -0.05669861788845301\n",
      "498 --- Value: -0.056462915327802704\n",
      "820 --- Value: -0.056341637981161534\n",
      "bio_sim_words --- Value: -0.056298301298439886\n",
      "528 --- Value: -0.056186313621750916\n",
      "603 --- Value: -0.0560687850093401\n",
      "120 --- Value: -0.05574333088692007\n",
      "727 --- Value: -0.055639244063961035\n",
      "691 --- Value: -0.05541802209771422\n",
      "529 --- Value: -0.05536938133351553\n",
      "842 --- Value: -0.0545635259718398\n",
      "92 --- Value: -0.05441544357198856\n",
      "366 --- Value: -0.054365506797367597\n",
      "73 --- Value: -0.054290535041541424\n",
      "332 --- Value: -0.054137866710741764\n",
      "642 --- Value: -0.05408309412274713\n",
      "402 --- Value: -0.05399413328386486\n",
      "654 --- Value: -0.05397856307131346\n",
      "781 --- Value: -0.05389481494284269\n",
      "493 --- Value: -0.05379719745811644\n",
      "8 --- Value: -0.05361128401393074\n",
      "597 --- Value: -0.053149431824651115\n",
      "545 --- Value: -0.05304100203969408\n",
      "112 --- Value: -0.05292238020719118\n",
      "201 --- Value: -0.0528391336236344\n",
      "200 --- Value: -0.05255665408996242\n",
      "251 --- Value: -0.052442511515706315\n",
      "760 --- Value: -0.052060265699664494\n",
      "58 --- Value: -0.05184961138213697\n",
      "372 --- Value: -0.051636750207851315\n",
      "553 --- Value: -0.05149559721360835\n",
      "584 --- Value: -0.05149559721360835\n",
      "240 --- Value: -0.05125772414242926\n",
      "embedding58 --- Value: -0.05110062333728875\n",
      "175 --- Value: -0.05089321936184334\n",
      "532 --- Value: -0.05078197716869985\n",
      "558 --- Value: -0.05049642016634443\n",
      "46 --- Value: -0.05013613953663422\n",
      "53 --- Value: -0.050133273188822575\n",
      "83 --- Value: -0.050111262182677044\n",
      "264 --- Value: -0.04968353227804685\n",
      "143 --- Value: -0.049643126938914275\n",
      "329 --- Value: -0.0495800755714259\n",
      "630 --- Value: -0.04954918961737812\n",
      "45 --- Value: -0.049494975230428756\n",
      "414 --- Value: -0.04931538443307273\n",
      "948 --- Value: -0.049076106096334005\n",
      "182 --- Value: -0.048578174475839\n",
      "623 --- Value: -0.04852528854385253\n",
      "712 --- Value: -0.04839989288577415\n",
      "260 --- Value: -0.048381410838288154\n",
      "285 --- Value: -0.04834379446659501\n",
      "837 --- Value: -0.04800126839440331\n",
      "178 --- Value: -0.04769695516917139\n",
      "447 --- Value: -0.0476657417331664\n",
      "embedding59 --- Value: -0.04734892440985889\n",
      "795 --- Value: -0.04728783554036275\n",
      "986 --- Value: -0.047234383970548056\n",
      "308 --- Value: -0.04678865856891\n",
      "158 --- Value: -0.046748094731568636\n",
      "72 --- Value: -0.046481756986632275\n",
      "411 --- Value: -0.04645612232133039\n",
      "413 --- Value: -0.04645612232133039\n",
      "496 --- Value: -0.04645612232133039\n",
      "540 --- Value: -0.04645612232133039\n",
      "782 --- Value: -0.046228202000759555\n",
      "367 --- Value: -0.04575566474532278\n",
      "195 --- Value: -0.04560982092078113\n",
      "587 --- Value: -0.045501297392686914\n",
      "547 --- Value: -0.04541355521154566\n",
      "583 --- Value: -0.04535813305526134\n",
      "788 --- Value: -0.04535813305526134\n",
      "91 --- Value: -0.045270601636862536\n",
      "316 --- Value: -0.045184006726467725\n",
      "716 --- Value: -0.04482722750008727\n",
      "55 --- Value: -0.04477502317642505\n",
      "128 --- Value: -0.044571471046472556\n",
      "284 --- Value: -0.04396340784857061\n",
      "327 --- Value: -0.043879173812218474\n",
      "442 --- Value: -0.04377970447152334\n",
      "401 --- Value: -0.043721544066099856\n",
      "976 --- Value: -0.043703072622171504\n",
      "676 --- Value: -0.04361706665656895\n",
      "600 --- Value: -0.04333692038502198\n",
      "632 --- Value: -0.04333692038502198\n",
      "214 --- Value: -0.043249996503331625\n",
      "embedding97 --- Value: -0.043160568267653715\n",
      "embedding49 --- Value: -0.04312844429407885\n",
      "594 --- Value: -0.04310118040631296\n",
      "566 --- Value: -0.04304418375563553\n",
      "893 --- Value: -0.04287070058803112\n",
      "39 --- Value: -0.042842125088350454\n",
      "624 --- Value: -0.042665047573636365\n",
      "33 --- Value: -0.042467758268804515\n",
      "636 --- Value: -0.042443171247915364\n",
      "138 --- Value: -0.04230825286309823\n",
      "593 --- Value: -0.042256937179139045\n",
      "786 --- Value: -0.042256937179139045\n",
      "947 --- Value: -0.04198168309769568\n",
      "159 --- Value: -0.041783910655454976\n",
      "270 --- Value: -0.04171638565414462\n",
      "919 --- Value: -0.0416553415033462\n",
      "13 --- Value: -0.04134749917694762\n",
      "657 --- Value: -0.04110878027952063\n",
      "379 --- Value: -0.04099309312855309\n",
      "123 --- Value: -0.04098964774263496\n",
      "422 --- Value: -0.04093110532200539\n",
      "95 --- Value: -0.04063343335503152\n",
      "326 --- Value: -0.04061332598010868\n",
      "456 --- Value: -0.04054640541085482\n",
      "472 --- Value: -0.04029751496125724\n",
      "516 --- Value: -0.04029751496125724\n",
      "104 --- Value: -0.040292988986442436\n",
      "179 --- Value: -0.040292988986442436\n",
      "975 --- Value: -0.040198981126072314\n",
      "320 --- Value: -0.03977802330567917\n",
      "7 --- Value: -0.03977198007680082\n",
      "203 --- Value: -0.03963461163811483\n",
      "938 --- Value: -0.03962614641289851\n",
      "955 --- Value: -0.03957549290577326\n",
      "69 --- Value: -0.03943538449022896\n",
      "815 --- Value: -0.039341657513821904\n",
      "17 --- Value: -0.038945017286334274\n",
      "89 --- Value: -0.03887199183568953\n",
      "990 --- Value: -0.03880535001980897\n",
      "736 --- Value: -0.03876723868017302\n",
      "804 --- Value: -0.038748541412768726\n",
      "768 --- Value: -0.038601776212941474\n",
      "678 --- Value: -0.038576089639565304\n",
      "663 --- Value: -0.03849966957278995\n",
      "290 --- Value: -0.0383480962252418\n",
      "180 --- Value: -0.0383031889344601\n",
      "127 --- Value: -0.03827407058034705\n",
      "2 --- Value: -0.038155789836461514\n",
      "57 --- Value: -0.03779051351200221\n",
      "425 --- Value: -0.03774777831897038\n",
      "75 --- Value: -0.03761521596746958\n",
      "945 --- Value: -0.03734230771487239\n",
      "ner --- Value: -0.03731774327683743\n",
      "145 --- Value: -0.037004046551373\n",
      "965 --- Value: -0.03693522653438552\n",
      "435 --- Value: -0.036707052125868556\n",
      "427 --- Value: -0.036634748155905\n",
      "944 --- Value: -0.036551116420560914\n",
      "114 --- Value: -0.036473485583182974\n",
      "50 --- Value: -0.036239009244767376\n",
      "776 --- Value: -0.03619270435535833\n",
      "512 --- Value: -0.03618576744290629\n",
      "667 --- Value: -0.03610252755621075\n",
      "188 --- Value: -0.03588099011345857\n",
      "377 --- Value: -0.03586893578819948\n",
      "579 --- Value: -0.035597453557622095\n",
      "26 --- Value: -0.035545890976070235\n",
      "717 --- Value: -0.03547434979178942\n",
      "97 --- Value: -0.03539615371291139\n",
      "230 --- Value: -0.03527404681878474\n",
      "448 --- Value: -0.03513069540238269\n",
      "631 --- Value: -0.03512148480094585\n",
      "embedding55 --- Value: -0.03504184800037907\n",
      "441 --- Value: -0.034881047537148005\n",
      "243 --- Value: -0.034802499127742116\n",
      "52 --- Value: -0.03464908156911784\n",
      "863 --- Value: -0.03464908156911784\n",
      "383 --- Value: -0.03463918564285777\n",
      "843 --- Value: -0.03460539056482921\n",
      "563 --- Value: -0.03424294029119964\n",
      "16 --- Value: -0.034087072210835824\n",
      "embedding60 --- Value: -0.03407088682761873\n",
      "430 --- Value: -0.034047194264456584\n",
      "753 --- Value: -0.03397646127811901\n",
      "471 --- Value: -0.03386959285315076\n",
      "279 --- Value: -0.03367164522028801\n",
      "866 --- Value: -0.03356481640781616\n",
      "265 --- Value: -0.03353187031950787\n",
      "357 --- Value: -0.033408711584492955\n",
      "722 --- Value: -0.03333261554518556\n",
      "606 --- Value: -0.03323144666013732\n",
      "659 --- Value: -0.03313506564885732\n",
      "625 --- Value: -0.033013898459651\n",
      "700 --- Value: -0.03300379761197129\n",
      "737 --- Value: -0.032657324326102515\n",
      "825 --- Value: -0.03236704073212936\n",
      "769 --- Value: -0.03234163182858726\n",
      "902 --- Value: -0.032253827679708046\n",
      "778 --- Value: -0.03219779897391076\n",
      "849 --- Value: -0.03215895166735929\n",
      "933 --- Value: -0.03215106660970062\n",
      "embedding20 --- Value: -0.03204286354824797\n",
      "172 --- Value: -0.031990062182529984\n",
      "715 --- Value: -0.031990062182529984\n",
      "719 --- Value: -0.031837626256842805\n",
      "351 --- Value: -0.03179365754503335\n",
      "609 --- Value: -0.03153362008280172\n",
      "907 --- Value: -0.031523038371928984\n",
      "908 --- Value: -0.03140246199871795\n",
      "673 --- Value: -0.0313480136928444\n",
      "184 --- Value: -0.03128631810120364\n",
      "216 --- Value: -0.031149351421689873\n",
      "490 --- Value: -0.03113879932707714\n",
      "705 --- Value: -0.031071769542979393\n",
      "84 --- Value: -0.031049416480917875\n",
      "256 --- Value: -0.031025711047949007\n",
      "347 --- Value: -0.03099554273189018\n",
      "148 --- Value: -0.030824965700407953\n",
      "186 --- Value: -0.030678954990145807\n",
      "12 --- Value: -0.030210297631182288\n",
      "245 --- Value: -0.030210297631182288\n",
      "27 --- Value: -0.030210297631182288\n",
      "365 --- Value: -0.030210297631182288\n",
      "724 --- Value: -0.030210297631182288\n",
      "626 --- Value: -0.030200356670046297\n",
      "451 --- Value: -0.030154245206729696\n",
      "391 --- Value: -0.03006546322598218\n",
      "224 --- Value: -0.030036947400978037\n",
      "225 --- Value: -0.030036947400978037\n",
      "464 --- Value: -0.030036947400978037\n",
      "912 --- Value: -0.029530354691703425\n",
      "885 --- Value: -0.029520064406673978\n",
      "144 --- Value: -0.029516795555893902\n",
      "139 --- Value: -0.02949156749157532\n",
      "640 --- Value: -0.02910859537335611\n",
      "928 --- Value: -0.02909599698374395\n",
      "111 --- Value: -0.028726487236066866\n",
      "249 --- Value: -0.028726487236066866\n",
      "605 --- Value: -0.028719381144612314\n",
      "380 --- Value: -0.028717101256553287\n",
      "585 --- Value: -0.02844039517244343\n",
      "658 --- Value: -0.028400601252471225\n",
      "70 --- Value: -0.028382474225531043\n",
      "96 --- Value: -0.027875591549557203\n",
      "262 --- Value: -0.027595140773882455\n",
      "453 --- Value: -0.02749011187397965\n",
      "599 --- Value: -0.02747292352249529\n",
      "913 --- Value: -0.027433170216457737\n",
      "199 --- Value: -0.02736779056730481\n",
      "78 --- Value: -0.027360906786829477\n",
      "37 --- Value: -0.02719128134696579\n",
      "787 --- Value: -0.02719128134696579\n",
      "98 --- Value: -0.026984833339448285\n",
      "294 --- Value: -0.02695544701547757\n",
      "419 --- Value: -0.02678511316174473\n",
      "879 --- Value: -0.026772753244943735\n",
      "477 --- Value: -0.02674075299164497\n",
      "436 --- Value: -0.026718302467351098\n",
      "142 --- Value: -0.02668334124546405\n",
      "953 --- Value: -0.026682928477500152\n",
      "903 --- Value: -0.026658219155355205\n",
      "246 --- Value: -0.026355488947651964\n",
      "611 --- Value: -0.026269375547352838\n",
      "816 --- Value: -0.026146944553983965\n",
      "154 --- Value: -0.025965545558432904\n",
      "487 --- Value: -0.02586492347661079\n",
      "881 --- Value: -0.02583177121816109\n",
      "386 --- Value: -0.025614442205037596\n",
      "488 --- Value: -0.02553588241876461\n",
      "15 --- Value: -0.025431831656734825\n",
      "24 --- Value: -0.025431831656734825\n",
      "443 --- Value: -0.02541383192983538\n",
      "522 --- Value: -0.025396816442948247\n",
      "491 --- Value: -0.025385467050625263\n",
      "281 --- Value: -0.025377089954146222\n",
      "497 --- Value: -0.025377089954146222\n",
      "328 --- Value: -0.025365507343043124\n",
      "embedding19 --- Value: -0.025356043302617374\n",
      "458 --- Value: -0.024982283246562705\n",
      "101 --- Value: -0.02496256957609549\n",
      "38 --- Value: -0.02490411946740714\n",
      "276 --- Value: -0.024903097437793592\n",
      "426 --- Value: -0.024854665607505783\n",
      "136 --- Value: -0.02479015674114312\n",
      "77 --- Value: -0.024621573913501053\n",
      "900 --- Value: -0.02461070874499147\n",
      "14 --- Value: -0.024438298698753635\n",
      "86 --- Value: -0.024438298698753635\n",
      "770 --- Value: -0.024431029200501463\n",
      "247 --- Value: -0.024377745913339435\n",
      "169 --- Value: -0.02431260433605412\n",
      "25 --- Value: -0.024081167459119444\n",
      "634 --- Value: -0.024063498787656075\n",
      "381 --- Value: -0.02401626612601147\n",
      "785 --- Value: -0.023890729822379747\n",
      "432 --- Value: -0.023820150834982613\n",
      "282 --- Value: -0.023379767988425566\n",
      "750 --- Value: -0.023233000038939872\n",
      "835 --- Value: -0.023084279044060713\n",
      "931 --- Value: -0.02290154570718225\n",
      "360 --- Value: -0.022730009829462734\n",
      "789 --- Value: -0.02263498838706591\n",
      "554 --- Value: -0.022342609546175056\n",
      "694 --- Value: -0.022342609546175056\n",
      "191 --- Value: -0.022338395973102013\n",
      "681 --- Value: -0.022244628672954864\n",
      "943 --- Value: -0.02195364483025862\n",
      "958 --- Value: -0.021905212565555915\n",
      "31 --- Value: -0.02180217549353227\n",
      "802 --- Value: -0.0217530229513328\n",
      "557 --- Value: -0.021630586326570875\n",
      "164 --- Value: -0.021370221873431048\n",
      "876 --- Value: -0.021254237441711565\n",
      "197 --- Value: -0.021229063223731633\n",
      "424 --- Value: -0.021203525788780663\n",
      "434 --- Value: -0.02114666561312259\n",
      "696 --- Value: -0.02114666561312259\n",
      "355 --- Value: -0.021130846191965355\n",
      "59 --- Value: -0.02100375308448903\n",
      "74 --- Value: -0.02037717230096279\n",
      "495 --- Value: -0.020184873834549644\n",
      "561 --- Value: -0.020164466633826667\n",
      "257 --- Value: -0.019962114666398044\n",
      "702 --- Value: -0.019962114666398044\n",
      "891 --- Value: -0.019962114666398044\n",
      "368 --- Value: -0.019868607934665255\n",
      "592 --- Value: -0.019599352651992398\n",
      "511 --- Value: -0.019511267306032525\n",
      "651 --- Value: -0.019482831941530256\n",
      "450 --- Value: -0.019471224889295462\n",
      "543 --- Value: -0.019095946081127944\n",
      "361 --- Value: -0.019025597042395183\n",
      "582 --- Value: -0.018943462427498114\n",
      "476 --- Value: -0.01887127310334814\n",
      "852 --- Value: -0.018843313290107945\n",
      "305 --- Value: -0.018830524733028573\n",
      "670 --- Value: -0.018811940448084465\n",
      "165 --- Value: -0.018748389021157264\n",
      "901 --- Value: -0.018748389021157264\n",
      "970 --- Value: -0.018702251647951165\n",
      "530 --- Value: -0.018350221794530813\n",
      "739 --- Value: -0.018320577749598336\n",
      "388 --- Value: -0.018120783705470794\n",
      "263 --- Value: -0.017928696118813637\n",
      "370 --- Value: -0.01785054655304364\n",
      "455 --- Value: -0.017714755718172256\n",
      "343 --- Value: -0.017482040905854725\n",
      "103 --- Value: -0.017407578774023987\n",
      "985 --- Value: -0.017256604665096646\n",
      "904 --- Value: -0.016757719206317027\n",
      "105 --- Value: -0.016714028663664865\n",
      "99 --- Value: -0.016570173329633477\n",
      "618 --- Value: -0.016552158192965412\n",
      "922 --- Value: -0.01651275573217073\n",
      "562 --- Value: -0.01617499208617904\n",
      "428 --- Value: -0.01612016673201867\n",
      "389 --- Value: -0.016096763698779826\n",
      "embedding96 --- Value: -0.015875579800181977\n",
      "62 --- Value: -0.01577460621204943\n",
      "780 --- Value: -0.015536282308514107\n",
      "481 --- Value: -0.015474525759166096\n",
      "40 --- Value: -0.015199499268518702\n",
      "662 --- Value: -0.015042750756705154\n",
      "473 --- Value: -0.014818333818247943\n",
      "595 --- Value: -0.014818333818247943\n",
      "699 --- Value: -0.014818333818247943\n",
      "822 --- Value: -0.014551752370792545\n",
      "452 --- Value: -0.01409966978617943\n",
      "704 --- Value: -0.014074577933396162\n",
      "405 --- Value: -0.014007153400427299\n",
      "790 --- Value: -0.013519408925640691\n",
      "348 --- Value: -0.013489381710565053\n",
      "514 --- Value: -0.013162978141288511\n",
      "882 --- Value: -0.013128017504482366\n",
      "826 --- Value: -0.012999700284164877\n",
      "310 --- Value: -0.012745354472137684\n",
      "244 --- Value: -0.01252166426486399\n",
      "60 --- Value: -0.01252166426486399\n",
      "846 --- Value: -0.01252166426486399\n",
      "212 --- Value: -0.012380222736282188\n",
      "315 --- Value: -0.012380222736282188\n",
      "920 --- Value: -0.012296592457698069\n",
      "926 --- Value: -0.01158960515983623\n",
      "363 --- Value: -0.011493329423040165\n",
      "239 --- Value: -0.010846584043355331\n",
      "952 --- Value: -0.010225105604878985\n",
      "746 --- Value: -0.009828205058913877\n",
      "189 --- Value: -0.008596773379369475\n",
      "403 --- Value: -0.008595289509343986\n",
      "331 --- Value: -0.007656187239212227\n",
      "792 --- Value: -0.007613521338942043\n",
      "274 --- Value: -0.007478523217536738\n",
      "500 --- Value: -0.006849878533403659\n",
      "982 --- Value: -0.005227170122680024\n",
      "981 --- Value: -0.003704000973966273\n",
      "9 --- Value: -0.002456251057627954\n",
      "664 --- Value: -0.0013367459148723785\n",
      "809 --- Value: -0.00061881550623751\n",
      "embedding14 --- Value: 0.0007185148838554016\n",
      "853 --- Value: 0.0017477455854145482\n",
      "841 --- Value: 0.002250507140759449\n",
      "656 --- Value: 0.003958774176689949\n",
      "embedding31 --- Value: 0.004905700084695946\n",
      "141 --- Value: 0.005834821201127014\n",
      "387 --- Value: 0.006208766081595773\n",
      "956 --- Value: 0.007070213297691636\n",
      "319 --- Value: 0.007156675890809379\n",
      "phy_sim_words --- Value: 0.007953402765431702\n",
      "130 --- Value: 0.008601135060229649\n",
      "335 --- Value: 0.008656992616931071\n",
      "346 --- Value: 0.010049462225158517\n",
      "51 --- Value: 0.010465595832452212\n",
      "772 --- Value: 0.010759574951950185\n",
      "337 --- Value: 0.011820406249225835\n",
      "eng_sim_words --- Value: 0.012362250848212242\n",
      "tech_sim_words --- Value: 0.012362250848212242\n",
      "18 --- Value: 0.013883368123512875\n",
      "508 --- Value: 0.014193789606589973\n",
      "813 --- Value: 0.01452454127193873\n",
      "117 --- Value: 0.01592003857129765\n",
      "757 --- Value: 0.016066603725356246\n",
      "340 --- Value: 0.016182285047067334\n",
      "468 --- Value: 0.0162683768627585\n",
      "295 --- Value: 0.01767213185953614\n",
      "559 --- Value: 0.01809692337615243\n",
      "204 --- Value: 0.01817026911115173\n",
      "754 --- Value: 0.018770201283640858\n",
      "674 --- Value: 0.01980426462568376\n",
      "253 --- Value: 0.020667486944256568\n",
      "82 --- Value: 0.020817721940863915\n",
      "665 --- Value: 0.022520132893524617\n",
      "726 --- Value: 0.02448051653045261\n",
      "483 --- Value: 0.02495853232081489\n",
      "613 --- Value: 0.025890929549571385\n",
      "814 --- Value: 0.026210325464472876\n",
      "977 --- Value: 0.027643520266305295\n",
      "601 --- Value: 0.02827109760647246\n",
      "303 --- Value: 0.029138241010643492\n",
      "359 --- Value: 0.029410593020804624\n",
      "644 --- Value: 0.029740749923551503\n",
      "395 --- Value: 0.03240973631280157\n",
      "116 --- Value: 0.033377167438267796\n",
      "23 --- Value: 0.034718551830532685\n",
      "311 --- Value: 0.03609355960376134\n",
      "580 --- Value: 0.03615738587019033\n",
      "207 --- Value: 0.036530917055936814\n",
      "485 --- Value: 0.03662268473347997\n",
      "409 --- Value: 0.03693646083830821\n",
      "298 --- Value: 0.03714780247950912\n",
      "552 --- Value: 0.037652500550480035\n",
      "85 --- Value: 0.03770540253373036\n",
      "420 --- Value: 0.03859002256086888\n",
      "829 --- Value: 0.03895430858110183\n",
      "embedding89 --- Value: 0.04002045586204141\n",
      "embedding17 --- Value: 0.04006900930419201\n",
      "883 --- Value: 0.041644169192471756\n",
      "61 --- Value: 0.04244320456188564\n",
      "embedding71 --- Value: 0.042506359122755855\n",
      "738 --- Value: 0.04585457548120227\n",
      "146 --- Value: 0.045946669616079246\n",
      "934 --- Value: 0.046040782753568\n",
      "968 --- Value: 0.04617086258060481\n",
      "887 --- Value: 0.04672393943503341\n",
      "341 --- Value: 0.04743030471920462\n",
      "28 --- Value: 0.0475252703624261\n",
      "949 --- Value: 0.04821117817651288\n",
      "embedding4 --- Value: 0.05072483879119653\n",
      "731 --- Value: 0.05072967889660607\n",
      "300 --- Value: 0.05251873267777292\n",
      "924 --- Value: 0.0531597792470417\n",
      "523 --- Value: 0.053677339585682965\n",
      "914 --- Value: 0.05449451459252268\n",
      "211 --- Value: 0.05486594044175833\n",
      "171 --- Value: 0.05497413314756786\n",
      "759 --- Value: 0.0549869510318157\n",
      "706 --- Value: 0.05568453062495937\n",
      "525 --- Value: 0.05645736911922348\n",
      "921 --- Value: 0.05714369665660504\n",
      "291 --- Value: 0.057638164800945635\n",
      "438 --- Value: 0.060338872865119964\n",
      "586 --- Value: 0.06069643279461883\n",
      "917 --- Value: 0.06107240504397976\n",
      "318 --- Value: 0.061542906672665626\n",
      "embedding64 --- Value: 0.061985806315019064\n",
      "874 --- Value: 0.062089554924392144\n",
      "131 --- Value: 0.06221571826166882\n",
      "90 --- Value: 0.06253749152539023\n",
      "685 --- Value: 0.06346126447130825\n",
      "445 --- Value: 0.06439064589080179\n",
      "266 --- Value: 0.06822246415947653\n",
      "720 --- Value: 0.0686866563750122\n",
      "989 --- Value: 0.06913832403977177\n",
      "857 --- Value: 0.06980917241197361\n",
      "672 --- Value: 0.06987547503677251\n",
      "871 --- Value: 0.07027575521332365\n",
      "376 --- Value: 0.07097176194089162\n",
      "811 --- Value: 0.07224350919829826\n",
      "333 --- Value: 0.07263348193502832\n",
      "510 --- Value: 0.07263348193502832\n",
      "524 --- Value: 0.07263348193502832\n",
      "637 --- Value: 0.07321067312603857\n",
      "202 --- Value: 0.07340319348227585\n",
      "129 --- Value: 0.0742434862711829\n",
      "889 --- Value: 0.07511567382356561\n",
      "21 --- Value: 0.07560081397716746\n",
      "480 --- Value: 0.07848204560373354\n",
      "766 --- Value: 0.0800340982374391\n",
      "690 --- Value: 0.08086779860322886\n",
      "406 --- Value: 0.08173190977544847\n",
      "617 --- Value: 0.08174340105970312\n",
      "344 --- Value: 0.08240404033650314\n",
      "embedding16 --- Value: 0.0827480331356065\n",
      "140 --- Value: 0.08377439073656488\n",
      "embedding70 --- Value: 0.08415398671851426\n",
      "embedding65 --- Value: 0.08648156560329003\n",
      "232 --- Value: 0.08701171032477187\n",
      "286 --- Value: 0.08703116193771936\n",
      "596 --- Value: 0.0871257962507404\n",
      "979 --- Value: 0.08734640407249844\n",
      "4 --- Value: 0.0887990559564912\n",
      "878 --- Value: 0.09103542276366272\n",
      "339 --- Value: 0.09229336835690864\n",
      "302 --- Value: 0.09254023378430738\n",
      "774 --- Value: 0.09261416772822778\n",
      "236 --- Value: 0.0926588194353579\n",
      "356 --- Value: 0.09284932552291153\n",
      "743 --- Value: 0.09284932552291153\n",
      "518 --- Value: 0.09298959820524541\n",
      "862 --- Value: 0.0931169620743386\n",
      "572 --- Value: 0.09320311868427432\n",
      "764 --- Value: 0.09330670929278226\n",
      "121 --- Value: 0.09335249021999092\n",
      "56 --- Value: 0.09432476810479666\n",
      "570 --- Value: 0.09486060943840924\n",
      "679 --- Value: 0.09548214640290165\n",
      "48 --- Value: 0.09551071179994924\n",
      "35 --- Value: 0.0955466054096005\n",
      "embedding1 --- Value: 0.09564884498225437\n",
      "210 --- Value: 0.09666562119589805\n",
      "185 --- Value: 0.09733673713037545\n",
      "87 --- Value: 0.09808189277883234\n",
      "embedding29 --- Value: 0.09841279129904293\n",
      "11 --- Value: 0.09843536544822204\n",
      "688 --- Value: 0.09921176034124318\n",
      "312 --- Value: 0.10018174562413248\n",
      "828 --- Value: 0.10044952041291033\n",
      "embedding57 --- Value: 0.10053616590892211\n",
      "896 --- Value: 0.10054704643911538\n",
      "6 --- Value: 0.10094566924149874\n",
      "652 --- Value: 0.10176075682610354\n",
      "192 --- Value: 0.10277128813818225\n",
      "638 --- Value: 0.10299605439360487\n",
      "181 --- Value: 0.10310590412719033\n",
      "589 --- Value: 0.10358062576325443\n",
      "454 --- Value: 0.10358467632087846\n",
      "526 --- Value: 0.10360755131831106\n",
      "794 --- Value: 0.10456839540310836\n",
      "227 --- Value: 0.10457750513307724\n",
      "773 --- Value: 0.10576949635131049\n",
      "278 --- Value: 0.10632318005683104\n",
      "34 --- Value: 0.10704461266059732\n",
      "984 --- Value: 0.10731601790617927\n",
      "162 --- Value: 0.10898979293836833\n",
      "embedding36 --- Value: 0.10932319169128288\n",
      "808 --- Value: 0.1099115151624932\n",
      "437 --- Value: 0.11027997631802552\n",
      "embedding28 --- Value: 0.11145872518683428\n",
      "81 --- Value: 0.11175768892988916\n",
      "535 --- Value: 0.11192249302135782\n",
      "30 --- Value: 0.11430431369419043\n",
      "119 --- Value: 0.11501632272489595\n",
      "677 --- Value: 0.11501980536989545\n",
      "836 --- Value: 0.11501980536989545\n",
      "267 --- Value: 0.11557239819604612\n",
      "193 --- Value: 0.11615523921218512\n",
      "102 --- Value: 0.11905056360402956\n",
      "228 --- Value: 0.1193276193475035\n",
      "330 --- Value: 0.11932772415209618\n",
      "718 --- Value: 0.11956631614235422\n",
      "574 --- Value: 0.11958113685713147\n",
      "641 --- Value: 0.12021642260257287\n",
      "550 --- Value: 0.12072115165166154\n",
      "206 --- Value: 0.1209397179611298\n",
      "36 --- Value: 0.12217166255883768\n",
      "embedding40 --- Value: 0.12247158022094964\n",
      "845 --- Value: 0.1231078770306323\n",
      "687 --- Value: 0.1233618102258468\n",
      "711 --- Value: 0.12348524582426881\n",
      "177 --- Value: 0.1258540107788839\n",
      "839 --- Value: 0.12604707930544556\n",
      "293 --- Value: 0.12612487451063584\n",
      "183 --- Value: 0.12698840101260303\n",
      "503 --- Value: 0.12764006843057354\n",
      "390 --- Value: 0.1291341639799213\n",
      "407 --- Value: 0.1293625941433925\n",
      "88 --- Value: 0.12946967458499395\n",
      "364 --- Value: 0.13042715014579107\n",
      "160 --- Value: 0.1305567867033437\n",
      "695 --- Value: 0.13263725748861882\n",
      "517 --- Value: 0.132965444721528\n",
      "168 --- Value: 0.1331078699143491\n",
      "779 --- Value: 0.13551710199093586\n",
      "396 --- Value: 0.13561999796934898\n",
      "307 --- Value: 0.13717021004855937\n",
      "54 --- Value: 0.13766532515330132\n",
      "163 --- Value: 0.1378536938209993\n",
      "647 --- Value: 0.13870225032547004\n",
      "983 --- Value: 0.13918209377217516\n",
      "504 --- Value: 0.13936698102510664\n",
      "520 --- Value: 0.1394951916817475\n",
      "807 --- Value: 0.1397648181015889\n",
      "embedding73 --- Value: 0.1398423689152229\n",
      "71 --- Value: 0.14009644801609045\n",
      "322 --- Value: 0.14023142019518167\n",
      "94 --- Value: 0.14058877988663412\n",
      "479 --- Value: 0.14085615707119573\n",
      "155 --- Value: 0.1423340485350432\n",
      "embedding56 --- Value: 0.14277815136982233\n",
      "math_sim_words --- Value: 0.14280085100761475\n",
      "568 --- Value: 0.1429668710292971\n",
      "404 --- Value: 0.1442057005922674\n",
      "870 --- Value: 0.1450329355710359\n",
      "951 --- Value: 0.14537968771904602\n",
      "991 --- Value: 0.14590433766145172\n",
      "375 --- Value: 0.14628716405004866\n",
      "embedding8 --- Value: 0.14858637403679772\n",
      "939 --- Value: 0.14913541511469594\n",
      "692 --- Value: 0.14947667374335352\n",
      "942 --- Value: 0.14999063322855308\n",
      "126 --- Value: 0.15010975899197243\n",
      "531 --- Value: 0.15135106721526245\n",
      "317 --- Value: 0.15183407305421184\n",
      "936 --- Value: 0.15261611880929368\n",
      "3 --- Value: 0.15402465600917373\n",
      "859 --- Value: 0.1545319783558969\n",
      "106 --- Value: 0.1554383625097994\n",
      "embedding26 --- Value: 0.155736612329869\n",
      "735 --- Value: 0.1560164778155026\n",
      "embedding53 --- Value: 0.15625425047348768\n",
      "embedding21 --- Value: 0.15722300700418995\n",
      "252 --- Value: 0.15762259024532468\n",
      "79 --- Value: 0.1605726786115689\n",
      "492 --- Value: 0.16193601633619595\n",
      "393 --- Value: 0.16270648423583087\n",
      "embedding34 --- Value: 0.16365602343314506\n",
      "838 --- Value: 0.16404013633615236\n",
      "507 --- Value: 0.1652566551054652\n",
      "43 --- Value: 0.16581391248272898\n",
      "869 --- Value: 0.16708029250869413\n",
      "960 --- Value: 0.16769312328009192\n",
      "219 --- Value: 0.16983195705693832\n",
      "946 --- Value: 0.1713649696371946\n",
      "854 --- Value: 0.1727316242560611\n",
      "884 --- Value: 0.17281135924845809\n",
      "embedding69 --- Value: 0.1728583644139173\n",
      "899 --- Value: 0.1740104223800681\n",
      "661 --- Value: 0.1744646801078461\n",
      "embedding63 --- Value: 0.1745846834869938\n",
      "650 --- Value: 0.17527080401945047\n",
      "755 --- Value: 0.17527080401945047\n",
      "149 --- Value: 0.17576001694696125\n",
      "818 --- Value: 0.17767423690323983\n",
      "embedding84 --- Value: 0.17805211725608538\n",
      "784 --- Value: 0.17837113879639985\n",
      "306 --- Value: 0.1785381362762835\n",
      "313 --- Value: 0.1785728502891595\n",
      "590 --- Value: 0.1785728502891595\n",
      "113 --- Value: 0.18049184416386635\n",
      "602 --- Value: 0.18049184416386635\n",
      "44 --- Value: 0.18081204906352172\n",
      "542 --- Value: 0.18142918831831317\n",
      "748 --- Value: 0.18203246334077872\n",
      "238 --- Value: 0.18285810676553468\n",
      "708 --- Value: 0.18336586682523984\n",
      "416 --- Value: 0.18352189633152835\n",
      "833 --- Value: 0.1842683390444626\n",
      "932 --- Value: 0.1855904152113602\n",
      "509 --- Value: 0.18624727157570023\n",
      "157 --- Value: 0.18718192504573544\n",
      "354 --- Value: 0.19111261536527732\n",
      "806 --- Value: 0.19178761112627474\n",
      "783 --- Value: 0.1922214918315463\n",
      "713 --- Value: 0.19241443213725337\n",
      "embedding6 --- Value: 0.19319772012239342\n",
      "648 --- Value: 0.1956456999502334\n",
      "137 --- Value: 0.1956853784056663\n",
      "578 --- Value: 0.19772052858343334\n",
      "709 --- Value: 0.1989397886788759\n",
      "689 --- Value: 0.19923963149091978\n",
      "80 --- Value: 0.2017461189007141\n",
      "607 --- Value: 0.20432967812046723\n",
      "353 --- Value: 0.2059219289960881\n",
      "791 --- Value: 0.20734943088713736\n",
      "277 --- Value: 0.20840534134074362\n",
      "embedding47 --- Value: 0.21044197772564907\n",
      "777 --- Value: 0.21500549723151038\n",
      "321 --- Value: 0.2162301734354441\n",
      "668 --- Value: 0.2219834528850566\n",
      "620 --- Value: 0.22264435203145383\n",
      "338 --- Value: 0.2235689619907757\n",
      "289 --- Value: 0.22529060380332372\n",
      "817 --- Value: 0.23272506646420224\n",
      "875 --- Value: 0.23517439387273287\n",
      "832 --- Value: 0.23603503559820424\n",
      "109 --- Value: 0.23610222040122933\n",
      "190 --- Value: 0.23761713745987112\n",
      "710 --- Value: 0.2394842624765033\n",
      "embedding23 --- Value: 0.24028549836173355\n",
      "embedding10 --- Value: 0.2423219799001569\n",
      "embedding38 --- Value: 0.24533389292594399\n",
      "235 --- Value: 0.24594639001281857\n",
      "10 --- Value: 0.24993316799051737\n",
      "66 --- Value: 0.2541700971000996\n",
      "embedding13 --- Value: 0.2553471485307923\n",
      "827 --- Value: 0.25852355094322504\n",
      "embedding68 --- Value: 0.25854193480712945\n",
      "576 --- Value: 0.27246762327579094\n",
      "226 --- Value: 0.2746026724391161\n",
      "831 --- Value: 0.27511841481174854\n",
      "723 --- Value: 0.2775859983169045\n",
      "chem_sim_words --- Value: 0.2816381288063469\n",
      "209 --- Value: 0.28680780114579213\n",
      "349 --- Value: 0.28933803910319095\n",
      "embedding46 --- Value: 0.289518300826448\n",
      "299 --- Value: 0.2986314621450271\n",
      "embedding41 --- Value: 0.29946462057797296\n",
      "135 --- Value: 0.2995907124553204\n",
      "273 --- Value: 0.30371264630230227\n",
      "729 --- Value: 0.30534022217794204\n",
      "803 --- Value: 0.30725873541671717\n",
      "499 --- Value: 0.308718917741945\n",
      "844 --- Value: 0.31075466544360464\n",
      "858 --- Value: 0.3124419365392779\n",
      "533 --- Value: 0.3189271187987127\n",
      "embedding24 --- Value: 0.3225811823553475\n",
      "544 --- Value: 0.32409760282381067\n",
      "64 --- Value: 0.32784667932184275\n",
      "63 --- Value: 0.3332805659035791\n",
      "460 --- Value: 0.33566477597803895\n",
      "embedding27 --- Value: 0.3357881962278326\n",
      "297 --- Value: 0.3440126967269672\n",
      "embedding82 --- Value: 0.34906139170987416\n",
      "embedding11 --- Value: 0.3490884594338013\n",
      "765 --- Value: 0.3511315994156417\n",
      "588 --- Value: 0.35478396659298195\n",
      "628 --- Value: 0.35481350089052865\n",
      "embedding54 --- Value: 0.35488001860216384\n",
      "855 --- Value: 0.35953142848683156\n",
      "761 --- Value: 0.368975396328178\n",
      "polarity --- Value: 0.37073983905961533\n",
      "embedding86 --- Value: 0.3779913820269303\n",
      "741 --- Value: 0.3781900768507409\n",
      "275 --- Value: 0.39171955306114714\n",
      "645 --- Value: 0.3924875331398883\n",
      "417 --- Value: 0.3959287796243723\n",
      "embedding25 --- Value: 0.3980606041110362\n",
      "embedding37 --- Value: 0.4083847657074524\n",
      "680 --- Value: 0.41087746304262357\n",
      "embedding15 --- Value: 0.41277896469790154\n",
      "embedding78 --- Value: 0.41621056004190654\n",
      "467 --- Value: 0.4323490306607848\n",
      "embedding92 --- Value: 0.43850621551524505\n",
      "963 --- Value: 0.44828325894616916\n",
      "812 --- Value: 0.4488642093079389\n",
      "615 --- Value: 0.4515028748401806\n",
      "embedding30 --- Value: 0.483400159400794\n",
      "733 --- Value: 0.5031249694576541\n",
      "embedding48 --- Value: 0.510617964765127\n",
      "643 --- Value: 0.5515015170369844\n",
      "229 --- Value: 0.5550983110243993\n",
      "embedding62 --- Value: 0.6217341227254487\n",
      "embedding99 --- Value: 0.6378478577294062\n",
      "medical_terms --- Value: 0.6414114628373009\n",
      "embedding22 --- Value: 0.6690136772469671\n",
      "embedding39 --- Value: 0.692128338846132\n",
      "701 --- Value: 0.7017931691493872\n",
      "865 --- Value: 0.712464628324808\n",
      "880 --- Value: 0.7195159999336109\n",
      "569 --- Value: 0.7205884762675324\n",
      "embedding66 --- Value: 0.7566482372110896\n",
      "961 --- Value: 0.7593800385678305\n",
      "301 --- Value: 0.8553642960897487\n",
      "864 --- Value: 0.8809336816652312\n",
      "515 --- Value: 0.9034060981982939\n",
      "959 --- Value: 1.1237021894697232\n"
     ]
    }
   ],
   "source": [
    "NameOfVariables = X_train_model_5.columns.values\n",
    "Var_coef = zip(lr_model_5.coef_[0,:],NameOfVariables)\n",
    "sorted_coef_val = sorted(Var_coef)\n",
    "for item in sorted_coef_val:\n",
    "    print(item[1],'--- Value:',item[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6: Without STEM Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_6 = X_train_final.iloc[:,np.r_[10:1113]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3763, 1103)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_6 = X_test_final.iloc[:,np.r_[10:1113]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(941, 1103)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.410\n",
      "Best parameters set:\n",
      "\tclf__C: 1\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.77      0.84       823\n",
      "           1       0.27      0.62      0.38       118\n",
      "\n",
      "    accuracy                           0.75       941\n",
      "   macro avg       0.60      0.69      0.61       941\n",
      "weighted avg       0.85      0.75      0.78       941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_6_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_6_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_6,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_6), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 630, False Positive: 193, False Negative: 45, True Positive: 73\n",
      "--------------------------------------------------------------------------------\n",
      "[[630 193]\n",
      " [ 45  73]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.77      0.84       823\n",
      "           1       0.27      0.62      0.38       118\n",
      "\n",
      "    accuracy                           0.75       941\n",
      "   macro avg       0.60      0.69      0.61       941\n",
      "weighted avg       0.85      0.75      0.78       941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_6 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_6, y_train)\n",
    "y_lr = lr_model_6.predict(X_test_model_6)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | Precision | Recall | F1 | Data Points |\n",
    "| --- | --- |--- | --- | --- |\n",
    "| Class 0 | 0.97 | 0.87 | 0.92 | 117 |\n",
    "| Class 1 |0.59 | 0.88 | 0.71 | 25 |\n",
    "| Macro Average Score | 0.78 | 0.88 | 0.81 | 142|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7: Without Sentiment Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_7 = X_train_final.iloc[:,np.r_[3:10,12:1113]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3763, 1108)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_7 = X_test_final.iloc[:,np.r_[3:10,12:1113]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(941, 1108)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.410\n",
      "Best parameters set:\n",
      "\tclf__C: 1\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.76      0.84       823\n",
      "           1       0.26      0.60      0.37       118\n",
      "\n",
      "    accuracy                           0.74       941\n",
      "   macro avg       0.60      0.68      0.60       941\n",
      "weighted avg       0.85      0.74      0.78       941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_7_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_7_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_7,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_7), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 624, False Positive: 199, False Negative: 47, True Positive: 71\n",
      "--------------------------------------------------------------------------------\n",
      "[[624 199]\n",
      " [ 47  71]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.76      0.84       823\n",
      "           1       0.26      0.60      0.37       118\n",
      "\n",
      "    accuracy                           0.74       941\n",
      "   macro avg       0.60      0.68      0.60       941\n",
      "weighted avg       0.85      0.74      0.78       941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_7 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_7, y_train)\n",
    "y_lr = lr_model_7.predict(X_test_model_7)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | Precision | Recall | F1 | Data Points |\n",
    "| --- | --- |--- | --- | --- |\n",
    "| Class 0 | 0.94 | 0.88 | 0.91 | 117 |\n",
    "| Class 1 |0.58 | 0.76 | 0.66 | 25 |\n",
    "| Macro Average Score | 0.76 | 0.82 | 0.78 | 142|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 8: Without NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_8 = X_train_final.iloc[:,np.r_[3:12,13:1113]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3763, 1109)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_8 = X_test_final.iloc[:,np.r_[3:12,13:1113]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(941, 1109)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.418\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.73      0.82       823\n",
      "           1       0.26      0.66      0.37       118\n",
      "\n",
      "    accuracy                           0.72       941\n",
      "   macro avg       0.60      0.70      0.60       941\n",
      "weighted avg       0.85      0.72      0.77       941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_8_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_8_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_8,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_8), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 602, False Positive: 221, False Negative: 40, True Positive: 78\n",
      "--------------------------------------------------------------------------------\n",
      "[[602 221]\n",
      " [ 40  78]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.73      0.82       823\n",
      "           1       0.26      0.66      0.37       118\n",
      "\n",
      "    accuracy                           0.72       941\n",
      "   macro avg       0.60      0.70      0.60       941\n",
      "weighted avg       0.85      0.72      0.77       941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_8 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_8, y_train)\n",
    "y_lr = lr_model_8.predict(X_test_model_8)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | Precision | Recall | F1 | Data Points |\n",
    "| --- | --- |--- | --- | --- |\n",
    "| Class 0 | 0.96 | 0.86 | 0.91 | 117 |\n",
    "| Class 1 |0.57 | 0.84 | 0.68 | 25 |\n",
    "| Macro Average Score | 0.76 | 0.85 | 0.79 | 142|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Summary\n",
    "\n",
    "| S.No|Features| Precision | Recall | F1 |\n",
    "| :-:| :-- | :-: |:-: | :-: |\n",
    "| 1 |Embeddings + POS Tag Count + Sentiment (Polarity and Subjectivity) + STEM Similarity + NER| 0.76 | 0.89 | 0.79 |\n",
    "| 2 |Unigrams + POS Tag Count + Sentiment (Polarity and Subjectivity) + STEM Similarity + NER  |0.76 | 0.80 | 0.78 |\n",
    "| 3 |Embeddings + Unigrams + Sentiment (Polarity and Subjectivity) + STEM Similarity + NER | 0.76 | 0.85 | 0.79 |\n",
    "| 4 |**Embeddings + Unigrams + Sentiment (Polarity and Subjectivity) + NER + POS Tag Count** | **0.78** | **0.88** | **0.81**|\n",
    "| 5 |Embeddings + Unigrams + STEM Similarity + NER + POS Tag Count | 0.76 | 0.82 | 0.78|\n",
    "| 6 |Embeddings + Unigrams + STEM Similarity + POS Tag Count + Sentiment (Polarity and Subjectivity) | 0.76 | 0.85 | 0.79|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
