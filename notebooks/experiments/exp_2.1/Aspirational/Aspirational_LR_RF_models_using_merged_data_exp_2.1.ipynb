{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aspirational Logistic Regression and Random Forest Models Using Merged Data Experiment 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229707282db042b39e854b574ebe8b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 23:44:37 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-10-15 23:44:37 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-10-15 23:44:38 INFO: File exists: /Users/gbaldonado/stanza_resources/en/default.zip\n",
      "2024-10-15 23:44:41 INFO: Finished downloading models and saved to /Users/gbaldonado/stanza_resources\n",
      "2024-10-15 23:44:41 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4d29fe3fe84399bc3412a6a4c64473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 23:44:41 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-10-15 23:44:42 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-10-15 23:44:42 INFO: Using device: cpu\n",
      "2024-10-15 23:44:42 INFO: Loading: tokenize\n",
      "2024-10-15 23:44:42 INFO: Loading: mwt\n",
      "2024-10-15 23:44:42 INFO: Loading: pos\n",
      "2024-10-15 23:44:42 INFO: Loading: lemma\n",
      "2024-10-15 23:44:42 INFO: Loading: constituency\n",
      "2024-10-15 23:44:43 INFO: Loading: depparse\n",
      "2024-10-15 23:44:43 INFO: Loading: sentiment\n",
      "2024-10-15 23:44:43 INFO: Loading: ner\n",
      "2024-10-15 23:44:43 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import pickle\n",
    "import warnings\n",
    "import stanza\n",
    "\n",
    "from random import shuffle\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, f1_score, r2_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Set random seed\n",
    "random.seed(18)\n",
    "seed = 18\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Initialize lemmatizer, stop words, and stanza\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline('en') # initialize English neural pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data and quick exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and test sets loaded.\n"
     ]
    }
   ],
   "source": [
    "merged_aspirational_df_batch_1 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/merged_Aspirational_sentence_level_batch_1_jaccard.csv\", encoding='utf-8')\n",
    "merged_aspirational_df_batch_2 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/Aspirational Plus_sentence_level_batch_2_jaccard.csv\", encoding='utf-8')\n",
    "\n",
    "merged_aspirational_df = pd.concat([merged_aspirational_df_batch_1, merged_aspirational_df_batch_2])\n",
    "\n",
    "# Shuffle the merged dataset\n",
    "merged_aspirational_df = shuffle(merged_aspirational_df, random_state=seed)\n",
    "\n",
    "\n",
    "# Function for undersampling or oversampling\n",
    "def resample_data(X, y, strategy='oversample', random_state=seed):\n",
    "    \"\"\"\n",
    "    Resample the data using either undersampling or oversampling.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Features\n",
    "    - y: Labels\n",
    "    - strategy: 'oversample' or 'undersample'\n",
    "    - random_state: Seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    - X_resampled, y_resampled: Resampled data and labels\n",
    "    \"\"\"\n",
    "    if strategy == 'oversample':\n",
    "        sampler = RandomOverSampler(random_state=random_state)\n",
    "    elif strategy == 'undersample':\n",
    "        sampler = RandomUnderSampler(random_state=random_state)\n",
    "    else:\n",
    "        raise ValueError(\"Strategy must be 'oversample' or 'undersample'\")\n",
    "\n",
    "    X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Separate features and labels\n",
    "X = merged_aspirational_df.drop(columns=['label'])  # Replace 'label' with your target column name\n",
    "y = merged_aspirational_df['label']\n",
    "\n",
    "# Toggle resampling\n",
    "resample = True  # Set this to False to turn off resampling\n",
    "\n",
    "if resample:\n",
    "    # Apply resampling (choose 'oversample' or 'undersample')\n",
    "    X_resampled, y_resampled = resample_data(X, y, strategy='undersample', random_state=seed)\n",
    "\n",
    "    # Combine resampled data into a single DataFrame\n",
    "    resampled_df = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "else:\n",
    "    # No resampling, use original dataset\n",
    "    resampled_df = merged_aspirational_df\n",
    "\n",
    "# Train-test split\n",
    "training_df, test_df = train_test_split(resampled_df, test_size=0.1, random_state=18, stratify=resampled_df['label'])\n",
    "\n",
    "\n",
    "# # Train-test split\n",
    "training_df, test_df = train_test_split(resampled_df, test_size=0.1, random_state=18, stratify=resampled_df['label'])\n",
    "\n",
    "training_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Training and test sets loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (1609, 3) \n",
      "Test dataset shape: (179, 3)\n",
      "Positive labels present in the dataset : 805  out of 1609 or 50.03107520198882%\n",
      "Positive labels present in the test dataset : 89  out of 179 or 49.72067039106145%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset shape: {training_df.shape} \\nTest dataset shape: {test_df.shape}\")\n",
    "pos_labels = len([n for n in training_df['label'] if n==1])\n",
    "print(\"Positive labels present in the dataset : {}  out of {} or {}%\".format(pos_labels, len(training_df['label']), (pos_labels/len(training_df['label']))*100))\n",
    "pos_labels = len([n for n in test_df['label'] if n==1])\n",
    "print(\"Positive labels present in the test dataset : {}  out of {} or {}%\".format(pos_labels, len(test_df['label']), (pos_labels/len(test_df['label']))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABWcAAAJICAYAAAANc1ZxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAA9hAAAPYQGoP6dpAABRxklEQVR4nO3de5hWdb03/jc4MDDDcBpMNBAx0kLoJEJmqeAxUvNUuWUiRDsYUWK5H0kNtRR19xSxhV8H2co2UjeStTNRUUGTCDrSbqi2mQkpqCgw0BggrN8fXc7TCMhxWKCv13WtK2d9v2utz7pn5ubTe9a9VquiKIoAAAAAALBbtS67AAAAAACA1yPhLAAAAABACYSzAAAAAAAlEM4CAAAAAJRAOAsAAAAAUALhLAAAAABACYSzAAAAAAAlEM4CAAAAAJRAOAvslKIoyi5hj6iBXcf3c/t4vQCg5e0J/97uCTWw67TU93N3/Zz4eYRdRzgLr2HHHntsWrVq1bS0bt06NTU1Ofzww/Pv//7v2bBhQ7P5Bx10UEaMGLHN+//v//7vfOxjH9vqvBEjRuSggw7a4eNsydq1a3PxxRfne9/73haPtSe49NJLU1tbm+rq6vznf/7nJuNz5sxJq1atMmfOnG3e545ssyXHHntsjj322B3e/i9/+UtatWqVW265Zafq+Otf/5pTTjklTz755E7t52WtWrXKlVde2eLblGnRokU56qijyi4DAEql590z6Hm3za7ueV+2cuXKfOxjH8tPfvKTXbrfzdGDwq4lnIXXuHe+852ZN29e5s2bl5/85Cf53ve+l4EDB+aiiy7Kueee2+wvnnfddVeuuOKKbd731772tSxevHir86644orcddddO1T/q1m6dGm+/vWvZ/369S1+rB31u9/9Ltdff33OOuus3HvvvXn/+99fdkm73P7775958+blAx/4wE7t54EHHsiPf/zjXVRVMm/evFxwwQUtvk2Z/uu//ivz5s0ruwwAKJ2et1x63m23q3vel/3mN7/Jf/7nf2bjxo27fN+vpAeFXaui7AKAltWxY8e8+93vbrbu1FNPzSGHHJKLL744p512WoYNG5bkH01tS3jTm97UIvst+1jb4vnnn0+S/Mu//Eve9773lVxNy6isrNzkZ2xPsCM17YnnAQBsnZ63XHpegB3nyll4nfrsZz+bAw44IN/85jeb1r3yo1d33HFH3v72t6d9+/bZd999U1dXl6VLlyb5x8eCHn744Tz88MNNHzV6+WNH3/rWt9KrV6/st99+uf/++zf7sav169fns5/9bLp06ZIuXbrkYx/7WJ577rmm8c1t888fJfrLX/6S3r17J0nOO++8prmv3G7Dhg2ZPHly+vfvn/bt2+fAAw/MpZdemr///e/NjnX88cfn5ptvziGHHJLKysq8/e1vzz333LPV1/GOO+7IgAED0qFDh3Tv3j2f+tSnsmLFiiTJlVde2fTRqSFDhmzXR89+8IMf5H3ve19qampSWVmZt7zlLbnxxhs3mbdo0aK8733vS7t27dKnT5/8+7//e7PxjRs35rrrrkufPn1SWVmZQw45ZJM5r/TAAw/kyCOPTIcOHdKlS5ecfvrp+eMf/7jF+a/8iNctt9ySioqKzJ8/P0ceeWTatWuXAw88MDfccMMW93HLLbfkvPPOS5L07t276efwoIMOypgxY3LcccelY8eO+dSnPpUk+e1vf5szzzwz++67b9q0aZM3vvGN+exnP5sXX3yxaZ//fIuCl382H3zwwZx44ompqqrKfvvtl0suuSQvvfTSTm2zevXqfPKTn8wb3vCGdOjQIeecc04mTJiQVq1averr/Gq/Xy+76aabcthhh6WysjIHHnhgrrzyyqZjX3nllbnqqqs2qRsA+H/0vHreLdmTet7k1fu+JFm+fHnq6urSvXv3tGvXLu94xzty6623JvlH3zp48OAkyeDBg1/19g16UNgDFcBr1jHHHFMcc8wxWxz/6Ec/WrRp06ZYv359URRF0atXr+JjH/tYURRF8eijjxb77LNPcdVVVxWzZ88ubr311qJ79+5N+6uvry/e+c53Fu985zuLefPmFatWrSpmz55dJCm6du1aTJ8+vbj11luLhoaG4mMf+1jRq1evpuP26tWr2GeffYojjzyy+OEPf1h85zvfKWpra4v3vOc9TXNeuU1RFMUTTzxRJCluvvnm4u9//3vx/e9/v0hSXH755cWvfvWrzW53/vnnFxUVFcVll11W3H///cX1119fVFVVFSeeeGKxcePGpm06depUvPWtby1uu+224p577ikOP/zwon379sULL7ywxdfvy1/+cpGk+PSnP13ce++9xeTJk4va2tribW97W9HY2FgsWbKkmDRpUpGkmDRpUlONr/Ty6zZ79uyiKIri7rvvLpIUn/vc54oHH3yw+NGPflScdNJJRZJi7ty5zbZp06ZN8YUvfKG49957i1GjRhVJim9/+9tN+/7EJz5RtGnTphg3blxx3333FV/84heL1q1bF1dffXXTnH/+OXn88ceL9u3bF6NGjSoeeuih4s477ywOPfTQ4uCDDy42bNiw2fr/+ftSFEVx8803F61atSoOPPDAYsKECcWDDz5YnHvuuUWS4t57793sPp599tni8ssvL5IU3//+94s//elPRVH842eloqKiuOiii4r777+/ePTRR4unn3666NixY3HiiScWd999dzFr1qzioosuKpIU11xzTdM+kxTjxo1r9nrtt99+xdVXX108+OCDxZgxY4okxTe/+c2d2mbIkCFF586di8mTJxd33313MXTo0KKysrJ4tX9it/b7VRRFce211xatWrUqPvvZzxb33Xdfcf311xft2rUrRo4cWRRFUSxZsqQ4//zziyTFvHnziiVLlmzxeADwWqbn1fPu7T3v1vq+oiiKE088sXjHO95R3HXXXcWDDz5YjBgxoun1XLVqVbPvQX19/WaPrweFPZNwFl7DttaoXnLJJUWSYtmyZUVRNG9Ux48fX3To0KF48cUXm+bfc889xZVXXtnU4L1y/y83T5dddlmz42yuUe3WrVvR0NDQtO4HP/hBkaS47777NrtNUWzaEL3y61duV19fXyQpvvKVrzTbz6233lokKe65556mbZI0NUdFURQPP/xwkaS48847N/vavfDCC0VlZWVxwQUXNFv/yCOPFEmKyZMnN3tNXm5CN+eVc2644YZi+PDhzeY8//zzRZLi2muvbbbNJz/5yWbzTj/99KJHjx7Fhg0bij/+8Y9Fq1atiuuuu67ZnMsvv7xo165dsXz58qIomn8fb7vttiJJ8de//rVp/vz584svfvGLxapVqzZb/+Ya1STFTTfd1DTn73//e9GuXbviM5/5zBZfh5e3e+KJJ5rW9erVqzjwwAObNcn33XdfcfTRR29ST//+/YsTTzyx6evNBa2XX355s2169+5dnHLKKTu8zYMPPlgkKWbMmNE0vmHDhqJv376vGs5u7fdr5cqVRVVVVfGpT32q2XY33XRTkaT43e9+VxRFUYwbN+5VjwMArwd6Xj3v3tzzbmvfV1lZ2ex7vGHDhuLzn/988ZOf/KTZa/Vq3wM9KOyZ3NYA2OzHr4855pg0Njamf//+ueyyyzJ37tyceOKJGTdu3FY/rt2/f/+tHnPo0KGpqalp+vrUU09NmzZt8sADD2z/CWzBww8/nCRN9xd72TnnnJN99tkns2fPblq37777Nrt3V48ePZIkf/vb3za775/97GdZu3btJvt+3/vel169ejXb9/a65JJLMnXq1Pztb3/LwoULM3369Fx33XVJknXr1jWb+5GPfKTZ12eeeWb++te/5g9/+EMeeuihFEWRU089NS+99FLTctppp+Xvf//7Zp/k+u53vzvt2rXLwIEDc/HFF+eBBx7IO97xjlxzzTXp2LHjdp3HkUce2fTflZWV2Xfffbf4er6avn37pnXr//fP1YknnpiHH3447du3z//+7//m7rvvzrXXXptnn312k9fn1WpK/vF93lpNr7bNQw89lDZt2uT0009vGm/dunU+/OEPv+o+t/b7NW/evDQ2Nua0005r9r079dRTkySzZs161f0DAJvS8+p5X7Yn9bzb2vcNHjw448aNy4c//OHccsstee655/LVr341733ve7f5WHpQ2DMJZ+F17Kmnnkr79u1TW1u7ydiRRx6Ze+65JwcffHDTP/o9evTIN77xja3ud7/99tvqnO7duzf7unXr1qmtrW26d9Wu8MILL2z2WBUVFenWrVtWrlzZtK6qqmqTepJs8WmnW9r3y+v+ed/ba/ny5TnrrLPSsWPHHH744fnSl77U9LoU//Sk4c0d/w1veEOSZMWKFU0PZjjssMPSpk2bpmXgwIFJkqeffnqTYx900EF5+OGHM2jQoHz729/OCSeckP322y+XXXbZdj/5dXOv6Y48PfaVP08bN27MpZdemq5du+bQQw/Npz/96fzqV79K+/btN3l9dkVNr7bNc889l9ra2mbhcbL5n4t/trXfr5e/d0OHDm32vXv5tdjc9w4A2Dw978qmdXref9iTet5t7ftuv/32fP7zn8+CBQty3nnn5YADDsjJJ5+cJ554YpuPpQeFPVNF2QUA5diwYUPmzJmTo446Kvvss89m55x00kk56aST0tjYmIceeijf+MY3ctFFF+Xd7353Bg0atFPHf2VDumHDhixfvryp0WrVqlU2bNjQbM6aNWu26xhdu3ZNkixbtqzZgwnWr1+f5cuXp1u3bjtQ+ab7fstb3tJsbOnSpTn44IN3eN/nnntufv/73+eBBx7Ie97znlRWVqaxsTE33XTTJnNf+TouW7YsyT8a1s6dOyf5x9Wd/3zFxssOPPDAzR5/4MCB+f73v59169bl0Ucfzbe+9a1ce+21edvb3rbJVQtluO666/K1r30t3/zmN3PWWWelU6dOSdLUgO9OPXr0yPLly7Nx48ZmAe2zzz671W1f7ffr5e/dtGnTcsghh2yy7bb8n0EAQM+r593ze95t7fs6deqU66+/Ptdff33++Mc/5oc//GGuvvrqfPrTn87MmTO3+Xh6UNjzuHIWXqe++c1v5umnn86FF1642fEvfOELGThwYIqiSFVVVU455ZR89atfTZIsWbIkSbbY4G6LBx54oNnTR++888689NJLTU8Z7dixY5YvX97sCbNz585tto+tHf+YY45J8o/m4p/dfvvt2bBhw3Z9BOiVBg0alMrKyk32/eijj2bx4sU7te9HH300Z599dgYPHpzKysokaWq4XvlX+HvvvbfZ17fffnt69uyZPn36NJ3/8uXLM2DAgKbl+eefz+WXX970l/F/NmHChBx00EFZu3Zt2rZtmyFDhuTb3/52kv/3fW8p2/rz9Oijj+awww7LyJEjm4LZp556Kv/zP/+zQ1fm7oxjjjkmL730Un70ox81W3/XXXe96nZb+/1697vfnbZt2+app55q9r1r27ZtLr300qYrJHbmdxAAXg/0vHrePb3n3Za+78knn0zPnj1z5513JkkOPfTQ/Ou//mtOOOGE7fo51YPCnsmVs/Aa19DQkJ/97GdJ/tHkLF++PPfdd1++9a1vpa6uLmeeeeZmtzv++OPzta99LSNGjEhdXV3WrVuXG264IV27ds2QIUOS/OOvvPPmzctDDz2Ud77zndtV17Jly3LWWWdl9OjReeyxxzJ27NiccMIJOe6445Ikp5xySiZOnJiRI0fm4x//eH73u9/lq1/9arNG4OVg7sEHH8xb3/rWTa5s6Nu3bz72sY/lyiuvzIsvvphjjz02v/nNb3LllVdm8ODBOfnkk7er5n/WtWvXXHrppbnqqqvStm3bfPCDH8wTTzyRK664In379s2IESN2eN8DBw7MtGnTcvjhh6dHjx756U9/mmuvvTatWrXa5P5VEydOTE1NTd75znfm9ttvz7333ptbb701rVq1Sr9+/VJXV5ePf/zj+ctf/pIBAwbkj3/8Y774xS+md+/em/1r+JAhQ/J//s//yRlnnJHPfOYzqaioyDe/+c1UVlY23Wuqpbz8l/rvf//7GTp06CZXZ7xs4MCB+fKXv5zrrrsuRx55ZP70pz/l2muvzdq1a3fonrY74+ijj84JJ5yQkSNH5tprr02vXr0yZcqULFy48FXvU7e136+uXbvmX//1X3PFFVekoaEhxx57bJ566qlcccUVadWqVd7+9rcn+X+v2W233ZZ3v/vd6d279+44bQDY4+h59bx7c8+7tb6vU6dO6dGjRz772c+moaEhb3rTm/KLX/wi99xzT8aOHdtsvz/+8Y/TpUuXpn7xn+lBYQ9V2qPIgBZ3zDHHFEmaltatWxfdu3cvjj322OK73/1u0xNoX/bPT64tiqL43ve+V7zrXe8qOnToUNTU1BTvf//7i9/+9rdN4w899FBx4IEHFm3bti2mTZu2xSeEbu7JtZ/73OeKj3/840WHDh2Krl27Fp/+9KeLNWvWNNvuq1/9anHggQcWlZWVxXve857il7/8ZVFZWdnsSbUXX3xxUV1dXXTu3LlYu3btJsd66aWXiq985SvFwQcfXLRp06Y46KCDirFjxzZ7Qum2PCV3S/6//+//K/r27Vu0bdu22H///YtPf/rTxQsvvNA0viNPrv3LX/5SnHLKKUWnTp2KTp06FUcccUTx3e9+tzj55JOLI444otk2t99+e3HEEUcUbdu2Ld7ylrcUt912W7N9r1+/vrj66qubzr9Hjx7FhRdeWDz//PNNc175BOL77ruvOOqoo4qOHTsWVVVVxdFHH108/PDDW6x/S0+uffkJtC975c/XK61evbo4/vjji7Zt2xZDhw7d4jZ///vfi1GjRhXdu3cv2rdvXxx66KHFuHHjiquuuqqorKxsev2TFOPGjdvsa7ylc9+RbV544YVixIgRRefOnYvq6upi2LBhxahRo4qampotnmtRbP33qyiKYtKkSU0/X/vtt18xbNiw4sknn2waf+qpp4ojjjiiaNOmTXHhhRe+6vEA4LVKz6vn3dt73qLYet+3dOnSYsSIEcUBBxxQtG3btnjTm95UXHPNNcWGDRuKoiiKDRs2FP/yL/9StGvXrjjssMO2eHw9KOx5WhXFVp6eAgBs1pNPPpl58+blgx/8YNq3b9+0/kMf+lAef/zx/OpXvyqxOgAAAPZ0bmsAADuodevWGTFiRD74wQ/m/PPPT0VFRe65557MmDEjN998c9nlAQAAsIdz5SwA7ITZs2fn6quvzq9//eusX78+ffv2zcUXX5x/+Zd/Kbs0AAAA9nDCWQAAAACAErQuuwAAAAAAgNcj4SwAAAAAQAmEswAAAAAAJagou4DdbePGjXn66adTU1OTVq1alV0OAADbqCiKrF69OgcccEBat359XGOgdwUA2Dtta+/6ugtnn3766fTs2bPsMgAA2EFLlixJjx49yi5jt9C7AgDs3bbWu77uwtmampok/3hhOnbsWHI1AABsq4aGhvTs2bOpn3s90LsCAOydtrV3fd2Fsy9/HKxjx44aXACAvdDr6eP9elcAgL3b1nrX18fNugAAAAAA9jDCWQAAAACAEghnAQAAAABKIJwFAAAAACiBcBYAAAAAoATCWQAAAACAEghnAQAAAABKIJwFAAAAACiBcBYAAAAAoATCWQAAAACAEghnAQAAAABKIJwFAAAAACiBcBYAAAAAoATCWQAAAACAEghnAQAAAABKIJwFAAAAACiBcBYAAAAAoATCWQAAAACAEghnAQAAAABKIJwFAAAAAChBKeHsr371qxx99NHp3Llz9t9//3zuc5/L2rVrkyTz58/PoEGD0qFDh/Tu3TtTpkxptu3UqVPTp0+fVFdXZ8CAAZk3b14ZpwAAAAAAsFMqdvcBN27cmFNOOSWXXnpp5syZk6effjrHH398unXrls985jMZOnRorr766nzyk5/MI488ktNPPz39+/fPwIEDM2fOnIwePTozZ87MwIEDc+ONN+a0007Lk08+maqqqt19KgC8wuGX/GfZJQC7wS//bXjZJbCbbNi4Mfu09mE7eK3zuw5Qnt0ezq5YsSJLly7Nxo0bUxRFkqR169apqqrKjBkzUltbm1GjRiVJhgwZkmHDhmXSpEkZOHBgbrrpppxzzjk56qijkiRjxozJt7/97dxxxx0577zzdvep7BDBBbz2CS0AeK3Yp3XrXP69n+SJZ1eVXQrQQnq/oVO+cu77yi4D4HVrt4eztbW1GTNmTD7/+c/nC1/4QjZs2JAPfvCDTev69+/fbH7fvn2bbm1QX1+fkSNHbjK+cOHCLR5v7dq1TbdMSJKGhoZdeDYAAPDa9sSzq/KHp14ouwwAgNek3f65hY0bN6Z9+/a58cYb87e//S2/+93vsmjRoowbNy6rV69OdXV1s/lVVVVZs2ZNkmx1fHPGjx+fTp06NS09e/bc9ScFAAAAALCddns4e9ddd2XGjBm58MILU1lZmcMOOyzjxo3L5MmTU11dncbGxmbzGxsbU1NTkyRbHd+csWPHZtWqVU3LkiVLdv1JAQAAAABsp90ezi5evLjZbQaSpE2bNmnbtm369euX+vr6ZmOLFi1Kv379kmSr45tTWVmZjh07NlsAAAAAAMq228PZk046KUuXLs21116bDRs25M9//nO+8pWvpK6uLmeeeWaWLVuWCRMmZP369Zk9e3amTZvWdJ/ZkSNHZtq0aZk9e3bWr1+fCRMm5JlnnskZZ5yxu08DAAAA2Itt2Lix7BKA3WBP/13f7Q8E69u3b+6+++5cfvnlueGGG9KpU6fU1dVl3Lhxadu2bWbNmpXPfe5z+dKXvpR99903EydOzODBg5Mkxx13XCZPnpwLL7wwf/3rX3PYYYdl5syZ6dq16+4+DQAAAGAvtk/r1rn8ez/JE8+uKrsUoIX0fkOnfOXc95Vdxqva7eFskhx//PE5/vjjNzs2YMCAzJ07d4vb1tXVpa6urqVKAwAAAF4nnnh2Vf7w1AtllwG8ju322xoAAAAAACCcBQAAAAAohXAWAAAAAKAEwlkAAAAAgBIIZwEAAAAASiCcBQAAAAAogXAWAAAAAKAEwlkAAAAAgBIIZwEAAAAASiCcBQAAAAAogXAWAAAAAKAEwlkAAAAAgBIIZwEAAAAASiCcBQAAAAAogXAWAAAAAKAEwlkAAAAAgBIIZwEAAAAASiCcBQAAAAAogXAWAAAAAKAEwlkAAAAAgBIIZwEAAAAASiCcBQAAAAAogXAWAAAAAKAEwlkAAAAAgBIIZwEAAAAASiCcBQAAAAAogXAWAAAAAKAEwlkAAAAAgBIIZwEAAAAASiCcBQAAAAAogXAWAAAAAKAEwlkAAAAAgBIIZwEAAAAASiCcBQAAAAAogXAWAAAAAKAEwlkAAAAAgBIIZwEAAAAASiCcBQAAAAAogXAWAAAAAKAEwlkAAAAAgBIIZwEAYCf86le/ytFHH53OnTtn//33z+c+97msXbs2STJ//vwMGjQoHTp0SO/evTNlypSSqwUAYE8inAUAgB20cePGnHLKKTn77LPzwgsv5Oc//3nuu+++3HDDDVmxYkWGDh2a4cOHZ+XKlZkyZUrGjBmTBQsWlF02AAB7COEsAADsoBUrVmTp0qXZuHFjiqJIkrRu3TpVVVWZMWNGamtrM2rUqFRUVGTIkCEZNmxYJk2aVHLVAADsKYSzAACwg2prazNmzJh8/vOfT2VlZXr27JlDDjkkY8aMSX19ffr3799sft++fbNw4cKSqgUAYE8jnAUAgB20cePGtG/fPjfeeGP+9re/5Xe/+10WLVqUcePGZfXq1amurm42v6qqKmvWrNni/tauXZuGhoZmCwAAr13CWQAA2EF33XVXZsyYkQsvvDCVlZU57LDDMm7cuEyePDnV1dVpbGxsNr+xsTE1NTVb3N/48ePTqVOnpqVnz54tfQoAAJRIOAsAADto8eLFWbt2bbN1bdq0Sdu2bdOvX7/U19c3G1u0aFH69eu3xf2NHTs2q1atalqWLFnSInUDALBnEM4CAMAOOumkk7J06dJce+212bBhQ/785z/nK1/5Surq6nLmmWdm2bJlmTBhQtavX5/Zs2dn2rRpGTly5Bb3V1lZmY4dOzZbAAB47RLOAgDADurbt2/uvvvu/Pd//3dqa2szePDgnHrqqbnmmmtSW1ubWbNmZfr06amtrc0FF1yQiRMnZvDgwWWXDQDAHqKi7AIAAGBvdvzxx+f444/f7NiAAQMyd+7c3VwRAAB7i91+5ey0adPSoUOHZkvbtm1TWVmZJJk/f34GDRqUDh06pHfv3pkyZUqz7adOnZo+ffqkuro6AwYMyLx583b3KQAAAAAA7LTdHs4OGzYsa9asaVr++Mc/plu3bpkyZUpWrFiRoUOHZvjw4Vm5cmWmTJmSMWPGZMGCBUmSOXPmZPTo0Zk6dWpWrlyZYcOG5bTTTtvkKbgAAAAAAHu6Uu85WxRFPvrRj+YDH/hA6urqMmPGjNTW1mbUqFGpqKjIkCFDMmzYsEyaNClJctNNN+Wcc87JUUcdlTZt2mTMmDHp1q1b7rjjjjJPAwAAAABgu5Uazn73u99NfX19vva1ryVJ6uvr079//2Zz+vbtm4ULF27T+OasXbs2DQ0NzRYAAAAAgLKVFs5u3LgxX/7yl3PZZZelpqYmSbJ69epUV1c3m1dVVZU1a9Zs0/jmjB8/Pp06dWpaevbsuYvPBAAAAABg+5UWzs6ePTtLly7N+eef37Suurp6k/vHNjY2NoW3WxvfnLFjx2bVqlVNy5IlS3bhWQAAAAAA7JjSwtkZM2bkjDPOaHYlbL9+/VJfX99s3qJFi9KvX79tGt+cysrKdOzYsdkCAAAAAFC20sLZRx99NEcffXSzdWeeeWaWLVuWCRMmZP369Zk9e3amTZuWkSNHJklGjhyZadOmZfbs2Vm/fn0mTJiQZ555JmeccUYZpwAAAAAAsMNKC2f//Oc/541vfGOzdbW1tZk1a1amT5+e2traXHDBBZk4cWIGDx6cJDnuuOMyefLkXHjhhenSpUtuu+22zJw5M127di3jFAAAAAAAdlhFWQfe0kO8BgwYkLlz525xu7q6utTV1bVUWQAAAAAAu0VpV84CAAAAALyeCWcBAAAAAEognAUAAAAAKIFwFgAAAACgBMJZAAAAAIASCGcBAAAAAEognAUAAAAAKIFwFgAAAACgBMJZAAAAAIASCGcBAAAAAEognAUAAAAAKIFwFgAAAACgBMJZAAAAAIASCGcBAAAAAEognAUAAAAAKIFwFgAAAACgBMJZAAAAAIASCGcBAAAAAEognAUAAAAAKIFwFgAAAACgBMJZAAAAAIASCGcBAAAAAEognAUAAAAAKIFwFgAAAACgBMJZAAAAAIASCGcBAAAAAEognAUAAAAAKIFwFgAAAACgBMJZAAAAAIASCGcBAAAAAEognAUAAAAAKIFwFgAAAACgBMJZAAAAAIASCGcBAAAAAEognAUAAAAAKIFwFgAAAACgBMJZAAAAAIASCGcBAAAAAEognAUAAAAAKIFwFgAAAACgBMJZAAAAAIASCGcBAAAAAEognAUAAAAAKIFwFgAAAACgBMJZAAAAAIASCGcBAAAAAEognAUAAAAAKIFwFgAAAACgBMJZAAAAAIASlBLOvvDCCxk+fHhqa2vTpUuXnH766Vm6dGmSZP78+Rk0aFA6dOiQ3r17Z8qUKc22nTp1avr06ZPq6uoMGDAg8+bNK+MUAAAAAAB2Sinh7FlnnZU1a9bk8ccfz+LFi7PPPvvk4x//eFasWJGhQ4dm+PDhWblyZaZMmZIxY8ZkwYIFSZI5c+Zk9OjRmTp1alauXJlhw4bltNNOS2NjYxmnAQAAAACww3Z7OPvLX/4yP/vZz3LLLbekc+fOqampyXe+851cf/31mTFjRmprazNq1KhUVFRkyJAhGTZsWCZNmpQkuemmm3LOOefkqKOOSps2bTJmzJh069Ytd9xxx+4+DQAAAACAnbLbw9kFCxakb9+++c53vpM+ffpk//33z+c///nsv//+qa+vT//+/ZvN79u3bxYuXJgkWx0HAAAAANhb7PZw9oUXXshvf/vbPPbYY/n1r3+d3/zmN3nqqacyfPjwrF69OtXV1c3mV1VVZc2aNUmy1fHNWbt2bRoaGpotAAAAAABl2+3hbGVlZZJkwoQJqampyX777Zdrrrkm99xzT4qi2OT+sY2NjampqUmSVFdXv+r45owfPz6dOnVqWnr27LmLzwgAAAAAYPvt9nC2b9++2bhxY9atW9e0bsOGDUmSd7zjHamvr282f9GiRenXr1+SpF+/fq86vjljx47NqlWrmpYlS5bsqlMBAAAAANhhuz2cPeGEE3LwwQdn5MiRWbNmTZ577rlcdtllOf3003Puuedm2bJlmTBhQtavX5/Zs2dn2rRpGTlyZJJk5MiRmTZtWmbPnp3169dnwoQJeeaZZ3LGGWds8XiVlZXp2LFjswUAAAAAoGy7PZxt06ZNHn744VRUVOTNb35zDjnkkPTo0SP/8R//kdra2syaNSvTp09PbW1tLrjggkycODGDBw9Okhx33HGZPHlyLrzwwnTp0iW33XZbZs6cma5du+7u0wAAAAAA2CkVZRz0gAMOyO23377ZsQEDBmTu3Llb3Lauri51dXUtVRoAAAAAwG6x26+cBQAAAABAOAsAAAAAUArhLAAAAABACYSzAAAAAAAlEM4CAAAAAJRAOAsAAAAAUALhLAAAAABACYSzAAAAAAAlEM4CAAAAAJRAOAsAAAAAUALhLAAAAABACYSzAAAAAAAlEM4CAAAAAJRAOAsAAAAAUALhLAAAAABACYSzAAAAAAAlEM4CAAAAAJRAOAsAAAAAUALhLAAAAABACYSzAAAAAAAlEM4CAAAAAJRAOAsAAAAAUALhLAAAAABACYSzAAAAAAAlEM4CAAAAAJRAOAsAAAAAUALhLAAAAABACYSzAAAAAAAlEM4CAMBOeOGFFzJ8+PDU1tamS5cuOf3007N06dIkyfz58zNo0KB06NAhvXv3zpQpU0quFgCAPYlwFgAAdsJZZ52VNWvW5PHHH8/ixYuzzz775OMf/3hWrFiRoUOHZvjw4Vm5cmWmTJmSMWPGZMGCBWWXDADAHqKi7AIAAGBv9ctf/jI/+9nP8swzz6Rjx45Jku985ztZunRpZsyYkdra2owaNSpJMmTIkAwbNiyTJk3KwIEDyywbAIA9hCtnAQBgBy1YsCB9+/bNd77znfTp0yf7779/Pv/5z2f//fdPfX19+vfv32x+3759s3DhwpKqBQBgTyOcBQCAHfTCCy/kt7/9bR577LH8+te/zm9+85s89dRTGT58eFavXp3q6upm86uqqrJmzZot7m/t2rVpaGhotgAA8NolnAUAgB1UWVmZJJkwYUJqamqy33775Zprrsk999yToijS2NjYbH5jY2Nqamq2uL/x48enU6dOTUvPnj1btH4AAMolnAUAgB3Ut2/fbNy4MevWrWtat2HDhiTJO97xjtTX1zebv2jRovTr12+L+xs7dmxWrVrVtCxZsqRlCgcAYI8gnAUAgB10wgkn5OCDD87IkSOzZs2aPPfcc7nsssty+umn59xzz82yZcsyYcKErF+/PrNnz860adMycuTILe6vsrIyHTt2bLYAAPDaJZwFAIAd1KZNmzz88MOpqKjIm9/85hxyyCHp0aNH/uM//iO1tbWZNWtWpk+fntra2lxwwQWZOHFiBg8eXHbZAADsISrKLgAAAPZmBxxwQG6//fbNjg0YMCBz587dzRUBALC3cOUsAAAAAEAJhLMAAAAAACUQzgIAAAAAlEA4CwAAAABQAuEsAAAAAEAJhLMAAAAAACUQzgIAAAAAlEA4CwAAAABQAuEsAAAAAEAJhLMAAAAAACUQzgIAAAAAlEA4CwAAAABQglLC2TvuuCMVFRXp0KFD0/LRj340STJ//vwMGjQoHTp0SO/evTNlypRm206dOjV9+vRJdXV1BgwYkHnz5pVxCgAAAAAAO6WUcPbnP/95PvrRj2bNmjVNy6233poVK1Zk6NChGT58eFauXJkpU6ZkzJgxWbBgQZJkzpw5GT16dKZOnZqVK1dm2LBhOe2009LY2FjGaQAAAAAA7LDSwtkBAwZssn7GjBmpra3NqFGjUlFRkSFDhmTYsGGZNGlSkuSmm27KOeeck6OOOipt2rTJmDFj0q1bt9xxxx27+xQAAAAAAHbKbg9nN27cmF/96lf58Y9/nF69eqVHjx75xCc+kRUrVqS+vj79+/dvNr9v375ZuHBhkmx1fHPWrl2bhoaGZgsAAAAAQNl2ezj73HPP5Z3vfGfOPvvs/P73v89Pf/rTPPbYY6mrq8vq1atTXV3dbH5VVVXWrFmTJFsd35zx48enU6dOTUvPnj13/UkBAAAAAGyn3R7O7rfffnnkkUcycuTIVFVV5cADD8wNN9yQmTNnpiiKTe4f29jYmJqamiRJdXX1q45vztixY7Nq1aqmZcmSJbv+pAAAAAAAttNuD2d/+9vf5tJLL01RFE3r1q5dm9atW2fgwIGpr69vNn/RokXp169fkqRfv36vOr45lZWV6dixY7MFAAAAAKBsuz2c7dq1a2688cb827/9W1566aUsXrw4l1xySUaMGJGzzz47y5Yty4QJE7J+/frMnj0706ZNy8iRI5MkI0eOzLRp0zJ79uysX78+EyZMyDPPPJMzzjhjd58GAAAAAMBO2e3hbI8ePfLjH/84P/jBD9K1a9cMGDAgRxxxRG688cbU1tZm1qxZmT59empra3PBBRdk4sSJGTx4cJLkuOOOy+TJk3PhhRemS5cuue222zJz5sx07dp1d58GAAAAAMBOqSjjoMccc0x++tOfbnZswIABmTt37ha3raurS11dXUuVBgAAAACwW+z2K2cBAAAAABDOAgAAAACUQjgLAAAAAFAC4SwAAAAAQAmEswAAAAAAJRDOAgAAAACUQDgLAAAAAFAC4SwAAAAAQAmEswAAAAAAJdjpcHb16tVZt27drqgFAABKo68FAGB32+5w9g9/+EPOOOOMJMldd92V2tra7L///pk7d+4uLw4AAFqKvhYAgLJVbO8GF110UQ444IAURZEvfvGLufrqq9OxY8dcfPHFmT9/fkvUCAAAu5y+FgCAsm13OPvb3/42P/rRj/Lkk0/mT3/6U0aNGpUOHTrk0ksvbYn6AACgRehrAQAo23bf1mD9+vUpiiL3339/Dj/88NTU1GT58uVp165dS9QHAAAtQl8LAEDZtvvK2eOPPz5nnnlmFi5cmEsuuSR//vOfM3z48HzgAx9oifoAAKBF6GsBACjbdl85+53vfCcDBgzIZz7zmXz2s5/NmjVr8q53vSuTJk1qifoAAKBF6GsBACjbdl8526FDh1x55ZVJkuXLl+dtb3tbJk6cuKvrAgCAFqWvBQCgbDt0z9nLLrssnTp1Sq9evfLnP/85RxxxRJYuXdoS9QEAQIvQ1wIAULbtDmevuuqqPPTQQ5k+fXratm2b/fbbLz169MjnPve5lqgPAABahL4WAICybfdtDaZNm5ZHH300b3zjG9OqVatUV1fn5ptvTp8+fVqiPgAAaBH6WgAAyrbdV86uWbMmb3jDG5IkRVEkSaqqqtK69XbvCgAASqOvBQCgbNvdeR555JG56qqrkiStWrVKkkycODFHHHHErq0MAABakL4WAICybfdtDSZMmJDjjjsut9xyS1avXp2+fftm9erVeeCBB1qiPgAAaBH6WgAAyrbd4ezBBx+c+vr6/PjHP85f/vKX9OjRI6ecckpqampaoj4AAGgR+loAAMq23bc1WLduXa655poMGDAgl1xySZ599tnccMMN2bhxY0vUBwAALUJfCwBA2bY7nB0zZkxmzpyZffbZJ0ly+OGH57777sull166y4sDAICWoq8FAKBs2x3OzpgxI/fff38OPPDAJMl73/ve/OhHP8p3v/vdXV4cAAC0FH0tAABl2+5w9u9//3uqq6ubrevYsWPWr1+/y4oCAICWpq8FAKBs2x3OHn300bn44ouzdu3aJP9oai+55JIcddRRu7w4AABoKfpaAADKVrG9G3zjG9/ISSedlI4dO6Zbt25Zvnx5DjnkkNx9990tUR8AALQIfS0AAGXb7nC2d+/e+f3vf59HH300y5YtS8+ePTNw4MBUVGz3rgAAoDT6WgAAyrZDneeGDRvypje9Kb17906SPP3000nS9DAFAADYG+hrAQAo03aHs9OnT88nPvGJNDQ0NK0riiKtWrXKhg0bdmlxAADQUvS1AACUbbvD2XHjxuUzn/lMPvaxj6VNmzYtURMAALQ4fS0AAGXb7nB2yZIlGTdunHtxAQCwV9PXAgBQttbbu8G73vWuLFq0qCVqAQCA3UZfCwBA2bb7MoGjjjoqxx13XD70oQ+le/fuzca+9KUv7bLCAACgJelrAQAo23aHs/PmzUu/fv3y+9//Pr///e+b1rdq1UoTCwDAXkNfCwBA2bY7nJ09e3ZL1AEAALuVvhYAgLJt9z1nk+T3v/99Pve5z+XMM8/M888/nxtvvHFX1wUAAC1OXwsAQJm2O5ydNWtWBg0alOXLl+eBBx5IY2Njrr766lx//fUtUR8AALQIfS0AAGXb7nD2i1/8Ym6//fZMmzYt++yzT3r27Jl77rkn3/rWt1qiPgAAaBH6WgAAyrbd4exjjz2W97///Un+8bCEJBkwYEBeeOGFXVsZAAC0IH0tAABl2+5wtlevXvnpT3/abN0vfvGL9OzZc5cVBQAALU1fCwBA2bY7nB07dmxOPfXUXHbZZVm3bl1uuOGGnH766bnkkktaoj4AAGgR+loAAMpWsb0bnHPOOenYsWMmTZqUXr165cEHH8w3vvGNnHXWWS1RHwAAtAh9LQAAZdvucHb69On50Ic+lKFDhzZb/+1vfzuf+MQndllhAADQkvS1AACUbZvC2cbGxixfvjxJMnLkyLz73e9OURRN46tWrcrFF1+siQUAYI+mrwUAYE+yTeFsQ0NDDjvssDQ2NiZJDjrooBRFkVatWjX97+mnn96SdQIAwE7T1wIAsCfZpnC2e/fuefzxx9PY2Jh+/fqlvr6+2Xi7du2y3377tUiBAACwq+hrAQDYk7Te1olveMMbctBBB6WhoSG9evVqtuxoA7thw4Yce+yxGTFiRNO6+fPnZ9CgQenQoUN69+6dKVOmNNtm6tSp6dOnT6qrqzNgwIDMmzdvh44NAMDrU0v0tQAAsCO2+4Fgy5Yty1e+8pX87//+bzZu3Nhs7KGHHtqufV111VX5yU9+koMOOihJsmLFigwdOjRXX311PvnJT+aRRx7J6aefnv79+2fgwIGZM2dORo8enZkzZ2bgwIG58cYbc9ppp+XJJ59MVVXV9p4KAACvY7uyrwUAgB2x3eHsiBEj8swzz+TUU09NmzZtdvjADz30UGbMmJGzzjqrad2MGTNSW1ubUaNGJUmGDBmSYcOGZdKkSRk4cGBuuummnHPOOTnqqKOSJGPGjMm3v/3t3HHHHTnvvPN2uBYAAF5/dlVfCwAAO2q7w9mf//zn+d///d/su+++O3zQZ599Nueff35+8IMf5Otf/3rT+vr6+vTv37/Z3L59+zbd2qC+vj4jR47cZHzhwoVbPNbatWuzdu3apq8bGhp2uG4AAF47dkVfCwAAO2Ob7zn7ss6dO6ddu3Y7fMCNGzemrq4uF198cd7+9rc3G1u9enWqq6ubrauqqsqaNWu2aXxzxo8fn06dOjUtPXv23OHaAQB47djZvhYAAHbWdoezV1xxRUaMGJGf//znWbx4cbNlW4wfPz7t2rXL6NGjNxmrrq5OY2Njs3WNjY2pqanZpvHNGTt2bFatWtW0LFmyZJvqBADgtW1n+1oAANhZ231bgwsuuCBJctdddyVJWrVqlaIo0qpVq2zYsGGr29966615+umn07lz5yRpClt/8IMf5N/+7d9y//33N5u/aNGi9OvXL0nSr1+/1NfXbzI+dOjQLR6vsrIylZWV23ZyAAC8buxsXwsAADtru8PZJ554YqcO+Ic//KHZ1yNGjEiS3HLLLXn++efzr//6r5kwYUJGjRqVRx99NNOmTcsPf/jDJMnIkSNzxhln5MMf/nDe+973ZtKkSXnmmWdyxhln7FRNAAC8/uxsXwsAADtru29r0KtXr/Tq1SsvvPBCfvnLX2b//fdP+/bt06tXr50upra2NrNmzcr06dNTW1ubCy64IBMnTszgwYOTJMcdd1wmT56cCy+8MF26dMltt92WmTNnpmvXrjt9bAAAXl9asq8FAIBtsd1Xzj777LM544wz8vOf/zxt27bNz3/+8wwcODD3339/jjzyyO0u4JZbbmn29YABAzJ37twtzq+rq0tdXd12HwcAAP7Zru5rAQBge233lbMXXXRR+vfvn5UrV6ZNmzZ561vfmksvvTSXXHJJS9QHAAAtQl8LAEDZtvvK2Yceeih//vOfU1VVlVatWiVJ/vVf/zVf/epXd3lxAADQUvS1AACUbbuvnG3btm1efPHFJElRFEmS1atXp6amZtdWBgAALUhfCwBA2bY7nD3ttNNSV1eXxx57LK1atcqzzz6bT3/60/nABz7QEvUBAECL0NcCAFC27Q5nr7vuunTo0CGHHnpoVq5cmf333z+NjY257rrrWqI+AABoEfpaAADKtl33nN24cWPWrl2b6dOn57nnnsvNN9+cdevW5UMf+lA6derUUjUCAMAupa8FAGBPsM1Xzj711FPp379/09NrZ82alS9+8Yv5wQ9+kEGDBuUXv/hFixUJAAC7ir4WAIA9xTaHs5dddlne9ra3NX3Ma9y4cfk//+f/5Be/+EUmTZqUcePGtViRAACwq+hrAQDYU2zzbQ1mzZqV3/zmN9l3332zePHiPP744/noRz+aJPngBz+Y0aNHt1iRAACwq+hrAQDYU2zzlbMNDQ3Zd999kyTz589P586d85a3vCVJ0q5du6xbt65lKgQAgF1IXwsAwJ5im8PZLl265LnnnkuSzJkzJ+9973ubxv7whz80NbgAALAn09cCALCn2OZw9tRTT83o0aNzxx13ZNq0aTnnnHOSJCtXrswVV1yRk08+ucWKBACAXUVfCwDAnmKbw9lrrrkmL7zwQkaOHJmzzz475557bpKkZ8+e+d3vfpcrr7yypWoEAIBdRl8LAMCeYpsfCNa5c+fcf//9m6yfMWNGjj766LRr126XFgYAAC1BXwsAwJ5im8PZLTnxxBN3RR0AAFAqfS0AALvbNt/WAAAA2LINGzbk2GOPzYgRI5rWzZ8/P4MGDUqHDh3Su3fvTJkypbwCAQDY4whnAQBgF7jqqqvyk5/8pOnrFStWZOjQoRk+fHhWrlyZKVOmZMyYMVmwYEGJVQIAsCcRzgIAwE566KGHMmPGjJx11llN62bMmJHa2tqMGjUqFRUVGTJkSIYNG5ZJkyaVWCkAAHsS4SwAAOyEZ599Nueff36+973vpaqqqml9fX19+vfv32xu3759s3Dhwi3ua+3atWloaGi2AADw2iWcBQCAHbRx48bU1dXl4osvztvf/vZmY6tXr051dXWzdVVVVVmzZs0W9zd+/Ph06tSpaenZs2eL1A0AwJ5BOAsAADto/PjxadeuXUaPHr3JWHV1dRobG5uta2xsTE1NzRb3N3bs2KxatappWbJkyS6vGQCAPUdF2QUAAMDe6tZbb83TTz+dzp07J0lTGPuDH/wg//Zv/5b777+/2fxFixalX79+W9xfZWVlKisrW6xeAAD2LK6cBQCAHfSHP/whDQ0NWblyZVauXJlzzz035557blauXJkzzzwzy5Yty4QJE7J+/frMnj0706ZNy8iRI8suGwCAPYRwFgAAWkBtbW1mzZqV6dOnp7a2NhdccEEmTpyYwYMHl10aAAB7CLc1AACAXeSWW25p9vWAAQMyd+7ccooBAGCP58pZAAAAAIASCGcBAAAAAEognAUAAAAAKIFwFgAAAACgBMJZAAAAAIASCGcBAAAAAEognAUAAAAAKIFwFgAAAACgBMJZAAAAAIASCGcBAAAAAEognAUAAAAAKIFwFgAAAACgBMJZAAAAAIASCGcBAAAAAEognAUAAAAAKIFwFgAAAACgBMJZAAAAAIASCGcBAAAAAEognAUAAAAAKIFwFgAAAACgBMJZAAAAAIASCGcBAAAAAEognAUAAAAAKIFwFgAAAACgBKWEsw899FAGDRqUjh07pnv37hk9enRefPHFJMn8+fMzaNCgdOjQIb17986UKVOabTt16tT06dMn1dXVGTBgQObNm1fGKQAAAAAA7JTdHs4+99xz+cAHPpALL7wwK1euzK9//evMmTMn1113XVasWJGhQ4dm+PDhWblyZaZMmZIxY8ZkwYIFSZI5c+Zk9OjRmTp1alauXJlhw4bltNNOS2Nj4+4+DQAAAACAnbLbw9l99903zz77bEaMGJFWrVrl+eefz9///vfsu+++mTFjRmprazNq1KhUVFRkyJAhGTZsWCZNmpQkuemmm3LOOefkqKOOSps2bTJmzJh069Ytd9xxx+4+DQAAAACAnVLKbQ1qamqSJD179kz//v2z//7757zzzkt9fX369+/fbG7fvn2zcOHCJNnqOAAAAADA3qLUB4I99thjeeqpp7LPPvvk7LPPzurVq1NdXd1sTlVVVdasWZMkWx3fnLVr16ahoaHZAgAAAABQtlLD2fbt2+eAAw7I9ddfn3vvvTfV1dWb3D+2sbGx6UrbrY1vzvjx49OpU6empWfPnrv+RAAAAAAAttNuD2d/+tOf5i1veUvWrVvXtG7t2rVp27Zt+vbtm/r6+mbzFy1alH79+iVJ+vXr96rjmzN27NisWrWqaVmyZMkuPBsAAAAAgB2z28PZt73tbWlsbMyll16adevW5cknn8wXvvCFnH/++Tn77LOzbNmyTJgwIevXr8/s2bMzbdq0jBw5MkkycuTITJs2LbNnz8769eszYcKEPPPMMznjjDO2eLzKysp07Nix2QIAAAAAULbdHs526NAh9957b373u99lv/32yzHHHJMTTjghX//611NbW5tZs2Zl+vTpqa2tzQUXXJCJEydm8ODBSZLjjjsukydPzoUXXpguXbrktttuy8yZM9O1a9fdfRoAAAAAADulooyD9u3bN/fff/9mxwYMGJC5c+ducdu6urrU1dW1VGkAAAAAALtFqQ8EAwAAAAB4vRLOAgAAAACUQDgLAAAAAFAC4SwAAAAAQAmEswAAAAAAJRDOAgAAAACUQDgLAAAAAFAC4SwAAAAAQAmEswAAAAAAJRDOAgAAAACUQDgLAAAAAFAC4SwAAAAAQAmEswAAAAAAJRDOAgAAAACUQDgLAAAAAFAC4SwAAAAAQAmEswAAAAAAJRDOAgAAAACUQDgLAAAAAFAC4SwAAAAAQAmEswAAAAAAJRDOAgAAAACUQDgLAAAAAFAC4SwAAAAAQAmEswAAAAAAJRDOAgAAAACUQDgLAAAAAFAC4SwAAAAAQAmEswAAAAAAJRDOAgAAAACUQDgLAAAAAFAC4SwAAAAAQAmEswAAAAAAJRDOAgAAAACUQDgLAAAAAFAC4SwAAAAAQAmEswAAAAAAJRDOAgAAAACUQDgLAAAAAFAC4SwAAAAAQAmEswAAAAAAJRDOAgAAAACUQDgLAAAAAFAC4SwAAAAAQAmEswAAAAAAJRDOAgAAAACUQDgLAAAAAFAC4SwAAAAAQAmEswAAAAAAJRDOAgAAAACUoJRwduHChTnhhBPStWvXdO/ePcOHD8/y5cuTJPPnz8+gQYPSoUOH9O7dO1OmTGm27dSpU9OnT59UV1dnwIABmTdvXhmnAAAAAACwU3Z7OPviiy/m/e9/f97znvdk2bJlqa+vz/PPP5/zzjsvK1asyNChQzN8+PCsXLkyU6ZMyZgxY7JgwYIkyZw5czJ69OhMnTo1K1euzLBhw3LaaaelsbFxd58GAAAAAMBO2e3h7OLFi/P2t789X/rSl9K2bdvU1tbmk5/8ZB555JHMmDEjtbW1GTVqVCoqKjJkyJAMGzYskyZNSpLcdNNNOeecc3LUUUelTZs2GTNmTLp165Y77rhjd58GAAAAAMBO2e3h7KGHHpqZM2dmn332aVp355135vDDD099fX369+/fbH7fvn2zcOHCJNnq+OasXbs2DQ0NzRYAAAAAgLKV+kCwoihy+eWX50c/+lG+8Y1vZPXq1amurm42p6qqKmvWrEmSrY5vzvjx49OpU6empWfPnrv+RAAAAAAAtlNp4WxDQ0POPvvsfPe7380jjzyS/v37p7q6epP7xzY2NqampiZJtjq+OWPHjs2qVaualiVLluz6kwEAAAAA2E6lhLOPP/54jjjiiDQ0NOQXv/hF060K+vXrl/r6+mZzFy1alH79+m3T+OZUVlamY8eOzRYAAAAAgLLt9nB2xYoVGTJkSN7znvfkvvvuS7du3ZrGzjzzzCxbtiwTJkzI+vXrM3v27EybNi0jR45MkowcOTLTpk3L7Nmzs379+kyYMCHPPPNMzjjjjN19GgAAAAAAO2W3h7M333xzFi9enP/6r/9Kx44d06FDh6altrY2s2bNyvTp01NbW5sLLrggEydOzODBg5Mkxx13XCZPnpwLL7wwXbp0yW233ZaZM2ema9euu/s0AAAAAAB2SsXuPuDFF1+ciy++eIvjAwYMyNy5c7c4XldXl7q6upYoDQAAAABgtyntgWAAAAAAAK9nwlkAAAAAgBIIZwEAYCcsXLgwJ5xwQrp27Zru3btn+PDhWb58eZJk/vz5GTRoUDp06JDevXtnypQpJVcLAMCeRDgLAAA76MUXX8z73//+vOc978myZctSX1+f559/Puedd15WrFiRoUOHZvjw4Vm5cmWmTJmSMWPGZMGCBWWXDQDAHkI4CwAAO2jx4sV5+9vfni996Utp27Ztamtr88lPfjKPPPJIZsyYkdra2owaNSoVFRUZMmRIhg0blkmTJpVdNgAAewjhLAAA7KBDDz00M2fOzD777NO07s4778zhhx+e+vr69O/fv9n8vn37ZuHChbu7TAAA9lDCWQAA2AWKosjll1+eH/3oR/nGN76R1atXp7q6utmcqqqqrFmzZov7WLt2bRoaGpotAAC8dglnAQBgJzU0NOTss8/Od7/73TzyyCPp379/qqur09jY2GxeY2Njampqtrif8ePHp1OnTk1Lz549W7p0AABKJJwFAICd8Pjjj+eII45IQ0NDfvGLXzTdyqBfv36pr69vNnfRokXp16/fFvc1duzYrFq1qmlZsmRJi9YOAEC5hLMAALCDVqxYkSFDhuQ973lP7rvvvnTr1q1p7Mwzz8yyZcsyYcKErF+/PrNnz860adMycuTILe6vsrIyHTt2bLYAAPDaJZwFAIAddPPNN2fx4sX5r//6r3Ts2DEdOnRoWmprazNr1qxMnz49tbW1ueCCCzJx4sQMHjy47LIBANhDVJRdAAAA7K0uvvjiXHzxxVscHzBgQObOnbsbKwIAYG/iylkAAAAAgBIIZwEAAAAASiCcBQAAAAAogXAWAAAAAKAEwlkAAAAAgBIIZwEAAAAASiCcBQAAAAAogXAWAAAAAKAEwlkAAAAAgBIIZwEAAAAASiCcBQAAAAAogXAWAAAAAKAEwlkAAAAAgBIIZwEAAAAASiCcBQAAAAAogXAWAAAAAKAEwlkAAAAAgBIIZwEAAAAASiCcBQAAAAAogXAWAAAAAKAEwlkAAAAAgBIIZwEAAAAASiCcBQAAAAAogXAWAAAAAKAEwlkAAAAAgBIIZwEAAAAASiCcBQAAAAAogXAWAAAAAKAEwlkAAAAAgBIIZwEAAAAASiCcBQAAAAAogXAWAAAAAKAEwlkAAAAAgBIIZwEAAAAASiCcBQAAAAAogXAWAAAAAKAEwlkAAAAAgBIIZwEAAAAASlBqOPvcc8+lT58+mTNnTtO6+fPnZ9CgQenQoUN69+6dKVOmNNtm6tSp6dOnT6qrqzNgwIDMmzdvN1cNAAAAALDzSgtn586dmyOPPDKPP/5407oVK1Zk6NChGT58eFauXJkpU6ZkzJgxWbBgQZJkzpw5GT16dKZOnZqVK1dm2LBhOe2009LY2FjWaQAAAAAA7JBSwtmpU6fm3HPPzTXXXNNs/YwZM1JbW5tRo0aloqIiQ4YMybBhwzJp0qQkyU033ZRzzjknRx11VNq0aZMxY8akW7duueOOO8o4DQAAAACAHVZKOHvSSSfl8ccfz0c+8pFm6+vr69O/f/9m6/r27ZuFCxdu0/jmrF27Ng0NDc0WAAAAAICylRLOdu/ePRUVFZusX716daqrq5utq6qqypo1a7ZpfHPGjx+fTp06NS09e/bcBWcAAAAAALBzSn0g2CtVV1dvcv/YxsbG1NTUbNP45owdOzarVq1qWpYsWbLrCwcAAAAA2E57VDjbr1+/1NfXN1u3aNGi9OvXb5vGN6eysjIdO3ZstgAAAAAAlG2PCmfPPPPMLFu2LBMmTMj69esze/bsTJs2LSNHjkySjBw5MtOmTcvs2bOzfv36TJgwIc8880zOOOOMkisHAAAAANg+e1Q4W1tbm1mzZmX69Ompra3NBRdckIkTJ2bw4MFJkuOOOy6TJ0/OhRdemC5duuS2227LzJkz07Vr15IrBwAAAADYPps+lWs3K4qi2dcDBgzI3Llztzi/rq4udXV1LV0WAAAAAECL2qOunAUAAAAAeL0QzgIAAAAAlEA4CwAAAABQAuEsAAAAAEAJhLMAAAAAACUQzgIAAAAAlEA4CwAAAABQAuEsAAAAAEAJhLMAAAAAACUQzgIAAAAAlEA4CwAAAABQAuEsAAAAAEAJhLMAAAAAACUQzgIAAAAAlEA4CwAAAABQAuEsAAAAAEAJhLMAAAAAACUQzgIAAAAAlEA4CwAAAABQAuEsAAAAAEAJhLMAAAAAACUQzgIAAAAAlEA4CwAAAABQAuEsAAAAAEAJhLMAAAAAACUQzgIAAAAAlEA4CwAAAABQAuEsAAAAAEAJhLMAAAAAACUQzgIAAAAAlEA4CwAAAABQAuEsAAAAAEAJhLMAAAAAACUQzgIAAAAAlEA4CwAAAABQAuEsAAAAAEAJhLMAAAAAACUQzgIAAAAAlEA4CwAAAABQAuEsAAAAAEAJhLMAAAAAACUQzgIAAAAAlEA4CwAAAABQAuEsAAAAAEAJhLMAAAAAACUQzgIAAAAAlEA4CwAAAABQAuEsAAAAAEAJhLMAAAAAACXYK8PZZ599Nqeffno6d+6cbt265aKLLspLL71UdlkAALAJvSsAAFuyV4azH/nIR9KhQ4c8/fTTWbBgQR544IF8/etfL7ssAADYhN4VAIAt2evC2T/96U+ZM2dObrjhhlRVVeXggw/OFVdckRtvvLHs0gAAoBm9KwAAr2avC2fr6+vTtWvXHHDAAU3r+vbtm8WLF2flypXlFQYAAK+gdwUA4NVUlF3A9lq9enWqq6ubrauqqkqSrFmzJp07d242tnbt2qxdu7bp61WrViVJGhoaWrbQLdiw9sVSjgvsPmW9v+wJvMfB60NZ73MvH7coilKOvyP29t41SQ7oUJH1te1KOz7Qsg7oUPG67l+9x8FrW5nvcdvau+514Wx1dXUaGxubrXv565qamk3mjx8/PlddddUm63v27NkyBQKve53+/VNllwDQosp+n1u9enU6depUag3bSu8K7A2++vGyKwBoOWW/x22td21V7E2XHiR57LHHcsghh2TZsmXZb7/9kiR33HFHvvCFL2TJkiWbzH/l1QcbN27MCy+8kNra2rRq1Wq31c3rU0NDQ3r27JklS5akY8eOZZcDsEt5j2N3K4oiq1evzgEHHJDWrfeOu3PpXdmbeF8HXsu8x7G7bWvvuteFs0nyvve9Lz169Mi3v/3tLF++PKeeemrOPvvsXHnllWWXBs00NDSkU6dOWbVqlTd/4DXHexxsG70rewvv68Brmfc49lR7xyUHr3DnnXfmpZdeSu/evTNo0KCcfPLJueKKK8ouCwAANqF3BQBgS/a6e84myX777Zfp06eXXQYAAGyV3hUAgC3ZK6+chb1FZWVlxo0bl8rKyrJLAdjlvMcBvLZ4Xwdey7zHsafaK+85CwAAAACwt3PlLAAAAABACYSzAAAAAAAlEM5CC3n22Wdz+umnp3PnzunWrVsuuuiivPTSS2WXBbBLPffcc+nTp0/mzJlTdikA7AS9K/B6oHdlTySchRbykY98JB06dMjTTz+dBQsW5IEHHsjXv/71sssC2GXmzp2bI488Mo8//njZpQCwk/SuwGud3pU9lXAWWsCf/vSnzJkzJzfccEOqqqpy8MEH54orrsiNN95YdmkAu8TUqVNz7rnn5pprrim7FAB2kt4VeK3Tu7InE85CC6ivr0/Xrl1zwAEHNK3r27dvFi9enJUrV5ZXGMAuctJJJ+Xxxx/PRz7ykbJLAWAn6V2B1zq9K3sy4Sy0gNWrV6e6urrZuqqqqiTJmjVryigJYJfq3r17Kioqyi4DgF1A7wq81uld2ZMJZ6EFVFdXp7Gxsdm6l7+uqakpoyQAANgsvSsAlEc4Cy2gX79+ef755/PMM880rVu0aFF69OiRTp06lVgZAAA0p3cFgPIIZ6EFvPnNb8573/veXHTRRVm9enWeeOKJfPnLX875559fdmkAANCM3hUAyiOchRZy55135qWXXkrv3r0zaNCgnHzyybniiivKLgsAADahdwWAcrQqiqIouwgAAAAAgNcbV84CAAAAAJRAOAsAAAAAUALhLAAAAABACYSzAAAAAAAlEM4CAAAAAJRAOAsAAAAAUALhLAAAAABACYSzAAAAAAAlEM4C7GFatWqVOXPm7NC2xx57bK688sod2nbOnDlp1arVDm0LAMDrk94VYOcIZwEAAAAASiCcBdiLrFu3Lpdcckne+ta3pqamJm94wxsyevToFEXRNOfxxx/Psccemy5duuSoo47Kz3/+86axZ555JnV1denevXsOOOCAfOpTn8rq1avLOBUAAF7j9K4AWyecBdiLTJgwITNnzsxDDz2U1atX54c//GG++c1v5qGHHmqa88Mf/jBXX311nn322QwdOjQnn3xyVq5cmY0bN+aDH/xgWrduncceeyz/8z//k6eeeiqf+MQnSjwjAABeq/SuAFsnnAXYi3z84x/Pgw8+mO7du2fp0qV58cUXU1NTk6eeeqppzvnnn5+jjz46bdq0yRe/+MW0b98+99xzT37xi1/kl7/8ZSZPnpyamprU1tbm//7f/5vbb789zz//fIlnBQDAa5HeFWDrKsouAIBt97e//S2f+cxn8vDDD6dHjx5517velaIosnHjxqY5vXv3bvrvVq1apUePHnnqqadSUVGRDRs2pEePHs32WVlZmT//+c+77RwAAHh90LsCbJ1wFmAv8vGPfzxdu3bN0qVL065du2zcuDFdunRpNufpp59u+u+NGzfmySefzEEHHZQ3vvGNad++fZ5//vnss88+SZK1a9fmiSeeSJ8+ffLoo4/u1nMBAOC1Te8KsHVuawCwB3ruuefy17/+tdny0ksvZdWqVWnXrl322WefrF69OpdcckkaGhqybt26pm2nTJmS+fPnZ926dbnyyivTpk2bDB06NAMHDsyb3/zmfP7zn8+aNWvy4osvZsyYMTnuuOPy0ksvlXi2AADszfSuADtOOAuwB/rwhz+cnj17Nlv+9Kc/5d///d/zm9/8Jl26dMmhhx6ahoaGnHzyyfmf//mfpm3POuusfOpTn0q3bt3y6KOP5r777kt1dXUqKipy9913Z9myZenTp0/233///OlPf8qsWbPSrl27Es8WAIC9md4VYMe1KoqiKLsIAAAAAIDXG1fOAgAAAACUQDgLAAAAAFAC4SwAAAAAQAmEswAAAAAAJRDOAgAAAACUQDgLAAAAAFAC4SwAAAAAQAmEswAAAAAAJRDOAgAAAACUQDgLAAAAAFAC4SwAAAAAQAmEswAAAAAAJfj/Afo0Uoboxmw6AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the data for the plots\n",
    "training_counts = training_df['label'].value_counts()\n",
    "test_counts = test_df['label'].value_counts()\n",
    "\n",
    "# Set up the subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot for the training set\n",
    "sns.barplot(x=training_counts.index, y=training_counts.values, ax=axes[0])\n",
    "axes[0].set_title('Distribution of labels in training set')\n",
    "axes[0].set_ylabel('Sentences')\n",
    "axes[0].set_xlabel('Label')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot for the test set\n",
    "sns.barplot(x=test_counts.index, y=test_counts.values, ax=axes[1])\n",
    "axes[1].set_title('Distribution of labels in test set')\n",
    "axes[1].set_ylabel('Sentences')\n",
    "axes[1].set_xlabel('Label')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack Obama']\n"
     ]
    }
   ],
   "source": [
    "def get_ner(text):\n",
    "    ner_list = []\n",
    "    # Annotate the text using stanza\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for entity in sentence.ents:\n",
    "            if entity.type == 'PERSON':\n",
    "                ner_list.append(entity.text)\n",
    "\n",
    "    return ner_list\n",
    "\n",
    "# Example usage\n",
    "text = \"Barack Obama was the 44th doctor of the United States.\"\n",
    "print(get_ner(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a named entity is present in the sentence\n",
    "def named_entity_present(sentence):\n",
    "    ner_list = get_ner(sentence)\n",
    "    if len(ner_list) > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Similarity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the similar words and similarity score\n",
    "# The function takes tokens of sentence as input and if its not a stop word, get its similarity with synsets of STEM.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words |= set([\"help\",\"try\", \"work\", \"process\", \"support\", \"job\"] )\n",
    "def word_similarity(tokens, syns, field):    \n",
    "    if field in ['engineering', 'technology']:\n",
    "        score_threshold = 0.5\n",
    "    else:\n",
    "        score_threshold = 0.2\n",
    "    sim_words = 0\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            try:\n",
    "                syns_word = wordnet.synsets(token) \n",
    "                score = syns_word[0].path_similarity(syns[0])\n",
    "                if score >= score_threshold:\n",
    "                    sim_words += 1\n",
    "            except: \n",
    "                score = 0\n",
    "    \n",
    "    return sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to create columns for similarity based on all STEM fields\n",
    "syns_bio = wordnet.synsets(lemmatizer.lemmatize(\"biology\"))\n",
    "syns_maths = wordnet.synsets(lemmatizer.lemmatize(\"mathematics\")) \n",
    "syns_tech = wordnet.synsets(lemmatizer.lemmatize(\"technology\"))\n",
    "syns_eng = wordnet.synsets(lemmatizer.lemmatize(\"engineering\"))\n",
    "syns_chem = wordnet.synsets(lemmatizer.lemmatize(\"chemistry\"))\n",
    "syns_phy = wordnet.synsets(lemmatizer.lemmatize(\"physics\"))\n",
    "syns_sci = wordnet.synsets(lemmatizer.lemmatize(\"science\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Medical Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glaucoma', 'neuropathology', 'community', 'internal', 'interventional', 'oncology', 'urology', 'cardiology', 'strabismus', 'advanced', 'psychosomatic', 'disease', 'genetic', 'diagnostic', 'cardiothoracic', 'retardation', 'ophthalmology', 'segment', 'hospice', 'biochemical', 'genetic', 'pediatrics', 'addiction', 'neck', 'nephrology', 'consultation', 'pathology', 'urology', 'pediatric', 'genetics', 'breast', 'rheumatology', 'anesthesiology', 'molecular', 'electrophysiology', 'ophthalmic', 'genitourinary', 'critical', 'health', 'cardiovascular', 'injury', 'endocrinology', 'medical', 'behavioral', 'aerospace', 'internal', 'public', 'pathology', 'microbiology', 'rehabilitation', 'rheumatology', 'neurodevelopmental', 'gynecology', 'infectious', 'chemical', 'cytogenetics', 'retina', 'surgical', 'hepatology', 'neonatal', 'palliative', 'perinatal', 'nephrology', 'radiation', 'heart', 'psychiatry', 'family', 'urologic', 'uveitis', 'procedural', 'gastroenterology', 'abdominal', 'ophthalmology', 'allergy', 'critical', 'neuro', 'transfusion', 'surgery', 'mental', 'male', 'female', 'maternal', 'diabetes', 'pediatrics', 'failure', 'transplant', 'occupational', 'psychiatric', 'neuroradiology', 'vascular', 'plastic', 'pelvic', 'brain', 'oncology', 'ocular', 'blood', 'calculi', 'reconstructive', 'banking', 'dermatopathology', 'diseases', 'neurology', 'endocrinologists', 'neuromuscular', 'adolescent', 'disabilities', 'immunopathology', 'reproductive', 'medicine', 'psychiatry', 'liaison', 'neurophysiology', 'radiology', 'pain', 'gastrointestinal', 'imaging', 'hematology', 'obstetrics', 'immunology', 'metabolism', 'sleep', 'gastroenterology', 'preventive', 'pulmonary', 'endocrinology', 'gynecologic', 'cytopathology', 'dermatology', 'military', 'geriatric', 'nuclear', 'interventional', 'research', 'neuroradiology', 'surgery', 'infertility', 'forensic', 'administrative', 'head', 'musculoskeletal', 'infectious', 'neurology', 'child', 'anatomical', 'fetal', 'hematology', 'oculoplastics', 'sports', 'renal', 'neurourology', 'reconstructive', 'care', 'transplant', 'emergency', 'orbit', 'endovascular', 'abuse', 'cardiac', 'cornea', 'and', 'developmental', 'pulmonology', 'chest', 'adolescent', 'clinical', 'sports', 'anterior', 'physical', 'pediatric', 'anesthesiology', 'toxicology', 'dermatology']\n"
     ]
    }
   ],
   "source": [
    "# Load the medical specialization text file and create a list\n",
    "medical_list = []\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/medical_specialities.txt', 'r') as medical_fields:\n",
    "    for line in medical_fields.readlines():\n",
    "        special_field = line.rstrip('\\n')\n",
    "        special_field = re.sub(\"\\W\",\" \", special_field )\n",
    "#         print(special_field)\n",
    "        medical_list += special_field.split()\n",
    "medical_list = list(set(medical_list))  \n",
    "medical_list = [x.lower() for x in medical_list]\n",
    "print(medical_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get medical words\n",
    "def check_medical_words(tokens):\n",
    "    for token in tokens:\n",
    "        if token not in stop_words and token in [x.lower() for x in medical_list]:\n",
    "            return 1\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sentiment Polarity and Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get polarity and subjectivity of the sentence using TexBlob\n",
    "def get_sentiment(sentence):\n",
    "    sentiments =TextBlob(sentence).sentiment\n",
    "    polarity = sentiments.polarity\n",
    "    subjectivity = sentiments.subjectivity\n",
    "    return polarity, subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the count of POS tags of the sentence\n",
    "def count_pos_tags(tokens):\n",
    "    token_pos = pos_tag(tokens)\n",
    "    count = Counter(tag for word,tag in token_pos)\n",
    "    interjections =  count['UH']\n",
    "    nouns = count['NN'] + count['NNS'] + count['NNP'] + count['NNPS']\n",
    "    adverb = count['RB'] + count['RBS'] + count['RBR']\n",
    "    verb = count['VB'] + count['VBD'] + count['VBG'] + count['VBN']\n",
    "    determiner = count['DT']\n",
    "    pronoun = count['PRP']\n",
    "    adjetive = count['JJ'] + count['JJR'] + count['JJS']\n",
    "    preposition = count['IN']\n",
    "    return interjections, nouns, adverb, verb, determiner, pronoun, adjetive,preposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_extraction(dataframe, field, func, column_names):\n",
    "    return pd.concat((\n",
    "        dataframe,\n",
    "        dataframe[field].apply(\n",
    "            lambda cell: pd.Series(func(cell), index=column_names))), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the w2v dict from pickle file\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/pickle/embeddings06122024.pickle', 'rb') as w2v_file:\n",
    "    w2v_dict = pickle.load(w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of word embeddings:  4762\n"
     ]
    }
   ],
   "source": [
    "print(\"length of word embeddings: \", len(w2v_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vectors for the essay\n",
    "def vectorizer(sequence):\n",
    "    vect = []\n",
    "    numw = 0\n",
    "    for w in sequence: \n",
    "        try :\n",
    "            if numw == 0:\n",
    "                vect = w2v_dict[w]\n",
    "            else:\n",
    "                vect = np.add(vect, w2v_dict[w])\n",
    "            numw += 1\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    return vect/ numw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into words\n",
    "def split_into_words(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vectorizer\n",
    "unigram_vect = CountVectorizer(ngram_range=(1, 1), min_df=2, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Putting them all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for feature engineering\n",
    "def feature_engineering(original_dataset):\n",
    "\n",
    "    dataset = original_dataset.copy()\n",
    "    # create a new column with sentence tokens\n",
    "    dataset['tokens'] = dataset['sentence'].apply(word_tokenize)\n",
    "    # 1. Similarity features\n",
    "    # biology\n",
    "    dataset['bio_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_bio,'biology',)) \n",
    "    # chemistry\n",
    "    dataset['chem_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_chem,'chemistry',))\n",
    "    # physics\n",
    "    dataset['phy_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_phy,'physics',))\n",
    "    # mathematics\n",
    "    dataset['math_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_maths,'mathematics',))\n",
    "    # technology\n",
    "    dataset['tech_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_tech,'technology',))\n",
    "    # engineering\n",
    "    dataset['eng_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_eng,'engineering',))\n",
    "    \n",
    "    # medical terms\n",
    "    dataset['medical_terms'] = dataset['tokens'].apply(check_medical_words)\n",
    "    \n",
    "    # polarity and subjectivity\n",
    "    dataset['polarity'], dataset['subjectivity'] = zip(*dataset['sentence'].apply(get_sentiment))\n",
    "    \n",
    "    # named entity recognition\n",
    "    dataset['ner'] = dataset['sentence'].apply(named_entity_present)\n",
    "    \n",
    "    # pos tag count\n",
    "    dataset = pos_tag_extraction(dataset, 'tokens', count_pos_tags, ['interjections', 'nouns', 'adverb', 'verb', 'determiner', 'pronoun', 'adjetive','preposition'])\n",
    "    \n",
    "    # labels\n",
    "    data_labels = dataset['label']\n",
    "    # X\n",
    "    data_x = dataset.drop(columns='label')\n",
    "\n",
    "    \n",
    "    # vectorize all the essays\n",
    "    vect_arr = data_x.tokens.apply(vectorizer)\n",
    "    for index in range(0, len(vect_arr)):\n",
    "        i = 0\n",
    "        for item in vect_arr[index]:\n",
    "            column_name= \"embedding\" + str(i)\n",
    "            data_x.loc[index, column_name] = item\n",
    "            i +=1\n",
    "    \n",
    "    return data_x,data_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = feature_engineering(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1609, 121)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = feature_engineering(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 121)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Unigram features for both train and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1609, 121)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 121)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2.1/Aspirational/saved_features/X_train_final.csv\", index=False)\n",
    "X_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2.1/Aspirational/saved_features/X_test_final.csv\", index=False)\n",
    "y_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2.1/Aspirational/saved_features/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2.1/Aspirational/saved_features/y_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the unigram df for train :  (1609, 1188)\n"
     ]
    }
   ],
   "source": [
    "# Unigrams for training set\n",
    "unigram_matrix = unigram_vect.fit_transform(X_train['sentence'])\n",
    "unigrams = pd.DataFrame(unigram_matrix.toarray())\n",
    "print(\"Shape of the unigram df for train : \",unigrams.shape)\n",
    "unigrams = unigrams.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = pd.concat([X_train, unigrams], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final.columns = X_train_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1609, 1309)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test unigram df shape :  (179, 1188)\n"
     ]
    }
   ],
   "source": [
    "unigram_matrix_test = unigram_vect.transform(X_test['sentence'])\n",
    "unigrams_test = pd.DataFrame(unigram_matrix_test.toarray())\n",
    "unigrams_test = unigrams_test.reset_index(drop=True)\n",
    "print(\"Test unigram df shape : \",unigrams_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 1309)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final = pd.concat([X_test, unigrams_test], axis = 1)\n",
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final.columns = X_test_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 1309)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ---- sentence\n",
      "1 ---- phrase\n",
      "2 ---- tokens\n",
      "3 ---- bio_sim_words\n",
      "4 ---- chem_sim_words\n",
      "5 ---- phy_sim_words\n",
      "6 ---- math_sim_words\n",
      "7 ---- tech_sim_words\n",
      "8 ---- eng_sim_words\n",
      "9 ---- medical_terms\n",
      "10 ---- polarity\n",
      "11 ---- subjectivity\n",
      "12 ---- ner\n",
      "13 ---- interjections\n",
      "14 ---- nouns\n",
      "15 ---- adverb\n",
      "16 ---- verb\n",
      "17 ---- determiner\n",
      "18 ---- pronoun\n",
      "19 ---- adjetive\n",
      "20 ---- preposition\n",
      "21 ---- embedding0\n",
      "22 ---- embedding1\n",
      "23 ---- embedding2\n",
      "24 ---- embedding3\n",
      "25 ---- embedding4\n",
      "26 ---- embedding5\n",
      "27 ---- embedding6\n",
      "28 ---- embedding7\n",
      "29 ---- embedding8\n",
      "30 ---- embedding9\n",
      "31 ---- embedding10\n",
      "32 ---- embedding11\n",
      "33 ---- embedding12\n",
      "34 ---- embedding13\n",
      "35 ---- embedding14\n",
      "36 ---- embedding15\n",
      "37 ---- embedding16\n",
      "38 ---- embedding17\n",
      "39 ---- embedding18\n",
      "40 ---- embedding19\n",
      "41 ---- embedding20\n",
      "42 ---- embedding21\n",
      "43 ---- embedding22\n",
      "44 ---- embedding23\n",
      "45 ---- embedding24\n",
      "46 ---- embedding25\n",
      "47 ---- embedding26\n",
      "48 ---- embedding27\n",
      "49 ---- embedding28\n",
      "50 ---- embedding29\n",
      "51 ---- embedding30\n",
      "52 ---- embedding31\n",
      "53 ---- embedding32\n",
      "54 ---- embedding33\n",
      "55 ---- embedding34\n",
      "56 ---- embedding35\n",
      "57 ---- embedding36\n",
      "58 ---- embedding37\n",
      "59 ---- embedding38\n",
      "60 ---- embedding39\n",
      "61 ---- embedding40\n",
      "62 ---- embedding41\n",
      "63 ---- embedding42\n",
      "64 ---- embedding43\n",
      "65 ---- embedding44\n",
      "66 ---- embedding45\n",
      "67 ---- embedding46\n",
      "68 ---- embedding47\n",
      "69 ---- embedding48\n",
      "70 ---- embedding49\n",
      "71 ---- embedding50\n",
      "72 ---- embedding51\n",
      "73 ---- embedding52\n",
      "74 ---- embedding53\n",
      "75 ---- embedding54\n",
      "76 ---- embedding55\n",
      "77 ---- embedding56\n",
      "78 ---- embedding57\n",
      "79 ---- embedding58\n",
      "80 ---- embedding59\n",
      "81 ---- embedding60\n",
      "82 ---- embedding61\n",
      "83 ---- embedding62\n",
      "84 ---- embedding63\n",
      "85 ---- embedding64\n",
      "86 ---- embedding65\n",
      "87 ---- embedding66\n",
      "88 ---- embedding67\n",
      "89 ---- embedding68\n",
      "90 ---- embedding69\n",
      "91 ---- embedding70\n",
      "92 ---- embedding71\n",
      "93 ---- embedding72\n",
      "94 ---- embedding73\n",
      "95 ---- embedding74\n",
      "96 ---- embedding75\n",
      "97 ---- embedding76\n",
      "98 ---- embedding77\n",
      "99 ---- embedding78\n",
      "100 ---- embedding79\n",
      "101 ---- embedding80\n",
      "102 ---- embedding81\n",
      "103 ---- embedding82\n",
      "104 ---- embedding83\n",
      "105 ---- embedding84\n",
      "106 ---- embedding85\n",
      "107 ---- embedding86\n",
      "108 ---- embedding87\n",
      "109 ---- embedding88\n",
      "110 ---- embedding89\n",
      "111 ---- embedding90\n",
      "112 ---- embedding91\n",
      "113 ---- embedding92\n",
      "114 ---- embedding93\n",
      "115 ---- embedding94\n",
      "116 ---- embedding95\n",
      "117 ---- embedding96\n",
      "118 ---- embedding97\n",
      "119 ---- embedding98\n",
      "120 ---- embedding99\n",
      "121 ---- 0\n",
      "122 ---- 1\n",
      "123 ---- 2\n",
      "124 ---- 3\n",
      "125 ---- 4\n",
      "126 ---- 5\n",
      "127 ---- 6\n",
      "128 ---- 7\n",
      "129 ---- 8\n",
      "130 ---- 9\n",
      "131 ---- 10\n",
      "132 ---- 11\n",
      "133 ---- 12\n",
      "134 ---- 13\n",
      "135 ---- 14\n",
      "136 ---- 15\n",
      "137 ---- 16\n",
      "138 ---- 17\n",
      "139 ---- 18\n",
      "140 ---- 19\n",
      "141 ---- 20\n",
      "142 ---- 21\n",
      "143 ---- 22\n",
      "144 ---- 23\n",
      "145 ---- 24\n",
      "146 ---- 25\n",
      "147 ---- 26\n",
      "148 ---- 27\n",
      "149 ---- 28\n",
      "150 ---- 29\n",
      "151 ---- 30\n",
      "152 ---- 31\n",
      "153 ---- 32\n",
      "154 ---- 33\n",
      "155 ---- 34\n",
      "156 ---- 35\n",
      "157 ---- 36\n",
      "158 ---- 37\n",
      "159 ---- 38\n",
      "160 ---- 39\n",
      "161 ---- 40\n",
      "162 ---- 41\n",
      "163 ---- 42\n",
      "164 ---- 43\n",
      "165 ---- 44\n",
      "166 ---- 45\n",
      "167 ---- 46\n",
      "168 ---- 47\n",
      "169 ---- 48\n",
      "170 ---- 49\n",
      "171 ---- 50\n",
      "172 ---- 51\n",
      "173 ---- 52\n",
      "174 ---- 53\n",
      "175 ---- 54\n",
      "176 ---- 55\n",
      "177 ---- 56\n",
      "178 ---- 57\n",
      "179 ---- 58\n",
      "180 ---- 59\n",
      "181 ---- 60\n",
      "182 ---- 61\n",
      "183 ---- 62\n",
      "184 ---- 63\n",
      "185 ---- 64\n",
      "186 ---- 65\n",
      "187 ---- 66\n",
      "188 ---- 67\n",
      "189 ---- 68\n",
      "190 ---- 69\n",
      "191 ---- 70\n",
      "192 ---- 71\n",
      "193 ---- 72\n",
      "194 ---- 73\n",
      "195 ---- 74\n",
      "196 ---- 75\n",
      "197 ---- 76\n",
      "198 ---- 77\n",
      "199 ---- 78\n",
      "200 ---- 79\n",
      "201 ---- 80\n",
      "202 ---- 81\n",
      "203 ---- 82\n",
      "204 ---- 83\n",
      "205 ---- 84\n",
      "206 ---- 85\n",
      "207 ---- 86\n",
      "208 ---- 87\n",
      "209 ---- 88\n",
      "210 ---- 89\n",
      "211 ---- 90\n",
      "212 ---- 91\n",
      "213 ---- 92\n",
      "214 ---- 93\n",
      "215 ---- 94\n",
      "216 ---- 95\n",
      "217 ---- 96\n",
      "218 ---- 97\n",
      "219 ---- 98\n",
      "220 ---- 99\n",
      "221 ---- 100\n",
      "222 ---- 101\n",
      "223 ---- 102\n",
      "224 ---- 103\n",
      "225 ---- 104\n",
      "226 ---- 105\n",
      "227 ---- 106\n",
      "228 ---- 107\n",
      "229 ---- 108\n",
      "230 ---- 109\n",
      "231 ---- 110\n",
      "232 ---- 111\n",
      "233 ---- 112\n",
      "234 ---- 113\n",
      "235 ---- 114\n",
      "236 ---- 115\n",
      "237 ---- 116\n",
      "238 ---- 117\n",
      "239 ---- 118\n",
      "240 ---- 119\n",
      "241 ---- 120\n",
      "242 ---- 121\n",
      "243 ---- 122\n",
      "244 ---- 123\n",
      "245 ---- 124\n",
      "246 ---- 125\n",
      "247 ---- 126\n",
      "248 ---- 127\n",
      "249 ---- 128\n",
      "250 ---- 129\n",
      "251 ---- 130\n",
      "252 ---- 131\n",
      "253 ---- 132\n",
      "254 ---- 133\n",
      "255 ---- 134\n",
      "256 ---- 135\n",
      "257 ---- 136\n",
      "258 ---- 137\n",
      "259 ---- 138\n",
      "260 ---- 139\n",
      "261 ---- 140\n",
      "262 ---- 141\n",
      "263 ---- 142\n",
      "264 ---- 143\n",
      "265 ---- 144\n",
      "266 ---- 145\n",
      "267 ---- 146\n",
      "268 ---- 147\n",
      "269 ---- 148\n",
      "270 ---- 149\n",
      "271 ---- 150\n",
      "272 ---- 151\n",
      "273 ---- 152\n",
      "274 ---- 153\n",
      "275 ---- 154\n",
      "276 ---- 155\n",
      "277 ---- 156\n",
      "278 ---- 157\n",
      "279 ---- 158\n",
      "280 ---- 159\n",
      "281 ---- 160\n",
      "282 ---- 161\n",
      "283 ---- 162\n",
      "284 ---- 163\n",
      "285 ---- 164\n",
      "286 ---- 165\n",
      "287 ---- 166\n",
      "288 ---- 167\n",
      "289 ---- 168\n",
      "290 ---- 169\n",
      "291 ---- 170\n",
      "292 ---- 171\n",
      "293 ---- 172\n",
      "294 ---- 173\n",
      "295 ---- 174\n",
      "296 ---- 175\n",
      "297 ---- 176\n",
      "298 ---- 177\n",
      "299 ---- 178\n",
      "300 ---- 179\n",
      "301 ---- 180\n",
      "302 ---- 181\n",
      "303 ---- 182\n",
      "304 ---- 183\n",
      "305 ---- 184\n",
      "306 ---- 185\n",
      "307 ---- 186\n",
      "308 ---- 187\n",
      "309 ---- 188\n",
      "310 ---- 189\n",
      "311 ---- 190\n",
      "312 ---- 191\n",
      "313 ---- 192\n",
      "314 ---- 193\n",
      "315 ---- 194\n",
      "316 ---- 195\n",
      "317 ---- 196\n",
      "318 ---- 197\n",
      "319 ---- 198\n",
      "320 ---- 199\n",
      "321 ---- 200\n",
      "322 ---- 201\n",
      "323 ---- 202\n",
      "324 ---- 203\n",
      "325 ---- 204\n",
      "326 ---- 205\n",
      "327 ---- 206\n",
      "328 ---- 207\n",
      "329 ---- 208\n",
      "330 ---- 209\n",
      "331 ---- 210\n",
      "332 ---- 211\n",
      "333 ---- 212\n",
      "334 ---- 213\n",
      "335 ---- 214\n",
      "336 ---- 215\n",
      "337 ---- 216\n",
      "338 ---- 217\n",
      "339 ---- 218\n",
      "340 ---- 219\n",
      "341 ---- 220\n",
      "342 ---- 221\n",
      "343 ---- 222\n",
      "344 ---- 223\n",
      "345 ---- 224\n",
      "346 ---- 225\n",
      "347 ---- 226\n",
      "348 ---- 227\n",
      "349 ---- 228\n",
      "350 ---- 229\n",
      "351 ---- 230\n",
      "352 ---- 231\n",
      "353 ---- 232\n",
      "354 ---- 233\n",
      "355 ---- 234\n",
      "356 ---- 235\n",
      "357 ---- 236\n",
      "358 ---- 237\n",
      "359 ---- 238\n",
      "360 ---- 239\n",
      "361 ---- 240\n",
      "362 ---- 241\n",
      "363 ---- 242\n",
      "364 ---- 243\n",
      "365 ---- 244\n",
      "366 ---- 245\n",
      "367 ---- 246\n",
      "368 ---- 247\n",
      "369 ---- 248\n",
      "370 ---- 249\n",
      "371 ---- 250\n",
      "372 ---- 251\n",
      "373 ---- 252\n",
      "374 ---- 253\n",
      "375 ---- 254\n",
      "376 ---- 255\n",
      "377 ---- 256\n",
      "378 ---- 257\n",
      "379 ---- 258\n",
      "380 ---- 259\n",
      "381 ---- 260\n",
      "382 ---- 261\n",
      "383 ---- 262\n",
      "384 ---- 263\n",
      "385 ---- 264\n",
      "386 ---- 265\n",
      "387 ---- 266\n",
      "388 ---- 267\n",
      "389 ---- 268\n",
      "390 ---- 269\n",
      "391 ---- 270\n",
      "392 ---- 271\n",
      "393 ---- 272\n",
      "394 ---- 273\n",
      "395 ---- 274\n",
      "396 ---- 275\n",
      "397 ---- 276\n",
      "398 ---- 277\n",
      "399 ---- 278\n",
      "400 ---- 279\n",
      "401 ---- 280\n",
      "402 ---- 281\n",
      "403 ---- 282\n",
      "404 ---- 283\n",
      "405 ---- 284\n",
      "406 ---- 285\n",
      "407 ---- 286\n",
      "408 ---- 287\n",
      "409 ---- 288\n",
      "410 ---- 289\n",
      "411 ---- 290\n",
      "412 ---- 291\n",
      "413 ---- 292\n",
      "414 ---- 293\n",
      "415 ---- 294\n",
      "416 ---- 295\n",
      "417 ---- 296\n",
      "418 ---- 297\n",
      "419 ---- 298\n",
      "420 ---- 299\n",
      "421 ---- 300\n",
      "422 ---- 301\n",
      "423 ---- 302\n",
      "424 ---- 303\n",
      "425 ---- 304\n",
      "426 ---- 305\n",
      "427 ---- 306\n",
      "428 ---- 307\n",
      "429 ---- 308\n",
      "430 ---- 309\n",
      "431 ---- 310\n",
      "432 ---- 311\n",
      "433 ---- 312\n",
      "434 ---- 313\n",
      "435 ---- 314\n",
      "436 ---- 315\n",
      "437 ---- 316\n",
      "438 ---- 317\n",
      "439 ---- 318\n",
      "440 ---- 319\n",
      "441 ---- 320\n",
      "442 ---- 321\n",
      "443 ---- 322\n",
      "444 ---- 323\n",
      "445 ---- 324\n",
      "446 ---- 325\n",
      "447 ---- 326\n",
      "448 ---- 327\n",
      "449 ---- 328\n",
      "450 ---- 329\n",
      "451 ---- 330\n",
      "452 ---- 331\n",
      "453 ---- 332\n",
      "454 ---- 333\n",
      "455 ---- 334\n",
      "456 ---- 335\n",
      "457 ---- 336\n",
      "458 ---- 337\n",
      "459 ---- 338\n",
      "460 ---- 339\n",
      "461 ---- 340\n",
      "462 ---- 341\n",
      "463 ---- 342\n",
      "464 ---- 343\n",
      "465 ---- 344\n",
      "466 ---- 345\n",
      "467 ---- 346\n",
      "468 ---- 347\n",
      "469 ---- 348\n",
      "470 ---- 349\n",
      "471 ---- 350\n",
      "472 ---- 351\n",
      "473 ---- 352\n",
      "474 ---- 353\n",
      "475 ---- 354\n",
      "476 ---- 355\n",
      "477 ---- 356\n",
      "478 ---- 357\n",
      "479 ---- 358\n",
      "480 ---- 359\n",
      "481 ---- 360\n",
      "482 ---- 361\n",
      "483 ---- 362\n",
      "484 ---- 363\n",
      "485 ---- 364\n",
      "486 ---- 365\n",
      "487 ---- 366\n",
      "488 ---- 367\n",
      "489 ---- 368\n",
      "490 ---- 369\n",
      "491 ---- 370\n",
      "492 ---- 371\n",
      "493 ---- 372\n",
      "494 ---- 373\n",
      "495 ---- 374\n",
      "496 ---- 375\n",
      "497 ---- 376\n",
      "498 ---- 377\n",
      "499 ---- 378\n",
      "500 ---- 379\n",
      "501 ---- 380\n",
      "502 ---- 381\n",
      "503 ---- 382\n",
      "504 ---- 383\n",
      "505 ---- 384\n",
      "506 ---- 385\n",
      "507 ---- 386\n",
      "508 ---- 387\n",
      "509 ---- 388\n",
      "510 ---- 389\n",
      "511 ---- 390\n",
      "512 ---- 391\n",
      "513 ---- 392\n",
      "514 ---- 393\n",
      "515 ---- 394\n",
      "516 ---- 395\n",
      "517 ---- 396\n",
      "518 ---- 397\n",
      "519 ---- 398\n",
      "520 ---- 399\n",
      "521 ---- 400\n",
      "522 ---- 401\n",
      "523 ---- 402\n",
      "524 ---- 403\n",
      "525 ---- 404\n",
      "526 ---- 405\n",
      "527 ---- 406\n",
      "528 ---- 407\n",
      "529 ---- 408\n",
      "530 ---- 409\n",
      "531 ---- 410\n",
      "532 ---- 411\n",
      "533 ---- 412\n",
      "534 ---- 413\n",
      "535 ---- 414\n",
      "536 ---- 415\n",
      "537 ---- 416\n",
      "538 ---- 417\n",
      "539 ---- 418\n",
      "540 ---- 419\n",
      "541 ---- 420\n",
      "542 ---- 421\n",
      "543 ---- 422\n",
      "544 ---- 423\n",
      "545 ---- 424\n",
      "546 ---- 425\n",
      "547 ---- 426\n",
      "548 ---- 427\n",
      "549 ---- 428\n",
      "550 ---- 429\n",
      "551 ---- 430\n",
      "552 ---- 431\n",
      "553 ---- 432\n",
      "554 ---- 433\n",
      "555 ---- 434\n",
      "556 ---- 435\n",
      "557 ---- 436\n",
      "558 ---- 437\n",
      "559 ---- 438\n",
      "560 ---- 439\n",
      "561 ---- 440\n",
      "562 ---- 441\n",
      "563 ---- 442\n",
      "564 ---- 443\n",
      "565 ---- 444\n",
      "566 ---- 445\n",
      "567 ---- 446\n",
      "568 ---- 447\n",
      "569 ---- 448\n",
      "570 ---- 449\n",
      "571 ---- 450\n",
      "572 ---- 451\n",
      "573 ---- 452\n",
      "574 ---- 453\n",
      "575 ---- 454\n",
      "576 ---- 455\n",
      "577 ---- 456\n",
      "578 ---- 457\n",
      "579 ---- 458\n",
      "580 ---- 459\n",
      "581 ---- 460\n",
      "582 ---- 461\n",
      "583 ---- 462\n",
      "584 ---- 463\n",
      "585 ---- 464\n",
      "586 ---- 465\n",
      "587 ---- 466\n",
      "588 ---- 467\n",
      "589 ---- 468\n",
      "590 ---- 469\n",
      "591 ---- 470\n",
      "592 ---- 471\n",
      "593 ---- 472\n",
      "594 ---- 473\n",
      "595 ---- 474\n",
      "596 ---- 475\n",
      "597 ---- 476\n",
      "598 ---- 477\n",
      "599 ---- 478\n",
      "600 ---- 479\n",
      "601 ---- 480\n",
      "602 ---- 481\n",
      "603 ---- 482\n",
      "604 ---- 483\n",
      "605 ---- 484\n",
      "606 ---- 485\n",
      "607 ---- 486\n",
      "608 ---- 487\n",
      "609 ---- 488\n",
      "610 ---- 489\n",
      "611 ---- 490\n",
      "612 ---- 491\n",
      "613 ---- 492\n",
      "614 ---- 493\n",
      "615 ---- 494\n",
      "616 ---- 495\n",
      "617 ---- 496\n",
      "618 ---- 497\n",
      "619 ---- 498\n",
      "620 ---- 499\n",
      "621 ---- 500\n",
      "622 ---- 501\n",
      "623 ---- 502\n",
      "624 ---- 503\n",
      "625 ---- 504\n",
      "626 ---- 505\n",
      "627 ---- 506\n",
      "628 ---- 507\n",
      "629 ---- 508\n",
      "630 ---- 509\n",
      "631 ---- 510\n",
      "632 ---- 511\n",
      "633 ---- 512\n",
      "634 ---- 513\n",
      "635 ---- 514\n",
      "636 ---- 515\n",
      "637 ---- 516\n",
      "638 ---- 517\n",
      "639 ---- 518\n",
      "640 ---- 519\n",
      "641 ---- 520\n",
      "642 ---- 521\n",
      "643 ---- 522\n",
      "644 ---- 523\n",
      "645 ---- 524\n",
      "646 ---- 525\n",
      "647 ---- 526\n",
      "648 ---- 527\n",
      "649 ---- 528\n",
      "650 ---- 529\n",
      "651 ---- 530\n",
      "652 ---- 531\n",
      "653 ---- 532\n",
      "654 ---- 533\n",
      "655 ---- 534\n",
      "656 ---- 535\n",
      "657 ---- 536\n",
      "658 ---- 537\n",
      "659 ---- 538\n",
      "660 ---- 539\n",
      "661 ---- 540\n",
      "662 ---- 541\n",
      "663 ---- 542\n",
      "664 ---- 543\n",
      "665 ---- 544\n",
      "666 ---- 545\n",
      "667 ---- 546\n",
      "668 ---- 547\n",
      "669 ---- 548\n",
      "670 ---- 549\n",
      "671 ---- 550\n",
      "672 ---- 551\n",
      "673 ---- 552\n",
      "674 ---- 553\n",
      "675 ---- 554\n",
      "676 ---- 555\n",
      "677 ---- 556\n",
      "678 ---- 557\n",
      "679 ---- 558\n",
      "680 ---- 559\n",
      "681 ---- 560\n",
      "682 ---- 561\n",
      "683 ---- 562\n",
      "684 ---- 563\n",
      "685 ---- 564\n",
      "686 ---- 565\n",
      "687 ---- 566\n",
      "688 ---- 567\n",
      "689 ---- 568\n",
      "690 ---- 569\n",
      "691 ---- 570\n",
      "692 ---- 571\n",
      "693 ---- 572\n",
      "694 ---- 573\n",
      "695 ---- 574\n",
      "696 ---- 575\n",
      "697 ---- 576\n",
      "698 ---- 577\n",
      "699 ---- 578\n",
      "700 ---- 579\n",
      "701 ---- 580\n",
      "702 ---- 581\n",
      "703 ---- 582\n",
      "704 ---- 583\n",
      "705 ---- 584\n",
      "706 ---- 585\n",
      "707 ---- 586\n",
      "708 ---- 587\n",
      "709 ---- 588\n",
      "710 ---- 589\n",
      "711 ---- 590\n",
      "712 ---- 591\n",
      "713 ---- 592\n",
      "714 ---- 593\n",
      "715 ---- 594\n",
      "716 ---- 595\n",
      "717 ---- 596\n",
      "718 ---- 597\n",
      "719 ---- 598\n",
      "720 ---- 599\n",
      "721 ---- 600\n",
      "722 ---- 601\n",
      "723 ---- 602\n",
      "724 ---- 603\n",
      "725 ---- 604\n",
      "726 ---- 605\n",
      "727 ---- 606\n",
      "728 ---- 607\n",
      "729 ---- 608\n",
      "730 ---- 609\n",
      "731 ---- 610\n",
      "732 ---- 611\n",
      "733 ---- 612\n",
      "734 ---- 613\n",
      "735 ---- 614\n",
      "736 ---- 615\n",
      "737 ---- 616\n",
      "738 ---- 617\n",
      "739 ---- 618\n",
      "740 ---- 619\n",
      "741 ---- 620\n",
      "742 ---- 621\n",
      "743 ---- 622\n",
      "744 ---- 623\n",
      "745 ---- 624\n",
      "746 ---- 625\n",
      "747 ---- 626\n",
      "748 ---- 627\n",
      "749 ---- 628\n",
      "750 ---- 629\n",
      "751 ---- 630\n",
      "752 ---- 631\n",
      "753 ---- 632\n",
      "754 ---- 633\n",
      "755 ---- 634\n",
      "756 ---- 635\n",
      "757 ---- 636\n",
      "758 ---- 637\n",
      "759 ---- 638\n",
      "760 ---- 639\n",
      "761 ---- 640\n",
      "762 ---- 641\n",
      "763 ---- 642\n",
      "764 ---- 643\n",
      "765 ---- 644\n",
      "766 ---- 645\n",
      "767 ---- 646\n",
      "768 ---- 647\n",
      "769 ---- 648\n",
      "770 ---- 649\n",
      "771 ---- 650\n",
      "772 ---- 651\n",
      "773 ---- 652\n",
      "774 ---- 653\n",
      "775 ---- 654\n",
      "776 ---- 655\n",
      "777 ---- 656\n",
      "778 ---- 657\n",
      "779 ---- 658\n",
      "780 ---- 659\n",
      "781 ---- 660\n",
      "782 ---- 661\n",
      "783 ---- 662\n",
      "784 ---- 663\n",
      "785 ---- 664\n",
      "786 ---- 665\n",
      "787 ---- 666\n",
      "788 ---- 667\n",
      "789 ---- 668\n",
      "790 ---- 669\n",
      "791 ---- 670\n",
      "792 ---- 671\n",
      "793 ---- 672\n",
      "794 ---- 673\n",
      "795 ---- 674\n",
      "796 ---- 675\n",
      "797 ---- 676\n",
      "798 ---- 677\n",
      "799 ---- 678\n",
      "800 ---- 679\n",
      "801 ---- 680\n",
      "802 ---- 681\n",
      "803 ---- 682\n",
      "804 ---- 683\n",
      "805 ---- 684\n",
      "806 ---- 685\n",
      "807 ---- 686\n",
      "808 ---- 687\n",
      "809 ---- 688\n",
      "810 ---- 689\n",
      "811 ---- 690\n",
      "812 ---- 691\n",
      "813 ---- 692\n",
      "814 ---- 693\n",
      "815 ---- 694\n",
      "816 ---- 695\n",
      "817 ---- 696\n",
      "818 ---- 697\n",
      "819 ---- 698\n",
      "820 ---- 699\n",
      "821 ---- 700\n",
      "822 ---- 701\n",
      "823 ---- 702\n",
      "824 ---- 703\n",
      "825 ---- 704\n",
      "826 ---- 705\n",
      "827 ---- 706\n",
      "828 ---- 707\n",
      "829 ---- 708\n",
      "830 ---- 709\n",
      "831 ---- 710\n",
      "832 ---- 711\n",
      "833 ---- 712\n",
      "834 ---- 713\n",
      "835 ---- 714\n",
      "836 ---- 715\n",
      "837 ---- 716\n",
      "838 ---- 717\n",
      "839 ---- 718\n",
      "840 ---- 719\n",
      "841 ---- 720\n",
      "842 ---- 721\n",
      "843 ---- 722\n",
      "844 ---- 723\n",
      "845 ---- 724\n",
      "846 ---- 725\n",
      "847 ---- 726\n",
      "848 ---- 727\n",
      "849 ---- 728\n",
      "850 ---- 729\n",
      "851 ---- 730\n",
      "852 ---- 731\n",
      "853 ---- 732\n",
      "854 ---- 733\n",
      "855 ---- 734\n",
      "856 ---- 735\n",
      "857 ---- 736\n",
      "858 ---- 737\n",
      "859 ---- 738\n",
      "860 ---- 739\n",
      "861 ---- 740\n",
      "862 ---- 741\n",
      "863 ---- 742\n",
      "864 ---- 743\n",
      "865 ---- 744\n",
      "866 ---- 745\n",
      "867 ---- 746\n",
      "868 ---- 747\n",
      "869 ---- 748\n",
      "870 ---- 749\n",
      "871 ---- 750\n",
      "872 ---- 751\n",
      "873 ---- 752\n",
      "874 ---- 753\n",
      "875 ---- 754\n",
      "876 ---- 755\n",
      "877 ---- 756\n",
      "878 ---- 757\n",
      "879 ---- 758\n",
      "880 ---- 759\n",
      "881 ---- 760\n",
      "882 ---- 761\n",
      "883 ---- 762\n",
      "884 ---- 763\n",
      "885 ---- 764\n",
      "886 ---- 765\n",
      "887 ---- 766\n",
      "888 ---- 767\n",
      "889 ---- 768\n",
      "890 ---- 769\n",
      "891 ---- 770\n",
      "892 ---- 771\n",
      "893 ---- 772\n",
      "894 ---- 773\n",
      "895 ---- 774\n",
      "896 ---- 775\n",
      "897 ---- 776\n",
      "898 ---- 777\n",
      "899 ---- 778\n",
      "900 ---- 779\n",
      "901 ---- 780\n",
      "902 ---- 781\n",
      "903 ---- 782\n",
      "904 ---- 783\n",
      "905 ---- 784\n",
      "906 ---- 785\n",
      "907 ---- 786\n",
      "908 ---- 787\n",
      "909 ---- 788\n",
      "910 ---- 789\n",
      "911 ---- 790\n",
      "912 ---- 791\n",
      "913 ---- 792\n",
      "914 ---- 793\n",
      "915 ---- 794\n",
      "916 ---- 795\n",
      "917 ---- 796\n",
      "918 ---- 797\n",
      "919 ---- 798\n",
      "920 ---- 799\n",
      "921 ---- 800\n",
      "922 ---- 801\n",
      "923 ---- 802\n",
      "924 ---- 803\n",
      "925 ---- 804\n",
      "926 ---- 805\n",
      "927 ---- 806\n",
      "928 ---- 807\n",
      "929 ---- 808\n",
      "930 ---- 809\n",
      "931 ---- 810\n",
      "932 ---- 811\n",
      "933 ---- 812\n",
      "934 ---- 813\n",
      "935 ---- 814\n",
      "936 ---- 815\n",
      "937 ---- 816\n",
      "938 ---- 817\n",
      "939 ---- 818\n",
      "940 ---- 819\n",
      "941 ---- 820\n",
      "942 ---- 821\n",
      "943 ---- 822\n",
      "944 ---- 823\n",
      "945 ---- 824\n",
      "946 ---- 825\n",
      "947 ---- 826\n",
      "948 ---- 827\n",
      "949 ---- 828\n",
      "950 ---- 829\n",
      "951 ---- 830\n",
      "952 ---- 831\n",
      "953 ---- 832\n",
      "954 ---- 833\n",
      "955 ---- 834\n",
      "956 ---- 835\n",
      "957 ---- 836\n",
      "958 ---- 837\n",
      "959 ---- 838\n",
      "960 ---- 839\n",
      "961 ---- 840\n",
      "962 ---- 841\n",
      "963 ---- 842\n",
      "964 ---- 843\n",
      "965 ---- 844\n",
      "966 ---- 845\n",
      "967 ---- 846\n",
      "968 ---- 847\n",
      "969 ---- 848\n",
      "970 ---- 849\n",
      "971 ---- 850\n",
      "972 ---- 851\n",
      "973 ---- 852\n",
      "974 ---- 853\n",
      "975 ---- 854\n",
      "976 ---- 855\n",
      "977 ---- 856\n",
      "978 ---- 857\n",
      "979 ---- 858\n",
      "980 ---- 859\n",
      "981 ---- 860\n",
      "982 ---- 861\n",
      "983 ---- 862\n",
      "984 ---- 863\n",
      "985 ---- 864\n",
      "986 ---- 865\n",
      "987 ---- 866\n",
      "988 ---- 867\n",
      "989 ---- 868\n",
      "990 ---- 869\n",
      "991 ---- 870\n",
      "992 ---- 871\n",
      "993 ---- 872\n",
      "994 ---- 873\n",
      "995 ---- 874\n",
      "996 ---- 875\n",
      "997 ---- 876\n",
      "998 ---- 877\n",
      "999 ---- 878\n",
      "1000 ---- 879\n",
      "1001 ---- 880\n",
      "1002 ---- 881\n",
      "1003 ---- 882\n",
      "1004 ---- 883\n",
      "1005 ---- 884\n",
      "1006 ---- 885\n",
      "1007 ---- 886\n",
      "1008 ---- 887\n",
      "1009 ---- 888\n",
      "1010 ---- 889\n",
      "1011 ---- 890\n",
      "1012 ---- 891\n",
      "1013 ---- 892\n",
      "1014 ---- 893\n",
      "1015 ---- 894\n",
      "1016 ---- 895\n",
      "1017 ---- 896\n",
      "1018 ---- 897\n",
      "1019 ---- 898\n",
      "1020 ---- 899\n",
      "1021 ---- 900\n",
      "1022 ---- 901\n",
      "1023 ---- 902\n",
      "1024 ---- 903\n",
      "1025 ---- 904\n",
      "1026 ---- 905\n",
      "1027 ---- 906\n",
      "1028 ---- 907\n",
      "1029 ---- 908\n",
      "1030 ---- 909\n",
      "1031 ---- 910\n",
      "1032 ---- 911\n",
      "1033 ---- 912\n",
      "1034 ---- 913\n",
      "1035 ---- 914\n",
      "1036 ---- 915\n",
      "1037 ---- 916\n",
      "1038 ---- 917\n",
      "1039 ---- 918\n",
      "1040 ---- 919\n",
      "1041 ---- 920\n",
      "1042 ---- 921\n",
      "1043 ---- 922\n",
      "1044 ---- 923\n",
      "1045 ---- 924\n",
      "1046 ---- 925\n",
      "1047 ---- 926\n",
      "1048 ---- 927\n",
      "1049 ---- 928\n",
      "1050 ---- 929\n",
      "1051 ---- 930\n",
      "1052 ---- 931\n",
      "1053 ---- 932\n",
      "1054 ---- 933\n",
      "1055 ---- 934\n",
      "1056 ---- 935\n",
      "1057 ---- 936\n",
      "1058 ---- 937\n",
      "1059 ---- 938\n",
      "1060 ---- 939\n",
      "1061 ---- 940\n",
      "1062 ---- 941\n",
      "1063 ---- 942\n",
      "1064 ---- 943\n",
      "1065 ---- 944\n",
      "1066 ---- 945\n",
      "1067 ---- 946\n",
      "1068 ---- 947\n",
      "1069 ---- 948\n",
      "1070 ---- 949\n",
      "1071 ---- 950\n",
      "1072 ---- 951\n",
      "1073 ---- 952\n",
      "1074 ---- 953\n",
      "1075 ---- 954\n",
      "1076 ---- 955\n",
      "1077 ---- 956\n",
      "1078 ---- 957\n",
      "1079 ---- 958\n",
      "1080 ---- 959\n",
      "1081 ---- 960\n",
      "1082 ---- 961\n",
      "1083 ---- 962\n",
      "1084 ---- 963\n",
      "1085 ---- 964\n",
      "1086 ---- 965\n",
      "1087 ---- 966\n",
      "1088 ---- 967\n",
      "1089 ---- 968\n",
      "1090 ---- 969\n",
      "1091 ---- 970\n",
      "1092 ---- 971\n",
      "1093 ---- 972\n",
      "1094 ---- 973\n",
      "1095 ---- 974\n",
      "1096 ---- 975\n",
      "1097 ---- 976\n",
      "1098 ---- 977\n",
      "1099 ---- 978\n",
      "1100 ---- 979\n",
      "1101 ---- 980\n",
      "1102 ---- 981\n",
      "1103 ---- 982\n",
      "1104 ---- 983\n",
      "1105 ---- 984\n",
      "1106 ---- 985\n",
      "1107 ---- 986\n",
      "1108 ---- 987\n",
      "1109 ---- 988\n",
      "1110 ---- 989\n",
      "1111 ---- 990\n",
      "1112 ---- 991\n",
      "1113 ---- 992\n",
      "1114 ---- 993\n",
      "1115 ---- 994\n",
      "1116 ---- 995\n",
      "1117 ---- 996\n",
      "1118 ---- 997\n",
      "1119 ---- 998\n",
      "1120 ---- 999\n",
      "1121 ---- 1000\n",
      "1122 ---- 1001\n",
      "1123 ---- 1002\n",
      "1124 ---- 1003\n",
      "1125 ---- 1004\n",
      "1126 ---- 1005\n",
      "1127 ---- 1006\n",
      "1128 ---- 1007\n",
      "1129 ---- 1008\n",
      "1130 ---- 1009\n",
      "1131 ---- 1010\n",
      "1132 ---- 1011\n",
      "1133 ---- 1012\n",
      "1134 ---- 1013\n",
      "1135 ---- 1014\n",
      "1136 ---- 1015\n",
      "1137 ---- 1016\n",
      "1138 ---- 1017\n",
      "1139 ---- 1018\n",
      "1140 ---- 1019\n",
      "1141 ---- 1020\n",
      "1142 ---- 1021\n",
      "1143 ---- 1022\n",
      "1144 ---- 1023\n",
      "1145 ---- 1024\n",
      "1146 ---- 1025\n",
      "1147 ---- 1026\n",
      "1148 ---- 1027\n",
      "1149 ---- 1028\n",
      "1150 ---- 1029\n",
      "1151 ---- 1030\n",
      "1152 ---- 1031\n",
      "1153 ---- 1032\n",
      "1154 ---- 1033\n",
      "1155 ---- 1034\n",
      "1156 ---- 1035\n",
      "1157 ---- 1036\n",
      "1158 ---- 1037\n",
      "1159 ---- 1038\n",
      "1160 ---- 1039\n",
      "1161 ---- 1040\n",
      "1162 ---- 1041\n",
      "1163 ---- 1042\n",
      "1164 ---- 1043\n",
      "1165 ---- 1044\n",
      "1166 ---- 1045\n",
      "1167 ---- 1046\n",
      "1168 ---- 1047\n",
      "1169 ---- 1048\n",
      "1170 ---- 1049\n",
      "1171 ---- 1050\n",
      "1172 ---- 1051\n",
      "1173 ---- 1052\n",
      "1174 ---- 1053\n",
      "1175 ---- 1054\n",
      "1176 ---- 1055\n",
      "1177 ---- 1056\n",
      "1178 ---- 1057\n",
      "1179 ---- 1058\n",
      "1180 ---- 1059\n",
      "1181 ---- 1060\n",
      "1182 ---- 1061\n",
      "1183 ---- 1062\n",
      "1184 ---- 1063\n",
      "1185 ---- 1064\n",
      "1186 ---- 1065\n",
      "1187 ---- 1066\n",
      "1188 ---- 1067\n",
      "1189 ---- 1068\n",
      "1190 ---- 1069\n",
      "1191 ---- 1070\n",
      "1192 ---- 1071\n",
      "1193 ---- 1072\n",
      "1194 ---- 1073\n",
      "1195 ---- 1074\n",
      "1196 ---- 1075\n",
      "1197 ---- 1076\n",
      "1198 ---- 1077\n",
      "1199 ---- 1078\n",
      "1200 ---- 1079\n",
      "1201 ---- 1080\n",
      "1202 ---- 1081\n",
      "1203 ---- 1082\n",
      "1204 ---- 1083\n",
      "1205 ---- 1084\n",
      "1206 ---- 1085\n",
      "1207 ---- 1086\n",
      "1208 ---- 1087\n",
      "1209 ---- 1088\n",
      "1210 ---- 1089\n",
      "1211 ---- 1090\n",
      "1212 ---- 1091\n",
      "1213 ---- 1092\n",
      "1214 ---- 1093\n",
      "1215 ---- 1094\n",
      "1216 ---- 1095\n",
      "1217 ---- 1096\n",
      "1218 ---- 1097\n",
      "1219 ---- 1098\n",
      "1220 ---- 1099\n",
      "1221 ---- 1100\n",
      "1222 ---- 1101\n",
      "1223 ---- 1102\n",
      "1224 ---- 1103\n",
      "1225 ---- 1104\n",
      "1226 ---- 1105\n",
      "1227 ---- 1106\n",
      "1228 ---- 1107\n",
      "1229 ---- 1108\n",
      "1230 ---- 1109\n",
      "1231 ---- 1110\n",
      "1232 ---- 1111\n",
      "1233 ---- 1112\n",
      "1234 ---- 1113\n",
      "1235 ---- 1114\n",
      "1236 ---- 1115\n",
      "1237 ---- 1116\n",
      "1238 ---- 1117\n",
      "1239 ---- 1118\n",
      "1240 ---- 1119\n",
      "1241 ---- 1120\n",
      "1242 ---- 1121\n",
      "1243 ---- 1122\n",
      "1244 ---- 1123\n",
      "1245 ---- 1124\n",
      "1246 ---- 1125\n",
      "1247 ---- 1126\n",
      "1248 ---- 1127\n",
      "1249 ---- 1128\n",
      "1250 ---- 1129\n",
      "1251 ---- 1130\n",
      "1252 ---- 1131\n",
      "1253 ---- 1132\n",
      "1254 ---- 1133\n",
      "1255 ---- 1134\n",
      "1256 ---- 1135\n",
      "1257 ---- 1136\n",
      "1258 ---- 1137\n",
      "1259 ---- 1138\n",
      "1260 ---- 1139\n",
      "1261 ---- 1140\n",
      "1262 ---- 1141\n",
      "1263 ---- 1142\n",
      "1264 ---- 1143\n",
      "1265 ---- 1144\n",
      "1266 ---- 1145\n",
      "1267 ---- 1146\n",
      "1268 ---- 1147\n",
      "1269 ---- 1148\n",
      "1270 ---- 1149\n",
      "1271 ---- 1150\n",
      "1272 ---- 1151\n",
      "1273 ---- 1152\n",
      "1274 ---- 1153\n",
      "1275 ---- 1154\n",
      "1276 ---- 1155\n",
      "1277 ---- 1156\n",
      "1278 ---- 1157\n",
      "1279 ---- 1158\n",
      "1280 ---- 1159\n",
      "1281 ---- 1160\n",
      "1282 ---- 1161\n",
      "1283 ---- 1162\n",
      "1284 ---- 1163\n",
      "1285 ---- 1164\n",
      "1286 ---- 1165\n",
      "1287 ---- 1166\n",
      "1288 ---- 1167\n",
      "1289 ---- 1168\n",
      "1290 ---- 1169\n",
      "1291 ---- 1170\n",
      "1292 ---- 1171\n",
      "1293 ---- 1172\n",
      "1294 ---- 1173\n",
      "1295 ---- 1174\n",
      "1296 ---- 1175\n",
      "1297 ---- 1176\n",
      "1298 ---- 1177\n",
      "1299 ---- 1178\n",
      "1300 ---- 1179\n",
      "1301 ---- 1180\n",
      "1302 ---- 1181\n",
      "1303 ---- 1182\n",
      "1304 ---- 1183\n",
      "1305 ---- 1184\n",
      "1306 ---- 1185\n",
      "1307 ---- 1186\n",
      "1308 ---- 1187\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(X_train_final.columns)):\n",
    "    print('{} ---- {}'.format(i, X_train_final.columns[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Model 6: Without STEM Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_6 = X_train_final.iloc[:,np.r_[10:1308]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1609, 1298)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_6 = X_test_final.iloc[:,np.r_[10:1308]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 1298)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.741\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.79      0.78        90\n",
      "           1       0.78      0.78      0.78        89\n",
      "\n",
      "    accuracy                           0.78       179\n",
      "   macro avg       0.78      0.78      0.78       179\n",
      "weighted avg       0.78      0.78      0.78       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_6_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_6_pipeline, parameters, scoring=\"average_precision\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_6,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_6), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 71, False Positive: 19, False Negative: 20, True Positive: 69\n",
      "--------------------------------------------------------------------------------\n",
      "[[71 19]\n",
      " [20 69]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.79      0.78        90\n",
      "           1       0.78      0.78      0.78        89\n",
      "\n",
      "    accuracy                           0.78       179\n",
      "   macro avg       0.78      0.78      0.78       179\n",
      "weighted avg       0.78      0.78      0.78       179\n",
      "\n",
      "Average Precision: 0.7196\n"
     ]
    }
   ],
   "source": [
    "lr_model_6 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_6, y_train)\n",
    "y_lr = lr_model_6.predict(X_test_model_6)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))\n",
    "\n",
    "# Calculate and print the average precision score\n",
    "avg_precision = average_precision_score(y_test, y_lr)\n",
    "print(f'Average Precision: {avg_precision:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF Model 4: Without Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [10,20,50,100,200,300]\n",
    "max_depth = [2,5,8,10,15,20]\n",
    "min_samples_split = [2, 5, 10, 15, 20]\n",
    "min_samples_leaf = [1, 2, 5, 10,20]\n",
    "rf_parameters = dict(n_estimators = n_estimators, max_depth = max_depth,  \n",
    "              min_samples_split = min_samples_split, \n",
    "              min_samples_leaf = min_samples_leaf)\n",
    "\n",
    "## reduced parameters\n",
    "# n_estimators = [50, 100, 200]       # Reduced options\n",
    "# max_depth = [5, 10, 15]             # Reduced options\n",
    "# min_samples_split = [2, 10]         # Reduced options\n",
    "# min_samples_leaf = [1, 5]           # Reduced options\n",
    "\n",
    "# rf_parameters = dict(\n",
    "#     n_estimators = n_estimators, \n",
    "#     max_depth = max_depth,  \n",
    "#     min_samples_split = min_samples_split, \n",
    "#     min_samples_leaf = min_samples_leaf\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_4 = X_train_final.iloc[:,np.r_[3:21,121:1308]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1609, 1205)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_4 = X_test_final.iloc[:,np.r_[3:21,121:1308]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 1205)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 900 candidates, totalling 9000 fits\n",
      "Best score: 0.736\n",
      "Best parameters set:\n",
      "\tmax_depth: 20\n",
      "\tmin_samples_leaf: 2\n",
      "\tmin_samples_split: 2\n",
      "\tn_estimators: 200\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.79      0.79        90\n",
      "           1       0.79      0.79      0.79        89\n",
      "\n",
      "    accuracy                           0.79       179\n",
      "   macro avg       0.79      0.79      0.79       179\n",
      "weighted avg       0.79      0.79      0.79       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_model_4 = RandomForestClassifier(random_state=18,class_weight='balanced')\n",
    "grid_search = GridSearchCV(rf_model_4, rf_parameters, scoring=\"average_precision\", cv = 10, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_model_4,y_train)\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(rf_parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_4), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 71, False Positive: 19, False Negative: 19, True Positive: 70\n",
      "--------------------------------------------------------------------------------\n",
      "[[71 19]\n",
      " [19 70]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.79      0.79        90\n",
      "           1       0.79      0.79      0.79        89\n",
      "\n",
      "    accuracy                           0.79       179\n",
      "   macro avg       0.79      0.79      0.79       179\n",
      "weighted avg       0.79      0.79      0.79       179\n",
      "\n",
      "Average Precision: 0.7248\n"
     ]
    }
   ],
   "source": [
    "randomForest_4 = RandomForestClassifier(random_state=18,\n",
    "                                        class_weight=best_parameters['class_weight'],\n",
    "                                        max_depth=best_parameters['max_depth'],\n",
    "                                        min_samples_leaf=best_parameters['min_samples_leaf'],\n",
    "                                        min_samples_split=best_parameters['min_samples_split'],\n",
    "                                        n_estimators=best_parameters['n_estimators']).fit(X_train_model_4, y_train)\n",
    "\n",
    "y_lr = randomForest_4.predict(X_test_model_4)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))\n",
    "\n",
    "# Calculate and print the average precision score\n",
    "avg_precision = average_precision_score(y_test, y_lr)\n",
    "print(f'Average Precision: {avg_precision:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
