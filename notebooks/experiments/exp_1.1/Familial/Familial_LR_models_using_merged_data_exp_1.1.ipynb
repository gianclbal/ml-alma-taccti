{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Familial Logistic Regression Models Using Merged Data Experiment 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4a41b56fc449ddbcee19fbd2c3bb8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 18:00:15 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-07-19 18:00:15 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-07-19 18:00:16 INFO: File exists: /Users/gbaldonado/stanza_resources/en/default.zip\n",
      "2024-07-19 18:00:21 INFO: Finished downloading models and saved to /Users/gbaldonado/stanza_resources\n",
      "2024-07-19 18:00:21 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b6abbff2894cd5af9a593c3e0481e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 18:00:21 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-07-19 18:00:23 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-07-19 18:00:23 INFO: Using device: cpu\n",
      "2024-07-19 18:00:23 INFO: Loading: tokenize\n",
      "2024-07-19 18:00:23 INFO: Loading: mwt\n",
      "2024-07-19 18:00:24 INFO: Loading: pos\n",
      "2024-07-19 18:00:24 INFO: Loading: lemma\n",
      "2024-07-19 18:00:24 INFO: Loading: constituency\n",
      "2024-07-19 18:00:24 INFO: Loading: depparse\n",
      "2024-07-19 18:00:25 INFO: Loading: sentiment\n",
      "2024-07-19 18:00:25 INFO: Loading: ner\n",
      "2024-07-19 18:00:27 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import pickle\n",
    "import warnings\n",
    "import stanza\n",
    "\n",
    "from random import shuffle\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, f1_score, r2_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Set random seed\n",
    "random.seed(18)\n",
    "seed = 18\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Initialize lemmatizer, stop words, and stanza\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline('en') # initialize English neural pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data and quick exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_familial_df = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/merged_Familial_sentence_level_batch_1_jaccard.csv\", encoding='utf-8')\n",
    "\n",
    "# Shuffle the merged dataset\n",
    "merged_familial_df = shuffle(merged_familial_df, random_state=seed)\n",
    "\n",
    "# Train-test split \n",
    "training_df, test_df = train_test_split(merged_familial_df, test_size=0.1, random_state=18, stratify=merged_familial_df['label'])\n",
    "\n",
    "training_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im not shy to say it how it is and it will drive me to success.</td>\n",
       "      <td>0</td>\n",
       "      <td>['That everything I do with my life will not only help my family but everyone in my community.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what's more, my dream is to become a veterinarian, but in china, there is only limited program that focus on veterinarian science.i transferred from university of california, santa barbara because san francisco is the city i want to stay for the rest of my life and i love everything about sf state.i am here because physics 122 is one of my major lower division class.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I am an International student from China, and my parents send me here because they want me to have better education resources.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i am here in united states because my father migrated us from philippines to have a better opportunity and to be able to have a degree that i can have and use my degree where ever i go.</td>\n",
       "      <td>0</td>\n",
       "      <td>['Having a degree will have more opportunity such as better job offers in order to support the family needs']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a lot of what my family taught us throughout the years has always been through education and knowing that, thats what our biggest goal was.</td>\n",
       "      <td>1</td>\n",
       "      <td>['A lot of what my family taught us throughout the years has always been through education and knowing that, thats what our biggest goal was.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this is why im here at san francisco state university majoring in biology concentration in physiology to in hopes one day become a pediatric nurse and to work as hard as i can so that one day i could return something back to my parents and make them proud for what theyve given up for me to live a better life.</td>\n",
       "      <td>1</td>\n",
       "      <td>['This is why Im here at San Francisco State University majoring in Biology concentration in Physiology to in hopes one day become a pediatric nurse and to work as hard as I can so that one day I could return something back to my parents and make them proud for what theyve given up for me to live a better life.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                            sentence  \\\n",
       "0                                                                                                                                                                                                                                                                                                                    im not shy to say it how it is and it will drive me to success.   \n",
       "1  what's more, my dream is to become a veterinarian, but in china, there is only limited program that focus on veterinarian science.i transferred from university of california, santa barbara because san francisco is the city i want to stay for the rest of my life and i love everything about sf state.i am here because physics 122 is one of my major lower division class.   \n",
       "2                                                                                                                                                                                          i am here in united states because my father migrated us from philippines to have a better opportunity and to be able to have a degree that i can have and use my degree where ever i go.   \n",
       "3                                                                                                                                                                                                                                        a lot of what my family taught us throughout the years has always been through education and knowing that, thats what our biggest goal was.   \n",
       "4                                                             this is why im here at san francisco state university majoring in biology concentration in physiology to in hopes one day become a pediatric nurse and to work as hard as i can so that one day i could return something back to my parents and make them proud for what theyve given up for me to live a better life.   \n",
       "\n",
       "   label  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      0   \n",
       "3      1   \n",
       "4      1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                       phrase  \n",
       "0                                                                                                                                                                                                                            ['That everything I do with my life will not only help my family but everyone in my community.']  \n",
       "1                                                                                                                                                                                          ['I am an International student from China, and my parents send me here because they want me to have better education resources.']  \n",
       "2                                                                                                                                                                                                               ['Having a degree will have more opportunity such as better job offers in order to support the family needs']  \n",
       "3                                                                                                                                                                             ['A lot of what my family taught us throughout the years has always been through education and knowing that, thats what our biggest goal was.']  \n",
       "4  ['This is why Im here at San Francisco State University majoring in Biology concentration in Physiology to in hopes one day become a pediatric nurse and to work as hard as I can so that one day I could return something back to my parents and make them proud for what theyve given up for me to live a better life.']  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>however, the decision to go to college was completely up to us.</td>\n",
       "      <td>0</td>\n",
       "      <td>['My parents immigrated to U.S. in large part to provide their future kids with better opportunities and the financial stability required for us to go to college.', 'Im happy it makes my parents proud as well.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i am here for myself and for everyone that has supported me throughout my journey at san francisco state.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I am here to make my familys sacrifices worth it.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>when asked the question of why i am here i think about my parents and their experience and message to me.</td>\n",
       "      <td>1</td>\n",
       "      <td>['When asked the question of why I am here I think about my parents and their experience and message to me.', 'When I started high school my parents talked to me about the struggle with balancing our family along with school and work. My mom and dad cemented in me that the only way to avoid such a hectic beginning to life was to go to college right after high school']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>she always told me that you had to study more so you wont get the same path as she did in the past.</td>\n",
       "      <td>0</td>\n",
       "      <td>['because of that I have to make a lot effort to finish my degree and get a good job after school and give them a well living life.', 'so its my time to finishing her dream and give her a better life in the future.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im here in the astronomy 116 class because of all the general education lab choices, astronomy piqued my interest the most.</td>\n",
       "      <td>0</td>\n",
       "      <td>['When I was younger, my sister introduced astrology to me, and I thought that Astronomy was the exact same thing.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                      sentence  \\\n",
       "0                                                              however, the decision to go to college was completely up to us.   \n",
       "1                    i am here for myself and for everyone that has supported me throughout my journey at san francisco state.   \n",
       "2                    when asked the question of why i am here i think about my parents and their experience and message to me.   \n",
       "3                          she always told me that you had to study more so you wont get the same path as she did in the past.   \n",
       "4  im here in the astronomy 116 class because of all the general education lab choices, astronomy piqued my interest the most.   \n",
       "\n",
       "   label  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      1   \n",
       "3      0   \n",
       "4      0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                               phrase  \n",
       "0                                                                                                                                                                 ['My parents immigrated to U.S. in large part to provide their future kids with better opportunities and the financial stability required for us to go to college.', 'Im happy it makes my parents proud as well.']  \n",
       "1                                                                                                                                                                                                                                                                                                                               ['I am here to make my familys sacrifices worth it.']  \n",
       "2  ['When asked the question of why I am here I think about my parents and their experience and message to me.', 'When I started high school my parents talked to me about the struggle with balancing our family along with school and work. My mom and dad cemented in me that the only way to avoid such a hectic beginning to life was to go to college right after high school']  \n",
       "3                                                                                                                                                            ['because of that I have to make a lot effort to finish my degree and get a good job after school and give them a well living life.', 'so its my time to finishing her dream and give her a better life in the future.']  \n",
       "4                                                                                                                                                                                                                                                                ['When I was younger, my sister introduced astrology to me, and I thought that Astronomy was the exact same thing.']  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (1296, 3) \n",
      "Test dataset shape: (144, 3)\n",
      "Positive labels present in the dataset : 141  out of 1296 or 10.87962962962963%\n",
      "Positive labels present in the test dataset : 16  out of 144 or 11.11111111111111%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset shape: {training_df.shape} \\nTest dataset shape: {test_df.shape}\")\n",
    "pos_labels = len([n for n in training_df['label'] if n==1])\n",
    "print(\"Positive labels present in the dataset : {}  out of {} or {}%\".format(pos_labels, len(training_df['label']), (pos_labels/len(training_df['label']))*100))\n",
    "pos_labels = len([n for n in test_df['label'] if n==1])\n",
    "print(\"Positive labels present in the test dataset : {}  out of {} or {}%\".format(pos_labels, len(test_df['label']), (pos_labels/len(test_df['label']))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the data for the plots\n",
    "training_counts = training_df['label'].value_counts()\n",
    "test_counts = test_df['label'].value_counts()\n",
    "\n",
    "# Set up the subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot for the training set\n",
    "sns.barplot(x=training_counts.index, y=training_counts.values, ax=axes[0])\n",
    "axes[0].set_title('Distribution of labels in training set')\n",
    "axes[0].set_ylabel('Sentences')\n",
    "axes[0].set_xlabel('Label')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot for the test set\n",
    "sns.barplot(x=test_counts.index, y=test_counts.values, ax=axes[1])\n",
    "axes[1].set_title('Distribution of labels in test set')\n",
    "axes[1].set_ylabel('Sentences')\n",
    "axes[1].set_xlabel('Label')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack Obama']\n"
     ]
    }
   ],
   "source": [
    "def get_ner(text):\n",
    "    ner_list = []\n",
    "    # Annotate the text using stanza\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for entity in sentence.ents:\n",
    "            if entity.type == 'PERSON':\n",
    "                ner_list.append(entity.text)\n",
    "\n",
    "    return ner_list\n",
    "\n",
    "# Example usage\n",
    "text = \"Barack Obama was the 44th doctor of the United States.\"\n",
    "print(get_ner(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a named entity is present in the sentence\n",
    "def named_entity_present(sentence):\n",
    "    ner_list = get_ner(sentence)\n",
    "    if len(ner_list) > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Similarity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the similar words and similarity score\n",
    "# The function takes tokens of sentence as input and if its not a stop word, get its similarity with synsets of STEM.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words |= set([\"help\",\"try\", \"work\", \"process\", \"support\", \"job\"] )\n",
    "def word_similarity(tokens, syns, field):    \n",
    "    if field in ['engineering', 'technology']:\n",
    "        score_threshold = 0.5\n",
    "    else:\n",
    "        score_threshold = 0.2\n",
    "    sim_words = 0\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            try:\n",
    "                syns_word = wordnet.synsets(token) \n",
    "                score = syns_word[0].path_similarity(syns[0])\n",
    "                if score >= score_threshold:\n",
    "                    sim_words += 1\n",
    "            except: \n",
    "                score = 0\n",
    "    \n",
    "    return sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to create columns for similarity based on all STEM fields\n",
    "syns_bio = wordnet.synsets(lemmatizer.lemmatize(\"biology\"))\n",
    "syns_maths = wordnet.synsets(lemmatizer.lemmatize(\"mathematics\")) \n",
    "syns_tech = wordnet.synsets(lemmatizer.lemmatize(\"technology\"))\n",
    "syns_eng = wordnet.synsets(lemmatizer.lemmatize(\"engineering\"))\n",
    "syns_chem = wordnet.synsets(lemmatizer.lemmatize(\"chemistry\"))\n",
    "syns_phy = wordnet.synsets(lemmatizer.lemmatize(\"physics\"))\n",
    "syns_sci = wordnet.synsets(lemmatizer.lemmatize(\"science\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Medical Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abuse', 'care', 'pulmonary', 'glaucoma', 'genetic', 'male', 'rehabilitation', 'genetic', 'medical', 'ocular', 'nephrology', 'medicine', 'disease', 'clinical', 'electrophysiology', 'anesthesiology', 'adolescent', 'hospice', 'nuclear', 'endovascular', 'family', 'pediatric', 'cornea', 'ophthalmology', 'abdominal', 'radiology', 'interventional', 'procedural', 'forensic', 'endocrinology', 'surgical', 'behavioral', 'cardiothoracic', 'disabilities', 'obstetrics', 'liaison', 'pelvic', 'sports', 'neuroradiology', 'neurourology', 'research', 'interventional', 'sleep', 'geriatric', 'psychosomatic', 'preventive', 'public', 'gastrointestinal', 'community', 'ophthalmology', 'plastic', 'retina', 'adolescent', 'chest', 'infectious', 'rheumatology', 'fetal', 'urology', 'infectious', 'toxicology', 'renal', 'neonatal', 'female', 'dermatopathology', 'pain', 'hematology', 'transplant', 'cytopathology', 'neuromuscular', 'maternal', 'anesthesiology', 'sports', 'psychiatry', 'injury', 'banking', 'military', 'administrative', 'neurodevelopmental', 'palliative', 'cardiology', 'chemical', 'developmental', 'diagnostic', 'hepatology', 'surgery', 'anterior', 'surgery', 'gastroenterology', 'neuropathology', 'perinatal', 'imaging', 'heart', 'and', 'immunopathology', 'urology', 'endocrinologists', 'diseases', 'infertility', 'head', 'neurophysiology', 'orbit', 'nephrology', 'metabolism', 'allergy', 'pathology', 'pediatrics', 'gastroenterology', 'ophthalmic', 'blood', 'dermatology', 'critical', 'aerospace', 'reconstructive', 'mental', 'advanced', 'radiation', 'pulmonology', 'physical', 'occupational', 'breast', 'health', 'cytogenetics', 'cardiovascular', 'failure', 'immunology', 'neuro', 'internal', 'hematology', 'dermatology', 'microbiology', 'urologic', 'rheumatology', 'oncology', 'musculoskeletal', 'reconstructive', 'genetics', 'strabismus', 'vascular', 'pediatrics', 'pathology', 'neurology', 'genitourinary', 'segment', 'gynecology', 'endocrinology', 'neurology', 'diabetes', 'biochemical', 'addiction', 'child', 'neuroradiology', 'anatomical', 'brain', 'critical', 'psychiatry', 'pediatric', 'consultation', 'oncology', 'reproductive', 'oculoplastics', 'internal', 'emergency', 'transplant', 'retardation', 'calculi', 'molecular', 'transfusion', 'psychiatric', 'cardiac', 'uveitis', 'neck', 'gynecologic']\n"
     ]
    }
   ],
   "source": [
    "# Load the medical specialization text file and create a list\n",
    "medical_list = []\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/medical_specialities.txt', 'r') as medical_fields:\n",
    "    for line in medical_fields.readlines():\n",
    "        special_field = line.rstrip('\\n')\n",
    "        special_field = re.sub(\"\\W\",\" \", special_field )\n",
    "#         print(special_field)\n",
    "        medical_list += special_field.split()\n",
    "medical_list = list(set(medical_list))  \n",
    "medical_list = [x.lower() for x in medical_list]\n",
    "print(medical_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get medical words\n",
    "def check_medical_words(tokens):\n",
    "    for token in tokens:\n",
    "        if token not in stop_words and token in [x.lower() for x in medical_list]:\n",
    "            return 1\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sentiment Polarity and Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get polarity and subjectivity of the sentence using TexBlob\n",
    "def get_sentiment(sentence):\n",
    "    sentiments =TextBlob(sentence).sentiment\n",
    "    polarity = sentiments.polarity\n",
    "    subjectivity = sentiments.subjectivity\n",
    "    return polarity, subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the count of POS tags of the sentence\n",
    "def count_pos_tags(tokens):\n",
    "    token_pos = pos_tag(tokens)\n",
    "    count = Counter(tag for word,tag in token_pos)\n",
    "    interjections =  count['UH']\n",
    "    nouns = count['NN'] + count['NNS'] + count['NNP'] + count['NNPS']\n",
    "    adverb = count['RB'] + count['RBS'] + count['RBR']\n",
    "    verb = count['VB'] + count['VBD'] + count['VBG'] + count['VBN']\n",
    "    determiner = count['DT']\n",
    "    pronoun = count['PRP']\n",
    "    adjetive = count['JJ'] + count['JJR'] + count['JJS']\n",
    "    preposition = count['IN']\n",
    "    return interjections, nouns, adverb, verb, determiner, pronoun, adjetive,preposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_extraction(dataframe, field, func, column_names):\n",
    "    return pd.concat((\n",
    "        dataframe,\n",
    "        dataframe[field].apply(\n",
    "            lambda cell: pd.Series(func(cell), index=column_names))), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the w2v dict from pickle file\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/pickle/embeddings06122024.pickle', 'rb') as w2v_file:\n",
    "    w2v_dict = pickle.load(w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of word embeddings:  4762\n"
     ]
    }
   ],
   "source": [
    "print(\"length of word embeddings: \", len(w2v_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vectors for the essay\n",
    "def vectorizer(sequence):\n",
    "    vect = []\n",
    "    numw = 0\n",
    "    for w in sequence: \n",
    "        try :\n",
    "            if numw == 0:\n",
    "                vect = w2v_dict[w]\n",
    "            else:\n",
    "                vect = np.add(vect, w2v_dict[w])\n",
    "            numw += 1\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    return vect/ numw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into words\n",
    "def split_into_words(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vectorizer\n",
    "unigram_vect = CountVectorizer(ngram_range=(1, 1), min_df=2, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Putting them all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for feature engineering\n",
    "def feature_engineering(original_dataset):\n",
    "\n",
    "    dataset = original_dataset.copy()\n",
    "    # create a new column with sentence tokens\n",
    "    dataset['tokens'] = dataset['sentence'].apply(word_tokenize)\n",
    "    # 1. Similarity features\n",
    "    # biology\n",
    "    dataset['bio_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_bio,'biology',)) \n",
    "    # chemistry\n",
    "    dataset['chem_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_chem,'chemistry',))\n",
    "    # physics\n",
    "    dataset['phy_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_phy,'physics',))\n",
    "    # mathematics\n",
    "    dataset['math_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_maths,'mathematics',))\n",
    "    # technology\n",
    "    dataset['tech_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_tech,'technology',))\n",
    "    # engineering\n",
    "    dataset['eng_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_eng,'engineering',))\n",
    "    \n",
    "    # medical terms\n",
    "    dataset['medical_terms'] = dataset['tokens'].apply(check_medical_words)\n",
    "    \n",
    "    # polarity and subjectivity\n",
    "    dataset['polarity'], dataset['subjectivity'] = zip(*dataset['sentence'].apply(get_sentiment))\n",
    "    \n",
    "    # named entity recognition\n",
    "    dataset['ner'] = dataset['sentence'].apply(named_entity_present)\n",
    "    \n",
    "    # pos tag count\n",
    "    dataset = pos_tag_extraction(dataset, 'tokens', count_pos_tags, ['interjections', 'nouns', 'adverb', 'verb', 'determiner', 'pronoun', 'adjetive','preposition'])\n",
    "    \n",
    "    # labels\n",
    "    data_labels = dataset['label']\n",
    "    # X\n",
    "    data_x = dataset.drop(columns='label')\n",
    "\n",
    "    \n",
    "    # vectorize all the essays\n",
    "    vect_arr = data_x.tokens.apply(vectorizer)\n",
    "    for index in range(0, len(vect_arr)):\n",
    "        i = 0\n",
    "        for item in vect_arr[index]:\n",
    "            column_name= \"embedding\" + str(i)\n",
    "            data_x.loc[index, column_name] = item\n",
    "            i +=1\n",
    "    \n",
    "    return data_x,data_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = feature_engineering(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296, 121)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = feature_engineering(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 121)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Unigram features for both train and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296, 121)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 121)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Familial/saved_features/X_test_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mX_train\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Familial/saved_features/X_train_final.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m X_test\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Familial/saved_features/X_test_final.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m y_train\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Familial/saved_features/y_train.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "X_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Familial/saved_features/X_train_final.csv\", index=False)\n",
    "X_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Familial/saved_features/X_test_final.csv\", index=False)\n",
    "y_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Familial/saved_features/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Familial/saved_features/y_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the unigram df for train :  (1296, 1160)\n"
     ]
    }
   ],
   "source": [
    "# Unigrams for training set\n",
    "unigram_matrix = unigram_vect.fit_transform(X_train['sentence'])\n",
    "unigrams = pd.DataFrame(unigram_matrix.toarray())\n",
    "print(\"Shape of the unigram df for train : \",unigrams.shape)\n",
    "unigrams = unigrams.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = pd.concat([X_train, unigrams], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final.columns = X_train_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296, 1281)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test unigram df shape :  (144, 1160)\n"
     ]
    }
   ],
   "source": [
    "unigram_matrix_test = unigram_vect.transform(X_test['sentence'])\n",
    "unigrams_test = pd.DataFrame(unigram_matrix_test.toarray())\n",
    "unigrams_test = unigrams_test.reset_index(drop=True)\n",
    "print(\"Test unigram df shape : \",unigrams_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 1281)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final = pd.concat([X_test, unigrams_test], axis = 1)\n",
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final.columns = X_test_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 1281)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ---- sentence\n",
      "1 ---- phrase\n",
      "2 ---- tokens\n",
      "3 ---- bio_sim_words\n",
      "4 ---- chem_sim_words\n",
      "5 ---- phy_sim_words\n",
      "6 ---- math_sim_words\n",
      "7 ---- tech_sim_words\n",
      "8 ---- eng_sim_words\n",
      "9 ---- medical_terms\n",
      "10 ---- polarity\n",
      "11 ---- subjectivity\n",
      "12 ---- ner\n",
      "13 ---- interjections\n",
      "14 ---- nouns\n",
      "15 ---- adverb\n",
      "16 ---- verb\n",
      "17 ---- determiner\n",
      "18 ---- pronoun\n",
      "19 ---- adjetive\n",
      "20 ---- preposition\n",
      "21 ---- embedding0\n",
      "22 ---- embedding1\n",
      "23 ---- embedding2\n",
      "24 ---- embedding3\n",
      "25 ---- embedding4\n",
      "26 ---- embedding5\n",
      "27 ---- embedding6\n",
      "28 ---- embedding7\n",
      "29 ---- embedding8\n",
      "30 ---- embedding9\n",
      "31 ---- embedding10\n",
      "32 ---- embedding11\n",
      "33 ---- embedding12\n",
      "34 ---- embedding13\n",
      "35 ---- embedding14\n",
      "36 ---- embedding15\n",
      "37 ---- embedding16\n",
      "38 ---- embedding17\n",
      "39 ---- embedding18\n",
      "40 ---- embedding19\n",
      "41 ---- embedding20\n",
      "42 ---- embedding21\n",
      "43 ---- embedding22\n",
      "44 ---- embedding23\n",
      "45 ---- embedding24\n",
      "46 ---- embedding25\n",
      "47 ---- embedding26\n",
      "48 ---- embedding27\n",
      "49 ---- embedding28\n",
      "50 ---- embedding29\n",
      "51 ---- embedding30\n",
      "52 ---- embedding31\n",
      "53 ---- embedding32\n",
      "54 ---- embedding33\n",
      "55 ---- embedding34\n",
      "56 ---- embedding35\n",
      "57 ---- embedding36\n",
      "58 ---- embedding37\n",
      "59 ---- embedding38\n",
      "60 ---- embedding39\n",
      "61 ---- embedding40\n",
      "62 ---- embedding41\n",
      "63 ---- embedding42\n",
      "64 ---- embedding43\n",
      "65 ---- embedding44\n",
      "66 ---- embedding45\n",
      "67 ---- embedding46\n",
      "68 ---- embedding47\n",
      "69 ---- embedding48\n",
      "70 ---- embedding49\n",
      "71 ---- embedding50\n",
      "72 ---- embedding51\n",
      "73 ---- embedding52\n",
      "74 ---- embedding53\n",
      "75 ---- embedding54\n",
      "76 ---- embedding55\n",
      "77 ---- embedding56\n",
      "78 ---- embedding57\n",
      "79 ---- embedding58\n",
      "80 ---- embedding59\n",
      "81 ---- embedding60\n",
      "82 ---- embedding61\n",
      "83 ---- embedding62\n",
      "84 ---- embedding63\n",
      "85 ---- embedding64\n",
      "86 ---- embedding65\n",
      "87 ---- embedding66\n",
      "88 ---- embedding67\n",
      "89 ---- embedding68\n",
      "90 ---- embedding69\n",
      "91 ---- embedding70\n",
      "92 ---- embedding71\n",
      "93 ---- embedding72\n",
      "94 ---- embedding73\n",
      "95 ---- embedding74\n",
      "96 ---- embedding75\n",
      "97 ---- embedding76\n",
      "98 ---- embedding77\n",
      "99 ---- embedding78\n",
      "100 ---- embedding79\n",
      "101 ---- embedding80\n",
      "102 ---- embedding81\n",
      "103 ---- embedding82\n",
      "104 ---- embedding83\n",
      "105 ---- embedding84\n",
      "106 ---- embedding85\n",
      "107 ---- embedding86\n",
      "108 ---- embedding87\n",
      "109 ---- embedding88\n",
      "110 ---- embedding89\n",
      "111 ---- embedding90\n",
      "112 ---- embedding91\n",
      "113 ---- embedding92\n",
      "114 ---- embedding93\n",
      "115 ---- embedding94\n",
      "116 ---- embedding95\n",
      "117 ---- embedding96\n",
      "118 ---- embedding97\n",
      "119 ---- embedding98\n",
      "120 ---- embedding99\n",
      "121 ---- 0\n",
      "122 ---- 1\n",
      "123 ---- 2\n",
      "124 ---- 3\n",
      "125 ---- 4\n",
      "126 ---- 5\n",
      "127 ---- 6\n",
      "128 ---- 7\n",
      "129 ---- 8\n",
      "130 ---- 9\n",
      "131 ---- 10\n",
      "132 ---- 11\n",
      "133 ---- 12\n",
      "134 ---- 13\n",
      "135 ---- 14\n",
      "136 ---- 15\n",
      "137 ---- 16\n",
      "138 ---- 17\n",
      "139 ---- 18\n",
      "140 ---- 19\n",
      "141 ---- 20\n",
      "142 ---- 21\n",
      "143 ---- 22\n",
      "144 ---- 23\n",
      "145 ---- 24\n",
      "146 ---- 25\n",
      "147 ---- 26\n",
      "148 ---- 27\n",
      "149 ---- 28\n",
      "150 ---- 29\n",
      "151 ---- 30\n",
      "152 ---- 31\n",
      "153 ---- 32\n",
      "154 ---- 33\n",
      "155 ---- 34\n",
      "156 ---- 35\n",
      "157 ---- 36\n",
      "158 ---- 37\n",
      "159 ---- 38\n",
      "160 ---- 39\n",
      "161 ---- 40\n",
      "162 ---- 41\n",
      "163 ---- 42\n",
      "164 ---- 43\n",
      "165 ---- 44\n",
      "166 ---- 45\n",
      "167 ---- 46\n",
      "168 ---- 47\n",
      "169 ---- 48\n",
      "170 ---- 49\n",
      "171 ---- 50\n",
      "172 ---- 51\n",
      "173 ---- 52\n",
      "174 ---- 53\n",
      "175 ---- 54\n",
      "176 ---- 55\n",
      "177 ---- 56\n",
      "178 ---- 57\n",
      "179 ---- 58\n",
      "180 ---- 59\n",
      "181 ---- 60\n",
      "182 ---- 61\n",
      "183 ---- 62\n",
      "184 ---- 63\n",
      "185 ---- 64\n",
      "186 ---- 65\n",
      "187 ---- 66\n",
      "188 ---- 67\n",
      "189 ---- 68\n",
      "190 ---- 69\n",
      "191 ---- 70\n",
      "192 ---- 71\n",
      "193 ---- 72\n",
      "194 ---- 73\n",
      "195 ---- 74\n",
      "196 ---- 75\n",
      "197 ---- 76\n",
      "198 ---- 77\n",
      "199 ---- 78\n",
      "200 ---- 79\n",
      "201 ---- 80\n",
      "202 ---- 81\n",
      "203 ---- 82\n",
      "204 ---- 83\n",
      "205 ---- 84\n",
      "206 ---- 85\n",
      "207 ---- 86\n",
      "208 ---- 87\n",
      "209 ---- 88\n",
      "210 ---- 89\n",
      "211 ---- 90\n",
      "212 ---- 91\n",
      "213 ---- 92\n",
      "214 ---- 93\n",
      "215 ---- 94\n",
      "216 ---- 95\n",
      "217 ---- 96\n",
      "218 ---- 97\n",
      "219 ---- 98\n",
      "220 ---- 99\n",
      "221 ---- 100\n",
      "222 ---- 101\n",
      "223 ---- 102\n",
      "224 ---- 103\n",
      "225 ---- 104\n",
      "226 ---- 105\n",
      "227 ---- 106\n",
      "228 ---- 107\n",
      "229 ---- 108\n",
      "230 ---- 109\n",
      "231 ---- 110\n",
      "232 ---- 111\n",
      "233 ---- 112\n",
      "234 ---- 113\n",
      "235 ---- 114\n",
      "236 ---- 115\n",
      "237 ---- 116\n",
      "238 ---- 117\n",
      "239 ---- 118\n",
      "240 ---- 119\n",
      "241 ---- 120\n",
      "242 ---- 121\n",
      "243 ---- 122\n",
      "244 ---- 123\n",
      "245 ---- 124\n",
      "246 ---- 125\n",
      "247 ---- 126\n",
      "248 ---- 127\n",
      "249 ---- 128\n",
      "250 ---- 129\n",
      "251 ---- 130\n",
      "252 ---- 131\n",
      "253 ---- 132\n",
      "254 ---- 133\n",
      "255 ---- 134\n",
      "256 ---- 135\n",
      "257 ---- 136\n",
      "258 ---- 137\n",
      "259 ---- 138\n",
      "260 ---- 139\n",
      "261 ---- 140\n",
      "262 ---- 141\n",
      "263 ---- 142\n",
      "264 ---- 143\n",
      "265 ---- 144\n",
      "266 ---- 145\n",
      "267 ---- 146\n",
      "268 ---- 147\n",
      "269 ---- 148\n",
      "270 ---- 149\n",
      "271 ---- 150\n",
      "272 ---- 151\n",
      "273 ---- 152\n",
      "274 ---- 153\n",
      "275 ---- 154\n",
      "276 ---- 155\n",
      "277 ---- 156\n",
      "278 ---- 157\n",
      "279 ---- 158\n",
      "280 ---- 159\n",
      "281 ---- 160\n",
      "282 ---- 161\n",
      "283 ---- 162\n",
      "284 ---- 163\n",
      "285 ---- 164\n",
      "286 ---- 165\n",
      "287 ---- 166\n",
      "288 ---- 167\n",
      "289 ---- 168\n",
      "290 ---- 169\n",
      "291 ---- 170\n",
      "292 ---- 171\n",
      "293 ---- 172\n",
      "294 ---- 173\n",
      "295 ---- 174\n",
      "296 ---- 175\n",
      "297 ---- 176\n",
      "298 ---- 177\n",
      "299 ---- 178\n",
      "300 ---- 179\n",
      "301 ---- 180\n",
      "302 ---- 181\n",
      "303 ---- 182\n",
      "304 ---- 183\n",
      "305 ---- 184\n",
      "306 ---- 185\n",
      "307 ---- 186\n",
      "308 ---- 187\n",
      "309 ---- 188\n",
      "310 ---- 189\n",
      "311 ---- 190\n",
      "312 ---- 191\n",
      "313 ---- 192\n",
      "314 ---- 193\n",
      "315 ---- 194\n",
      "316 ---- 195\n",
      "317 ---- 196\n",
      "318 ---- 197\n",
      "319 ---- 198\n",
      "320 ---- 199\n",
      "321 ---- 200\n",
      "322 ---- 201\n",
      "323 ---- 202\n",
      "324 ---- 203\n",
      "325 ---- 204\n",
      "326 ---- 205\n",
      "327 ---- 206\n",
      "328 ---- 207\n",
      "329 ---- 208\n",
      "330 ---- 209\n",
      "331 ---- 210\n",
      "332 ---- 211\n",
      "333 ---- 212\n",
      "334 ---- 213\n",
      "335 ---- 214\n",
      "336 ---- 215\n",
      "337 ---- 216\n",
      "338 ---- 217\n",
      "339 ---- 218\n",
      "340 ---- 219\n",
      "341 ---- 220\n",
      "342 ---- 221\n",
      "343 ---- 222\n",
      "344 ---- 223\n",
      "345 ---- 224\n",
      "346 ---- 225\n",
      "347 ---- 226\n",
      "348 ---- 227\n",
      "349 ---- 228\n",
      "350 ---- 229\n",
      "351 ---- 230\n",
      "352 ---- 231\n",
      "353 ---- 232\n",
      "354 ---- 233\n",
      "355 ---- 234\n",
      "356 ---- 235\n",
      "357 ---- 236\n",
      "358 ---- 237\n",
      "359 ---- 238\n",
      "360 ---- 239\n",
      "361 ---- 240\n",
      "362 ---- 241\n",
      "363 ---- 242\n",
      "364 ---- 243\n",
      "365 ---- 244\n",
      "366 ---- 245\n",
      "367 ---- 246\n",
      "368 ---- 247\n",
      "369 ---- 248\n",
      "370 ---- 249\n",
      "371 ---- 250\n",
      "372 ---- 251\n",
      "373 ---- 252\n",
      "374 ---- 253\n",
      "375 ---- 254\n",
      "376 ---- 255\n",
      "377 ---- 256\n",
      "378 ---- 257\n",
      "379 ---- 258\n",
      "380 ---- 259\n",
      "381 ---- 260\n",
      "382 ---- 261\n",
      "383 ---- 262\n",
      "384 ---- 263\n",
      "385 ---- 264\n",
      "386 ---- 265\n",
      "387 ---- 266\n",
      "388 ---- 267\n",
      "389 ---- 268\n",
      "390 ---- 269\n",
      "391 ---- 270\n",
      "392 ---- 271\n",
      "393 ---- 272\n",
      "394 ---- 273\n",
      "395 ---- 274\n",
      "396 ---- 275\n",
      "397 ---- 276\n",
      "398 ---- 277\n",
      "399 ---- 278\n",
      "400 ---- 279\n",
      "401 ---- 280\n",
      "402 ---- 281\n",
      "403 ---- 282\n",
      "404 ---- 283\n",
      "405 ---- 284\n",
      "406 ---- 285\n",
      "407 ---- 286\n",
      "408 ---- 287\n",
      "409 ---- 288\n",
      "410 ---- 289\n",
      "411 ---- 290\n",
      "412 ---- 291\n",
      "413 ---- 292\n",
      "414 ---- 293\n",
      "415 ---- 294\n",
      "416 ---- 295\n",
      "417 ---- 296\n",
      "418 ---- 297\n",
      "419 ---- 298\n",
      "420 ---- 299\n",
      "421 ---- 300\n",
      "422 ---- 301\n",
      "423 ---- 302\n",
      "424 ---- 303\n",
      "425 ---- 304\n",
      "426 ---- 305\n",
      "427 ---- 306\n",
      "428 ---- 307\n",
      "429 ---- 308\n",
      "430 ---- 309\n",
      "431 ---- 310\n",
      "432 ---- 311\n",
      "433 ---- 312\n",
      "434 ---- 313\n",
      "435 ---- 314\n",
      "436 ---- 315\n",
      "437 ---- 316\n",
      "438 ---- 317\n",
      "439 ---- 318\n",
      "440 ---- 319\n",
      "441 ---- 320\n",
      "442 ---- 321\n",
      "443 ---- 322\n",
      "444 ---- 323\n",
      "445 ---- 324\n",
      "446 ---- 325\n",
      "447 ---- 326\n",
      "448 ---- 327\n",
      "449 ---- 328\n",
      "450 ---- 329\n",
      "451 ---- 330\n",
      "452 ---- 331\n",
      "453 ---- 332\n",
      "454 ---- 333\n",
      "455 ---- 334\n",
      "456 ---- 335\n",
      "457 ---- 336\n",
      "458 ---- 337\n",
      "459 ---- 338\n",
      "460 ---- 339\n",
      "461 ---- 340\n",
      "462 ---- 341\n",
      "463 ---- 342\n",
      "464 ---- 343\n",
      "465 ---- 344\n",
      "466 ---- 345\n",
      "467 ---- 346\n",
      "468 ---- 347\n",
      "469 ---- 348\n",
      "470 ---- 349\n",
      "471 ---- 350\n",
      "472 ---- 351\n",
      "473 ---- 352\n",
      "474 ---- 353\n",
      "475 ---- 354\n",
      "476 ---- 355\n",
      "477 ---- 356\n",
      "478 ---- 357\n",
      "479 ---- 358\n",
      "480 ---- 359\n",
      "481 ---- 360\n",
      "482 ---- 361\n",
      "483 ---- 362\n",
      "484 ---- 363\n",
      "485 ---- 364\n",
      "486 ---- 365\n",
      "487 ---- 366\n",
      "488 ---- 367\n",
      "489 ---- 368\n",
      "490 ---- 369\n",
      "491 ---- 370\n",
      "492 ---- 371\n",
      "493 ---- 372\n",
      "494 ---- 373\n",
      "495 ---- 374\n",
      "496 ---- 375\n",
      "497 ---- 376\n",
      "498 ---- 377\n",
      "499 ---- 378\n",
      "500 ---- 379\n",
      "501 ---- 380\n",
      "502 ---- 381\n",
      "503 ---- 382\n",
      "504 ---- 383\n",
      "505 ---- 384\n",
      "506 ---- 385\n",
      "507 ---- 386\n",
      "508 ---- 387\n",
      "509 ---- 388\n",
      "510 ---- 389\n",
      "511 ---- 390\n",
      "512 ---- 391\n",
      "513 ---- 392\n",
      "514 ---- 393\n",
      "515 ---- 394\n",
      "516 ---- 395\n",
      "517 ---- 396\n",
      "518 ---- 397\n",
      "519 ---- 398\n",
      "520 ---- 399\n",
      "521 ---- 400\n",
      "522 ---- 401\n",
      "523 ---- 402\n",
      "524 ---- 403\n",
      "525 ---- 404\n",
      "526 ---- 405\n",
      "527 ---- 406\n",
      "528 ---- 407\n",
      "529 ---- 408\n",
      "530 ---- 409\n",
      "531 ---- 410\n",
      "532 ---- 411\n",
      "533 ---- 412\n",
      "534 ---- 413\n",
      "535 ---- 414\n",
      "536 ---- 415\n",
      "537 ---- 416\n",
      "538 ---- 417\n",
      "539 ---- 418\n",
      "540 ---- 419\n",
      "541 ---- 420\n",
      "542 ---- 421\n",
      "543 ---- 422\n",
      "544 ---- 423\n",
      "545 ---- 424\n",
      "546 ---- 425\n",
      "547 ---- 426\n",
      "548 ---- 427\n",
      "549 ---- 428\n",
      "550 ---- 429\n",
      "551 ---- 430\n",
      "552 ---- 431\n",
      "553 ---- 432\n",
      "554 ---- 433\n",
      "555 ---- 434\n",
      "556 ---- 435\n",
      "557 ---- 436\n",
      "558 ---- 437\n",
      "559 ---- 438\n",
      "560 ---- 439\n",
      "561 ---- 440\n",
      "562 ---- 441\n",
      "563 ---- 442\n",
      "564 ---- 443\n",
      "565 ---- 444\n",
      "566 ---- 445\n",
      "567 ---- 446\n",
      "568 ---- 447\n",
      "569 ---- 448\n",
      "570 ---- 449\n",
      "571 ---- 450\n",
      "572 ---- 451\n",
      "573 ---- 452\n",
      "574 ---- 453\n",
      "575 ---- 454\n",
      "576 ---- 455\n",
      "577 ---- 456\n",
      "578 ---- 457\n",
      "579 ---- 458\n",
      "580 ---- 459\n",
      "581 ---- 460\n",
      "582 ---- 461\n",
      "583 ---- 462\n",
      "584 ---- 463\n",
      "585 ---- 464\n",
      "586 ---- 465\n",
      "587 ---- 466\n",
      "588 ---- 467\n",
      "589 ---- 468\n",
      "590 ---- 469\n",
      "591 ---- 470\n",
      "592 ---- 471\n",
      "593 ---- 472\n",
      "594 ---- 473\n",
      "595 ---- 474\n",
      "596 ---- 475\n",
      "597 ---- 476\n",
      "598 ---- 477\n",
      "599 ---- 478\n",
      "600 ---- 479\n",
      "601 ---- 480\n",
      "602 ---- 481\n",
      "603 ---- 482\n",
      "604 ---- 483\n",
      "605 ---- 484\n",
      "606 ---- 485\n",
      "607 ---- 486\n",
      "608 ---- 487\n",
      "609 ---- 488\n",
      "610 ---- 489\n",
      "611 ---- 490\n",
      "612 ---- 491\n",
      "613 ---- 492\n",
      "614 ---- 493\n",
      "615 ---- 494\n",
      "616 ---- 495\n",
      "617 ---- 496\n",
      "618 ---- 497\n",
      "619 ---- 498\n",
      "620 ---- 499\n",
      "621 ---- 500\n",
      "622 ---- 501\n",
      "623 ---- 502\n",
      "624 ---- 503\n",
      "625 ---- 504\n",
      "626 ---- 505\n",
      "627 ---- 506\n",
      "628 ---- 507\n",
      "629 ---- 508\n",
      "630 ---- 509\n",
      "631 ---- 510\n",
      "632 ---- 511\n",
      "633 ---- 512\n",
      "634 ---- 513\n",
      "635 ---- 514\n",
      "636 ---- 515\n",
      "637 ---- 516\n",
      "638 ---- 517\n",
      "639 ---- 518\n",
      "640 ---- 519\n",
      "641 ---- 520\n",
      "642 ---- 521\n",
      "643 ---- 522\n",
      "644 ---- 523\n",
      "645 ---- 524\n",
      "646 ---- 525\n",
      "647 ---- 526\n",
      "648 ---- 527\n",
      "649 ---- 528\n",
      "650 ---- 529\n",
      "651 ---- 530\n",
      "652 ---- 531\n",
      "653 ---- 532\n",
      "654 ---- 533\n",
      "655 ---- 534\n",
      "656 ---- 535\n",
      "657 ---- 536\n",
      "658 ---- 537\n",
      "659 ---- 538\n",
      "660 ---- 539\n",
      "661 ---- 540\n",
      "662 ---- 541\n",
      "663 ---- 542\n",
      "664 ---- 543\n",
      "665 ---- 544\n",
      "666 ---- 545\n",
      "667 ---- 546\n",
      "668 ---- 547\n",
      "669 ---- 548\n",
      "670 ---- 549\n",
      "671 ---- 550\n",
      "672 ---- 551\n",
      "673 ---- 552\n",
      "674 ---- 553\n",
      "675 ---- 554\n",
      "676 ---- 555\n",
      "677 ---- 556\n",
      "678 ---- 557\n",
      "679 ---- 558\n",
      "680 ---- 559\n",
      "681 ---- 560\n",
      "682 ---- 561\n",
      "683 ---- 562\n",
      "684 ---- 563\n",
      "685 ---- 564\n",
      "686 ---- 565\n",
      "687 ---- 566\n",
      "688 ---- 567\n",
      "689 ---- 568\n",
      "690 ---- 569\n",
      "691 ---- 570\n",
      "692 ---- 571\n",
      "693 ---- 572\n",
      "694 ---- 573\n",
      "695 ---- 574\n",
      "696 ---- 575\n",
      "697 ---- 576\n",
      "698 ---- 577\n",
      "699 ---- 578\n",
      "700 ---- 579\n",
      "701 ---- 580\n",
      "702 ---- 581\n",
      "703 ---- 582\n",
      "704 ---- 583\n",
      "705 ---- 584\n",
      "706 ---- 585\n",
      "707 ---- 586\n",
      "708 ---- 587\n",
      "709 ---- 588\n",
      "710 ---- 589\n",
      "711 ---- 590\n",
      "712 ---- 591\n",
      "713 ---- 592\n",
      "714 ---- 593\n",
      "715 ---- 594\n",
      "716 ---- 595\n",
      "717 ---- 596\n",
      "718 ---- 597\n",
      "719 ---- 598\n",
      "720 ---- 599\n",
      "721 ---- 600\n",
      "722 ---- 601\n",
      "723 ---- 602\n",
      "724 ---- 603\n",
      "725 ---- 604\n",
      "726 ---- 605\n",
      "727 ---- 606\n",
      "728 ---- 607\n",
      "729 ---- 608\n",
      "730 ---- 609\n",
      "731 ---- 610\n",
      "732 ---- 611\n",
      "733 ---- 612\n",
      "734 ---- 613\n",
      "735 ---- 614\n",
      "736 ---- 615\n",
      "737 ---- 616\n",
      "738 ---- 617\n",
      "739 ---- 618\n",
      "740 ---- 619\n",
      "741 ---- 620\n",
      "742 ---- 621\n",
      "743 ---- 622\n",
      "744 ---- 623\n",
      "745 ---- 624\n",
      "746 ---- 625\n",
      "747 ---- 626\n",
      "748 ---- 627\n",
      "749 ---- 628\n",
      "750 ---- 629\n",
      "751 ---- 630\n",
      "752 ---- 631\n",
      "753 ---- 632\n",
      "754 ---- 633\n",
      "755 ---- 634\n",
      "756 ---- 635\n",
      "757 ---- 636\n",
      "758 ---- 637\n",
      "759 ---- 638\n",
      "760 ---- 639\n",
      "761 ---- 640\n",
      "762 ---- 641\n",
      "763 ---- 642\n",
      "764 ---- 643\n",
      "765 ---- 644\n",
      "766 ---- 645\n",
      "767 ---- 646\n",
      "768 ---- 647\n",
      "769 ---- 648\n",
      "770 ---- 649\n",
      "771 ---- 650\n",
      "772 ---- 651\n",
      "773 ---- 652\n",
      "774 ---- 653\n",
      "775 ---- 654\n",
      "776 ---- 655\n",
      "777 ---- 656\n",
      "778 ---- 657\n",
      "779 ---- 658\n",
      "780 ---- 659\n",
      "781 ---- 660\n",
      "782 ---- 661\n",
      "783 ---- 662\n",
      "784 ---- 663\n",
      "785 ---- 664\n",
      "786 ---- 665\n",
      "787 ---- 666\n",
      "788 ---- 667\n",
      "789 ---- 668\n",
      "790 ---- 669\n",
      "791 ---- 670\n",
      "792 ---- 671\n",
      "793 ---- 672\n",
      "794 ---- 673\n",
      "795 ---- 674\n",
      "796 ---- 675\n",
      "797 ---- 676\n",
      "798 ---- 677\n",
      "799 ---- 678\n",
      "800 ---- 679\n",
      "801 ---- 680\n",
      "802 ---- 681\n",
      "803 ---- 682\n",
      "804 ---- 683\n",
      "805 ---- 684\n",
      "806 ---- 685\n",
      "807 ---- 686\n",
      "808 ---- 687\n",
      "809 ---- 688\n",
      "810 ---- 689\n",
      "811 ---- 690\n",
      "812 ---- 691\n",
      "813 ---- 692\n",
      "814 ---- 693\n",
      "815 ---- 694\n",
      "816 ---- 695\n",
      "817 ---- 696\n",
      "818 ---- 697\n",
      "819 ---- 698\n",
      "820 ---- 699\n",
      "821 ---- 700\n",
      "822 ---- 701\n",
      "823 ---- 702\n",
      "824 ---- 703\n",
      "825 ---- 704\n",
      "826 ---- 705\n",
      "827 ---- 706\n",
      "828 ---- 707\n",
      "829 ---- 708\n",
      "830 ---- 709\n",
      "831 ---- 710\n",
      "832 ---- 711\n",
      "833 ---- 712\n",
      "834 ---- 713\n",
      "835 ---- 714\n",
      "836 ---- 715\n",
      "837 ---- 716\n",
      "838 ---- 717\n",
      "839 ---- 718\n",
      "840 ---- 719\n",
      "841 ---- 720\n",
      "842 ---- 721\n",
      "843 ---- 722\n",
      "844 ---- 723\n",
      "845 ---- 724\n",
      "846 ---- 725\n",
      "847 ---- 726\n",
      "848 ---- 727\n",
      "849 ---- 728\n",
      "850 ---- 729\n",
      "851 ---- 730\n",
      "852 ---- 731\n",
      "853 ---- 732\n",
      "854 ---- 733\n",
      "855 ---- 734\n",
      "856 ---- 735\n",
      "857 ---- 736\n",
      "858 ---- 737\n",
      "859 ---- 738\n",
      "860 ---- 739\n",
      "861 ---- 740\n",
      "862 ---- 741\n",
      "863 ---- 742\n",
      "864 ---- 743\n",
      "865 ---- 744\n",
      "866 ---- 745\n",
      "867 ---- 746\n",
      "868 ---- 747\n",
      "869 ---- 748\n",
      "870 ---- 749\n",
      "871 ---- 750\n",
      "872 ---- 751\n",
      "873 ---- 752\n",
      "874 ---- 753\n",
      "875 ---- 754\n",
      "876 ---- 755\n",
      "877 ---- 756\n",
      "878 ---- 757\n",
      "879 ---- 758\n",
      "880 ---- 759\n",
      "881 ---- 760\n",
      "882 ---- 761\n",
      "883 ---- 762\n",
      "884 ---- 763\n",
      "885 ---- 764\n",
      "886 ---- 765\n",
      "887 ---- 766\n",
      "888 ---- 767\n",
      "889 ---- 768\n",
      "890 ---- 769\n",
      "891 ---- 770\n",
      "892 ---- 771\n",
      "893 ---- 772\n",
      "894 ---- 773\n",
      "895 ---- 774\n",
      "896 ---- 775\n",
      "897 ---- 776\n",
      "898 ---- 777\n",
      "899 ---- 778\n",
      "900 ---- 779\n",
      "901 ---- 780\n",
      "902 ---- 781\n",
      "903 ---- 782\n",
      "904 ---- 783\n",
      "905 ---- 784\n",
      "906 ---- 785\n",
      "907 ---- 786\n",
      "908 ---- 787\n",
      "909 ---- 788\n",
      "910 ---- 789\n",
      "911 ---- 790\n",
      "912 ---- 791\n",
      "913 ---- 792\n",
      "914 ---- 793\n",
      "915 ---- 794\n",
      "916 ---- 795\n",
      "917 ---- 796\n",
      "918 ---- 797\n",
      "919 ---- 798\n",
      "920 ---- 799\n",
      "921 ---- 800\n",
      "922 ---- 801\n",
      "923 ---- 802\n",
      "924 ---- 803\n",
      "925 ---- 804\n",
      "926 ---- 805\n",
      "927 ---- 806\n",
      "928 ---- 807\n",
      "929 ---- 808\n",
      "930 ---- 809\n",
      "931 ---- 810\n",
      "932 ---- 811\n",
      "933 ---- 812\n",
      "934 ---- 813\n",
      "935 ---- 814\n",
      "936 ---- 815\n",
      "937 ---- 816\n",
      "938 ---- 817\n",
      "939 ---- 818\n",
      "940 ---- 819\n",
      "941 ---- 820\n",
      "942 ---- 821\n",
      "943 ---- 822\n",
      "944 ---- 823\n",
      "945 ---- 824\n",
      "946 ---- 825\n",
      "947 ---- 826\n",
      "948 ---- 827\n",
      "949 ---- 828\n",
      "950 ---- 829\n",
      "951 ---- 830\n",
      "952 ---- 831\n",
      "953 ---- 832\n",
      "954 ---- 833\n",
      "955 ---- 834\n",
      "956 ---- 835\n",
      "957 ---- 836\n",
      "958 ---- 837\n",
      "959 ---- 838\n",
      "960 ---- 839\n",
      "961 ---- 840\n",
      "962 ---- 841\n",
      "963 ---- 842\n",
      "964 ---- 843\n",
      "965 ---- 844\n",
      "966 ---- 845\n",
      "967 ---- 846\n",
      "968 ---- 847\n",
      "969 ---- 848\n",
      "970 ---- 849\n",
      "971 ---- 850\n",
      "972 ---- 851\n",
      "973 ---- 852\n",
      "974 ---- 853\n",
      "975 ---- 854\n",
      "976 ---- 855\n",
      "977 ---- 856\n",
      "978 ---- 857\n",
      "979 ---- 858\n",
      "980 ---- 859\n",
      "981 ---- 860\n",
      "982 ---- 861\n",
      "983 ---- 862\n",
      "984 ---- 863\n",
      "985 ---- 864\n",
      "986 ---- 865\n",
      "987 ---- 866\n",
      "988 ---- 867\n",
      "989 ---- 868\n",
      "990 ---- 869\n",
      "991 ---- 870\n",
      "992 ---- 871\n",
      "993 ---- 872\n",
      "994 ---- 873\n",
      "995 ---- 874\n",
      "996 ---- 875\n",
      "997 ---- 876\n",
      "998 ---- 877\n",
      "999 ---- 878\n",
      "1000 ---- 879\n",
      "1001 ---- 880\n",
      "1002 ---- 881\n",
      "1003 ---- 882\n",
      "1004 ---- 883\n",
      "1005 ---- 884\n",
      "1006 ---- 885\n",
      "1007 ---- 886\n",
      "1008 ---- 887\n",
      "1009 ---- 888\n",
      "1010 ---- 889\n",
      "1011 ---- 890\n",
      "1012 ---- 891\n",
      "1013 ---- 892\n",
      "1014 ---- 893\n",
      "1015 ---- 894\n",
      "1016 ---- 895\n",
      "1017 ---- 896\n",
      "1018 ---- 897\n",
      "1019 ---- 898\n",
      "1020 ---- 899\n",
      "1021 ---- 900\n",
      "1022 ---- 901\n",
      "1023 ---- 902\n",
      "1024 ---- 903\n",
      "1025 ---- 904\n",
      "1026 ---- 905\n",
      "1027 ---- 906\n",
      "1028 ---- 907\n",
      "1029 ---- 908\n",
      "1030 ---- 909\n",
      "1031 ---- 910\n",
      "1032 ---- 911\n",
      "1033 ---- 912\n",
      "1034 ---- 913\n",
      "1035 ---- 914\n",
      "1036 ---- 915\n",
      "1037 ---- 916\n",
      "1038 ---- 917\n",
      "1039 ---- 918\n",
      "1040 ---- 919\n",
      "1041 ---- 920\n",
      "1042 ---- 921\n",
      "1043 ---- 922\n",
      "1044 ---- 923\n",
      "1045 ---- 924\n",
      "1046 ---- 925\n",
      "1047 ---- 926\n",
      "1048 ---- 927\n",
      "1049 ---- 928\n",
      "1050 ---- 929\n",
      "1051 ---- 930\n",
      "1052 ---- 931\n",
      "1053 ---- 932\n",
      "1054 ---- 933\n",
      "1055 ---- 934\n",
      "1056 ---- 935\n",
      "1057 ---- 936\n",
      "1058 ---- 937\n",
      "1059 ---- 938\n",
      "1060 ---- 939\n",
      "1061 ---- 940\n",
      "1062 ---- 941\n",
      "1063 ---- 942\n",
      "1064 ---- 943\n",
      "1065 ---- 944\n",
      "1066 ---- 945\n",
      "1067 ---- 946\n",
      "1068 ---- 947\n",
      "1069 ---- 948\n",
      "1070 ---- 949\n",
      "1071 ---- 950\n",
      "1072 ---- 951\n",
      "1073 ---- 952\n",
      "1074 ---- 953\n",
      "1075 ---- 954\n",
      "1076 ---- 955\n",
      "1077 ---- 956\n",
      "1078 ---- 957\n",
      "1079 ---- 958\n",
      "1080 ---- 959\n",
      "1081 ---- 960\n",
      "1082 ---- 961\n",
      "1083 ---- 962\n",
      "1084 ---- 963\n",
      "1085 ---- 964\n",
      "1086 ---- 965\n",
      "1087 ---- 966\n",
      "1088 ---- 967\n",
      "1089 ---- 968\n",
      "1090 ---- 969\n",
      "1091 ---- 970\n",
      "1092 ---- 971\n",
      "1093 ---- 972\n",
      "1094 ---- 973\n",
      "1095 ---- 974\n",
      "1096 ---- 975\n",
      "1097 ---- 976\n",
      "1098 ---- 977\n",
      "1099 ---- 978\n",
      "1100 ---- 979\n",
      "1101 ---- 980\n",
      "1102 ---- 981\n",
      "1103 ---- 982\n",
      "1104 ---- 983\n",
      "1105 ---- 984\n",
      "1106 ---- 985\n",
      "1107 ---- 986\n",
      "1108 ---- 987\n",
      "1109 ---- 988\n",
      "1110 ---- 989\n",
      "1111 ---- 990\n",
      "1112 ---- 991\n",
      "1113 ---- 992\n",
      "1114 ---- 993\n",
      "1115 ---- 994\n",
      "1116 ---- 995\n",
      "1117 ---- 996\n",
      "1118 ---- 997\n",
      "1119 ---- 998\n",
      "1120 ---- 999\n",
      "1121 ---- 1000\n",
      "1122 ---- 1001\n",
      "1123 ---- 1002\n",
      "1124 ---- 1003\n",
      "1125 ---- 1004\n",
      "1126 ---- 1005\n",
      "1127 ---- 1006\n",
      "1128 ---- 1007\n",
      "1129 ---- 1008\n",
      "1130 ---- 1009\n",
      "1131 ---- 1010\n",
      "1132 ---- 1011\n",
      "1133 ---- 1012\n",
      "1134 ---- 1013\n",
      "1135 ---- 1014\n",
      "1136 ---- 1015\n",
      "1137 ---- 1016\n",
      "1138 ---- 1017\n",
      "1139 ---- 1018\n",
      "1140 ---- 1019\n",
      "1141 ---- 1020\n",
      "1142 ---- 1021\n",
      "1143 ---- 1022\n",
      "1144 ---- 1023\n",
      "1145 ---- 1024\n",
      "1146 ---- 1025\n",
      "1147 ---- 1026\n",
      "1148 ---- 1027\n",
      "1149 ---- 1028\n",
      "1150 ---- 1029\n",
      "1151 ---- 1030\n",
      "1152 ---- 1031\n",
      "1153 ---- 1032\n",
      "1154 ---- 1033\n",
      "1155 ---- 1034\n",
      "1156 ---- 1035\n",
      "1157 ---- 1036\n",
      "1158 ---- 1037\n",
      "1159 ---- 1038\n",
      "1160 ---- 1039\n",
      "1161 ---- 1040\n",
      "1162 ---- 1041\n",
      "1163 ---- 1042\n",
      "1164 ---- 1043\n",
      "1165 ---- 1044\n",
      "1166 ---- 1045\n",
      "1167 ---- 1046\n",
      "1168 ---- 1047\n",
      "1169 ---- 1048\n",
      "1170 ---- 1049\n",
      "1171 ---- 1050\n",
      "1172 ---- 1051\n",
      "1173 ---- 1052\n",
      "1174 ---- 1053\n",
      "1175 ---- 1054\n",
      "1176 ---- 1055\n",
      "1177 ---- 1056\n",
      "1178 ---- 1057\n",
      "1179 ---- 1058\n",
      "1180 ---- 1059\n",
      "1181 ---- 1060\n",
      "1182 ---- 1061\n",
      "1183 ---- 1062\n",
      "1184 ---- 1063\n",
      "1185 ---- 1064\n",
      "1186 ---- 1065\n",
      "1187 ---- 1066\n",
      "1188 ---- 1067\n",
      "1189 ---- 1068\n",
      "1190 ---- 1069\n",
      "1191 ---- 1070\n",
      "1192 ---- 1071\n",
      "1193 ---- 1072\n",
      "1194 ---- 1073\n",
      "1195 ---- 1074\n",
      "1196 ---- 1075\n",
      "1197 ---- 1076\n",
      "1198 ---- 1077\n",
      "1199 ---- 1078\n",
      "1200 ---- 1079\n",
      "1201 ---- 1080\n",
      "1202 ---- 1081\n",
      "1203 ---- 1082\n",
      "1204 ---- 1083\n",
      "1205 ---- 1084\n",
      "1206 ---- 1085\n",
      "1207 ---- 1086\n",
      "1208 ---- 1087\n",
      "1209 ---- 1088\n",
      "1210 ---- 1089\n",
      "1211 ---- 1090\n",
      "1212 ---- 1091\n",
      "1213 ---- 1092\n",
      "1214 ---- 1093\n",
      "1215 ---- 1094\n",
      "1216 ---- 1095\n",
      "1217 ---- 1096\n",
      "1218 ---- 1097\n",
      "1219 ---- 1098\n",
      "1220 ---- 1099\n",
      "1221 ---- 1100\n",
      "1222 ---- 1101\n",
      "1223 ---- 1102\n",
      "1224 ---- 1103\n",
      "1225 ---- 1104\n",
      "1226 ---- 1105\n",
      "1227 ---- 1106\n",
      "1228 ---- 1107\n",
      "1229 ---- 1108\n",
      "1230 ---- 1109\n",
      "1231 ---- 1110\n",
      "1232 ---- 1111\n",
      "1233 ---- 1112\n",
      "1234 ---- 1113\n",
      "1235 ---- 1114\n",
      "1236 ---- 1115\n",
      "1237 ---- 1116\n",
      "1238 ---- 1117\n",
      "1239 ---- 1118\n",
      "1240 ---- 1119\n",
      "1241 ---- 1120\n",
      "1242 ---- 1121\n",
      "1243 ---- 1122\n",
      "1244 ---- 1123\n",
      "1245 ---- 1124\n",
      "1246 ---- 1125\n",
      "1247 ---- 1126\n",
      "1248 ---- 1127\n",
      "1249 ---- 1128\n",
      "1250 ---- 1129\n",
      "1251 ---- 1130\n",
      "1252 ---- 1131\n",
      "1253 ---- 1132\n",
      "1254 ---- 1133\n",
      "1255 ---- 1134\n",
      "1256 ---- 1135\n",
      "1257 ---- 1136\n",
      "1258 ---- 1137\n",
      "1259 ---- 1138\n",
      "1260 ---- 1139\n",
      "1261 ---- 1140\n",
      "1262 ---- 1141\n",
      "1263 ---- 1142\n",
      "1264 ---- 1143\n",
      "1265 ---- 1144\n",
      "1266 ---- 1145\n",
      "1267 ---- 1146\n",
      "1268 ---- 1147\n",
      "1269 ---- 1148\n",
      "1270 ---- 1149\n",
      "1271 ---- 1150\n",
      "1272 ---- 1151\n",
      "1273 ---- 1152\n",
      "1274 ---- 1153\n",
      "1275 ---- 1154\n",
      "1276 ---- 1155\n",
      "1277 ---- 1156\n",
      "1278 ---- 1157\n",
      "1279 ---- 1158\n",
      "1280 ---- 1159\n",
      "1281 ---- 1160\n",
      "1282 ---- 1161\n",
      "1283 ---- 1162\n",
      "1284 ---- 1163\n",
      "1285 ---- 1164\n",
      "1286 ---- 1165\n",
      "1287 ---- 1166\n",
      "1288 ---- 1167\n",
      "1289 ---- 1168\n",
      "1290 ---- 1169\n",
      "1291 ---- 1170\n",
      "1292 ---- 1171\n",
      "1293 ---- 1172\n",
      "1294 ---- 1173\n",
      "1295 ---- 1174\n",
      "1296 ---- 1175\n",
      "1297 ---- 1176\n",
      "1298 ---- 1177\n",
      "1299 ---- 1178\n",
      "1300 ---- 1179\n",
      "1301 ---- 1180\n",
      "1302 ---- 1181\n",
      "1303 ---- 1182\n",
      "1304 ---- 1183\n",
      "1305 ---- 1184\n",
      "1306 ---- 1185\n",
      "1307 ---- 1186\n",
      "1308 ---- 1187\n",
      "1309 ---- 1188\n",
      "1310 ---- 1189\n",
      "1311 ---- 1190\n",
      "1312 ---- 1191\n",
      "1313 ---- 1192\n",
      "1314 ---- 1193\n",
      "1315 ---- 1194\n",
      "1316 ---- 1195\n",
      "1317 ---- 1196\n",
      "1318 ---- 1197\n",
      "1319 ---- 1198\n",
      "1320 ---- 1199\n",
      "1321 ---- 1200\n",
      "1322 ---- 1201\n",
      "1323 ---- 1202\n",
      "1324 ---- 1203\n",
      "1325 ---- 1204\n",
      "1326 ---- 1205\n",
      "1327 ---- 1206\n",
      "1328 ---- 1207\n",
      "1329 ---- 1208\n",
      "1330 ---- 1209\n",
      "1331 ---- 1210\n",
      "1332 ---- 1211\n",
      "1333 ---- 1212\n",
      "1334 ---- 1213\n",
      "1335 ---- 1214\n",
      "1336 ---- 1215\n",
      "1337 ---- 1216\n",
      "1338 ---- 1217\n",
      "1339 ---- 1218\n",
      "1340 ---- 1219\n",
      "1341 ---- 1220\n",
      "1342 ---- 1221\n",
      "1343 ---- 1222\n",
      "1344 ---- 1223\n",
      "1345 ---- 1224\n",
      "1346 ---- 1225\n",
      "1347 ---- 1226\n",
      "1348 ---- 1227\n",
      "1349 ---- 1228\n",
      "1350 ---- 1229\n",
      "1351 ---- 1230\n",
      "1352 ---- 1231\n",
      "1353 ---- 1232\n",
      "1354 ---- 1233\n",
      "1355 ---- 1234\n",
      "1356 ---- 1235\n",
      "1357 ---- 1236\n",
      "1358 ---- 1237\n",
      "1359 ---- 1238\n",
      "1360 ---- 1239\n",
      "1361 ---- 1240\n",
      "1362 ---- 1241\n",
      "1363 ---- 1242\n",
      "1364 ---- 1243\n",
      "1365 ---- 1244\n",
      "1366 ---- 1245\n",
      "1367 ---- 1246\n",
      "1368 ---- 1247\n",
      "1369 ---- 1248\n",
      "1370 ---- 1249\n",
      "1371 ---- 1250\n",
      "1372 ---- 1251\n",
      "1373 ---- 1252\n",
      "1374 ---- 1253\n",
      "1375 ---- 1254\n",
      "1376 ---- 1255\n",
      "1377 ---- 1256\n",
      "1378 ---- 1257\n",
      "1379 ---- 1258\n",
      "1380 ---- 1259\n",
      "1381 ---- 1260\n",
      "1382 ---- 1261\n",
      "1383 ---- 1262\n",
      "1384 ---- 1263\n",
      "1385 ---- 1264\n",
      "1386 ---- 1265\n",
      "1387 ---- 1266\n",
      "1388 ---- 1267\n",
      "1389 ---- 1268\n",
      "1390 ---- 1269\n",
      "1391 ---- 1270\n",
      "1392 ---- 1271\n",
      "1393 ---- 1272\n",
      "1394 ---- 1273\n",
      "1395 ---- 1274\n",
      "1396 ---- 1275\n",
      "1397 ---- 1276\n",
      "1398 ---- 1277\n",
      "1399 ---- 1278\n",
      "1400 ---- 1279\n",
      "1401 ---- 1280\n",
      "1402 ---- 1281\n",
      "1403 ---- 1282\n",
      "1404 ---- 1283\n",
      "1405 ---- 1284\n",
      "1406 ---- 1285\n",
      "1407 ---- 1286\n",
      "1408 ---- 1287\n",
      "1409 ---- 1288\n",
      "1410 ---- 1289\n",
      "1411 ---- 1290\n",
      "1412 ---- 1291\n",
      "1413 ---- 1292\n",
      "1414 ---- 1293\n",
      "1415 ---- 1294\n",
      "1416 ---- 1295\n",
      "1417 ---- 1296\n",
      "1418 ---- 1297\n",
      "1419 ---- 1298\n",
      "1420 ---- 1299\n",
      "1421 ---- 1300\n",
      "1422 ---- 1301\n",
      "1423 ---- 1302\n",
      "1424 ---- 1303\n",
      "1425 ---- 1304\n",
      "1426 ---- 1305\n",
      "1427 ---- 1306\n",
      "1428 ---- 1307\n",
      "1429 ---- 1308\n",
      "1430 ---- 1309\n",
      "1431 ---- 1310\n",
      "1432 ---- 1311\n",
      "1433 ---- 1312\n",
      "1434 ---- 1313\n",
      "1435 ---- 1314\n",
      "1436 ---- 1315\n",
      "1437 ---- 1316\n",
      "1438 ---- 1317\n",
      "1439 ---- 1318\n",
      "1440 ---- 1319\n",
      "1441 ---- 1320\n",
      "1442 ---- 1321\n",
      "1443 ---- 1322\n",
      "1444 ---- 1323\n",
      "1445 ---- 1324\n",
      "1446 ---- 1325\n",
      "1447 ---- 1326\n",
      "1448 ---- 1327\n",
      "1449 ---- 1328\n",
      "1450 ---- 1329\n",
      "1451 ---- 1330\n",
      "1452 ---- 1331\n",
      "1453 ---- 1332\n",
      "1454 ---- 1333\n",
      "1455 ---- 1334\n",
      "1456 ---- 1335\n",
      "1457 ---- 1336\n",
      "1458 ---- 1337\n",
      "1459 ---- 1338\n",
      "1460 ---- 1339\n",
      "1461 ---- 1340\n",
      "1462 ---- 1341\n",
      "1463 ---- 1342\n",
      "1464 ---- 1343\n",
      "1465 ---- 1344\n",
      "1466 ---- 1345\n",
      "1467 ---- 1346\n",
      "1468 ---- 1347\n",
      "1469 ---- 1348\n",
      "1470 ---- 1349\n",
      "1471 ---- 1350\n",
      "1472 ---- 1351\n",
      "1473 ---- 1352\n",
      "1474 ---- 1353\n",
      "1475 ---- 1354\n",
      "1476 ---- 1355\n",
      "1477 ---- 1356\n",
      "1478 ---- 1357\n",
      "1479 ---- 1358\n",
      "1480 ---- 1359\n",
      "1481 ---- 1360\n",
      "1482 ---- 1361\n",
      "1483 ---- 1362\n",
      "1484 ---- 1363\n",
      "1485 ---- 1364\n",
      "1486 ---- 1365\n",
      "1487 ---- 1366\n",
      "1488 ---- 1367\n",
      "1489 ---- 1368\n",
      "1490 ---- 1369\n",
      "1491 ---- 1370\n",
      "1492 ---- 1371\n",
      "1493 ---- 1372\n",
      "1494 ---- 1373\n",
      "1495 ---- 1374\n",
      "1496 ---- 1375\n",
      "1497 ---- 1376\n",
      "1498 ---- 1377\n",
      "1499 ---- 1378\n",
      "1500 ---- 1379\n",
      "1501 ---- 1380\n",
      "1502 ---- 1381\n",
      "1503 ---- 1382\n",
      "1504 ---- 1383\n",
      "1505 ---- 1384\n",
      "1506 ---- 1385\n",
      "1507 ---- 1386\n",
      "1508 ---- 1387\n",
      "1509 ---- 1388\n",
      "1510 ---- 1389\n",
      "1511 ---- 1390\n",
      "1512 ---- 1391\n",
      "1513 ---- 1392\n",
      "1514 ---- 1393\n",
      "1515 ---- 1394\n",
      "1516 ---- 1395\n",
      "1517 ---- 1396\n",
      "1518 ---- 1397\n",
      "1519 ---- 1398\n",
      "1520 ---- 1399\n",
      "1521 ---- 1400\n",
      "1522 ---- 1401\n",
      "1523 ---- 1402\n",
      "1524 ---- 1403\n",
      "1525 ---- 1404\n",
      "1526 ---- 1405\n",
      "1527 ---- 1406\n",
      "1528 ---- 1407\n",
      "1529 ---- 1408\n",
      "1530 ---- 1409\n",
      "1531 ---- 1410\n",
      "1532 ---- 1411\n",
      "1533 ---- 1412\n",
      "1534 ---- 1413\n",
      "1535 ---- 1414\n",
      "1536 ---- 1415\n",
      "1537 ---- 1416\n",
      "1538 ---- 1417\n",
      "1539 ---- 1418\n",
      "1540 ---- 1419\n",
      "1541 ---- 1420\n",
      "1542 ---- 1421\n",
      "1543 ---- 1422\n",
      "1544 ---- 1423\n",
      "1545 ---- 1424\n",
      "1546 ---- 1425\n",
      "1547 ---- 1426\n",
      "1548 ---- 1427\n",
      "1549 ---- 1428\n",
      "1550 ---- 1429\n",
      "1551 ---- 1430\n",
      "1552 ---- 1431\n",
      "1553 ---- 1432\n",
      "1554 ---- 1433\n",
      "1555 ---- 1434\n",
      "1556 ---- 1435\n",
      "1557 ---- 1436\n",
      "1558 ---- 1437\n",
      "1559 ---- 1438\n",
      "1560 ---- 1439\n",
      "1561 ---- 1440\n",
      "1562 ---- 1441\n",
      "1563 ---- 1442\n",
      "1564 ---- 1443\n",
      "1565 ---- 1444\n",
      "1566 ---- 1445\n",
      "1567 ---- 1446\n",
      "1568 ---- 1447\n",
      "1569 ---- 1448\n",
      "1570 ---- 1449\n",
      "1571 ---- 1450\n",
      "1572 ---- 1451\n",
      "1573 ---- 1452\n",
      "1574 ---- 1453\n",
      "1575 ---- 1454\n",
      "1576 ---- 1455\n",
      "1577 ---- 1456\n",
      "1578 ---- 1457\n",
      "1579 ---- 1458\n",
      "1580 ---- 1459\n",
      "1581 ---- 1460\n",
      "1582 ---- 1461\n",
      "1583 ---- 1462\n",
      "1584 ---- 1463\n",
      "1585 ---- 1464\n",
      "1586 ---- 1465\n",
      "1587 ---- 1466\n",
      "1588 ---- 1467\n",
      "1589 ---- 1468\n",
      "1590 ---- 1469\n",
      "1591 ---- 1470\n",
      "1592 ---- 1471\n",
      "1593 ---- 1472\n",
      "1594 ---- 1473\n",
      "1595 ---- 1474\n",
      "1596 ---- 1475\n",
      "1597 ---- 1476\n",
      "1598 ---- 1477\n",
      "1599 ---- 1478\n",
      "1600 ---- 1479\n",
      "1601 ---- 1480\n",
      "1602 ---- 1481\n",
      "1603 ---- 1482\n",
      "1604 ---- 1483\n",
      "1605 ---- 1484\n",
      "1606 ---- 1485\n",
      "1607 ---- 1486\n",
      "1608 ---- 1487\n",
      "1609 ---- 1488\n",
      "1610 ---- 1489\n",
      "1611 ---- 1490\n",
      "1612 ---- 1491\n",
      "1613 ---- 1492\n",
      "1614 ---- 1493\n",
      "1615 ---- 1494\n",
      "1616 ---- 1495\n",
      "1617 ---- 1496\n",
      "1618 ---- 1497\n",
      "1619 ---- 1498\n",
      "1620 ---- 1499\n",
      "1621 ---- 1500\n",
      "1622 ---- 1501\n",
      "1623 ---- 1502\n",
      "1624 ---- 1503\n",
      "1625 ---- 1504\n",
      "1626 ---- 1505\n",
      "1627 ---- 1506\n",
      "1628 ---- 1507\n",
      "1629 ---- 1508\n",
      "1630 ---- 1509\n",
      "1631 ---- 1510\n",
      "1632 ---- 1511\n",
      "1633 ---- 1512\n",
      "1634 ---- 1513\n",
      "1635 ---- 1514\n",
      "1636 ---- 1515\n",
      "1637 ---- 1516\n",
      "1638 ---- 1517\n",
      "1639 ---- 1518\n",
      "1640 ---- 1519\n",
      "1641 ---- 1520\n",
      "1642 ---- 1521\n",
      "1643 ---- 1522\n",
      "1644 ---- 1523\n",
      "1645 ---- 1524\n",
      "1646 ---- 1525\n",
      "1647 ---- 1526\n",
      "1648 ---- 1527\n",
      "1649 ---- 1528\n",
      "1650 ---- 1529\n",
      "1651 ---- 1530\n",
      "1652 ---- 1531\n",
      "1653 ---- 1532\n",
      "1654 ---- 1533\n",
      "1655 ---- 1534\n",
      "1656 ---- 1535\n",
      "1657 ---- 1536\n",
      "1658 ---- 1537\n",
      "1659 ---- 1538\n",
      "1660 ---- 1539\n",
      "1661 ---- 1540\n",
      "1662 ---- 1541\n",
      "1663 ---- 1542\n",
      "1664 ---- 1543\n",
      "1665 ---- 1544\n",
      "1666 ---- 1545\n",
      "1667 ---- 1546\n",
      "1668 ---- 1547\n",
      "1669 ---- 1548\n",
      "1670 ---- 1549\n",
      "1671 ---- 1550\n",
      "1672 ---- 1551\n",
      "1673 ---- 1552\n",
      "1674 ---- 1553\n",
      "1675 ---- 1554\n",
      "1676 ---- 1555\n",
      "1677 ---- 1556\n",
      "1678 ---- 1557\n",
      "1679 ---- 1558\n",
      "1680 ---- 1559\n",
      "1681 ---- 1560\n",
      "1682 ---- 1561\n",
      "1683 ---- 1562\n",
      "1684 ---- 1563\n",
      "1685 ---- 1564\n",
      "1686 ---- 1565\n",
      "1687 ---- 1566\n",
      "1688 ---- 1567\n",
      "1689 ---- 1568\n",
      "1690 ---- 1569\n",
      "1691 ---- 1570\n",
      "1692 ---- 1571\n",
      "1693 ---- 1572\n",
      "1694 ---- 1573\n",
      "1695 ---- 1574\n",
      "1696 ---- 1575\n",
      "1697 ---- 1576\n",
      "1698 ---- 1577\n",
      "1699 ---- 1578\n",
      "1700 ---- 1579\n",
      "1701 ---- 1580\n",
      "1702 ---- 1581\n",
      "1703 ---- 1582\n",
      "1704 ---- 1583\n",
      "1705 ---- 1584\n",
      "1706 ---- 1585\n",
      "1707 ---- 1586\n",
      "1708 ---- 1587\n",
      "1709 ---- 1588\n",
      "1710 ---- 1589\n",
      "1711 ---- 1590\n",
      "1712 ---- 1591\n",
      "1713 ---- 1592\n",
      "1714 ---- 1593\n",
      "1715 ---- 1594\n",
      "1716 ---- 1595\n",
      "1717 ---- 1596\n",
      "1718 ---- 1597\n",
      "1719 ---- 1598\n",
      "1720 ---- 1599\n",
      "1721 ---- 1600\n",
      "1722 ---- 1601\n",
      "1723 ---- 1602\n",
      "1724 ---- 1603\n",
      "1725 ---- 1604\n",
      "1726 ---- 1605\n",
      "1727 ---- 1606\n",
      "1728 ---- 1607\n",
      "1729 ---- 1608\n",
      "1730 ---- 1609\n",
      "1731 ---- 1610\n",
      "1732 ---- 1611\n",
      "1733 ---- 1612\n",
      "1734 ---- 1613\n",
      "1735 ---- 1614\n",
      "1736 ---- 1615\n",
      "1737 ---- 1616\n",
      "1738 ---- 1617\n",
      "1739 ---- 1618\n",
      "1740 ---- 1619\n",
      "1741 ---- 1620\n",
      "1742 ---- 1621\n",
      "1743 ---- 1622\n",
      "1744 ---- 1623\n",
      "1745 ---- 1624\n",
      "1746 ---- 1625\n",
      "1747 ---- 1626\n",
      "1748 ---- 1627\n",
      "1749 ---- 1628\n",
      "1750 ---- 1629\n",
      "1751 ---- 1630\n",
      "1752 ---- 1631\n",
      "1753 ---- 1632\n",
      "1754 ---- 1633\n",
      "1755 ---- 1634\n",
      "1756 ---- 1635\n",
      "1757 ---- 1636\n",
      "1758 ---- 1637\n",
      "1759 ---- 1638\n",
      "1760 ---- 1639\n",
      "1761 ---- 1640\n",
      "1762 ---- 1641\n",
      "1763 ---- 1642\n",
      "1764 ---- 1643\n",
      "1765 ---- 1644\n",
      "1766 ---- 1645\n",
      "1767 ---- 1646\n",
      "1768 ---- 1647\n",
      "1769 ---- 1648\n",
      "1770 ---- 1649\n",
      "1771 ---- 1650\n",
      "1772 ---- 1651\n",
      "1773 ---- 1652\n",
      "1774 ---- 1653\n",
      "1775 ---- 1654\n",
      "1776 ---- 1655\n",
      "1777 ---- 1656\n",
      "1778 ---- 1657\n",
      "1779 ---- 1658\n",
      "1780 ---- 1659\n",
      "1781 ---- 1660\n",
      "1782 ---- 1661\n",
      "1783 ---- 1662\n",
      "1784 ---- 1663\n",
      "1785 ---- 1664\n",
      "1786 ---- 1665\n",
      "1787 ---- 1666\n",
      "1788 ---- 1667\n",
      "1789 ---- 1668\n",
      "1790 ---- 1669\n",
      "1791 ---- 1670\n",
      "1792 ---- 1671\n",
      "1793 ---- 1672\n",
      "1794 ---- 1673\n",
      "1795 ---- 1674\n",
      "1796 ---- 1675\n",
      "1797 ---- 1676\n",
      "1798 ---- 1677\n",
      "1799 ---- 1678\n",
      "1800 ---- 1679\n",
      "1801 ---- 1680\n",
      "1802 ---- 1681\n",
      "1803 ---- 1682\n",
      "1804 ---- 1683\n",
      "1805 ---- 1684\n",
      "1806 ---- 1685\n",
      "1807 ---- 1686\n",
      "1808 ---- 1687\n",
      "1809 ---- 1688\n",
      "1810 ---- 1689\n",
      "1811 ---- 1690\n",
      "1812 ---- 1691\n",
      "1813 ---- 1692\n",
      "1814 ---- 1693\n",
      "1815 ---- 1694\n",
      "1816 ---- 1695\n",
      "1817 ---- 1696\n",
      "1818 ---- 1697\n",
      "1819 ---- 1698\n",
      "1820 ---- 1699\n",
      "1821 ---- 1700\n",
      "1822 ---- 1701\n",
      "1823 ---- 1702\n",
      "1824 ---- 1703\n",
      "1825 ---- 1704\n",
      "1826 ---- 1705\n",
      "1827 ---- 1706\n",
      "1828 ---- 1707\n",
      "1829 ---- 1708\n",
      "1830 ---- 1709\n",
      "1831 ---- 1710\n",
      "1832 ---- 1711\n",
      "1833 ---- 1712\n",
      "1834 ---- 1713\n",
      "1835 ---- 1714\n",
      "1836 ---- 1715\n",
      "1837 ---- 1716\n",
      "1838 ---- 1717\n",
      "1839 ---- 1718\n",
      "1840 ---- 1719\n",
      "1841 ---- 1720\n",
      "1842 ---- 1721\n",
      "1843 ---- 1722\n",
      "1844 ---- 1723\n",
      "1845 ---- 1724\n",
      "1846 ---- 1725\n",
      "1847 ---- 1726\n",
      "1848 ---- 1727\n",
      "1849 ---- 1728\n",
      "1850 ---- 1729\n",
      "1851 ---- 1730\n",
      "1852 ---- 1731\n",
      "1853 ---- 1732\n",
      "1854 ---- 1733\n",
      "1855 ---- 1734\n",
      "1856 ---- 1735\n",
      "1857 ---- 1736\n",
      "1858 ---- 1737\n",
      "1859 ---- 1738\n",
      "1860 ---- 1739\n",
      "1861 ---- 1740\n",
      "1862 ---- 1741\n",
      "1863 ---- 1742\n",
      "1864 ---- 1743\n",
      "1865 ---- 1744\n",
      "1866 ---- 1745\n",
      "1867 ---- 1746\n",
      "1868 ---- 1747\n",
      "1869 ---- 1748\n",
      "1870 ---- 1749\n",
      "1871 ---- 1750\n",
      "1872 ---- 1751\n",
      "1873 ---- 1752\n",
      "1874 ---- 1753\n",
      "1875 ---- 1754\n",
      "1876 ---- 1755\n",
      "1877 ---- 1756\n",
      "1878 ---- 1757\n",
      "1879 ---- 1758\n",
      "1880 ---- 1759\n",
      "1881 ---- 1760\n",
      "1882 ---- 1761\n",
      "1883 ---- 1762\n",
      "1884 ---- 1763\n",
      "1885 ---- 1764\n",
      "1886 ---- 1765\n",
      "1887 ---- 1766\n",
      "1888 ---- 1767\n",
      "1889 ---- 1768\n",
      "1890 ---- 1769\n",
      "1891 ---- 1770\n",
      "1892 ---- 1771\n",
      "1893 ---- 1772\n",
      "1894 ---- 1773\n",
      "1895 ---- 1774\n",
      "1896 ---- 1775\n",
      "1897 ---- 1776\n",
      "1898 ---- 1777\n",
      "1899 ---- 1778\n",
      "1900 ---- 1779\n",
      "1901 ---- 1780\n",
      "1902 ---- 1781\n",
      "1903 ---- 1782\n",
      "1904 ---- 1783\n",
      "1905 ---- 1784\n",
      "1906 ---- 1785\n",
      "1907 ---- 1786\n",
      "1908 ---- 1787\n",
      "1909 ---- 1788\n",
      "1910 ---- 1789\n",
      "1911 ---- 1790\n",
      "1912 ---- 1791\n",
      "1913 ---- 1792\n",
      "1914 ---- 1793\n",
      "1915 ---- 1794\n",
      "1916 ---- 1795\n",
      "1917 ---- 1796\n",
      "1918 ---- 1797\n",
      "1919 ---- 1798\n",
      "1920 ---- 1799\n",
      "1921 ---- 1800\n",
      "1922 ---- 1801\n",
      "1923 ---- 1802\n",
      "1924 ---- 1803\n",
      "1925 ---- 1804\n",
      "1926 ---- 1805\n",
      "1927 ---- 1806\n",
      "1928 ---- 1807\n",
      "1929 ---- 1808\n",
      "1930 ---- 1809\n",
      "1931 ---- 1810\n",
      "1932 ---- 1811\n",
      "1933 ---- 1812\n",
      "1934 ---- 1813\n",
      "1935 ---- 1814\n",
      "1936 ---- 1815\n",
      "1937 ---- 1816\n",
      "1938 ---- 1817\n",
      "1939 ---- 1818\n",
      "1940 ---- 1819\n",
      "1941 ---- 1820\n",
      "1942 ---- 1821\n",
      "1943 ---- 1822\n",
      "1944 ---- 1823\n",
      "1945 ---- 1824\n",
      "1946 ---- 1825\n",
      "1947 ---- 1826\n",
      "1948 ---- 1827\n",
      "1949 ---- 1828\n",
      "1950 ---- 1829\n",
      "1951 ---- 1830\n",
      "1952 ---- 1831\n",
      "1953 ---- 1832\n",
      "1954 ---- 1833\n",
      "1955 ---- 1834\n",
      "1956 ---- 1835\n",
      "1957 ---- 1836\n",
      "1958 ---- 1837\n",
      "1959 ---- 1838\n",
      "1960 ---- 1839\n",
      "1961 ---- 1840\n",
      "1962 ---- 1841\n",
      "1963 ---- 1842\n",
      "1964 ---- 1843\n",
      "1965 ---- 1844\n",
      "1966 ---- 1845\n",
      "1967 ---- 1846\n",
      "1968 ---- 1847\n",
      "1969 ---- 1848\n",
      "1970 ---- 1849\n",
      "1971 ---- 1850\n",
      "1972 ---- 1851\n",
      "1973 ---- 1852\n",
      "1974 ---- 1853\n",
      "1975 ---- 1854\n",
      "1976 ---- 1855\n",
      "1977 ---- 1856\n",
      "1978 ---- 1857\n",
      "1979 ---- 1858\n",
      "1980 ---- 1859\n",
      "1981 ---- 1860\n",
      "1982 ---- 1861\n",
      "1983 ---- 1862\n",
      "1984 ---- 1863\n",
      "1985 ---- 1864\n",
      "1986 ---- 1865\n",
      "1987 ---- 1866\n",
      "1988 ---- 1867\n",
      "1989 ---- 1868\n",
      "1990 ---- 1869\n",
      "1991 ---- 1870\n",
      "1992 ---- 1871\n",
      "1993 ---- 1872\n",
      "1994 ---- 1873\n",
      "1995 ---- 1874\n",
      "1996 ---- 1875\n",
      "1997 ---- 1876\n",
      "1998 ---- 1877\n",
      "1999 ---- 1878\n",
      "2000 ---- 1879\n",
      "2001 ---- 1880\n",
      "2002 ---- 1881\n",
      "2003 ---- 1882\n",
      "2004 ---- 1883\n",
      "2005 ---- 1884\n",
      "2006 ---- 1885\n",
      "2007 ---- 1886\n",
      "2008 ---- 1887\n",
      "2009 ---- 1888\n",
      "2010 ---- 1889\n",
      "2011 ---- 1890\n",
      "2012 ---- 1891\n",
      "2013 ---- 1892\n",
      "2014 ---- 1893\n",
      "2015 ---- 1894\n",
      "2016 ---- 1895\n",
      "2017 ---- 1896\n",
      "2018 ---- 1897\n",
      "2019 ---- 1898\n",
      "2020 ---- 1899\n",
      "2021 ---- 1900\n",
      "2022 ---- 1901\n",
      "2023 ---- 1902\n",
      "2024 ---- 1903\n",
      "2025 ---- 1904\n",
      "2026 ---- 1905\n",
      "2027 ---- 1906\n",
      "2028 ---- 1907\n",
      "2029 ---- 1908\n",
      "2030 ---- 1909\n",
      "2031 ---- 1910\n",
      "2032 ---- 1911\n",
      "2033 ---- 1912\n",
      "2034 ---- 1913\n",
      "2035 ---- 1914\n",
      "2036 ---- 1915\n",
      "2037 ---- 1916\n",
      "2038 ---- 1917\n",
      "2039 ---- 1918\n",
      "2040 ---- 1919\n",
      "2041 ---- 1920\n",
      "2042 ---- 1921\n",
      "2043 ---- 1922\n",
      "2044 ---- 1923\n",
      "2045 ---- 1924\n",
      "2046 ---- 1925\n",
      "2047 ---- 1926\n",
      "2048 ---- 1927\n",
      "2049 ---- 1928\n",
      "2050 ---- 1929\n",
      "2051 ---- 1930\n",
      "2052 ---- 1931\n",
      "2053 ---- 1932\n",
      "2054 ---- 1933\n",
      "2055 ---- 1934\n",
      "2056 ---- 1935\n",
      "2057 ---- 1936\n",
      "2058 ---- 1937\n",
      "2059 ---- 1938\n",
      "2060 ---- 1939\n",
      "2061 ---- 1940\n",
      "2062 ---- 1941\n",
      "2063 ---- 1942\n",
      "2064 ---- 1943\n",
      "2065 ---- 1944\n",
      "2066 ---- 1945\n",
      "2067 ---- 1946\n",
      "2068 ---- 1947\n",
      "2069 ---- 1948\n",
      "2070 ---- 1949\n",
      "2071 ---- 1950\n",
      "2072 ---- 1951\n",
      "2073 ---- 1952\n",
      "2074 ---- 1953\n",
      "2075 ---- 1954\n",
      "2076 ---- 1955\n",
      "2077 ---- 1956\n",
      "2078 ---- 1957\n",
      "2079 ---- 1958\n",
      "2080 ---- 1959\n",
      "2081 ---- 1960\n",
      "2082 ---- 1961\n",
      "2083 ---- 1962\n",
      "2084 ---- 1963\n",
      "2085 ---- 1964\n",
      "2086 ---- 1965\n",
      "2087 ---- 1966\n",
      "2088 ---- 1967\n",
      "2089 ---- 1968\n",
      "2090 ---- 1969\n",
      "2091 ---- 1970\n",
      "2092 ---- 1971\n",
      "2093 ---- 1972\n",
      "2094 ---- 1973\n",
      "2095 ---- 1974\n",
      "2096 ---- 1975\n",
      "2097 ---- 1976\n",
      "2098 ---- 1977\n",
      "2099 ---- 1978\n",
      "2100 ---- 1979\n",
      "2101 ---- 1980\n",
      "2102 ---- 1981\n",
      "2103 ---- 1982\n",
      "2104 ---- 1983\n",
      "2105 ---- 1984\n",
      "2106 ---- 1985\n",
      "2107 ---- 1986\n",
      "2108 ---- 1987\n",
      "2109 ---- 1988\n",
      "2110 ---- 1989\n",
      "2111 ---- 1990\n",
      "2112 ---- 1991\n",
      "2113 ---- 1992\n",
      "2114 ---- 1993\n",
      "2115 ---- 1994\n",
      "2116 ---- 1995\n",
      "2117 ---- 1996\n",
      "2118 ---- 1997\n",
      "2119 ---- 1998\n",
      "2120 ---- 1999\n",
      "2121 ---- 2000\n",
      "2122 ---- 2001\n",
      "2123 ---- 2002\n",
      "2124 ---- 2003\n",
      "2125 ---- 2004\n",
      "2126 ---- 2005\n",
      "2127 ---- 2006\n",
      "2128 ---- 2007\n",
      "2129 ---- 2008\n",
      "2130 ---- 2009\n",
      "2131 ---- 2010\n",
      "2132 ---- 2011\n",
      "2133 ---- 2012\n",
      "2134 ---- 2013\n",
      "2135 ---- 2014\n",
      "2136 ---- 2015\n",
      "2137 ---- 2016\n",
      "2138 ---- 2017\n",
      "2139 ---- 2018\n",
      "2140 ---- 2019\n",
      "2141 ---- 2020\n",
      "2142 ---- 2021\n",
      "2143 ---- 2022\n",
      "2144 ---- 2023\n",
      "2145 ---- 2024\n",
      "2146 ---- 2025\n",
      "2147 ---- 2026\n",
      "2148 ---- 2027\n",
      "2149 ---- 2028\n",
      "2150 ---- 2029\n",
      "2151 ---- 2030\n",
      "2152 ---- 2031\n",
      "2153 ---- 2032\n",
      "2154 ---- 2033\n",
      "2155 ---- 2034\n",
      "2156 ---- 2035\n",
      "2157 ---- 2036\n",
      "2158 ---- 2037\n",
      "2159 ---- 2038\n",
      "2160 ---- 2039\n",
      "2161 ---- 2040\n",
      "2162 ---- 2041\n",
      "2163 ---- 2042\n",
      "2164 ---- 2043\n",
      "2165 ---- 2044\n",
      "2166 ---- 2045\n",
      "2167 ---- 2046\n",
      "2168 ---- 2047\n",
      "2169 ---- 2048\n",
      "2170 ---- 2049\n",
      "2171 ---- 2050\n",
      "2172 ---- 2051\n",
      "2173 ---- 2052\n",
      "2174 ---- 2053\n",
      "2175 ---- 2054\n",
      "2176 ---- 2055\n",
      "2177 ---- 2056\n",
      "2178 ---- 2057\n",
      "2179 ---- 2058\n",
      "2180 ---- 2059\n",
      "2181 ---- 2060\n",
      "2182 ---- 2061\n",
      "2183 ---- 2062\n",
      "2184 ---- 2063\n",
      "2185 ---- 2064\n",
      "2186 ---- 2065\n",
      "2187 ---- 2066\n",
      "2188 ---- 2067\n",
      "2189 ---- 2068\n",
      "2190 ---- 2069\n",
      "2191 ---- 2070\n",
      "2192 ---- 2071\n",
      "2193 ---- 2072\n",
      "2194 ---- 2073\n",
      "2195 ---- 2074\n",
      "2196 ---- 2075\n",
      "2197 ---- 2076\n",
      "2198 ---- 2077\n",
      "2199 ---- 2078\n",
      "2200 ---- 2079\n",
      "2201 ---- 2080\n",
      "2202 ---- 2081\n",
      "2203 ---- 2082\n",
      "2204 ---- 2083\n",
      "2205 ---- 2084\n",
      "2206 ---- 2085\n",
      "2207 ---- 2086\n",
      "2208 ---- 2087\n",
      "2209 ---- 2088\n",
      "2210 ---- 2089\n",
      "2211 ---- 2090\n",
      "2212 ---- 2091\n",
      "2213 ---- 2092\n",
      "2214 ---- 2093\n",
      "2215 ---- 2094\n",
      "2216 ---- 2095\n",
      "2217 ---- 2096\n",
      "2218 ---- 2097\n",
      "2219 ---- 2098\n",
      "2220 ---- 2099\n",
      "2221 ---- 2100\n",
      "2222 ---- 2101\n",
      "2223 ---- 2102\n",
      "2224 ---- 2103\n",
      "2225 ---- 2104\n",
      "2226 ---- 2105\n",
      "2227 ---- 2106\n",
      "2228 ---- 2107\n",
      "2229 ---- 2108\n",
      "2230 ---- 2109\n",
      "2231 ---- 2110\n",
      "2232 ---- 2111\n",
      "2233 ---- 2112\n",
      "2234 ---- 2113\n",
      "2235 ---- 2114\n",
      "2236 ---- 2115\n",
      "2237 ---- 2116\n",
      "2238 ---- 2117\n",
      "2239 ---- 2118\n",
      "2240 ---- 2119\n",
      "2241 ---- 2120\n",
      "2242 ---- 2121\n",
      "2243 ---- 2122\n",
      "2244 ---- 2123\n",
      "2245 ---- 2124\n",
      "2246 ---- 2125\n",
      "2247 ---- 2126\n",
      "2248 ---- 2127\n",
      "2249 ---- 2128\n",
      "2250 ---- 2129\n",
      "2251 ---- 2130\n",
      "2252 ---- 2131\n",
      "2253 ---- 2132\n",
      "2254 ---- 2133\n",
      "2255 ---- 2134\n",
      "2256 ---- 2135\n",
      "2257 ---- 2136\n",
      "2258 ---- 2137\n",
      "2259 ---- 2138\n",
      "2260 ---- 2139\n",
      "2261 ---- 2140\n",
      "2262 ---- 2141\n",
      "2263 ---- 2142\n",
      "2264 ---- 2143\n",
      "2265 ---- 2144\n",
      "2266 ---- 2145\n",
      "2267 ---- 2146\n",
      "2268 ---- 2147\n",
      "2269 ---- 2148\n",
      "2270 ---- 2149\n",
      "2271 ---- 2150\n",
      "2272 ---- 2151\n",
      "2273 ---- 2152\n",
      "2274 ---- 2153\n",
      "2275 ---- 2154\n",
      "2276 ---- 2155\n",
      "2277 ---- 2156\n",
      "2278 ---- 2157\n",
      "2279 ---- 2158\n",
      "2280 ---- 2159\n",
      "2281 ---- 2160\n",
      "2282 ---- 2161\n",
      "2283 ---- 2162\n",
      "2284 ---- 2163\n",
      "2285 ---- 2164\n",
      "2286 ---- 2165\n",
      "2287 ---- 2166\n",
      "2288 ---- 2167\n",
      "2289 ---- 2168\n",
      "2290 ---- 2169\n",
      "2291 ---- 2170\n",
      "2292 ---- 2171\n",
      "2293 ---- 2172\n",
      "2294 ---- 2173\n",
      "2295 ---- 2174\n",
      "2296 ---- 2175\n",
      "2297 ---- 2176\n",
      "2298 ---- 2177\n",
      "2299 ---- 2178\n",
      "2300 ---- 2179\n",
      "2301 ---- 2180\n",
      "2302 ---- 2181\n",
      "2303 ---- 2182\n",
      "2304 ---- 2183\n",
      "2305 ---- 2184\n",
      "2306 ---- 2185\n",
      "2307 ---- 2186\n",
      "2308 ---- 2187\n",
      "2309 ---- 2188\n",
      "2310 ---- 2189\n",
      "2311 ---- 2190\n",
      "2312 ---- 2191\n",
      "2313 ---- 2192\n",
      "2314 ---- 2193\n",
      "2315 ---- 2194\n",
      "2316 ---- 2195\n",
      "2317 ---- 2196\n",
      "2318 ---- 2197\n",
      "2319 ---- 2198\n",
      "2320 ---- 2199\n",
      "2321 ---- 2200\n",
      "2322 ---- 2201\n",
      "2323 ---- 2202\n",
      "2324 ---- 2203\n",
      "2325 ---- 2204\n",
      "2326 ---- 2205\n",
      "2327 ---- 2206\n",
      "2328 ---- 2207\n",
      "2329 ---- 2208\n",
      "2330 ---- 2209\n",
      "2331 ---- 2210\n",
      "2332 ---- 2211\n",
      "2333 ---- 2212\n",
      "2334 ---- 2213\n",
      "2335 ---- 2214\n",
      "2336 ---- 2215\n",
      "2337 ---- 2216\n",
      "2338 ---- 2217\n",
      "2339 ---- 2218\n",
      "2340 ---- 2219\n",
      "2341 ---- 2220\n",
      "2342 ---- 2221\n",
      "2343 ---- 2222\n",
      "2344 ---- 2223\n",
      "2345 ---- 2224\n",
      "2346 ---- 2225\n",
      "2347 ---- 2226\n",
      "2348 ---- 2227\n",
      "2349 ---- 2228\n",
      "2350 ---- 2229\n",
      "2351 ---- 2230\n",
      "2352 ---- 2231\n",
      "2353 ---- 2232\n",
      "2354 ---- 2233\n",
      "2355 ---- 2234\n",
      "2356 ---- 2235\n",
      "2357 ---- 2236\n",
      "2358 ---- 2237\n",
      "2359 ---- 2238\n",
      "2360 ---- 2239\n",
      "2361 ---- 2240\n",
      "2362 ---- 2241\n",
      "2363 ---- 2242\n",
      "2364 ---- 2243\n",
      "2365 ---- 2244\n",
      "2366 ---- 2245\n",
      "2367 ---- 2246\n",
      "2368 ---- 2247\n",
      "2369 ---- 2248\n",
      "2370 ---- 2249\n",
      "2371 ---- 2250\n",
      "2372 ---- 2251\n",
      "2373 ---- 2252\n",
      "2374 ---- 2253\n",
      "2375 ---- 2254\n",
      "2376 ---- 2255\n",
      "2377 ---- 2256\n",
      "2378 ---- 2257\n",
      "2379 ---- 2258\n",
      "2380 ---- 2259\n",
      "2381 ---- 2260\n",
      "2382 ---- 2261\n",
      "2383 ---- 2262\n",
      "2384 ---- 2263\n",
      "2385 ---- 2264\n",
      "2386 ---- 2265\n",
      "2387 ---- 2266\n",
      "2388 ---- 2267\n",
      "2389 ---- 2268\n",
      "2390 ---- 2269\n",
      "2391 ---- 2270\n",
      "2392 ---- 2271\n",
      "2393 ---- 2272\n",
      "2394 ---- 2273\n",
      "2395 ---- 2274\n",
      "2396 ---- 2275\n",
      "2397 ---- 2276\n",
      "2398 ---- 2277\n",
      "2399 ---- 2278\n",
      "2400 ---- 2279\n",
      "2401 ---- 2280\n",
      "2402 ---- 2281\n",
      "2403 ---- 2282\n",
      "2404 ---- 2283\n",
      "2405 ---- 2284\n",
      "2406 ---- 2285\n",
      "2407 ---- 2286\n",
      "2408 ---- 2287\n",
      "2409 ---- 2288\n",
      "2410 ---- 2289\n",
      "2411 ---- 2290\n",
      "2412 ---- 2291\n",
      "2413 ---- 2292\n",
      "2414 ---- 2293\n",
      "2415 ---- 2294\n",
      "2416 ---- 2295\n",
      "2417 ---- 2296\n",
      "2418 ---- 2297\n",
      "2419 ---- 2298\n",
      "2420 ---- 2299\n",
      "2421 ---- 2300\n",
      "2422 ---- 2301\n",
      "2423 ---- 2302\n",
      "2424 ---- 2303\n",
      "2425 ---- 2304\n",
      "2426 ---- 2305\n",
      "2427 ---- 2306\n",
      "2428 ---- 2307\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(X_train_final.columns)):\n",
    "    print('{} ---- {}'.format(i, X_train_final.columns[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Unigrams, POS Tag Count, Sentiment Polarity, Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_1 = X_train_final.iloc[:,np.r_[10:12,13:21,121:1281]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296, 1170)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_1 = X_test_final.iloc[:,np.r_[10:12,13:21,121:1281]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 1170)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.514\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9550    0.8281    0.8870       128\n",
      "           1     0.3333    0.6875    0.4490        16\n",
      "\n",
      "    accuracy                         0.8125       144\n",
      "   macro avg     0.6441    0.7578    0.6680       144\n",
      "weighted avg     0.8859    0.8125    0.8384       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_1_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_1_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_1,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_1), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 106, False Positive: 22, False Negative: 5, True Positive: 11\n",
      "--------------------------------------------------------------------------------\n",
      "[[106  22]\n",
      " [  5  11]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.83      0.89       128\n",
      "           1       0.33      0.69      0.45        16\n",
      "\n",
      "    accuracy                           0.81       144\n",
      "   macro avg       0.64      0.76      0.67       144\n",
      "weighted avg       0.89      0.81      0.84       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_1 = LogisticRegression(random_state=18, \n",
    "                                solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], \n",
    "                                class_weight='balanced').fit(X_train_model_1, y_train)\n",
    "y_lr = lr_model_1.predict(X_test_model_1)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_model_2 = X_train_final.iloc[:,np.r_[3:1113]]\n",
    "X_train_model_2 = X_train_final.iloc[:, np.r_[3:1281]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296, 1278)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_2 = X_test_final.iloc[:,np.r_[3:1281]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 1278)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.500\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9459    0.8203    0.8787       128\n",
      "           1     0.3030    0.6250    0.4082        16\n",
      "\n",
      "    accuracy                         0.7986       144\n",
      "   macro avg     0.6245    0.7227    0.6434       144\n",
      "weighted avg     0.8745    0.7986    0.8264       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_2_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_2_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_2,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_2), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 105, False Positive: 23, False Negative: 6, True Positive: 10\n",
      "--------------------------------------------------------------------------------\n",
      "[[105  23]\n",
      " [  6  10]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.82      0.88       128\n",
      "           1       0.30      0.62      0.41        16\n",
      "\n",
      "    accuracy                           0.80       144\n",
      "   macro avg       0.62      0.72      0.64       144\n",
      "weighted avg       0.87      0.80      0.83       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_2 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_2, y_train)\n",
    "y_lr = lr_model_2.predict(X_test_model_2)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Without Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_3 = X_train_final.iloc[:,np.r_[3:121]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296, 118)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_3 = X_test_final.iloc[:,np.r_[3:121]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 118)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n",
      "Best score: 0.421\n",
      "Best parameters set:\n",
      "\tclf__C: 25\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9450    0.8047    0.8692       128\n",
      "           1     0.2857    0.6250    0.3922        16\n",
      "\n",
      "    accuracy                         0.7847       144\n",
      "   macro avg     0.6153    0.7148    0.6307       144\n",
      "weighted avg     0.8717    0.7847    0.8162       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_3_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.0001, 0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\", \"elasticnet\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_3_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_3,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_3), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 103, False Positive: 25, False Negative: 6, True Positive: 10\n",
      "--------------------------------------------------------------------------------\n",
      "[[103  25]\n",
      " [  6  10]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.80      0.87       128\n",
      "           1       0.29      0.62      0.39        16\n",
      "\n",
      "    accuracy                           0.78       144\n",
      "   macro avg       0.62      0.71      0.63       144\n",
      "weighted avg       0.87      0.78      0.82       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_3 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_3, y_train)\n",
    "y_lr = lr_model_3.predict(X_test_model_3)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Without Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_4 = X_train_final.iloc[:,np.r_[3:21,121:1281]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296, 1178)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_4 = X_test_final.iloc[:,np.r_[3:21,121:1281]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 1178)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.512\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9550    0.8281    0.8870       128\n",
      "           1     0.3333    0.6875    0.4490        16\n",
      "\n",
      "    accuracy                         0.8125       144\n",
      "   macro avg     0.6441    0.7578    0.6680       144\n",
      "weighted avg     0.8859    0.8125    0.8384       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_4_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_4_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_4,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_4), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 106, False Positive: 22, False Negative: 5, True Positive: 11\n",
      "--------------------------------------------------------------------------------\n",
      "[[106  22]\n",
      " [  5  11]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.83      0.89       128\n",
      "           1       0.33      0.69      0.45        16\n",
      "\n",
      "    accuracy                           0.81       144\n",
      "   macro avg       0.64      0.76      0.67       144\n",
      "weighted avg       0.89      0.81      0.84       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_4 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_4, y_train)\n",
    "y_lr = lr_model_4.predict(X_test_model_4)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Without POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_5 = X_train_final.iloc[:,np.r_[3:13,21:1281]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296, 1270)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_5 = X_test_final.iloc[:,np.r_[3:13,21:1281]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 1270)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.499\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.86      0.91       128\n",
      "           1       0.40      0.75      0.52        16\n",
      "\n",
      "    accuracy                           0.85       144\n",
      "   macro avg       0.68      0.80      0.72       144\n",
      "weighted avg       0.90      0.85      0.87       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_5_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_5_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_5,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_5), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 110, False Positive: 18, False Negative: 4, True Positive: 12\n",
      "--------------------------------------------------------------------------------\n",
      "[[110  18]\n",
      " [  4  12]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.86      0.91       128\n",
      "           1       0.40      0.75      0.52        16\n",
      "\n",
      "    accuracy                           0.85       144\n",
      "   macro avg       0.68      0.80      0.72       144\n",
      "weighted avg       0.90      0.85      0.87       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_5 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_5, y_train)\n",
    "y_lr = lr_model_5.predict(X_test_model_5)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6: Without STEM Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_6 = X_train_final.iloc[:,np.r_[10:1281]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296, 1271)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_6 = X_test_final.iloc[:,np.r_[10:1281]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 1271)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.511\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.82      0.88       128\n",
      "           1       0.32      0.69      0.44        16\n",
      "\n",
      "    accuracy                           0.81       144\n",
      "   macro avg       0.64      0.75      0.66       144\n",
      "weighted avg       0.88      0.81      0.83       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_6_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_6_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_6,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_6), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 105, False Positive: 23, False Negative: 5, True Positive: 11\n",
      "--------------------------------------------------------------------------------\n",
      "[[105  23]\n",
      " [  5  11]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.82      0.88       128\n",
      "           1       0.32      0.69      0.44        16\n",
      "\n",
      "    accuracy                           0.81       144\n",
      "   macro avg       0.64      0.75      0.66       144\n",
      "weighted avg       0.88      0.81      0.83       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_6 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_6, y_train)\n",
    "y_lr = lr_model_6.predict(X_test_model_6)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7: Without Sentiment Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_7 = X_train_final.iloc[:,np.r_[3:10,12:1281]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296, 1276)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_7 = X_test_final.iloc[:,np.r_[3:10,12:1281]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 1276)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.505\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.84      0.89       128\n",
      "           1       0.33      0.62      0.43        16\n",
      "\n",
      "    accuracy                           0.82       144\n",
      "   macro avg       0.64      0.73      0.66       144\n",
      "weighted avg       0.88      0.82      0.84       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_7_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_7_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_7,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_7), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 108, False Positive: 20, False Negative: 6, True Positive: 10\n",
      "--------------------------------------------------------------------------------\n",
      "[[108  20]\n",
      " [  6  10]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.84      0.89       128\n",
      "           1       0.33      0.62      0.43        16\n",
      "\n",
      "    accuracy                           0.82       144\n",
      "   macro avg       0.64      0.73      0.66       144\n",
      "weighted avg       0.88      0.82      0.84       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_7 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_7, y_train)\n",
    "y_lr = lr_model_7.predict(X_test_model_7)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 8: Without NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_8 = X_train_final.iloc[:,np.r_[3:12,13:1281]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296, 1277)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_8 = X_test_final.iloc[:,np.r_[3:12,13:1281]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 1277)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.500\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.82      0.88       128\n",
      "           1       0.30      0.62      0.41        16\n",
      "\n",
      "    accuracy                           0.80       144\n",
      "   macro avg       0.62      0.72      0.64       144\n",
      "weighted avg       0.87      0.80      0.83       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_8_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_8_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_8,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_8), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 105, False Positive: 23, False Negative: 6, True Positive: 10\n",
      "--------------------------------------------------------------------------------\n",
      "[[105  23]\n",
      " [  6  10]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.82      0.88       128\n",
      "           1       0.30      0.62      0.41        16\n",
      "\n",
      "    accuracy                           0.80       144\n",
      "   macro avg       0.62      0.72      0.64       144\n",
      "weighted avg       0.87      0.80      0.83       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_8 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_8, y_train)\n",
    "y_lr = lr_model_8.predict(X_test_model_8)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Summary\n",
    "| Experiment | Model Number | Features Used                                                | Precision | Recall | Macro F1 | Ranking               |\n",
    "| :--------: | :----------: | :----------------------------------------------------------: | :-------: | :----: | :------: | :-------------------: |\n",
    "| Baseline   | 1            | Unigrams, POS Tag Count, Sentiment Polarity and Subjectivity | 0.64      | 0.76   | 0.67     |                       |\n",
    "| Baseline   | 2            | All features (baseline)                                      | 0.62      | 0.72   | 0.64     |                       |\n",
    "| Baseline   | 3            | Without Unigrams                                             | 0.62      | 0.71   | 0.63     |                       |\n",
    "| Baseline   | 4            | Without Embeddings                                           | 0.64      | 0.76   | 0.67     |                       |\n",
    "| Baseline   | 5            | **Without POS tag**                                          | 0.68      | 0.8    | 0.72     |      1                |\n",
    "| Baseline   | 6            | Without STEM similarity (paper baseline)                     | 0.64      | 0.75   | 0.66     |                       |\n",
    "| Baseline   | 7            | Without sentiment features                                   | 0.64      | 0.73   | 0.66     |                       |\n",
    "| Baseline   | 8            | Without NER                                                  | 0.62      | 0.72   | 0.64     |                       |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
