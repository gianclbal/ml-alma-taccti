{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resistance Logistic Regression Models Using Merged Data Experiment 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34922cb298a64700b1e095162f9c30c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 06:11:29 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-09-25 06:11:29 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-09-25 06:11:30 INFO: File exists: /Users/gbaldonado/stanza_resources/en/default.zip\n",
      "2024-09-25 06:11:32 INFO: Finished downloading models and saved to /Users/gbaldonado/stanza_resources\n",
      "2024-09-25 06:11:32 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a1070d7843e47fb94d4f2bd3f465f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 06:11:33 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-09-25 06:11:34 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-09-25 06:11:34 INFO: Using device: cpu\n",
      "2024-09-25 06:11:34 INFO: Loading: tokenize\n",
      "2024-09-25 06:11:34 INFO: Loading: mwt\n",
      "2024-09-25 06:11:34 INFO: Loading: pos\n",
      "2024-09-25 06:11:34 INFO: Loading: lemma\n",
      "2024-09-25 06:11:34 INFO: Loading: constituency\n",
      "2024-09-25 06:11:34 INFO: Loading: depparse\n",
      "2024-09-25 06:11:35 INFO: Loading: sentiment\n",
      "2024-09-25 06:11:35 INFO: Loading: ner\n",
      "2024-09-25 06:11:35 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import pickle\n",
    "import warnings\n",
    "import stanza\n",
    "\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, f1_score, r2_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Set random seed\n",
    "random.seed(18)\n",
    "seed = 18\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Initialize lemmatizer, stop words, and stanza\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline('en') # initialize English neural pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data and quick exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_resistance_df = pd.read_csv(\"../../../../data/processed_for_model/merged_themes_using_jaccard_method/merged_Resistance_sentence_level_batch_1_jaccard.csv\", encoding='utf-8')\n",
    "\n",
    "# Shuffle the merged dataset\n",
    "merged_resistance_df = shuffle(merged_resistance_df, random_state=seed)\n",
    "\n",
    "# Train-test split \n",
    "training_df, test_df = train_test_split(merged_resistance_df, test_size=0.3, random_state=18, stratify=merged_resistance_df['label'])\n",
    "\n",
    "training_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>math always seems to give me a little bit of trouble, but i get a hang of it after pushing myself to do the best i can.</td>\n",
       "      <td>1</td>\n",
       "      <td>['Math always seems to give me a little bit of trouble, but I get a hang of it after pushing myself to do the best I can.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>after working minimum wage part time jobs while in community college, i've learned that there is no way i can do that for the rest of my life.</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"After working minimum wage part time jobs while in community college, i've learned that there is no way I can do that for the rest of my life. If I didn't chose to be here, I would honestly have no idea or guess to where I would be now but all the decision making and experiences i've had in my life has led me here\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this would cause a sharp pain in my elbow whenever i would do pushups, bench press, or simply bending my arm would result in a cracking like noise.</td>\n",
       "      <td>0</td>\n",
       "      <td>['When I went to physical therapy, my therapist and I got really close during my process of rehabilitation. After healing my elbow completely, my physical therapist taught me important tips on taking care of my body overall as well as continuing my dreams of playing basketball professionally. My overall goal as to why I am in college is to become a physical therapist.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i believe i am here to better my life.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I want to be able to say I went back and finished school, but not only finished, but completed a major that most never thought I would be able to do.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am here because i want to better myself my family, not only financially but in health.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I know that as a child I never thought of education and a career for someone who is undocumented.', 'I Hope to reach a position in which I can inspire and prove to all of the \"immigrants\" in the USA that achieving and surpassing struggle is possible.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                              sentence  \\\n",
       "0                              math always seems to give me a little bit of trouble, but i get a hang of it after pushing myself to do the best i can.   \n",
       "1       after working minimum wage part time jobs while in community college, i've learned that there is no way i can do that for the rest of my life.   \n",
       "2  this would cause a sharp pain in my elbow whenever i would do pushups, bench press, or simply bending my arm would result in a cracking like noise.   \n",
       "3                                                                                                               i believe i am here to better my life.   \n",
       "4                                                             i am here because i want to better myself my family, not only financially but in health.   \n",
       "\n",
       "   label  \\\n",
       "0      1   \n",
       "1      0   \n",
       "2      0   \n",
       "3      0   \n",
       "4      0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                 phrase  \n",
       "0                                                                                                                                                                                                                                                           ['Math always seems to give me a little bit of trouble, but I get a hang of it after pushing myself to do the best I can.']  \n",
       "1                                                       [\"After working minimum wage part time jobs while in community college, i've learned that there is no way I can do that for the rest of my life. If I didn't chose to be here, I would honestly have no idea or guess to where I would be now but all the decision making and experiences i've had in my life has led me here\"]  \n",
       "2  ['When I went to physical therapy, my therapist and I got really close during my process of rehabilitation. After healing my elbow completely, my physical therapist taught me important tips on taking care of my body overall as well as continuing my dreams of playing basketball professionally. My overall goal as to why I am in college is to become a physical therapist.']  \n",
       "3                                                                                                                                                                                                                             ['I want to be able to say I went back and finished school, but not only finished, but completed a major that most never thought I would be able to do.']  \n",
       "4                                                                                                                        ['I know that as a child I never thought of education and a career for someone who is undocumented.', 'I Hope to reach a position in which I can inspire and prove to all of the \"immigrants\" in the USA that achieving and surpassing struggle is possible.']  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>keeping an open mind i am looking forward to this transition and know it will be difficult but after the military nothing really can be that bad, or can it? expectations for this class are high i suppose in the fact that this course takes prerequisites to even get into.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I am a Disabled Veteran and am coming back to school to learn skills more properly suited for civilian interaction, basically here to learn something thats not related to killing and dealing with guns since all those skills do is help me stay in the cycle of protection lines of work (cop, security, etc...). Keeping an open mind I am looking forward to this transition and know it will be difficult but after the military nothing really can be that bad, or can it?']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i am here to prove that i can do anything i set my mind to, not only to others, but to myself as well.</td>\n",
       "      <td>1</td>\n",
       "      <td>['I am here to prove that I can do anything I set my mind to, not only to others, but to myself as well.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>being in physics is difficult for me because there are many formulas i need to memorize.</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"Being in physics is difficult for me because there are many formulas I need to memorize. So another reason I'm in the lab is to form groups that will help me learn the terms and formulas better.\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>physics why am i here?</td>\n",
       "      <td>0</td>\n",
       "      <td>['Despite, all these concepts that is definitely interesting, I feel physics can be sometimes hard to be understood with all the details and mind boggling questions it includes. Yet, these concepts will be an important part of my work I will be doing in the future. Thus, I am here to learn physics and more to get better.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i used to always be a quitter, and very early on in high school i decided that i was not smart enough to understand science but i have learned that dedication, passion, and hard work are key.</td>\n",
       "      <td>1</td>\n",
       "      <td>['I used to always be a quitter, and very early on in high school I decided that I was not smart enough to understand science but I have learned that dedication, passion, and hard work are key.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                         sentence  \\\n",
       "0  keeping an open mind i am looking forward to this transition and know it will be difficult but after the military nothing really can be that bad, or can it? expectations for this class are high i suppose in the fact that this course takes prerequisites to even get into.   \n",
       "1                                                                                                                                                                          i am here to prove that i can do anything i set my mind to, not only to others, but to myself as well.   \n",
       "2                                                                                                                                                                                        being in physics is difficult for me because there are many formulas i need to memorize.   \n",
       "3                                                                                                                                                                                                                                                          physics why am i here?   \n",
       "4                                                                                 i used to always be a quitter, and very early on in high school i decided that i was not smart enough to understand science but i have learned that dedication, passion, and hard work are key.   \n",
       "\n",
       "   label  \\\n",
       "0      0   \n",
       "1      1   \n",
       "2      0   \n",
       "3      0   \n",
       "4      1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  phrase  \n",
       "0  ['I am a Disabled Veteran and am coming back to school to learn skills more properly suited for civilian interaction, basically here to learn something thats not related to killing and dealing with guns since all those skills do is help me stay in the cycle of protection lines of work (cop, security, etc...). Keeping an open mind I am looking forward to this transition and know it will be difficult but after the military nothing really can be that bad, or can it?']  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                             ['I am here to prove that I can do anything I set my mind to, not only to others, but to myself as well.']  \n",
       "2                                                                                                                                                                                                                                                                                 [\"Being in physics is difficult for me because there are many formulas I need to memorize. So another reason I'm in the lab is to form groups that will help me learn the terms and formulas better.\"]  \n",
       "3                                                                                                                                                   ['Despite, all these concepts that is definitely interesting, I feel physics can be sometimes hard to be understood with all the details and mind boggling questions it includes. Yet, these concepts will be an important part of my work I will be doing in the future. Thus, I am here to learn physics and more to get better.']  \n",
       "4                                                                                                                                                                                                                                                                                    ['I used to always be a quitter, and very early on in high school I decided that I was not smart enough to understand science but I have learned that dedication, passion, and hard work are key.']  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (842, 3) \n",
      "Test dataset shape: (362, 3)\n",
      "Positive labels present in the dataset : 61  out of 842 or 7.244655581947744%\n",
      "Positive labels present in the test dataset : 26  out of 362 or 7.18232044198895%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset shape: {training_df.shape} \\nTest dataset shape: {test_df.shape}\")\n",
    "pos_labels = len([n for n in training_df['label'] if n==1])\n",
    "print(\"Positive labels present in the dataset : {}  out of {} or {}%\".format(pos_labels, len(training_df['label']), (pos_labels/len(training_df['label']))*100))\n",
    "pos_labels = len([n for n in test_df['label'] if n==1])\n",
    "print(\"Positive labels present in the test dataset : {}  out of {} or {}%\".format(pos_labels, len(test_df['label']), (pos_labels/len(test_df['label']))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the data for the plots\n",
    "training_counts = training_df['label'].value_counts()\n",
    "test_counts = test_df['label'].value_counts()\n",
    "\n",
    "# Set up the subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot for the training set\n",
    "sns.barplot(x=training_counts.index, y=training_counts.values, ax=axes[0])\n",
    "axes[0].set_title('Distribution of labels in training set')\n",
    "axes[0].set_ylabel('Sentences')\n",
    "axes[0].set_xlabel('Label')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot for the test set\n",
    "sns.barplot(x=test_counts.index, y=test_counts.values, ax=axes[1])\n",
    "axes[1].set_title('Distribution of labels in test set')\n",
    "axes[1].set_ylabel('Sentences')\n",
    "axes[1].set_xlabel('Label')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack Obama']\n"
     ]
    }
   ],
   "source": [
    "def get_ner(text):\n",
    "    ner_list = []\n",
    "    # Annotate the text using stanza\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for entity in sentence.ents:\n",
    "            if entity.type == 'PERSON':\n",
    "                ner_list.append(entity.text)\n",
    "\n",
    "    return ner_list\n",
    "\n",
    "# Example usage\n",
    "text = \"Barack Obama was the 44th doctor of the United States.\"\n",
    "print(get_ner(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a named entity is present in the sentence\n",
    "def named_entity_present(sentence):\n",
    "    ner_list = get_ner(sentence)\n",
    "    if len(ner_list) > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Similarity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the similar words and similarity score\n",
    "# The function takes tokens of sentence as input and if its not a stop word, get its similarity with synsets of STEM.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words |= set([\"help\",\"try\", \"work\", \"process\", \"support\", \"job\"] )\n",
    "def word_similarity(tokens, syns, field):    \n",
    "    if field in ['engineering', 'technology']:\n",
    "        score_threshold = 0.5\n",
    "    else:\n",
    "        score_threshold = 0.2\n",
    "    sim_words = 0\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            try:\n",
    "                syns_word = wordnet.synsets(token) \n",
    "                score = syns_word[0].path_similarity(syns[0])\n",
    "                if score >= score_threshold:\n",
    "                    sim_words += 1\n",
    "            except: \n",
    "                score = 0\n",
    "    \n",
    "    return sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to create columns for similarity based on all STEM fields\n",
    "syns_bio = wordnet.synsets(lemmatizer.lemmatize(\"biology\"))\n",
    "syns_maths = wordnet.synsets(lemmatizer.lemmatize(\"mathematics\")) \n",
    "syns_tech = wordnet.synsets(lemmatizer.lemmatize(\"technology\"))\n",
    "syns_eng = wordnet.synsets(lemmatizer.lemmatize(\"engineering\"))\n",
    "syns_chem = wordnet.synsets(lemmatizer.lemmatize(\"chemistry\"))\n",
    "syns_phy = wordnet.synsets(lemmatizer.lemmatize(\"physics\"))\n",
    "syns_sci = wordnet.synsets(lemmatizer.lemmatize(\"science\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Medical Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gynecology', 'heart', 'sports', 'transfusion', 'orbit', 'brain', 'hepatology', 'neonatal', 'neurology', 'abuse', 'perinatal', 'dermatopathology', 'urologic', 'toxicology', 'gastrointestinal', 'urology', 'pain', 'care', 'glaucoma', 'anesthesiology', 'cytogenetics', 'oncology', 'cardiovascular', 'surgery', 'anesthesiology', 'behavioral', 'interventional', 'neuro', 'pelvic', 'pediatrics', 'male', 'musculoskeletal', 'liaison', 'military', 'retina', 'psychiatry', 'sports', 'neurodevelopmental', 'nephrology', 'administrative', 'medicine', 'genetics', 'reconstructive', 'molecular', 'neuroradiology', 'psychiatry', 'cytopathology', 'immunopathology', 'emergency', 'nuclear', 'psychiatric', 'adolescent', 'neurourology', 'mental', 'medical', 'surgery', 'anterior', 'transplant', 'pediatrics', 'disease', 'neuromuscular', 'endocrinologists', 'occupational', 'microbiology', 'biochemical', 'infectious', 'calculi', 'reconstructive', 'blood', 'gynecologic', 'genitourinary', 'child', 'pulmonary', 'allergy', 'cardiac', 'clinical', 'internal', 'strabismus', 'neuropathology', 'gastroenterology', 'diagnostic', 'surgical', 'endocrinology', 'fetal', 'physical', 'neuroradiology', 'hematology', 'anatomical', 'banking', 'abdominal', 'cardiology', 'rehabilitation', 'family', 'metabolism', 'ophthalmic', 'oncology', 'ophthalmology', 'dermatology', 'consultation', 'rheumatology', 'and', 'dermatology', 'community', 'chemical', 'radiology', 'developmental', 'female', 'cardiothoracic', 'palliative', 'neck', 'aerospace', 'renal', 'vascular', 'reproductive', 'psychosomatic', 'advanced', 'ocular', 'endovascular', 'pulmonology', 'sleep', 'radiation', 'ophthalmology', 'uveitis', 'endocrinology', 'head', 'immunology', 'hematology', 'disabilities', 'transplant', 'geriatric', 'pathology', 'cornea', 'internal', 'chest', 'pediatric', 'oculoplastics', 'addiction', 'hospice', 'health', 'breast', 'infertility', 'gastroenterology', 'retardation', 'forensic', 'rheumatology', 'critical', 'maternal', 'genetic', 'genetic', 'failure', 'interventional', 'preventive', 'procedural', 'pediatric', 'segment', 'diseases', 'plastic', 'critical', 'urology', 'obstetrics', 'imaging', 'neurology', 'nephrology', 'diabetes', 'public', 'electrophysiology', 'pathology', 'adolescent', 'infectious', 'research', 'injury', 'neurophysiology']\n"
     ]
    }
   ],
   "source": [
    "# Load the medical specialization text file and create a list\n",
    "medical_list = []\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/medical_specialities.txt', 'r') as medical_fields:\n",
    "    for line in medical_fields.readlines():\n",
    "        special_field = line.rstrip('\\n')\n",
    "        special_field = re.sub(\"\\W\",\" \", special_field )\n",
    "#         print(special_field)\n",
    "        medical_list += special_field.split()\n",
    "medical_list = list(set(medical_list))  \n",
    "medical_list = [x.lower() for x in medical_list]\n",
    "print(medical_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get medical words\n",
    "def check_medical_words(tokens):\n",
    "    for token in tokens:\n",
    "        if token not in stop_words and token in [x.lower() for x in medical_list]:\n",
    "            return 1\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sentiment Polarity and Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get polarity and subjectivity of the sentence using TexBlob\n",
    "def get_sentiment(sentence):\n",
    "    sentiments =TextBlob(sentence).sentiment\n",
    "    polarity = sentiments.polarity\n",
    "    subjectivity = sentiments.subjectivity\n",
    "    return polarity, subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the count of POS tags of the sentence\n",
    "def count_pos_tags(tokens):\n",
    "    token_pos = pos_tag(tokens)\n",
    "    count = Counter(tag for word,tag in token_pos)\n",
    "    interjections =  count['UH']\n",
    "    nouns = count['NN'] + count['NNS'] + count['NNP'] + count['NNPS']\n",
    "    adverb = count['RB'] + count['RBS'] + count['RBR']\n",
    "    verb = count['VB'] + count['VBD'] + count['VBG'] + count['VBN']\n",
    "    determiner = count['DT']\n",
    "    pronoun = count['PRP']\n",
    "    adjetive = count['JJ'] + count['JJR'] + count['JJS']\n",
    "    preposition = count['IN']\n",
    "    return interjections, nouns, adverb, verb, determiner, pronoun, adjetive,preposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_extraction(dataframe, field, func, column_names):\n",
    "    return pd.concat((\n",
    "        dataframe,\n",
    "        dataframe[field].apply(\n",
    "            lambda cell: pd.Series(func(cell), index=column_names))), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the w2v dict from pickle file\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/pickle/embeddings06122024.pickle', 'rb') as w2v_file:\n",
    "    w2v_dict = pickle.load(w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of word embeddings:  4762\n"
     ]
    }
   ],
   "source": [
    "print(\"length of word embeddings: \", len(w2v_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vectors for the essay\n",
    "def vectorizer(sequence):\n",
    "    vect = []\n",
    "    numw = 0\n",
    "    for w in sequence: \n",
    "        try :\n",
    "            if numw == 0:\n",
    "                vect = w2v_dict[w]\n",
    "            else:\n",
    "                vect = np.add(vect, w2v_dict[w])\n",
    "            numw += 1\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    return vect/ numw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into words\n",
    "def split_into_words(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vectorizer\n",
    "unigram_vect = CountVectorizer(ngram_range=(1, 1), min_df=2, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Putting them all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for feature engineering\n",
    "def feature_engineering(original_dataset):\n",
    "\n",
    "    dataset = original_dataset.copy()\n",
    "    # create a new column with sentence tokens\n",
    "    dataset['tokens'] = dataset['sentence'].apply(word_tokenize)\n",
    "    # 1. Similarity features\n",
    "    # biology\n",
    "    dataset['bio_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_bio,'biology',)) \n",
    "    # chemistry\n",
    "    dataset['chem_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_chem,'chemistry',))\n",
    "    # physics\n",
    "    dataset['phy_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_phy,'physics',))\n",
    "    # mathematics\n",
    "    dataset['math_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_maths,'mathematics',))\n",
    "    # technology\n",
    "    dataset['tech_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_tech,'technology',))\n",
    "    # engineering\n",
    "    dataset['eng_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_eng,'engineering',))\n",
    "    \n",
    "    # medical terms\n",
    "    dataset['medical_terms'] = dataset['tokens'].apply(check_medical_words)\n",
    "    \n",
    "    # polarity and subjectivity\n",
    "    dataset['polarity'], dataset['subjectivity'] = zip(*dataset['sentence'].apply(get_sentiment))\n",
    "    \n",
    "    # named entity recognition\n",
    "    dataset['ner'] = dataset['sentence'].apply(named_entity_present)\n",
    "    \n",
    "    # pos tag count\n",
    "    dataset = pos_tag_extraction(dataset, 'tokens', count_pos_tags, ['interjections', 'nouns', 'adverb', 'verb', 'determiner', 'pronoun', 'adjetive','preposition'])\n",
    "    \n",
    "    # labels\n",
    "    data_labels = dataset['label']\n",
    "    # X\n",
    "    data_x = dataset.drop(columns='label')\n",
    "\n",
    "    \n",
    "    # vectorize all the essays\n",
    "    vect_arr = data_x.tokens.apply(vectorizer)\n",
    "    for index in range(0, len(vect_arr)):\n",
    "        i = 0\n",
    "        for item in vect_arr[index]:\n",
    "            column_name= \"embedding\" + str(i)\n",
    "            data_x.loc[index, column_name] = item\n",
    "            i +=1\n",
    "    \n",
    "    return data_x,data_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = feature_engineering(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(842, 121)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = feature_engineering(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362, 121)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Unigram features for both train and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(842, 121)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362, 121)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Resistance/saved_features/X_test_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Resistance/saved_features/X_train_final.csv\", index=False)\n",
    "X_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Resistance/saved_features/X_test_final.csv\", index=False)\n",
    "y_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Resistance/saved_features/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Resistance/saved_features/y_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the unigram df for train :  (842, 811)\n"
     ]
    }
   ],
   "source": [
    "# Unigrams for training set\n",
    "unigram_matrix = unigram_vect.fit_transform(X_train['sentence'])\n",
    "unigrams = pd.DataFrame(unigram_matrix.toarray())\n",
    "print(\"Shape of the unigram df for train : \",unigrams.shape)\n",
    "unigrams = unigrams.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = pd.concat([X_train, unigrams], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final.columns = X_train_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(842, 932)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test unigram df shape :  (362, 811)\n"
     ]
    }
   ],
   "source": [
    "unigram_matrix_test = unigram_vect.transform(X_test['sentence'])\n",
    "unigrams_test = pd.DataFrame(unigram_matrix_test.toarray())\n",
    "unigrams_test = unigrams_test.reset_index(drop=True)\n",
    "print(\"Test unigram df shape : \",unigrams_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362, 932)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final = pd.concat([X_test, unigrams_test], axis = 1)\n",
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final.columns = X_test_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362, 932)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ---- sentence\n",
      "1 ---- phrase\n",
      "2 ---- tokens\n",
      "3 ---- bio_sim_words\n",
      "4 ---- chem_sim_words\n",
      "5 ---- phy_sim_words\n",
      "6 ---- math_sim_words\n",
      "7 ---- tech_sim_words\n",
      "8 ---- eng_sim_words\n",
      "9 ---- medical_terms\n",
      "10 ---- polarity\n",
      "11 ---- subjectivity\n",
      "12 ---- ner\n",
      "13 ---- interjections\n",
      "14 ---- nouns\n",
      "15 ---- adverb\n",
      "16 ---- verb\n",
      "17 ---- determiner\n",
      "18 ---- pronoun\n",
      "19 ---- adjetive\n",
      "20 ---- preposition\n",
      "21 ---- embedding0\n",
      "22 ---- embedding1\n",
      "23 ---- embedding2\n",
      "24 ---- embedding3\n",
      "25 ---- embedding4\n",
      "26 ---- embedding5\n",
      "27 ---- embedding6\n",
      "28 ---- embedding7\n",
      "29 ---- embedding8\n",
      "30 ---- embedding9\n",
      "31 ---- embedding10\n",
      "32 ---- embedding11\n",
      "33 ---- embedding12\n",
      "34 ---- embedding13\n",
      "35 ---- embedding14\n",
      "36 ---- embedding15\n",
      "37 ---- embedding16\n",
      "38 ---- embedding17\n",
      "39 ---- embedding18\n",
      "40 ---- embedding19\n",
      "41 ---- embedding20\n",
      "42 ---- embedding21\n",
      "43 ---- embedding22\n",
      "44 ---- embedding23\n",
      "45 ---- embedding24\n",
      "46 ---- embedding25\n",
      "47 ---- embedding26\n",
      "48 ---- embedding27\n",
      "49 ---- embedding28\n",
      "50 ---- embedding29\n",
      "51 ---- embedding30\n",
      "52 ---- embedding31\n",
      "53 ---- embedding32\n",
      "54 ---- embedding33\n",
      "55 ---- embedding34\n",
      "56 ---- embedding35\n",
      "57 ---- embedding36\n",
      "58 ---- embedding37\n",
      "59 ---- embedding38\n",
      "60 ---- embedding39\n",
      "61 ---- embedding40\n",
      "62 ---- embedding41\n",
      "63 ---- embedding42\n",
      "64 ---- embedding43\n",
      "65 ---- embedding44\n",
      "66 ---- embedding45\n",
      "67 ---- embedding46\n",
      "68 ---- embedding47\n",
      "69 ---- embedding48\n",
      "70 ---- embedding49\n",
      "71 ---- embedding50\n",
      "72 ---- embedding51\n",
      "73 ---- embedding52\n",
      "74 ---- embedding53\n",
      "75 ---- embedding54\n",
      "76 ---- embedding55\n",
      "77 ---- embedding56\n",
      "78 ---- embedding57\n",
      "79 ---- embedding58\n",
      "80 ---- embedding59\n",
      "81 ---- embedding60\n",
      "82 ---- embedding61\n",
      "83 ---- embedding62\n",
      "84 ---- embedding63\n",
      "85 ---- embedding64\n",
      "86 ---- embedding65\n",
      "87 ---- embedding66\n",
      "88 ---- embedding67\n",
      "89 ---- embedding68\n",
      "90 ---- embedding69\n",
      "91 ---- embedding70\n",
      "92 ---- embedding71\n",
      "93 ---- embedding72\n",
      "94 ---- embedding73\n",
      "95 ---- embedding74\n",
      "96 ---- embedding75\n",
      "97 ---- embedding76\n",
      "98 ---- embedding77\n",
      "99 ---- embedding78\n",
      "100 ---- embedding79\n",
      "101 ---- embedding80\n",
      "102 ---- embedding81\n",
      "103 ---- embedding82\n",
      "104 ---- embedding83\n",
      "105 ---- embedding84\n",
      "106 ---- embedding85\n",
      "107 ---- embedding86\n",
      "108 ---- embedding87\n",
      "109 ---- embedding88\n",
      "110 ---- embedding89\n",
      "111 ---- embedding90\n",
      "112 ---- embedding91\n",
      "113 ---- embedding92\n",
      "114 ---- embedding93\n",
      "115 ---- embedding94\n",
      "116 ---- embedding95\n",
      "117 ---- embedding96\n",
      "118 ---- embedding97\n",
      "119 ---- embedding98\n",
      "120 ---- embedding99\n",
      "121 ---- 0\n",
      "122 ---- 1\n",
      "123 ---- 2\n",
      "124 ---- 3\n",
      "125 ---- 4\n",
      "126 ---- 5\n",
      "127 ---- 6\n",
      "128 ---- 7\n",
      "129 ---- 8\n",
      "130 ---- 9\n",
      "131 ---- 10\n",
      "132 ---- 11\n",
      "133 ---- 12\n",
      "134 ---- 13\n",
      "135 ---- 14\n",
      "136 ---- 15\n",
      "137 ---- 16\n",
      "138 ---- 17\n",
      "139 ---- 18\n",
      "140 ---- 19\n",
      "141 ---- 20\n",
      "142 ---- 21\n",
      "143 ---- 22\n",
      "144 ---- 23\n",
      "145 ---- 24\n",
      "146 ---- 25\n",
      "147 ---- 26\n",
      "148 ---- 27\n",
      "149 ---- 28\n",
      "150 ---- 29\n",
      "151 ---- 30\n",
      "152 ---- 31\n",
      "153 ---- 32\n",
      "154 ---- 33\n",
      "155 ---- 34\n",
      "156 ---- 35\n",
      "157 ---- 36\n",
      "158 ---- 37\n",
      "159 ---- 38\n",
      "160 ---- 39\n",
      "161 ---- 40\n",
      "162 ---- 41\n",
      "163 ---- 42\n",
      "164 ---- 43\n",
      "165 ---- 44\n",
      "166 ---- 45\n",
      "167 ---- 46\n",
      "168 ---- 47\n",
      "169 ---- 48\n",
      "170 ---- 49\n",
      "171 ---- 50\n",
      "172 ---- 51\n",
      "173 ---- 52\n",
      "174 ---- 53\n",
      "175 ---- 54\n",
      "176 ---- 55\n",
      "177 ---- 56\n",
      "178 ---- 57\n",
      "179 ---- 58\n",
      "180 ---- 59\n",
      "181 ---- 60\n",
      "182 ---- 61\n",
      "183 ---- 62\n",
      "184 ---- 63\n",
      "185 ---- 64\n",
      "186 ---- 65\n",
      "187 ---- 66\n",
      "188 ---- 67\n",
      "189 ---- 68\n",
      "190 ---- 69\n",
      "191 ---- 70\n",
      "192 ---- 71\n",
      "193 ---- 72\n",
      "194 ---- 73\n",
      "195 ---- 74\n",
      "196 ---- 75\n",
      "197 ---- 76\n",
      "198 ---- 77\n",
      "199 ---- 78\n",
      "200 ---- 79\n",
      "201 ---- 80\n",
      "202 ---- 81\n",
      "203 ---- 82\n",
      "204 ---- 83\n",
      "205 ---- 84\n",
      "206 ---- 85\n",
      "207 ---- 86\n",
      "208 ---- 87\n",
      "209 ---- 88\n",
      "210 ---- 89\n",
      "211 ---- 90\n",
      "212 ---- 91\n",
      "213 ---- 92\n",
      "214 ---- 93\n",
      "215 ---- 94\n",
      "216 ---- 95\n",
      "217 ---- 96\n",
      "218 ---- 97\n",
      "219 ---- 98\n",
      "220 ---- 99\n",
      "221 ---- 100\n",
      "222 ---- 101\n",
      "223 ---- 102\n",
      "224 ---- 103\n",
      "225 ---- 104\n",
      "226 ---- 105\n",
      "227 ---- 106\n",
      "228 ---- 107\n",
      "229 ---- 108\n",
      "230 ---- 109\n",
      "231 ---- 110\n",
      "232 ---- 111\n",
      "233 ---- 112\n",
      "234 ---- 113\n",
      "235 ---- 114\n",
      "236 ---- 115\n",
      "237 ---- 116\n",
      "238 ---- 117\n",
      "239 ---- 118\n",
      "240 ---- 119\n",
      "241 ---- 120\n",
      "242 ---- 121\n",
      "243 ---- 122\n",
      "244 ---- 123\n",
      "245 ---- 124\n",
      "246 ---- 125\n",
      "247 ---- 126\n",
      "248 ---- 127\n",
      "249 ---- 128\n",
      "250 ---- 129\n",
      "251 ---- 130\n",
      "252 ---- 131\n",
      "253 ---- 132\n",
      "254 ---- 133\n",
      "255 ---- 134\n",
      "256 ---- 135\n",
      "257 ---- 136\n",
      "258 ---- 137\n",
      "259 ---- 138\n",
      "260 ---- 139\n",
      "261 ---- 140\n",
      "262 ---- 141\n",
      "263 ---- 142\n",
      "264 ---- 143\n",
      "265 ---- 144\n",
      "266 ---- 145\n",
      "267 ---- 146\n",
      "268 ---- 147\n",
      "269 ---- 148\n",
      "270 ---- 149\n",
      "271 ---- 150\n",
      "272 ---- 151\n",
      "273 ---- 152\n",
      "274 ---- 153\n",
      "275 ---- 154\n",
      "276 ---- 155\n",
      "277 ---- 156\n",
      "278 ---- 157\n",
      "279 ---- 158\n",
      "280 ---- 159\n",
      "281 ---- 160\n",
      "282 ---- 161\n",
      "283 ---- 162\n",
      "284 ---- 163\n",
      "285 ---- 164\n",
      "286 ---- 165\n",
      "287 ---- 166\n",
      "288 ---- 167\n",
      "289 ---- 168\n",
      "290 ---- 169\n",
      "291 ---- 170\n",
      "292 ---- 171\n",
      "293 ---- 172\n",
      "294 ---- 173\n",
      "295 ---- 174\n",
      "296 ---- 175\n",
      "297 ---- 176\n",
      "298 ---- 177\n",
      "299 ---- 178\n",
      "300 ---- 179\n",
      "301 ---- 180\n",
      "302 ---- 181\n",
      "303 ---- 182\n",
      "304 ---- 183\n",
      "305 ---- 184\n",
      "306 ---- 185\n",
      "307 ---- 186\n",
      "308 ---- 187\n",
      "309 ---- 188\n",
      "310 ---- 189\n",
      "311 ---- 190\n",
      "312 ---- 191\n",
      "313 ---- 192\n",
      "314 ---- 193\n",
      "315 ---- 194\n",
      "316 ---- 195\n",
      "317 ---- 196\n",
      "318 ---- 197\n",
      "319 ---- 198\n",
      "320 ---- 199\n",
      "321 ---- 200\n",
      "322 ---- 201\n",
      "323 ---- 202\n",
      "324 ---- 203\n",
      "325 ---- 204\n",
      "326 ---- 205\n",
      "327 ---- 206\n",
      "328 ---- 207\n",
      "329 ---- 208\n",
      "330 ---- 209\n",
      "331 ---- 210\n",
      "332 ---- 211\n",
      "333 ---- 212\n",
      "334 ---- 213\n",
      "335 ---- 214\n",
      "336 ---- 215\n",
      "337 ---- 216\n",
      "338 ---- 217\n",
      "339 ---- 218\n",
      "340 ---- 219\n",
      "341 ---- 220\n",
      "342 ---- 221\n",
      "343 ---- 222\n",
      "344 ---- 223\n",
      "345 ---- 224\n",
      "346 ---- 225\n",
      "347 ---- 226\n",
      "348 ---- 227\n",
      "349 ---- 228\n",
      "350 ---- 229\n",
      "351 ---- 230\n",
      "352 ---- 231\n",
      "353 ---- 232\n",
      "354 ---- 233\n",
      "355 ---- 234\n",
      "356 ---- 235\n",
      "357 ---- 236\n",
      "358 ---- 237\n",
      "359 ---- 238\n",
      "360 ---- 239\n",
      "361 ---- 240\n",
      "362 ---- 241\n",
      "363 ---- 242\n",
      "364 ---- 243\n",
      "365 ---- 244\n",
      "366 ---- 245\n",
      "367 ---- 246\n",
      "368 ---- 247\n",
      "369 ---- 248\n",
      "370 ---- 249\n",
      "371 ---- 250\n",
      "372 ---- 251\n",
      "373 ---- 252\n",
      "374 ---- 253\n",
      "375 ---- 254\n",
      "376 ---- 255\n",
      "377 ---- 256\n",
      "378 ---- 257\n",
      "379 ---- 258\n",
      "380 ---- 259\n",
      "381 ---- 260\n",
      "382 ---- 261\n",
      "383 ---- 262\n",
      "384 ---- 263\n",
      "385 ---- 264\n",
      "386 ---- 265\n",
      "387 ---- 266\n",
      "388 ---- 267\n",
      "389 ---- 268\n",
      "390 ---- 269\n",
      "391 ---- 270\n",
      "392 ---- 271\n",
      "393 ---- 272\n",
      "394 ---- 273\n",
      "395 ---- 274\n",
      "396 ---- 275\n",
      "397 ---- 276\n",
      "398 ---- 277\n",
      "399 ---- 278\n",
      "400 ---- 279\n",
      "401 ---- 280\n",
      "402 ---- 281\n",
      "403 ---- 282\n",
      "404 ---- 283\n",
      "405 ---- 284\n",
      "406 ---- 285\n",
      "407 ---- 286\n",
      "408 ---- 287\n",
      "409 ---- 288\n",
      "410 ---- 289\n",
      "411 ---- 290\n",
      "412 ---- 291\n",
      "413 ---- 292\n",
      "414 ---- 293\n",
      "415 ---- 294\n",
      "416 ---- 295\n",
      "417 ---- 296\n",
      "418 ---- 297\n",
      "419 ---- 298\n",
      "420 ---- 299\n",
      "421 ---- 300\n",
      "422 ---- 301\n",
      "423 ---- 302\n",
      "424 ---- 303\n",
      "425 ---- 304\n",
      "426 ---- 305\n",
      "427 ---- 306\n",
      "428 ---- 307\n",
      "429 ---- 308\n",
      "430 ---- 309\n",
      "431 ---- 310\n",
      "432 ---- 311\n",
      "433 ---- 312\n",
      "434 ---- 313\n",
      "435 ---- 314\n",
      "436 ---- 315\n",
      "437 ---- 316\n",
      "438 ---- 317\n",
      "439 ---- 318\n",
      "440 ---- 319\n",
      "441 ---- 320\n",
      "442 ---- 321\n",
      "443 ---- 322\n",
      "444 ---- 323\n",
      "445 ---- 324\n",
      "446 ---- 325\n",
      "447 ---- 326\n",
      "448 ---- 327\n",
      "449 ---- 328\n",
      "450 ---- 329\n",
      "451 ---- 330\n",
      "452 ---- 331\n",
      "453 ---- 332\n",
      "454 ---- 333\n",
      "455 ---- 334\n",
      "456 ---- 335\n",
      "457 ---- 336\n",
      "458 ---- 337\n",
      "459 ---- 338\n",
      "460 ---- 339\n",
      "461 ---- 340\n",
      "462 ---- 341\n",
      "463 ---- 342\n",
      "464 ---- 343\n",
      "465 ---- 344\n",
      "466 ---- 345\n",
      "467 ---- 346\n",
      "468 ---- 347\n",
      "469 ---- 348\n",
      "470 ---- 349\n",
      "471 ---- 350\n",
      "472 ---- 351\n",
      "473 ---- 352\n",
      "474 ---- 353\n",
      "475 ---- 354\n",
      "476 ---- 355\n",
      "477 ---- 356\n",
      "478 ---- 357\n",
      "479 ---- 358\n",
      "480 ---- 359\n",
      "481 ---- 360\n",
      "482 ---- 361\n",
      "483 ---- 362\n",
      "484 ---- 363\n",
      "485 ---- 364\n",
      "486 ---- 365\n",
      "487 ---- 366\n",
      "488 ---- 367\n",
      "489 ---- 368\n",
      "490 ---- 369\n",
      "491 ---- 370\n",
      "492 ---- 371\n",
      "493 ---- 372\n",
      "494 ---- 373\n",
      "495 ---- 374\n",
      "496 ---- 375\n",
      "497 ---- 376\n",
      "498 ---- 377\n",
      "499 ---- 378\n",
      "500 ---- 379\n",
      "501 ---- 380\n",
      "502 ---- 381\n",
      "503 ---- 382\n",
      "504 ---- 383\n",
      "505 ---- 384\n",
      "506 ---- 385\n",
      "507 ---- 386\n",
      "508 ---- 387\n",
      "509 ---- 388\n",
      "510 ---- 389\n",
      "511 ---- 390\n",
      "512 ---- 391\n",
      "513 ---- 392\n",
      "514 ---- 393\n",
      "515 ---- 394\n",
      "516 ---- 395\n",
      "517 ---- 396\n",
      "518 ---- 397\n",
      "519 ---- 398\n",
      "520 ---- 399\n",
      "521 ---- 400\n",
      "522 ---- 401\n",
      "523 ---- 402\n",
      "524 ---- 403\n",
      "525 ---- 404\n",
      "526 ---- 405\n",
      "527 ---- 406\n",
      "528 ---- 407\n",
      "529 ---- 408\n",
      "530 ---- 409\n",
      "531 ---- 410\n",
      "532 ---- 411\n",
      "533 ---- 412\n",
      "534 ---- 413\n",
      "535 ---- 414\n",
      "536 ---- 415\n",
      "537 ---- 416\n",
      "538 ---- 417\n",
      "539 ---- 418\n",
      "540 ---- 419\n",
      "541 ---- 420\n",
      "542 ---- 421\n",
      "543 ---- 422\n",
      "544 ---- 423\n",
      "545 ---- 424\n",
      "546 ---- 425\n",
      "547 ---- 426\n",
      "548 ---- 427\n",
      "549 ---- 428\n",
      "550 ---- 429\n",
      "551 ---- 430\n",
      "552 ---- 431\n",
      "553 ---- 432\n",
      "554 ---- 433\n",
      "555 ---- 434\n",
      "556 ---- 435\n",
      "557 ---- 436\n",
      "558 ---- 437\n",
      "559 ---- 438\n",
      "560 ---- 439\n",
      "561 ---- 440\n",
      "562 ---- 441\n",
      "563 ---- 442\n",
      "564 ---- 443\n",
      "565 ---- 444\n",
      "566 ---- 445\n",
      "567 ---- 446\n",
      "568 ---- 447\n",
      "569 ---- 448\n",
      "570 ---- 449\n",
      "571 ---- 450\n",
      "572 ---- 451\n",
      "573 ---- 452\n",
      "574 ---- 453\n",
      "575 ---- 454\n",
      "576 ---- 455\n",
      "577 ---- 456\n",
      "578 ---- 457\n",
      "579 ---- 458\n",
      "580 ---- 459\n",
      "581 ---- 460\n",
      "582 ---- 461\n",
      "583 ---- 462\n",
      "584 ---- 463\n",
      "585 ---- 464\n",
      "586 ---- 465\n",
      "587 ---- 466\n",
      "588 ---- 467\n",
      "589 ---- 468\n",
      "590 ---- 469\n",
      "591 ---- 470\n",
      "592 ---- 471\n",
      "593 ---- 472\n",
      "594 ---- 473\n",
      "595 ---- 474\n",
      "596 ---- 475\n",
      "597 ---- 476\n",
      "598 ---- 477\n",
      "599 ---- 478\n",
      "600 ---- 479\n",
      "601 ---- 480\n",
      "602 ---- 481\n",
      "603 ---- 482\n",
      "604 ---- 483\n",
      "605 ---- 484\n",
      "606 ---- 485\n",
      "607 ---- 486\n",
      "608 ---- 487\n",
      "609 ---- 488\n",
      "610 ---- 489\n",
      "611 ---- 490\n",
      "612 ---- 491\n",
      "613 ---- 492\n",
      "614 ---- 493\n",
      "615 ---- 494\n",
      "616 ---- 495\n",
      "617 ---- 496\n",
      "618 ---- 497\n",
      "619 ---- 498\n",
      "620 ---- 499\n",
      "621 ---- 500\n",
      "622 ---- 501\n",
      "623 ---- 502\n",
      "624 ---- 503\n",
      "625 ---- 504\n",
      "626 ---- 505\n",
      "627 ---- 506\n",
      "628 ---- 507\n",
      "629 ---- 508\n",
      "630 ---- 509\n",
      "631 ---- 510\n",
      "632 ---- 511\n",
      "633 ---- 512\n",
      "634 ---- 513\n",
      "635 ---- 514\n",
      "636 ---- 515\n",
      "637 ---- 516\n",
      "638 ---- 517\n",
      "639 ---- 518\n",
      "640 ---- 519\n",
      "641 ---- 520\n",
      "642 ---- 521\n",
      "643 ---- 522\n",
      "644 ---- 523\n",
      "645 ---- 524\n",
      "646 ---- 525\n",
      "647 ---- 526\n",
      "648 ---- 527\n",
      "649 ---- 528\n",
      "650 ---- 529\n",
      "651 ---- 530\n",
      "652 ---- 531\n",
      "653 ---- 532\n",
      "654 ---- 533\n",
      "655 ---- 534\n",
      "656 ---- 535\n",
      "657 ---- 536\n",
      "658 ---- 537\n",
      "659 ---- 538\n",
      "660 ---- 539\n",
      "661 ---- 540\n",
      "662 ---- 541\n",
      "663 ---- 542\n",
      "664 ---- 543\n",
      "665 ---- 544\n",
      "666 ---- 545\n",
      "667 ---- 546\n",
      "668 ---- 547\n",
      "669 ---- 548\n",
      "670 ---- 549\n",
      "671 ---- 550\n",
      "672 ---- 551\n",
      "673 ---- 552\n",
      "674 ---- 553\n",
      "675 ---- 554\n",
      "676 ---- 555\n",
      "677 ---- 556\n",
      "678 ---- 557\n",
      "679 ---- 558\n",
      "680 ---- 559\n",
      "681 ---- 560\n",
      "682 ---- 561\n",
      "683 ---- 562\n",
      "684 ---- 563\n",
      "685 ---- 564\n",
      "686 ---- 565\n",
      "687 ---- 566\n",
      "688 ---- 567\n",
      "689 ---- 568\n",
      "690 ---- 569\n",
      "691 ---- 570\n",
      "692 ---- 571\n",
      "693 ---- 572\n",
      "694 ---- 573\n",
      "695 ---- 574\n",
      "696 ---- 575\n",
      "697 ---- 576\n",
      "698 ---- 577\n",
      "699 ---- 578\n",
      "700 ---- 579\n",
      "701 ---- 580\n",
      "702 ---- 581\n",
      "703 ---- 582\n",
      "704 ---- 583\n",
      "705 ---- 584\n",
      "706 ---- 585\n",
      "707 ---- 586\n",
      "708 ---- 587\n",
      "709 ---- 588\n",
      "710 ---- 589\n",
      "711 ---- 590\n",
      "712 ---- 591\n",
      "713 ---- 592\n",
      "714 ---- 593\n",
      "715 ---- 594\n",
      "716 ---- 595\n",
      "717 ---- 596\n",
      "718 ---- 597\n",
      "719 ---- 598\n",
      "720 ---- 599\n",
      "721 ---- 600\n",
      "722 ---- 601\n",
      "723 ---- 602\n",
      "724 ---- 603\n",
      "725 ---- 604\n",
      "726 ---- 605\n",
      "727 ---- 606\n",
      "728 ---- 607\n",
      "729 ---- 608\n",
      "730 ---- 609\n",
      "731 ---- 610\n",
      "732 ---- 611\n",
      "733 ---- 612\n",
      "734 ---- 613\n",
      "735 ---- 614\n",
      "736 ---- 615\n",
      "737 ---- 616\n",
      "738 ---- 617\n",
      "739 ---- 618\n",
      "740 ---- 619\n",
      "741 ---- 620\n",
      "742 ---- 621\n",
      "743 ---- 622\n",
      "744 ---- 623\n",
      "745 ---- 624\n",
      "746 ---- 625\n",
      "747 ---- 626\n",
      "748 ---- 627\n",
      "749 ---- 628\n",
      "750 ---- 629\n",
      "751 ---- 630\n",
      "752 ---- 631\n",
      "753 ---- 632\n",
      "754 ---- 633\n",
      "755 ---- 634\n",
      "756 ---- 635\n",
      "757 ---- 636\n",
      "758 ---- 637\n",
      "759 ---- 638\n",
      "760 ---- 639\n",
      "761 ---- 640\n",
      "762 ---- 641\n",
      "763 ---- 642\n",
      "764 ---- 643\n",
      "765 ---- 644\n",
      "766 ---- 645\n",
      "767 ---- 646\n",
      "768 ---- 647\n",
      "769 ---- 648\n",
      "770 ---- 649\n",
      "771 ---- 650\n",
      "772 ---- 651\n",
      "773 ---- 652\n",
      "774 ---- 653\n",
      "775 ---- 654\n",
      "776 ---- 655\n",
      "777 ---- 656\n",
      "778 ---- 657\n",
      "779 ---- 658\n",
      "780 ---- 659\n",
      "781 ---- 660\n",
      "782 ---- 661\n",
      "783 ---- 662\n",
      "784 ---- 663\n",
      "785 ---- 664\n",
      "786 ---- 665\n",
      "787 ---- 666\n",
      "788 ---- 667\n",
      "789 ---- 668\n",
      "790 ---- 669\n",
      "791 ---- 670\n",
      "792 ---- 671\n",
      "793 ---- 672\n",
      "794 ---- 673\n",
      "795 ---- 674\n",
      "796 ---- 675\n",
      "797 ---- 676\n",
      "798 ---- 677\n",
      "799 ---- 678\n",
      "800 ---- 679\n",
      "801 ---- 680\n",
      "802 ---- 681\n",
      "803 ---- 682\n",
      "804 ---- 683\n",
      "805 ---- 684\n",
      "806 ---- 685\n",
      "807 ---- 686\n",
      "808 ---- 687\n",
      "809 ---- 688\n",
      "810 ---- 689\n",
      "811 ---- 690\n",
      "812 ---- 691\n",
      "813 ---- 692\n",
      "814 ---- 693\n",
      "815 ---- 694\n",
      "816 ---- 695\n",
      "817 ---- 696\n",
      "818 ---- 697\n",
      "819 ---- 698\n",
      "820 ---- 699\n",
      "821 ---- 700\n",
      "822 ---- 701\n",
      "823 ---- 702\n",
      "824 ---- 703\n",
      "825 ---- 704\n",
      "826 ---- 705\n",
      "827 ---- 706\n",
      "828 ---- 707\n",
      "829 ---- 708\n",
      "830 ---- 709\n",
      "831 ---- 710\n",
      "832 ---- 711\n",
      "833 ---- 712\n",
      "834 ---- 713\n",
      "835 ---- 714\n",
      "836 ---- 715\n",
      "837 ---- 716\n",
      "838 ---- 717\n",
      "839 ---- 718\n",
      "840 ---- 719\n",
      "841 ---- 720\n",
      "842 ---- 721\n",
      "843 ---- 722\n",
      "844 ---- 723\n",
      "845 ---- 724\n",
      "846 ---- 725\n",
      "847 ---- 726\n",
      "848 ---- 727\n",
      "849 ---- 728\n",
      "850 ---- 729\n",
      "851 ---- 730\n",
      "852 ---- 731\n",
      "853 ---- 732\n",
      "854 ---- 733\n",
      "855 ---- 734\n",
      "856 ---- 735\n",
      "857 ---- 736\n",
      "858 ---- 737\n",
      "859 ---- 738\n",
      "860 ---- 739\n",
      "861 ---- 740\n",
      "862 ---- 741\n",
      "863 ---- 742\n",
      "864 ---- 743\n",
      "865 ---- 744\n",
      "866 ---- 745\n",
      "867 ---- 746\n",
      "868 ---- 747\n",
      "869 ---- 748\n",
      "870 ---- 749\n",
      "871 ---- 750\n",
      "872 ---- 751\n",
      "873 ---- 752\n",
      "874 ---- 753\n",
      "875 ---- 754\n",
      "876 ---- 755\n",
      "877 ---- 756\n",
      "878 ---- 757\n",
      "879 ---- 758\n",
      "880 ---- 759\n",
      "881 ---- 760\n",
      "882 ---- 761\n",
      "883 ---- 762\n",
      "884 ---- 763\n",
      "885 ---- 764\n",
      "886 ---- 765\n",
      "887 ---- 766\n",
      "888 ---- 767\n",
      "889 ---- 768\n",
      "890 ---- 769\n",
      "891 ---- 770\n",
      "892 ---- 771\n",
      "893 ---- 772\n",
      "894 ---- 773\n",
      "895 ---- 774\n",
      "896 ---- 775\n",
      "897 ---- 776\n",
      "898 ---- 777\n",
      "899 ---- 778\n",
      "900 ---- 779\n",
      "901 ---- 780\n",
      "902 ---- 781\n",
      "903 ---- 782\n",
      "904 ---- 783\n",
      "905 ---- 784\n",
      "906 ---- 785\n",
      "907 ---- 786\n",
      "908 ---- 787\n",
      "909 ---- 788\n",
      "910 ---- 789\n",
      "911 ---- 790\n",
      "912 ---- 791\n",
      "913 ---- 792\n",
      "914 ---- 793\n",
      "915 ---- 794\n",
      "916 ---- 795\n",
      "917 ---- 796\n",
      "918 ---- 797\n",
      "919 ---- 798\n",
      "920 ---- 799\n",
      "921 ---- 800\n",
      "922 ---- 801\n",
      "923 ---- 802\n",
      "924 ---- 803\n",
      "925 ---- 804\n",
      "926 ---- 805\n",
      "927 ---- 806\n",
      "928 ---- 807\n",
      "929 ---- 808\n",
      "930 ---- 809\n",
      "931 ---- 810\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(X_train_final.columns)):\n",
    "    print('{} ---- {}'.format(i, X_train_final.columns[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Unigrams, POS Tag Count, Sentiment Polarity, Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_1 = X_train_final.iloc[:,np.r_[10:12,13:21,121:932]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(842, 821)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_1 = X_test_final.iloc[:,np.r_[10:12,13:21,121:932]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362, 821)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.184\n",
      "Best parameters set:\n",
      "\tclf__C: 0.009\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9552    0.5714    0.7151       336\n",
      "           1     0.1056    0.6538    0.1818        26\n",
      "\n",
      "    accuracy                         0.5773       362\n",
      "   macro avg     0.5304    0.6126    0.4485       362\n",
      "weighted avg     0.8942    0.5773    0.6768       362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_1_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_1_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_1,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_1), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 192, False Positive: 144, False Negative: 9, True Positive: 17\n",
      "--------------------------------------------------------------------------------\n",
      "[[192 144]\n",
      " [  9  17]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.57      0.72       336\n",
      "           1       0.11      0.65      0.18        26\n",
      "\n",
      "    accuracy                           0.58       362\n",
      "   macro avg       0.53      0.61      0.45       362\n",
      "weighted avg       0.89      0.58      0.68       362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_1 = LogisticRegression(random_state=18, \n",
    "                                solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], \n",
    "                                class_weight='balanced').fit(X_train_model_1, y_train)\n",
    "y_lr = lr_model_1.predict(X_test_model_1)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_model_2 = X_train_final.iloc[:,np.r_[3:1113]]\n",
    "X_train_model_2 = X_train_final.iloc[:, np.r_[3:932]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(842, 929)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_2 = X_test_final.iloc[:,np.r_[3:932]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362, 929)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.192\n",
      "Best parameters set:\n",
      "\tclf__C: 0.009\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9551    0.6964    0.8055       336\n",
      "           1     0.1282    0.5769    0.2098        26\n",
      "\n",
      "    accuracy                         0.6878       362\n",
      "   macro avg     0.5417    0.6367    0.5076       362\n",
      "weighted avg     0.8957    0.6878    0.7627       362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_2_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_2_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_2,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_2), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 234, False Positive: 102, False Negative: 11, True Positive: 15\n",
      "--------------------------------------------------------------------------------\n",
      "[[234 102]\n",
      " [ 11  15]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.70      0.81       336\n",
      "           1       0.13      0.58      0.21        26\n",
      "\n",
      "    accuracy                           0.69       362\n",
      "   macro avg       0.54      0.64      0.51       362\n",
      "weighted avg       0.90      0.69      0.76       362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_2 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_2, y_train)\n",
    "y_lr = lr_model_2.predict(X_test_model_2)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Without Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_3 = X_train_final.iloc[:,np.r_[3:121]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(842, 118)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_3 = X_test_final.iloc[:,np.r_[3:121]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362, 118)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n",
      "Best score: 0.219\n",
      "Best parameters set:\n",
      "\tclf__C: 5\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9602    0.7173    0.8211       336\n",
      "           1     0.1441    0.6154    0.2336        26\n",
      "\n",
      "    accuracy                         0.7099       362\n",
      "   macro avg     0.5522    0.6663    0.5274       362\n",
      "weighted avg     0.9016    0.7099    0.7789       362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_3_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.0001, 0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\", \"elasticnet\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_3_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_3,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_3), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 241, False Positive: 95, False Negative: 10, True Positive: 16\n",
      "--------------------------------------------------------------------------------\n",
      "[[241  95]\n",
      " [ 10  16]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.72      0.82       336\n",
      "           1       0.14      0.62      0.23        26\n",
      "\n",
      "    accuracy                           0.71       362\n",
      "   macro avg       0.55      0.67      0.53       362\n",
      "weighted avg       0.90      0.71      0.78       362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_3 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_3, y_train)\n",
    "y_lr = lr_model_3.predict(X_test_model_3)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Without Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_4 = X_train_final.iloc[:,np.r_[3:21,121:932]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(842, 829)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_4 = X_test_final.iloc[:,np.r_[3:21,121:932]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362, 829)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.177\n",
      "Best parameters set:\n",
      "\tclf__C: 0.009\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9507    0.5744    0.7161       336\n",
      "           1     0.1006    0.6154    0.1730        26\n",
      "\n",
      "    accuracy                         0.5773       362\n",
      "   macro avg     0.5257    0.5949    0.4446       362\n",
      "weighted avg     0.8897    0.5773    0.6771       362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_4_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_4_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_4,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_4), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 193, False Positive: 143, False Negative: 10, True Positive: 16\n",
      "--------------------------------------------------------------------------------\n",
      "[[193 143]\n",
      " [ 10  16]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.57      0.72       336\n",
      "           1       0.10      0.62      0.17        26\n",
      "\n",
      "    accuracy                           0.58       362\n",
      "   macro avg       0.53      0.59      0.44       362\n",
      "weighted avg       0.89      0.58      0.68       362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_4 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_4, y_train)\n",
    "y_lr = lr_model_4.predict(X_test_model_4)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Without POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_5 = X_train_final.iloc[:,np.r_[3:13,21:932]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(842, 921)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_5 = X_test_final.iloc[:,np.r_[3:13,21:932]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362, 921)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.199\n",
      "Best parameters set:\n",
      "\tclf__C: 25\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95       336\n",
      "           1       0.30      0.23      0.26        26\n",
      "\n",
      "    accuracy                           0.91       362\n",
      "   macro avg       0.62      0.59      0.61       362\n",
      "weighted avg       0.90      0.91      0.90       362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_5_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_5_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_5,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_5), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 322, False Positive: 14, False Negative: 20, True Positive: 6\n",
      "--------------------------------------------------------------------------------\n",
      "[[322  14]\n",
      " [ 20   6]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95       336\n",
      "           1       0.30      0.23      0.26        26\n",
      "\n",
      "    accuracy                           0.91       362\n",
      "   macro avg       0.62      0.59      0.61       362\n",
      "weighted avg       0.90      0.91      0.90       362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_5 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_5, y_train)\n",
    "y_lr = lr_model_5.predict(X_test_model_5)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6: Without STEM Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_6 = X_train_final.iloc[:,np.r_[10:932]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(842, 922)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_6 = X_test_final.iloc[:,np.r_[10:932]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362, 922)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.197\n",
      "Best parameters set:\n",
      "\tclf__C: 0.009\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.69      0.80       336\n",
      "           1       0.13      0.58      0.21        26\n",
      "\n",
      "    accuracy                           0.68       362\n",
      "   macro avg       0.54      0.63      0.50       362\n",
      "weighted avg       0.90      0.68      0.76       362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_6_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_6_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_6,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_6), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 232, False Positive: 104, False Negative: 11, True Positive: 15\n",
      "--------------------------------------------------------------------------------\n",
      "[[232 104]\n",
      " [ 11  15]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.69      0.80       336\n",
      "           1       0.13      0.58      0.21        26\n",
      "\n",
      "    accuracy                           0.68       362\n",
      "   macro avg       0.54      0.63      0.50       362\n",
      "weighted avg       0.90      0.68      0.76       362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_6 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_6, y_train)\n",
    "y_lr = lr_model_6.predict(X_test_model_6)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7: Without Sentiment Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_7 = X_train_final.iloc[:,np.r_[3:10,12:932]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(842, 927)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_7 = X_test_final.iloc[:,np.r_[3:10,12:932]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362, 927)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.188\n",
      "Best parameters set:\n",
      "\tclf__C: 0.01\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.71      0.81       336\n",
      "           1       0.12      0.54      0.20        26\n",
      "\n",
      "    accuracy                           0.70       362\n",
      "   macro avg       0.54      0.62      0.51       362\n",
      "weighted avg       0.89      0.70      0.77       362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_7_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_7_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_7,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_7), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 238, False Positive: 98, False Negative: 12, True Positive: 14\n",
      "--------------------------------------------------------------------------------\n",
      "[[238  98]\n",
      " [ 12  14]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.71      0.81       336\n",
      "           1       0.12      0.54      0.20        26\n",
      "\n",
      "    accuracy                           0.70       362\n",
      "   macro avg       0.54      0.62      0.51       362\n",
      "weighted avg       0.89      0.70      0.77       362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_7 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_7, y_train)\n",
    "y_lr = lr_model_7.predict(X_test_model_7)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 8: Without NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_8 = X_train_final.iloc[:,np.r_[3:12,13:932]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(842, 928)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_8 = X_test_final.iloc[:,np.r_[3:12,13:932]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362, 928)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.192\n",
      "Best parameters set:\n",
      "\tclf__C: 0.009\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.70      0.81       336\n",
      "           1       0.13      0.58      0.21        26\n",
      "\n",
      "    accuracy                           0.69       362\n",
      "   macro avg       0.54      0.64      0.51       362\n",
      "weighted avg       0.90      0.69      0.76       362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_8_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_8_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_8,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_8), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 234, False Positive: 102, False Negative: 11, True Positive: 15\n",
      "--------------------------------------------------------------------------------\n",
      "[[234 102]\n",
      " [ 11  15]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.70      0.81       336\n",
      "           1       0.13      0.58      0.21        26\n",
      "\n",
      "    accuracy                           0.69       362\n",
      "   macro avg       0.54      0.64      0.51       362\n",
      "weighted avg       0.90      0.69      0.76       362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_8 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_8, y_train)\n",
    "y_lr = lr_model_8.predict(X_test_model_8)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Summary\n",
    "| Experiment | Model Number | Features Used                                                | Precision | Recall | Macro F1 |\n",
    "| :--------: | :----------: | :----------------------------------------------------------: | :-------: | :----: | :------: |\n",
    "| Baseline   | 1            | Unigrams, POS Tag Count, Sentiment Polarity and Subjectivity | 0.64      | 0.74   | 0.66     |\n",
    "| Baseline   | 2            | All features (baseline)                                      | 0.64      | 0.76   | 0.66     |\n",
    "| Baseline   | 3            | Without Unigrams                                             | 0.61      | 0.72   | 0.59     |\n",
    "| Baseline   | 4            | Without Embeddings                                           | 0.64      | 0.74   | 0.66     |\n",
    "| Baseline   | 5            | Without POS tag                                              | 0.64      | 0.75   | 0.66     |\n",
    "| Baseline   | 6            | Without STEM similarity (paper baseline)                     | 0.65      | 0.77   | 0.67     |\n",
    "| Baseline   | 7            | Without sentiment features                                   | 0.64      | 0.75   | 0.66     |\n",
    "| Baseline   | 8            | Without NER                                                  | 0.64      | 0.76   | 0.66     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
