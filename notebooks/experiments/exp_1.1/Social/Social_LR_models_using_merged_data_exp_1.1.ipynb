{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Logistic Regression Models Using Merged Data Experiment 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e78aa6b2407d4b80b1d4a613e70ba82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 01:06:38 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-10-09 01:06:38 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-10-09 01:06:39 INFO: File exists: /Users/gbaldonado/stanza_resources/en/default.zip\n",
      "2024-10-09 01:06:42 INFO: Finished downloading models and saved to /Users/gbaldonado/stanza_resources\n",
      "2024-10-09 01:06:42 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f16ea88872c412aaf798a9ec5bd4088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 01:06:42 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-10-09 01:06:43 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-10-09 01:06:43 INFO: Using device: cpu\n",
      "2024-10-09 01:06:43 INFO: Loading: tokenize\n",
      "2024-10-09 01:06:43 INFO: Loading: mwt\n",
      "2024-10-09 01:06:43 INFO: Loading: pos\n",
      "2024-10-09 01:06:44 INFO: Loading: lemma\n",
      "2024-10-09 01:06:44 INFO: Loading: constituency\n",
      "2024-10-09 01:06:44 INFO: Loading: depparse\n",
      "2024-10-09 01:06:44 INFO: Loading: sentiment\n",
      "2024-10-09 01:06:44 INFO: Loading: ner\n",
      "2024-10-09 01:06:45 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import pickle\n",
    "import warnings\n",
    "import stanza\n",
    "\n",
    "from random import shuffle\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, f1_score, r2_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Set random seed\n",
    "random.seed(18)\n",
    "seed = 18\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Initialize lemmatizer, stop words, and stanza\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline('en') # initialize English neural pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data and quick exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_social_df = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/merged_Social_sentence_level_batch_1_jaccard.csv\", encoding='utf-8')\n",
    "\n",
    "# Shuffle the merged dataset\n",
    "merged_social_df = shuffle(merged_social_df, random_state=seed)\n",
    "\n",
    "# Train-test split \n",
    "training_df, test_df = train_test_split(merged_social_df, test_size=0.1, random_state=18, stratify=merged_social_df['label'])\n",
    "\n",
    "training_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it does not hurt to get extra help and honestly it is an easy a that is one unit.</td>\n",
       "      <td>0</td>\n",
       "      <td>['So, I scammed my friend to come with me to the first class because I did not want to go alone.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i want to go to med school and become a doctor.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I want to help people and connect to patients. I want to connect to the individual on a personal level and get to have a better relationship with them. I would love to be able to help people physically and mentally, while being a friend. I would love to work with elders and get to know them, their stories, and experiences.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a large amount of co2 is made by burning fossil fuels.</td>\n",
       "      <td>0</td>\n",
       "      <td>['As most know right now, the environment is pretty fucked. From my standpoint, humanity needs to find a way to be less wasteful with their resources. Since using less is most likely not an option, I want to learn more about how electricity is produced and how it can be made more efficiently. A large amount of CO2 is made by burning fossil fuels. If we can reduce how much we use, then how much waste we produce is also cut down. That way we can reduce the amount of pollutants that we make and hopefully our race will live past the year 2100.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i believe i also will make an impact on climate change along with helping animals.</td>\n",
       "      <td>1</td>\n",
       "      <td>['So as all over the place this whole write up was I believe I am here on this planet to live my life long dream of being part of the fashion industry, while making a positive impact environmentally', 'I believe I also will make an impact on Climate Change along with helping animals.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>maybe i don't know.</td>\n",
       "      <td>0</td>\n",
       "      <td>['Our professor makes this class really enjoyable and hes really helpful.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             sentence  \\\n",
       "0   it does not hurt to get extra help and honestly it is an easy a that is one unit.   \n",
       "1                                     i want to go to med school and become a doctor.   \n",
       "2                              a large amount of co2 is made by burning fossil fuels.   \n",
       "3  i believe i also will make an impact on climate change along with helping animals.   \n",
       "4                                                                 maybe i don't know.   \n",
       "\n",
       "   label  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      0   \n",
       "3      1   \n",
       "4      0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                phrase  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ['So, I scammed my friend to come with me to the first class because I did not want to go alone.']  \n",
       "1                                                                                                                                                                                                                             ['I want to help people and connect to patients. I want to connect to the individual on a personal level and get to have a better relationship with them. I would love to be able to help people physically and mentally, while being a friend. I would love to work with elders and get to know them, their stories, and experiences.']  \n",
       "2  ['As most know right now, the environment is pretty fucked. From my standpoint, humanity needs to find a way to be less wasteful with their resources. Since using less is most likely not an option, I want to learn more about how electricity is produced and how it can be made more efficiently. A large amount of CO2 is made by burning fossil fuels. If we can reduce how much we use, then how much waste we produce is also cut down. That way we can reduce the amount of pollutants that we make and hopefully our race will live past the year 2100.']  \n",
       "3                                                                                                                                                                                                                                                                       ['So as all over the place this whole write up was I believe I am here on this planet to live my life long dream of being part of the fashion industry, while making a positive impact environmentally', 'I believe I also will make an impact on Climate Change along with helping animals.']  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ['Our professor makes this class really enjoyable and hes really helpful.']  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i have heard that the course itself is intense.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I think sci would definitely enhance my learning abilities but also my academic selfesteem since I wont be the only student struggling with the subject.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>but overall, mainly i want to make a difference in the medical community and use my skills to help others.</td>\n",
       "      <td>1</td>\n",
       "      <td>['mainly I want to make a difference in the medical community and use my skills to help others.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>although there may be some courses in my curriculum that do not relate directly to my focus of study, they are prerequisites to the classes that will have more of a direct impact to my field of study.</td>\n",
       "      <td>0</td>\n",
       "      <td>['Ultimately, I hope to get out as much as I can from my education in order to give back in the near future.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it applies to just about everything in my daily life, especially subjects like math and science.</td>\n",
       "      <td>0</td>\n",
       "      <td>['Overall I see my education as a gateway to help others in need']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>being here at san francisco state gives me an opportunity to get an education.</td>\n",
       "      <td>0</td>\n",
       "      <td>['Secondly, I am here to make friends and experience new things.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                   sentence  \\\n",
       "0                                                                                                                                                           i have heard that the course itself is intense.   \n",
       "1                                                                                                but overall, mainly i want to make a difference in the medical community and use my skills to help others.   \n",
       "2  although there may be some courses in my curriculum that do not relate directly to my focus of study, they are prerequisites to the classes that will have more of a direct impact to my field of study.   \n",
       "3                                                                                                          it applies to just about everything in my daily life, especially subjects like math and science.   \n",
       "4                                                                                                                            being here at san francisco state gives me an opportunity to get an education.   \n",
       "\n",
       "   label  \\\n",
       "0      0   \n",
       "1      1   \n",
       "2      0   \n",
       "3      0   \n",
       "4      0   \n",
       "\n",
       "                                                                                                                                                         phrase  \n",
       "0  ['I think sci would definitely enhance my learning abilities but also my academic selfesteem since I wont be the only student struggling with the subject.']  \n",
       "1                                                             ['mainly I want to make a difference in the medical community and use my skills to help others.']  \n",
       "2                                                ['Ultimately, I hope to get out as much as I can from my education in order to give back in the near future.']  \n",
       "3                                                                                            ['Overall I see my education as a gateway to help others in need']  \n",
       "4                                                                                            ['Secondly, I am here to make friends and experience new things.']  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (1462, 3) \n",
      "Test dataset shape: (163, 3)\n",
      "Positive labels present in the dataset : 166  out of 1462 or 11.354309165526676%\n",
      "Positive labels present in the test dataset : 19  out of 163 or 11.65644171779141%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset shape: {training_df.shape} \\nTest dataset shape: {test_df.shape}\")\n",
    "pos_labels = len([n for n in training_df['label'] if n==1])\n",
    "print(\"Positive labels present in the dataset : {}  out of {} or {}%\".format(pos_labels, len(training_df['label']), (pos_labels/len(training_df['label']))*100))\n",
    "pos_labels = len([n for n in test_df['label'] if n==1])\n",
    "print(\"Positive labels present in the test dataset : {}  out of {} or {}%\".format(pos_labels, len(test_df['label']), (pos_labels/len(test_df['label']))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the data for the plots\n",
    "training_counts = training_df['label'].value_counts()\n",
    "test_counts = test_df['label'].value_counts()\n",
    "\n",
    "# Set up the subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot for the training set\n",
    "sns.barplot(x=training_counts.index, y=training_counts.values, ax=axes[0])\n",
    "axes[0].set_title('Distribution of labels in training set')\n",
    "axes[0].set_ylabel('Sentences')\n",
    "axes[0].set_xlabel('Label')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot for the test set\n",
    "sns.barplot(x=test_counts.index, y=test_counts.values, ax=axes[1])\n",
    "axes[1].set_title('Distribution of labels in test set')\n",
    "axes[1].set_ylabel('Sentences')\n",
    "axes[1].set_xlabel('Label')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack Obama']\n"
     ]
    }
   ],
   "source": [
    "def get_ner(text):\n",
    "    ner_list = []\n",
    "    # Annotate the text using stanza\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for entity in sentence.ents:\n",
    "            if entity.type == 'PERSON':\n",
    "                ner_list.append(entity.text)\n",
    "\n",
    "    return ner_list\n",
    "\n",
    "# Example usage\n",
    "text = \"Barack Obama was the 44th doctor of the United States.\"\n",
    "print(get_ner(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a named entity is present in the sentence\n",
    "def named_entity_present(sentence):\n",
    "    ner_list = get_ner(sentence)\n",
    "    if len(ner_list) > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Similarity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the similar words and similarity score\n",
    "# The function takes tokens of sentence as input and if its not a stop word, get its similarity with synsets of STEM.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words |= set([\"help\",\"try\", \"work\", \"process\", \"support\", \"job\"] )\n",
    "def word_similarity(tokens, syns, field):    \n",
    "    if field in ['engineering', 'technology']:\n",
    "        score_threshold = 0.5\n",
    "    else:\n",
    "        score_threshold = 0.2\n",
    "    sim_words = 0\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            try:\n",
    "                syns_word = wordnet.synsets(token) \n",
    "                score = syns_word[0].path_similarity(syns[0])\n",
    "                if score >= score_threshold:\n",
    "                    sim_words += 1\n",
    "            except: \n",
    "                score = 0\n",
    "    \n",
    "    return sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to create columns for similarity based on all STEM fields\n",
    "syns_bio = wordnet.synsets(lemmatizer.lemmatize(\"biology\"))\n",
    "syns_maths = wordnet.synsets(lemmatizer.lemmatize(\"mathematics\")) \n",
    "syns_tech = wordnet.synsets(lemmatizer.lemmatize(\"technology\"))\n",
    "syns_eng = wordnet.synsets(lemmatizer.lemmatize(\"engineering\"))\n",
    "syns_chem = wordnet.synsets(lemmatizer.lemmatize(\"chemistry\"))\n",
    "syns_phy = wordnet.synsets(lemmatizer.lemmatize(\"physics\"))\n",
    "syns_sci = wordnet.synsets(lemmatizer.lemmatize(\"science\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Medical Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['microbiology', 'oncology', 'cardiology', 'chest', 'radiation', 'cardiothoracic', 'care', 'pathology', 'health', 'cytopathology', 'genetics', 'obstetrics', 'nuclear', 'neuromuscular', 'transfusion', 'segment', 'pediatrics', 'surgery', 'sports', 'psychiatry', 'dermatology', 'geriatric', 'molecular', 'research', 'pelvic', 'neurology', 'liaison', 'reconstructive', 'hospice', 'dermatopathology', 'dermatology', 'interventional', 'anatomical', 'neuro', 'pediatric', 'emergency', 'fetal', 'imaging', 'addiction', 'male', 'musculoskeletal', 'anterior', 'orbit', 'procedural', 'aerospace', 'surgical', 'adolescent', 'consultation', 'surgery', 'and', 'genetic', 'allergy', 'female', 'adolescent', 'cardiac', 'glaucoma', 'sports', 'plastic', 'oncology', 'infectious', 'physical', 'calculi', 'hematology', 'abdominal', 'neurodevelopmental', 'head', 'rehabilitation', 'genetic', 'internal', 'reproductive', 'critical', 'immunopathology', 'transplant', 'neurourology', 'advanced', 'heart', 'genitourinary', 'endocrinology', 'public', 'developmental', 'brain', 'immunology', 'retina', 'gastroenterology', 'disabilities', 'critical', 'nephrology', 'gynecologic', 'psychiatric', 'banking', 'abuse', 'toxicology', 'medicine', 'behavioral', 'pulmonology', 'pulmonary', 'neck', 'ocular', 'chemical', 'gastroenterology', 'pathology', 'nephrology', 'urologic', 'neonatal', 'endovascular', 'cornea', 'hematology', 'cardiovascular', 'failure', 'hepatology', 'anesthesiology', 'pediatrics', 'uveitis', 'rheumatology', 'urology', 'endocrinologists', 'psychosomatic', 'reconstructive', 'forensic', 'neurology', 'mental', 'medical', 'endocrinology', 'metabolism', 'diagnostic', 'maternal', 'gynecology', 'neuroradiology', 'interventional', 'diabetes', 'internal', 'sleep', 'cytogenetics', 'biochemical', 'oculoplastics', 'diseases', 'gastrointestinal', 'urology', 'vascular', 'community', 'neurophysiology', 'neuropathology', 'pediatric', 'strabismus', 'child', 'anesthesiology', 'ophthalmology', 'administrative', 'clinical', 'blood', 'retardation', 'neuroradiology', 'injury', 'infertility', 'preventive', 'military', 'breast', 'disease', 'psychiatry', 'infectious', 'palliative', 'transplant', 'rheumatology', 'renal', 'pain', 'family', 'ophthalmology', 'perinatal', 'ophthalmic', 'occupational', 'radiology', 'electrophysiology']\n"
     ]
    }
   ],
   "source": [
    "# Load the medical specialization text file and create a list\n",
    "medical_list = []\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/medical_specialities.txt', 'r') as medical_fields:\n",
    "    for line in medical_fields.readlines():\n",
    "        special_field = line.rstrip('\\n')\n",
    "        special_field = re.sub(\"\\W\",\" \", special_field )\n",
    "#         print(special_field)\n",
    "        medical_list += special_field.split()\n",
    "medical_list = list(set(medical_list))  \n",
    "medical_list = [x.lower() for x in medical_list]\n",
    "print(medical_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get medical words\n",
    "def check_medical_words(tokens):\n",
    "    for token in tokens:\n",
    "        if token not in stop_words and token in [x.lower() for x in medical_list]:\n",
    "            return 1\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sentiment Polarity and Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get polarity and subjectivity of the sentence using TexBlob\n",
    "def get_sentiment(sentence):\n",
    "    sentiments =TextBlob(sentence).sentiment\n",
    "    polarity = sentiments.polarity\n",
    "    subjectivity = sentiments.subjectivity\n",
    "    return polarity, subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the count of POS tags of the sentence\n",
    "def count_pos_tags(tokens):\n",
    "    token_pos = pos_tag(tokens)\n",
    "    count = Counter(tag for word,tag in token_pos)\n",
    "    interjections =  count['UH']\n",
    "    nouns = count['NN'] + count['NNS'] + count['NNP'] + count['NNPS']\n",
    "    adverb = count['RB'] + count['RBS'] + count['RBR']\n",
    "    verb = count['VB'] + count['VBD'] + count['VBG'] + count['VBN']\n",
    "    determiner = count['DT']\n",
    "    pronoun = count['PRP']\n",
    "    adjetive = count['JJ'] + count['JJR'] + count['JJS']\n",
    "    preposition = count['IN']\n",
    "    return interjections, nouns, adverb, verb, determiner, pronoun, adjetive,preposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_extraction(dataframe, field, func, column_names):\n",
    "    return pd.concat((\n",
    "        dataframe,\n",
    "        dataframe[field].apply(\n",
    "            lambda cell: pd.Series(func(cell), index=column_names))), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the w2v dict from pickle file\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/pickle/embeddings06122024.pickle', 'rb') as w2v_file:\n",
    "    w2v_dict = pickle.load(w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of word embeddings:  4762\n"
     ]
    }
   ],
   "source": [
    "print(\"length of word embeddings: \", len(w2v_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vectors for the essay\n",
    "def vectorizer(sequence):\n",
    "    vect = []\n",
    "    numw = 0\n",
    "    for w in sequence: \n",
    "        try :\n",
    "            if numw == 0:\n",
    "                vect = w2v_dict[w]\n",
    "            else:\n",
    "                vect = np.add(vect, w2v_dict[w])\n",
    "            numw += 1\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    return vect/ numw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into words\n",
    "def split_into_words(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vectorizer\n",
    "unigram_vect = CountVectorizer(ngram_range=(1, 1), min_df=2, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Putting them all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for feature engineering\n",
    "def feature_engineering(original_dataset):\n",
    "\n",
    "    dataset = original_dataset.copy()\n",
    "    # create a new column with sentence tokens\n",
    "    dataset['tokens'] = dataset['sentence'].apply(word_tokenize)\n",
    "    # 1. Similarity features\n",
    "    # biology\n",
    "    dataset['bio_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_bio,'biology',)) \n",
    "    # chemistry\n",
    "    dataset['chem_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_chem,'chemistry',))\n",
    "    # physics\n",
    "    dataset['phy_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_phy,'physics',))\n",
    "    # mathematics\n",
    "    dataset['math_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_maths,'mathematics',))\n",
    "    # technology\n",
    "    dataset['tech_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_tech,'technology',))\n",
    "    # engineering\n",
    "    dataset['eng_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_eng,'engineering',))\n",
    "    \n",
    "    # medical terms\n",
    "    dataset['medical_terms'] = dataset['tokens'].apply(check_medical_words)\n",
    "    \n",
    "    # polarity and subjectivity\n",
    "    dataset['polarity'], dataset['subjectivity'] = zip(*dataset['sentence'].apply(get_sentiment))\n",
    "    \n",
    "    # named entity recognition\n",
    "    dataset['ner'] = dataset['sentence'].apply(named_entity_present)\n",
    "    \n",
    "    # pos tag count\n",
    "    dataset = pos_tag_extraction(dataset, 'tokens', count_pos_tags, ['interjections', 'nouns', 'adverb', 'verb', 'determiner', 'pronoun', 'adjetive','preposition'])\n",
    "    \n",
    "    # labels\n",
    "    data_labels = dataset['label']\n",
    "    # X\n",
    "    data_x = dataset.drop(columns='label')\n",
    "\n",
    "    \n",
    "    # vectorize all the essays\n",
    "    vect_arr = data_x.tokens.apply(vectorizer)\n",
    "    for index in range(0, len(vect_arr)):\n",
    "        i = 0\n",
    "        for item in vect_arr[index]:\n",
    "            column_name= \"embedding\" + str(i)\n",
    "            data_x.loc[index, column_name] = item\n",
    "            i +=1\n",
    "    \n",
    "    return data_x,data_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = feature_engineering(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1462, 121)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = feature_engineering(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 121)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Unigram features for both train and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1462, 121)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 121)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Social/saved_features/X_test_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Social/saved_features/X_train_final.csv\", index=False)\n",
    "X_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Social/saved_features/X_test_final.csv\", index=False)\n",
    "y_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Social/saved_features/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Social/saved_features/y_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the unigram df for train :  (1462, 1170)\n"
     ]
    }
   ],
   "source": [
    "# Unigrams for training set\n",
    "unigram_matrix = unigram_vect.fit_transform(X_train['sentence'])\n",
    "unigrams = pd.DataFrame(unigram_matrix.toarray())\n",
    "print(\"Shape of the unigram df for train : \",unigrams.shape)\n",
    "unigrams = unigrams.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = pd.concat([X_train, unigrams], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final.columns = X_train_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1462, 1291)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test unigram df shape :  (163, 1170)\n"
     ]
    }
   ],
   "source": [
    "unigram_matrix_test = unigram_vect.transform(X_test['sentence'])\n",
    "unigrams_test = pd.DataFrame(unigram_matrix_test.toarray())\n",
    "unigrams_test = unigrams_test.reset_index(drop=True)\n",
    "print(\"Test unigram df shape : \",unigrams_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 1291)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final = pd.concat([X_test, unigrams_test], axis = 1)\n",
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final.columns = X_test_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 1291)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ---- sentence\n",
      "1 ---- phrase\n",
      "2 ---- tokens\n",
      "3 ---- bio_sim_words\n",
      "4 ---- chem_sim_words\n",
      "5 ---- phy_sim_words\n",
      "6 ---- math_sim_words\n",
      "7 ---- tech_sim_words\n",
      "8 ---- eng_sim_words\n",
      "9 ---- medical_terms\n",
      "10 ---- polarity\n",
      "11 ---- subjectivity\n",
      "12 ---- ner\n",
      "13 ---- interjections\n",
      "14 ---- nouns\n",
      "15 ---- adverb\n",
      "16 ---- verb\n",
      "17 ---- determiner\n",
      "18 ---- pronoun\n",
      "19 ---- adjetive\n",
      "20 ---- preposition\n",
      "21 ---- embedding0\n",
      "22 ---- embedding1\n",
      "23 ---- embedding2\n",
      "24 ---- embedding3\n",
      "25 ---- embedding4\n",
      "26 ---- embedding5\n",
      "27 ---- embedding6\n",
      "28 ---- embedding7\n",
      "29 ---- embedding8\n",
      "30 ---- embedding9\n",
      "31 ---- embedding10\n",
      "32 ---- embedding11\n",
      "33 ---- embedding12\n",
      "34 ---- embedding13\n",
      "35 ---- embedding14\n",
      "36 ---- embedding15\n",
      "37 ---- embedding16\n",
      "38 ---- embedding17\n",
      "39 ---- embedding18\n",
      "40 ---- embedding19\n",
      "41 ---- embedding20\n",
      "42 ---- embedding21\n",
      "43 ---- embedding22\n",
      "44 ---- embedding23\n",
      "45 ---- embedding24\n",
      "46 ---- embedding25\n",
      "47 ---- embedding26\n",
      "48 ---- embedding27\n",
      "49 ---- embedding28\n",
      "50 ---- embedding29\n",
      "51 ---- embedding30\n",
      "52 ---- embedding31\n",
      "53 ---- embedding32\n",
      "54 ---- embedding33\n",
      "55 ---- embedding34\n",
      "56 ---- embedding35\n",
      "57 ---- embedding36\n",
      "58 ---- embedding37\n",
      "59 ---- embedding38\n",
      "60 ---- embedding39\n",
      "61 ---- embedding40\n",
      "62 ---- embedding41\n",
      "63 ---- embedding42\n",
      "64 ---- embedding43\n",
      "65 ---- embedding44\n",
      "66 ---- embedding45\n",
      "67 ---- embedding46\n",
      "68 ---- embedding47\n",
      "69 ---- embedding48\n",
      "70 ---- embedding49\n",
      "71 ---- embedding50\n",
      "72 ---- embedding51\n",
      "73 ---- embedding52\n",
      "74 ---- embedding53\n",
      "75 ---- embedding54\n",
      "76 ---- embedding55\n",
      "77 ---- embedding56\n",
      "78 ---- embedding57\n",
      "79 ---- embedding58\n",
      "80 ---- embedding59\n",
      "81 ---- embedding60\n",
      "82 ---- embedding61\n",
      "83 ---- embedding62\n",
      "84 ---- embedding63\n",
      "85 ---- embedding64\n",
      "86 ---- embedding65\n",
      "87 ---- embedding66\n",
      "88 ---- embedding67\n",
      "89 ---- embedding68\n",
      "90 ---- embedding69\n",
      "91 ---- embedding70\n",
      "92 ---- embedding71\n",
      "93 ---- embedding72\n",
      "94 ---- embedding73\n",
      "95 ---- embedding74\n",
      "96 ---- embedding75\n",
      "97 ---- embedding76\n",
      "98 ---- embedding77\n",
      "99 ---- embedding78\n",
      "100 ---- embedding79\n",
      "101 ---- embedding80\n",
      "102 ---- embedding81\n",
      "103 ---- embedding82\n",
      "104 ---- embedding83\n",
      "105 ---- embedding84\n",
      "106 ---- embedding85\n",
      "107 ---- embedding86\n",
      "108 ---- embedding87\n",
      "109 ---- embedding88\n",
      "110 ---- embedding89\n",
      "111 ---- embedding90\n",
      "112 ---- embedding91\n",
      "113 ---- embedding92\n",
      "114 ---- embedding93\n",
      "115 ---- embedding94\n",
      "116 ---- embedding95\n",
      "117 ---- embedding96\n",
      "118 ---- embedding97\n",
      "119 ---- embedding98\n",
      "120 ---- embedding99\n",
      "121 ---- 0\n",
      "122 ---- 1\n",
      "123 ---- 2\n",
      "124 ---- 3\n",
      "125 ---- 4\n",
      "126 ---- 5\n",
      "127 ---- 6\n",
      "128 ---- 7\n",
      "129 ---- 8\n",
      "130 ---- 9\n",
      "131 ---- 10\n",
      "132 ---- 11\n",
      "133 ---- 12\n",
      "134 ---- 13\n",
      "135 ---- 14\n",
      "136 ---- 15\n",
      "137 ---- 16\n",
      "138 ---- 17\n",
      "139 ---- 18\n",
      "140 ---- 19\n",
      "141 ---- 20\n",
      "142 ---- 21\n",
      "143 ---- 22\n",
      "144 ---- 23\n",
      "145 ---- 24\n",
      "146 ---- 25\n",
      "147 ---- 26\n",
      "148 ---- 27\n",
      "149 ---- 28\n",
      "150 ---- 29\n",
      "151 ---- 30\n",
      "152 ---- 31\n",
      "153 ---- 32\n",
      "154 ---- 33\n",
      "155 ---- 34\n",
      "156 ---- 35\n",
      "157 ---- 36\n",
      "158 ---- 37\n",
      "159 ---- 38\n",
      "160 ---- 39\n",
      "161 ---- 40\n",
      "162 ---- 41\n",
      "163 ---- 42\n",
      "164 ---- 43\n",
      "165 ---- 44\n",
      "166 ---- 45\n",
      "167 ---- 46\n",
      "168 ---- 47\n",
      "169 ---- 48\n",
      "170 ---- 49\n",
      "171 ---- 50\n",
      "172 ---- 51\n",
      "173 ---- 52\n",
      "174 ---- 53\n",
      "175 ---- 54\n",
      "176 ---- 55\n",
      "177 ---- 56\n",
      "178 ---- 57\n",
      "179 ---- 58\n",
      "180 ---- 59\n",
      "181 ---- 60\n",
      "182 ---- 61\n",
      "183 ---- 62\n",
      "184 ---- 63\n",
      "185 ---- 64\n",
      "186 ---- 65\n",
      "187 ---- 66\n",
      "188 ---- 67\n",
      "189 ---- 68\n",
      "190 ---- 69\n",
      "191 ---- 70\n",
      "192 ---- 71\n",
      "193 ---- 72\n",
      "194 ---- 73\n",
      "195 ---- 74\n",
      "196 ---- 75\n",
      "197 ---- 76\n",
      "198 ---- 77\n",
      "199 ---- 78\n",
      "200 ---- 79\n",
      "201 ---- 80\n",
      "202 ---- 81\n",
      "203 ---- 82\n",
      "204 ---- 83\n",
      "205 ---- 84\n",
      "206 ---- 85\n",
      "207 ---- 86\n",
      "208 ---- 87\n",
      "209 ---- 88\n",
      "210 ---- 89\n",
      "211 ---- 90\n",
      "212 ---- 91\n",
      "213 ---- 92\n",
      "214 ---- 93\n",
      "215 ---- 94\n",
      "216 ---- 95\n",
      "217 ---- 96\n",
      "218 ---- 97\n",
      "219 ---- 98\n",
      "220 ---- 99\n",
      "221 ---- 100\n",
      "222 ---- 101\n",
      "223 ---- 102\n",
      "224 ---- 103\n",
      "225 ---- 104\n",
      "226 ---- 105\n",
      "227 ---- 106\n",
      "228 ---- 107\n",
      "229 ---- 108\n",
      "230 ---- 109\n",
      "231 ---- 110\n",
      "232 ---- 111\n",
      "233 ---- 112\n",
      "234 ---- 113\n",
      "235 ---- 114\n",
      "236 ---- 115\n",
      "237 ---- 116\n",
      "238 ---- 117\n",
      "239 ---- 118\n",
      "240 ---- 119\n",
      "241 ---- 120\n",
      "242 ---- 121\n",
      "243 ---- 122\n",
      "244 ---- 123\n",
      "245 ---- 124\n",
      "246 ---- 125\n",
      "247 ---- 126\n",
      "248 ---- 127\n",
      "249 ---- 128\n",
      "250 ---- 129\n",
      "251 ---- 130\n",
      "252 ---- 131\n",
      "253 ---- 132\n",
      "254 ---- 133\n",
      "255 ---- 134\n",
      "256 ---- 135\n",
      "257 ---- 136\n",
      "258 ---- 137\n",
      "259 ---- 138\n",
      "260 ---- 139\n",
      "261 ---- 140\n",
      "262 ---- 141\n",
      "263 ---- 142\n",
      "264 ---- 143\n",
      "265 ---- 144\n",
      "266 ---- 145\n",
      "267 ---- 146\n",
      "268 ---- 147\n",
      "269 ---- 148\n",
      "270 ---- 149\n",
      "271 ---- 150\n",
      "272 ---- 151\n",
      "273 ---- 152\n",
      "274 ---- 153\n",
      "275 ---- 154\n",
      "276 ---- 155\n",
      "277 ---- 156\n",
      "278 ---- 157\n",
      "279 ---- 158\n",
      "280 ---- 159\n",
      "281 ---- 160\n",
      "282 ---- 161\n",
      "283 ---- 162\n",
      "284 ---- 163\n",
      "285 ---- 164\n",
      "286 ---- 165\n",
      "287 ---- 166\n",
      "288 ---- 167\n",
      "289 ---- 168\n",
      "290 ---- 169\n",
      "291 ---- 170\n",
      "292 ---- 171\n",
      "293 ---- 172\n",
      "294 ---- 173\n",
      "295 ---- 174\n",
      "296 ---- 175\n",
      "297 ---- 176\n",
      "298 ---- 177\n",
      "299 ---- 178\n",
      "300 ---- 179\n",
      "301 ---- 180\n",
      "302 ---- 181\n",
      "303 ---- 182\n",
      "304 ---- 183\n",
      "305 ---- 184\n",
      "306 ---- 185\n",
      "307 ---- 186\n",
      "308 ---- 187\n",
      "309 ---- 188\n",
      "310 ---- 189\n",
      "311 ---- 190\n",
      "312 ---- 191\n",
      "313 ---- 192\n",
      "314 ---- 193\n",
      "315 ---- 194\n",
      "316 ---- 195\n",
      "317 ---- 196\n",
      "318 ---- 197\n",
      "319 ---- 198\n",
      "320 ---- 199\n",
      "321 ---- 200\n",
      "322 ---- 201\n",
      "323 ---- 202\n",
      "324 ---- 203\n",
      "325 ---- 204\n",
      "326 ---- 205\n",
      "327 ---- 206\n",
      "328 ---- 207\n",
      "329 ---- 208\n",
      "330 ---- 209\n",
      "331 ---- 210\n",
      "332 ---- 211\n",
      "333 ---- 212\n",
      "334 ---- 213\n",
      "335 ---- 214\n",
      "336 ---- 215\n",
      "337 ---- 216\n",
      "338 ---- 217\n",
      "339 ---- 218\n",
      "340 ---- 219\n",
      "341 ---- 220\n",
      "342 ---- 221\n",
      "343 ---- 222\n",
      "344 ---- 223\n",
      "345 ---- 224\n",
      "346 ---- 225\n",
      "347 ---- 226\n",
      "348 ---- 227\n",
      "349 ---- 228\n",
      "350 ---- 229\n",
      "351 ---- 230\n",
      "352 ---- 231\n",
      "353 ---- 232\n",
      "354 ---- 233\n",
      "355 ---- 234\n",
      "356 ---- 235\n",
      "357 ---- 236\n",
      "358 ---- 237\n",
      "359 ---- 238\n",
      "360 ---- 239\n",
      "361 ---- 240\n",
      "362 ---- 241\n",
      "363 ---- 242\n",
      "364 ---- 243\n",
      "365 ---- 244\n",
      "366 ---- 245\n",
      "367 ---- 246\n",
      "368 ---- 247\n",
      "369 ---- 248\n",
      "370 ---- 249\n",
      "371 ---- 250\n",
      "372 ---- 251\n",
      "373 ---- 252\n",
      "374 ---- 253\n",
      "375 ---- 254\n",
      "376 ---- 255\n",
      "377 ---- 256\n",
      "378 ---- 257\n",
      "379 ---- 258\n",
      "380 ---- 259\n",
      "381 ---- 260\n",
      "382 ---- 261\n",
      "383 ---- 262\n",
      "384 ---- 263\n",
      "385 ---- 264\n",
      "386 ---- 265\n",
      "387 ---- 266\n",
      "388 ---- 267\n",
      "389 ---- 268\n",
      "390 ---- 269\n",
      "391 ---- 270\n",
      "392 ---- 271\n",
      "393 ---- 272\n",
      "394 ---- 273\n",
      "395 ---- 274\n",
      "396 ---- 275\n",
      "397 ---- 276\n",
      "398 ---- 277\n",
      "399 ---- 278\n",
      "400 ---- 279\n",
      "401 ---- 280\n",
      "402 ---- 281\n",
      "403 ---- 282\n",
      "404 ---- 283\n",
      "405 ---- 284\n",
      "406 ---- 285\n",
      "407 ---- 286\n",
      "408 ---- 287\n",
      "409 ---- 288\n",
      "410 ---- 289\n",
      "411 ---- 290\n",
      "412 ---- 291\n",
      "413 ---- 292\n",
      "414 ---- 293\n",
      "415 ---- 294\n",
      "416 ---- 295\n",
      "417 ---- 296\n",
      "418 ---- 297\n",
      "419 ---- 298\n",
      "420 ---- 299\n",
      "421 ---- 300\n",
      "422 ---- 301\n",
      "423 ---- 302\n",
      "424 ---- 303\n",
      "425 ---- 304\n",
      "426 ---- 305\n",
      "427 ---- 306\n",
      "428 ---- 307\n",
      "429 ---- 308\n",
      "430 ---- 309\n",
      "431 ---- 310\n",
      "432 ---- 311\n",
      "433 ---- 312\n",
      "434 ---- 313\n",
      "435 ---- 314\n",
      "436 ---- 315\n",
      "437 ---- 316\n",
      "438 ---- 317\n",
      "439 ---- 318\n",
      "440 ---- 319\n",
      "441 ---- 320\n",
      "442 ---- 321\n",
      "443 ---- 322\n",
      "444 ---- 323\n",
      "445 ---- 324\n",
      "446 ---- 325\n",
      "447 ---- 326\n",
      "448 ---- 327\n",
      "449 ---- 328\n",
      "450 ---- 329\n",
      "451 ---- 330\n",
      "452 ---- 331\n",
      "453 ---- 332\n",
      "454 ---- 333\n",
      "455 ---- 334\n",
      "456 ---- 335\n",
      "457 ---- 336\n",
      "458 ---- 337\n",
      "459 ---- 338\n",
      "460 ---- 339\n",
      "461 ---- 340\n",
      "462 ---- 341\n",
      "463 ---- 342\n",
      "464 ---- 343\n",
      "465 ---- 344\n",
      "466 ---- 345\n",
      "467 ---- 346\n",
      "468 ---- 347\n",
      "469 ---- 348\n",
      "470 ---- 349\n",
      "471 ---- 350\n",
      "472 ---- 351\n",
      "473 ---- 352\n",
      "474 ---- 353\n",
      "475 ---- 354\n",
      "476 ---- 355\n",
      "477 ---- 356\n",
      "478 ---- 357\n",
      "479 ---- 358\n",
      "480 ---- 359\n",
      "481 ---- 360\n",
      "482 ---- 361\n",
      "483 ---- 362\n",
      "484 ---- 363\n",
      "485 ---- 364\n",
      "486 ---- 365\n",
      "487 ---- 366\n",
      "488 ---- 367\n",
      "489 ---- 368\n",
      "490 ---- 369\n",
      "491 ---- 370\n",
      "492 ---- 371\n",
      "493 ---- 372\n",
      "494 ---- 373\n",
      "495 ---- 374\n",
      "496 ---- 375\n",
      "497 ---- 376\n",
      "498 ---- 377\n",
      "499 ---- 378\n",
      "500 ---- 379\n",
      "501 ---- 380\n",
      "502 ---- 381\n",
      "503 ---- 382\n",
      "504 ---- 383\n",
      "505 ---- 384\n",
      "506 ---- 385\n",
      "507 ---- 386\n",
      "508 ---- 387\n",
      "509 ---- 388\n",
      "510 ---- 389\n",
      "511 ---- 390\n",
      "512 ---- 391\n",
      "513 ---- 392\n",
      "514 ---- 393\n",
      "515 ---- 394\n",
      "516 ---- 395\n",
      "517 ---- 396\n",
      "518 ---- 397\n",
      "519 ---- 398\n",
      "520 ---- 399\n",
      "521 ---- 400\n",
      "522 ---- 401\n",
      "523 ---- 402\n",
      "524 ---- 403\n",
      "525 ---- 404\n",
      "526 ---- 405\n",
      "527 ---- 406\n",
      "528 ---- 407\n",
      "529 ---- 408\n",
      "530 ---- 409\n",
      "531 ---- 410\n",
      "532 ---- 411\n",
      "533 ---- 412\n",
      "534 ---- 413\n",
      "535 ---- 414\n",
      "536 ---- 415\n",
      "537 ---- 416\n",
      "538 ---- 417\n",
      "539 ---- 418\n",
      "540 ---- 419\n",
      "541 ---- 420\n",
      "542 ---- 421\n",
      "543 ---- 422\n",
      "544 ---- 423\n",
      "545 ---- 424\n",
      "546 ---- 425\n",
      "547 ---- 426\n",
      "548 ---- 427\n",
      "549 ---- 428\n",
      "550 ---- 429\n",
      "551 ---- 430\n",
      "552 ---- 431\n",
      "553 ---- 432\n",
      "554 ---- 433\n",
      "555 ---- 434\n",
      "556 ---- 435\n",
      "557 ---- 436\n",
      "558 ---- 437\n",
      "559 ---- 438\n",
      "560 ---- 439\n",
      "561 ---- 440\n",
      "562 ---- 441\n",
      "563 ---- 442\n",
      "564 ---- 443\n",
      "565 ---- 444\n",
      "566 ---- 445\n",
      "567 ---- 446\n",
      "568 ---- 447\n",
      "569 ---- 448\n",
      "570 ---- 449\n",
      "571 ---- 450\n",
      "572 ---- 451\n",
      "573 ---- 452\n",
      "574 ---- 453\n",
      "575 ---- 454\n",
      "576 ---- 455\n",
      "577 ---- 456\n",
      "578 ---- 457\n",
      "579 ---- 458\n",
      "580 ---- 459\n",
      "581 ---- 460\n",
      "582 ---- 461\n",
      "583 ---- 462\n",
      "584 ---- 463\n",
      "585 ---- 464\n",
      "586 ---- 465\n",
      "587 ---- 466\n",
      "588 ---- 467\n",
      "589 ---- 468\n",
      "590 ---- 469\n",
      "591 ---- 470\n",
      "592 ---- 471\n",
      "593 ---- 472\n",
      "594 ---- 473\n",
      "595 ---- 474\n",
      "596 ---- 475\n",
      "597 ---- 476\n",
      "598 ---- 477\n",
      "599 ---- 478\n",
      "600 ---- 479\n",
      "601 ---- 480\n",
      "602 ---- 481\n",
      "603 ---- 482\n",
      "604 ---- 483\n",
      "605 ---- 484\n",
      "606 ---- 485\n",
      "607 ---- 486\n",
      "608 ---- 487\n",
      "609 ---- 488\n",
      "610 ---- 489\n",
      "611 ---- 490\n",
      "612 ---- 491\n",
      "613 ---- 492\n",
      "614 ---- 493\n",
      "615 ---- 494\n",
      "616 ---- 495\n",
      "617 ---- 496\n",
      "618 ---- 497\n",
      "619 ---- 498\n",
      "620 ---- 499\n",
      "621 ---- 500\n",
      "622 ---- 501\n",
      "623 ---- 502\n",
      "624 ---- 503\n",
      "625 ---- 504\n",
      "626 ---- 505\n",
      "627 ---- 506\n",
      "628 ---- 507\n",
      "629 ---- 508\n",
      "630 ---- 509\n",
      "631 ---- 510\n",
      "632 ---- 511\n",
      "633 ---- 512\n",
      "634 ---- 513\n",
      "635 ---- 514\n",
      "636 ---- 515\n",
      "637 ---- 516\n",
      "638 ---- 517\n",
      "639 ---- 518\n",
      "640 ---- 519\n",
      "641 ---- 520\n",
      "642 ---- 521\n",
      "643 ---- 522\n",
      "644 ---- 523\n",
      "645 ---- 524\n",
      "646 ---- 525\n",
      "647 ---- 526\n",
      "648 ---- 527\n",
      "649 ---- 528\n",
      "650 ---- 529\n",
      "651 ---- 530\n",
      "652 ---- 531\n",
      "653 ---- 532\n",
      "654 ---- 533\n",
      "655 ---- 534\n",
      "656 ---- 535\n",
      "657 ---- 536\n",
      "658 ---- 537\n",
      "659 ---- 538\n",
      "660 ---- 539\n",
      "661 ---- 540\n",
      "662 ---- 541\n",
      "663 ---- 542\n",
      "664 ---- 543\n",
      "665 ---- 544\n",
      "666 ---- 545\n",
      "667 ---- 546\n",
      "668 ---- 547\n",
      "669 ---- 548\n",
      "670 ---- 549\n",
      "671 ---- 550\n",
      "672 ---- 551\n",
      "673 ---- 552\n",
      "674 ---- 553\n",
      "675 ---- 554\n",
      "676 ---- 555\n",
      "677 ---- 556\n",
      "678 ---- 557\n",
      "679 ---- 558\n",
      "680 ---- 559\n",
      "681 ---- 560\n",
      "682 ---- 561\n",
      "683 ---- 562\n",
      "684 ---- 563\n",
      "685 ---- 564\n",
      "686 ---- 565\n",
      "687 ---- 566\n",
      "688 ---- 567\n",
      "689 ---- 568\n",
      "690 ---- 569\n",
      "691 ---- 570\n",
      "692 ---- 571\n",
      "693 ---- 572\n",
      "694 ---- 573\n",
      "695 ---- 574\n",
      "696 ---- 575\n",
      "697 ---- 576\n",
      "698 ---- 577\n",
      "699 ---- 578\n",
      "700 ---- 579\n",
      "701 ---- 580\n",
      "702 ---- 581\n",
      "703 ---- 582\n",
      "704 ---- 583\n",
      "705 ---- 584\n",
      "706 ---- 585\n",
      "707 ---- 586\n",
      "708 ---- 587\n",
      "709 ---- 588\n",
      "710 ---- 589\n",
      "711 ---- 590\n",
      "712 ---- 591\n",
      "713 ---- 592\n",
      "714 ---- 593\n",
      "715 ---- 594\n",
      "716 ---- 595\n",
      "717 ---- 596\n",
      "718 ---- 597\n",
      "719 ---- 598\n",
      "720 ---- 599\n",
      "721 ---- 600\n",
      "722 ---- 601\n",
      "723 ---- 602\n",
      "724 ---- 603\n",
      "725 ---- 604\n",
      "726 ---- 605\n",
      "727 ---- 606\n",
      "728 ---- 607\n",
      "729 ---- 608\n",
      "730 ---- 609\n",
      "731 ---- 610\n",
      "732 ---- 611\n",
      "733 ---- 612\n",
      "734 ---- 613\n",
      "735 ---- 614\n",
      "736 ---- 615\n",
      "737 ---- 616\n",
      "738 ---- 617\n",
      "739 ---- 618\n",
      "740 ---- 619\n",
      "741 ---- 620\n",
      "742 ---- 621\n",
      "743 ---- 622\n",
      "744 ---- 623\n",
      "745 ---- 624\n",
      "746 ---- 625\n",
      "747 ---- 626\n",
      "748 ---- 627\n",
      "749 ---- 628\n",
      "750 ---- 629\n",
      "751 ---- 630\n",
      "752 ---- 631\n",
      "753 ---- 632\n",
      "754 ---- 633\n",
      "755 ---- 634\n",
      "756 ---- 635\n",
      "757 ---- 636\n",
      "758 ---- 637\n",
      "759 ---- 638\n",
      "760 ---- 639\n",
      "761 ---- 640\n",
      "762 ---- 641\n",
      "763 ---- 642\n",
      "764 ---- 643\n",
      "765 ---- 644\n",
      "766 ---- 645\n",
      "767 ---- 646\n",
      "768 ---- 647\n",
      "769 ---- 648\n",
      "770 ---- 649\n",
      "771 ---- 650\n",
      "772 ---- 651\n",
      "773 ---- 652\n",
      "774 ---- 653\n",
      "775 ---- 654\n",
      "776 ---- 655\n",
      "777 ---- 656\n",
      "778 ---- 657\n",
      "779 ---- 658\n",
      "780 ---- 659\n",
      "781 ---- 660\n",
      "782 ---- 661\n",
      "783 ---- 662\n",
      "784 ---- 663\n",
      "785 ---- 664\n",
      "786 ---- 665\n",
      "787 ---- 666\n",
      "788 ---- 667\n",
      "789 ---- 668\n",
      "790 ---- 669\n",
      "791 ---- 670\n",
      "792 ---- 671\n",
      "793 ---- 672\n",
      "794 ---- 673\n",
      "795 ---- 674\n",
      "796 ---- 675\n",
      "797 ---- 676\n",
      "798 ---- 677\n",
      "799 ---- 678\n",
      "800 ---- 679\n",
      "801 ---- 680\n",
      "802 ---- 681\n",
      "803 ---- 682\n",
      "804 ---- 683\n",
      "805 ---- 684\n",
      "806 ---- 685\n",
      "807 ---- 686\n",
      "808 ---- 687\n",
      "809 ---- 688\n",
      "810 ---- 689\n",
      "811 ---- 690\n",
      "812 ---- 691\n",
      "813 ---- 692\n",
      "814 ---- 693\n",
      "815 ---- 694\n",
      "816 ---- 695\n",
      "817 ---- 696\n",
      "818 ---- 697\n",
      "819 ---- 698\n",
      "820 ---- 699\n",
      "821 ---- 700\n",
      "822 ---- 701\n",
      "823 ---- 702\n",
      "824 ---- 703\n",
      "825 ---- 704\n",
      "826 ---- 705\n",
      "827 ---- 706\n",
      "828 ---- 707\n",
      "829 ---- 708\n",
      "830 ---- 709\n",
      "831 ---- 710\n",
      "832 ---- 711\n",
      "833 ---- 712\n",
      "834 ---- 713\n",
      "835 ---- 714\n",
      "836 ---- 715\n",
      "837 ---- 716\n",
      "838 ---- 717\n",
      "839 ---- 718\n",
      "840 ---- 719\n",
      "841 ---- 720\n",
      "842 ---- 721\n",
      "843 ---- 722\n",
      "844 ---- 723\n",
      "845 ---- 724\n",
      "846 ---- 725\n",
      "847 ---- 726\n",
      "848 ---- 727\n",
      "849 ---- 728\n",
      "850 ---- 729\n",
      "851 ---- 730\n",
      "852 ---- 731\n",
      "853 ---- 732\n",
      "854 ---- 733\n",
      "855 ---- 734\n",
      "856 ---- 735\n",
      "857 ---- 736\n",
      "858 ---- 737\n",
      "859 ---- 738\n",
      "860 ---- 739\n",
      "861 ---- 740\n",
      "862 ---- 741\n",
      "863 ---- 742\n",
      "864 ---- 743\n",
      "865 ---- 744\n",
      "866 ---- 745\n",
      "867 ---- 746\n",
      "868 ---- 747\n",
      "869 ---- 748\n",
      "870 ---- 749\n",
      "871 ---- 750\n",
      "872 ---- 751\n",
      "873 ---- 752\n",
      "874 ---- 753\n",
      "875 ---- 754\n",
      "876 ---- 755\n",
      "877 ---- 756\n",
      "878 ---- 757\n",
      "879 ---- 758\n",
      "880 ---- 759\n",
      "881 ---- 760\n",
      "882 ---- 761\n",
      "883 ---- 762\n",
      "884 ---- 763\n",
      "885 ---- 764\n",
      "886 ---- 765\n",
      "887 ---- 766\n",
      "888 ---- 767\n",
      "889 ---- 768\n",
      "890 ---- 769\n",
      "891 ---- 770\n",
      "892 ---- 771\n",
      "893 ---- 772\n",
      "894 ---- 773\n",
      "895 ---- 774\n",
      "896 ---- 775\n",
      "897 ---- 776\n",
      "898 ---- 777\n",
      "899 ---- 778\n",
      "900 ---- 779\n",
      "901 ---- 780\n",
      "902 ---- 781\n",
      "903 ---- 782\n",
      "904 ---- 783\n",
      "905 ---- 784\n",
      "906 ---- 785\n",
      "907 ---- 786\n",
      "908 ---- 787\n",
      "909 ---- 788\n",
      "910 ---- 789\n",
      "911 ---- 790\n",
      "912 ---- 791\n",
      "913 ---- 792\n",
      "914 ---- 793\n",
      "915 ---- 794\n",
      "916 ---- 795\n",
      "917 ---- 796\n",
      "918 ---- 797\n",
      "919 ---- 798\n",
      "920 ---- 799\n",
      "921 ---- 800\n",
      "922 ---- 801\n",
      "923 ---- 802\n",
      "924 ---- 803\n",
      "925 ---- 804\n",
      "926 ---- 805\n",
      "927 ---- 806\n",
      "928 ---- 807\n",
      "929 ---- 808\n",
      "930 ---- 809\n",
      "931 ---- 810\n",
      "932 ---- 811\n",
      "933 ---- 812\n",
      "934 ---- 813\n",
      "935 ---- 814\n",
      "936 ---- 815\n",
      "937 ---- 816\n",
      "938 ---- 817\n",
      "939 ---- 818\n",
      "940 ---- 819\n",
      "941 ---- 820\n",
      "942 ---- 821\n",
      "943 ---- 822\n",
      "944 ---- 823\n",
      "945 ---- 824\n",
      "946 ---- 825\n",
      "947 ---- 826\n",
      "948 ---- 827\n",
      "949 ---- 828\n",
      "950 ---- 829\n",
      "951 ---- 830\n",
      "952 ---- 831\n",
      "953 ---- 832\n",
      "954 ---- 833\n",
      "955 ---- 834\n",
      "956 ---- 835\n",
      "957 ---- 836\n",
      "958 ---- 837\n",
      "959 ---- 838\n",
      "960 ---- 839\n",
      "961 ---- 840\n",
      "962 ---- 841\n",
      "963 ---- 842\n",
      "964 ---- 843\n",
      "965 ---- 844\n",
      "966 ---- 845\n",
      "967 ---- 846\n",
      "968 ---- 847\n",
      "969 ---- 848\n",
      "970 ---- 849\n",
      "971 ---- 850\n",
      "972 ---- 851\n",
      "973 ---- 852\n",
      "974 ---- 853\n",
      "975 ---- 854\n",
      "976 ---- 855\n",
      "977 ---- 856\n",
      "978 ---- 857\n",
      "979 ---- 858\n",
      "980 ---- 859\n",
      "981 ---- 860\n",
      "982 ---- 861\n",
      "983 ---- 862\n",
      "984 ---- 863\n",
      "985 ---- 864\n",
      "986 ---- 865\n",
      "987 ---- 866\n",
      "988 ---- 867\n",
      "989 ---- 868\n",
      "990 ---- 869\n",
      "991 ---- 870\n",
      "992 ---- 871\n",
      "993 ---- 872\n",
      "994 ---- 873\n",
      "995 ---- 874\n",
      "996 ---- 875\n",
      "997 ---- 876\n",
      "998 ---- 877\n",
      "999 ---- 878\n",
      "1000 ---- 879\n",
      "1001 ---- 880\n",
      "1002 ---- 881\n",
      "1003 ---- 882\n",
      "1004 ---- 883\n",
      "1005 ---- 884\n",
      "1006 ---- 885\n",
      "1007 ---- 886\n",
      "1008 ---- 887\n",
      "1009 ---- 888\n",
      "1010 ---- 889\n",
      "1011 ---- 890\n",
      "1012 ---- 891\n",
      "1013 ---- 892\n",
      "1014 ---- 893\n",
      "1015 ---- 894\n",
      "1016 ---- 895\n",
      "1017 ---- 896\n",
      "1018 ---- 897\n",
      "1019 ---- 898\n",
      "1020 ---- 899\n",
      "1021 ---- 900\n",
      "1022 ---- 901\n",
      "1023 ---- 902\n",
      "1024 ---- 903\n",
      "1025 ---- 904\n",
      "1026 ---- 905\n",
      "1027 ---- 906\n",
      "1028 ---- 907\n",
      "1029 ---- 908\n",
      "1030 ---- 909\n",
      "1031 ---- 910\n",
      "1032 ---- 911\n",
      "1033 ---- 912\n",
      "1034 ---- 913\n",
      "1035 ---- 914\n",
      "1036 ---- 915\n",
      "1037 ---- 916\n",
      "1038 ---- 917\n",
      "1039 ---- 918\n",
      "1040 ---- 919\n",
      "1041 ---- 920\n",
      "1042 ---- 921\n",
      "1043 ---- 922\n",
      "1044 ---- 923\n",
      "1045 ---- 924\n",
      "1046 ---- 925\n",
      "1047 ---- 926\n",
      "1048 ---- 927\n",
      "1049 ---- 928\n",
      "1050 ---- 929\n",
      "1051 ---- 930\n",
      "1052 ---- 931\n",
      "1053 ---- 932\n",
      "1054 ---- 933\n",
      "1055 ---- 934\n",
      "1056 ---- 935\n",
      "1057 ---- 936\n",
      "1058 ---- 937\n",
      "1059 ---- 938\n",
      "1060 ---- 939\n",
      "1061 ---- 940\n",
      "1062 ---- 941\n",
      "1063 ---- 942\n",
      "1064 ---- 943\n",
      "1065 ---- 944\n",
      "1066 ---- 945\n",
      "1067 ---- 946\n",
      "1068 ---- 947\n",
      "1069 ---- 948\n",
      "1070 ---- 949\n",
      "1071 ---- 950\n",
      "1072 ---- 951\n",
      "1073 ---- 952\n",
      "1074 ---- 953\n",
      "1075 ---- 954\n",
      "1076 ---- 955\n",
      "1077 ---- 956\n",
      "1078 ---- 957\n",
      "1079 ---- 958\n",
      "1080 ---- 959\n",
      "1081 ---- 960\n",
      "1082 ---- 961\n",
      "1083 ---- 962\n",
      "1084 ---- 963\n",
      "1085 ---- 964\n",
      "1086 ---- 965\n",
      "1087 ---- 966\n",
      "1088 ---- 967\n",
      "1089 ---- 968\n",
      "1090 ---- 969\n",
      "1091 ---- 970\n",
      "1092 ---- 971\n",
      "1093 ---- 972\n",
      "1094 ---- 973\n",
      "1095 ---- 974\n",
      "1096 ---- 975\n",
      "1097 ---- 976\n",
      "1098 ---- 977\n",
      "1099 ---- 978\n",
      "1100 ---- 979\n",
      "1101 ---- 980\n",
      "1102 ---- 981\n",
      "1103 ---- 982\n",
      "1104 ---- 983\n",
      "1105 ---- 984\n",
      "1106 ---- 985\n",
      "1107 ---- 986\n",
      "1108 ---- 987\n",
      "1109 ---- 988\n",
      "1110 ---- 989\n",
      "1111 ---- 990\n",
      "1112 ---- 991\n",
      "1113 ---- 992\n",
      "1114 ---- 993\n",
      "1115 ---- 994\n",
      "1116 ---- 995\n",
      "1117 ---- 996\n",
      "1118 ---- 997\n",
      "1119 ---- 998\n",
      "1120 ---- 999\n",
      "1121 ---- 1000\n",
      "1122 ---- 1001\n",
      "1123 ---- 1002\n",
      "1124 ---- 1003\n",
      "1125 ---- 1004\n",
      "1126 ---- 1005\n",
      "1127 ---- 1006\n",
      "1128 ---- 1007\n",
      "1129 ---- 1008\n",
      "1130 ---- 1009\n",
      "1131 ---- 1010\n",
      "1132 ---- 1011\n",
      "1133 ---- 1012\n",
      "1134 ---- 1013\n",
      "1135 ---- 1014\n",
      "1136 ---- 1015\n",
      "1137 ---- 1016\n",
      "1138 ---- 1017\n",
      "1139 ---- 1018\n",
      "1140 ---- 1019\n",
      "1141 ---- 1020\n",
      "1142 ---- 1021\n",
      "1143 ---- 1022\n",
      "1144 ---- 1023\n",
      "1145 ---- 1024\n",
      "1146 ---- 1025\n",
      "1147 ---- 1026\n",
      "1148 ---- 1027\n",
      "1149 ---- 1028\n",
      "1150 ---- 1029\n",
      "1151 ---- 1030\n",
      "1152 ---- 1031\n",
      "1153 ---- 1032\n",
      "1154 ---- 1033\n",
      "1155 ---- 1034\n",
      "1156 ---- 1035\n",
      "1157 ---- 1036\n",
      "1158 ---- 1037\n",
      "1159 ---- 1038\n",
      "1160 ---- 1039\n",
      "1161 ---- 1040\n",
      "1162 ---- 1041\n",
      "1163 ---- 1042\n",
      "1164 ---- 1043\n",
      "1165 ---- 1044\n",
      "1166 ---- 1045\n",
      "1167 ---- 1046\n",
      "1168 ---- 1047\n",
      "1169 ---- 1048\n",
      "1170 ---- 1049\n",
      "1171 ---- 1050\n",
      "1172 ---- 1051\n",
      "1173 ---- 1052\n",
      "1174 ---- 1053\n",
      "1175 ---- 1054\n",
      "1176 ---- 1055\n",
      "1177 ---- 1056\n",
      "1178 ---- 1057\n",
      "1179 ---- 1058\n",
      "1180 ---- 1059\n",
      "1181 ---- 1060\n",
      "1182 ---- 1061\n",
      "1183 ---- 1062\n",
      "1184 ---- 1063\n",
      "1185 ---- 1064\n",
      "1186 ---- 1065\n",
      "1187 ---- 1066\n",
      "1188 ---- 1067\n",
      "1189 ---- 1068\n",
      "1190 ---- 1069\n",
      "1191 ---- 1070\n",
      "1192 ---- 1071\n",
      "1193 ---- 1072\n",
      "1194 ---- 1073\n",
      "1195 ---- 1074\n",
      "1196 ---- 1075\n",
      "1197 ---- 1076\n",
      "1198 ---- 1077\n",
      "1199 ---- 1078\n",
      "1200 ---- 1079\n",
      "1201 ---- 1080\n",
      "1202 ---- 1081\n",
      "1203 ---- 1082\n",
      "1204 ---- 1083\n",
      "1205 ---- 1084\n",
      "1206 ---- 1085\n",
      "1207 ---- 1086\n",
      "1208 ---- 1087\n",
      "1209 ---- 1088\n",
      "1210 ---- 1089\n",
      "1211 ---- 1090\n",
      "1212 ---- 1091\n",
      "1213 ---- 1092\n",
      "1214 ---- 1093\n",
      "1215 ---- 1094\n",
      "1216 ---- 1095\n",
      "1217 ---- 1096\n",
      "1218 ---- 1097\n",
      "1219 ---- 1098\n",
      "1220 ---- 1099\n",
      "1221 ---- 1100\n",
      "1222 ---- 1101\n",
      "1223 ---- 1102\n",
      "1224 ---- 1103\n",
      "1225 ---- 1104\n",
      "1226 ---- 1105\n",
      "1227 ---- 1106\n",
      "1228 ---- 1107\n",
      "1229 ---- 1108\n",
      "1230 ---- 1109\n",
      "1231 ---- 1110\n",
      "1232 ---- 1111\n",
      "1233 ---- 1112\n",
      "1234 ---- 1113\n",
      "1235 ---- 1114\n",
      "1236 ---- 1115\n",
      "1237 ---- 1116\n",
      "1238 ---- 1117\n",
      "1239 ---- 1118\n",
      "1240 ---- 1119\n",
      "1241 ---- 1120\n",
      "1242 ---- 1121\n",
      "1243 ---- 1122\n",
      "1244 ---- 1123\n",
      "1245 ---- 1124\n",
      "1246 ---- 1125\n",
      "1247 ---- 1126\n",
      "1248 ---- 1127\n",
      "1249 ---- 1128\n",
      "1250 ---- 1129\n",
      "1251 ---- 1130\n",
      "1252 ---- 1131\n",
      "1253 ---- 1132\n",
      "1254 ---- 1133\n",
      "1255 ---- 1134\n",
      "1256 ---- 1135\n",
      "1257 ---- 1136\n",
      "1258 ---- 1137\n",
      "1259 ---- 1138\n",
      "1260 ---- 1139\n",
      "1261 ---- 1140\n",
      "1262 ---- 1141\n",
      "1263 ---- 1142\n",
      "1264 ---- 1143\n",
      "1265 ---- 1144\n",
      "1266 ---- 1145\n",
      "1267 ---- 1146\n",
      "1268 ---- 1147\n",
      "1269 ---- 1148\n",
      "1270 ---- 1149\n",
      "1271 ---- 1150\n",
      "1272 ---- 1151\n",
      "1273 ---- 1152\n",
      "1274 ---- 1153\n",
      "1275 ---- 1154\n",
      "1276 ---- 1155\n",
      "1277 ---- 1156\n",
      "1278 ---- 1157\n",
      "1279 ---- 1158\n",
      "1280 ---- 1159\n",
      "1281 ---- 1160\n",
      "1282 ---- 1161\n",
      "1283 ---- 1162\n",
      "1284 ---- 1163\n",
      "1285 ---- 1164\n",
      "1286 ---- 1165\n",
      "1287 ---- 1166\n",
      "1288 ---- 1167\n",
      "1289 ---- 1168\n",
      "1290 ---- 1169\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(X_train_final.columns)):\n",
    "    print('{} ---- {}'.format(i, X_train_final.columns[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Unigrams, POS Tag Count, Sentiment Polarity, Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_1 = X_train_final.iloc[:,np.r_[10:12,13:21,121:1291]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1462, 1180)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_1 = X_test_final.iloc[:,np.r_[10:12,13:21,121:1291]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 1180)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.449\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9758    0.8403    0.9030       144\n",
      "           1     0.4103    0.8421    0.5517        19\n",
      "\n",
      "    accuracy                         0.8405       163\n",
      "   macro avg     0.6930    0.8412    0.7274       163\n",
      "weighted avg     0.9099    0.8405    0.8620       163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_1_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_1_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_1,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_1), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 121, False Positive: 23, False Negative: 3, True Positive: 16\n",
      "--------------------------------------------------------------------------------\n",
      "[[121  23]\n",
      " [  3  16]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.84      0.90       144\n",
      "           1       0.41      0.84      0.55        19\n",
      "\n",
      "    accuracy                           0.84       163\n",
      "   macro avg       0.69      0.84      0.73       163\n",
      "weighted avg       0.91      0.84      0.86       163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_1 = LogisticRegression(random_state=18, \n",
    "                                solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], \n",
    "                                class_weight='balanced').fit(X_train_model_1, y_train)\n",
    "y_lr = lr_model_1.predict(X_test_model_1)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_model_2 = X_train_final.iloc[:,np.r_[3:1113]]\n",
    "X_train_model_2 = X_train_final.iloc[:, np.r_[3:1291]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1462, 1288)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_2 = X_test_final.iloc[:,np.r_[3:1291]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 1288)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.451\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9758    0.8403    0.9030       144\n",
      "           1     0.4103    0.8421    0.5517        19\n",
      "\n",
      "    accuracy                         0.8405       163\n",
      "   macro avg     0.6930    0.8412    0.7274       163\n",
      "weighted avg     0.9099    0.8405    0.8620       163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_2_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_2_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_2,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_2), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 121, False Positive: 23, False Negative: 3, True Positive: 16\n",
      "--------------------------------------------------------------------------------\n",
      "[[121  23]\n",
      " [  3  16]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.84      0.90       144\n",
      "           1       0.41      0.84      0.55        19\n",
      "\n",
      "    accuracy                           0.84       163\n",
      "   macro avg       0.69      0.84      0.73       163\n",
      "weighted avg       0.91      0.84      0.86       163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_2 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_2, y_train)\n",
    "y_lr = lr_model_2.predict(X_test_model_2)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Without Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_3 = X_train_final.iloc[:,np.r_[3:121]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1462, 118)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_3 = X_test_final.iloc[:,np.r_[3:121]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 118)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n",
      "Best score: 0.389\n",
      "Best parameters set:\n",
      "\tclf__C: 25\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9565    0.7639    0.8494       144\n",
      "           1     0.2917    0.7368    0.4179        19\n",
      "\n",
      "    accuracy                         0.7607       163\n",
      "   macro avg     0.6241    0.7504    0.6337       163\n",
      "weighted avg     0.8790    0.7607    0.7991       163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_3_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.0001, 0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\", \"elasticnet\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_3_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_3,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_3), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 110, False Positive: 34, False Negative: 5, True Positive: 14\n",
      "--------------------------------------------------------------------------------\n",
      "[[110  34]\n",
      " [  5  14]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.76      0.85       144\n",
      "           1       0.29      0.74      0.42        19\n",
      "\n",
      "    accuracy                           0.76       163\n",
      "   macro avg       0.62      0.75      0.63       163\n",
      "weighted avg       0.88      0.76      0.80       163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_3 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_3, y_train)\n",
    "y_lr = lr_model_3.predict(X_test_model_3)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Without Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_4 = X_train_final.iloc[:,np.r_[3:21,121:1291]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1462, 1188)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_4 = X_test_final.iloc[:,np.r_[3:21,121:1291]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 1188)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.456\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9756    0.8333    0.8989       144\n",
      "           1     0.4000    0.8421    0.5424        19\n",
      "\n",
      "    accuracy                         0.8344       163\n",
      "   macro avg     0.6878    0.8377    0.7206       163\n",
      "weighted avg     0.9085    0.8344    0.8573       163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_4_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_4_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_4,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_4), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 120, False Positive: 24, False Negative: 3, True Positive: 16\n",
      "--------------------------------------------------------------------------------\n",
      "[[120  24]\n",
      " [  3  16]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.83      0.90       144\n",
      "           1       0.40      0.84      0.54        19\n",
      "\n",
      "    accuracy                           0.83       163\n",
      "   macro avg       0.69      0.84      0.72       163\n",
      "weighted avg       0.91      0.83      0.86       163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_4 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_4, y_train)\n",
    "y_lr = lr_model_4.predict(X_test_model_4)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Without POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_5 = X_train_final.iloc[:,np.r_[3:13,21:1291]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1462, 1280)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_5 = X_test_final.iloc[:,np.r_[3:13,21:1291]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 1280)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.426\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.88      0.92       144\n",
      "           1       0.44      0.74      0.55        19\n",
      "\n",
      "    accuracy                           0.86       163\n",
      "   macro avg       0.70      0.81      0.73       163\n",
      "weighted avg       0.90      0.86      0.87       163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_5_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_5_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_5,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_5), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 126, False Positive: 18, False Negative: 5, True Positive: 14\n",
      "--------------------------------------------------------------------------------\n",
      "[[126  18]\n",
      " [  5  14]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.88      0.92       144\n",
      "           1       0.44      0.74      0.55        19\n",
      "\n",
      "    accuracy                           0.86       163\n",
      "   macro avg       0.70      0.81      0.73       163\n",
      "weighted avg       0.90      0.86      0.87       163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_5 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_5, y_train)\n",
    "y_lr = lr_model_5.predict(X_test_model_5)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6: Without STEM Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_6 = X_train_final.iloc[:,np.r_[10:1291]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1462, 1281)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_6 = X_test_final.iloc[:,np.r_[10:1291]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 1281)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.442\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.85      0.91       144\n",
      "           1       0.42      0.84      0.56        19\n",
      "\n",
      "    accuracy                           0.85       163\n",
      "   macro avg       0.70      0.84      0.73       163\n",
      "weighted avg       0.91      0.85      0.87       163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_6_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_6_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_6,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_6), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 122, False Positive: 22, False Negative: 3, True Positive: 16\n",
      "--------------------------------------------------------------------------------\n",
      "[[122  22]\n",
      " [  3  16]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.85      0.91       144\n",
      "           1       0.42      0.84      0.56        19\n",
      "\n",
      "    accuracy                           0.85       163\n",
      "   macro avg       0.70      0.84      0.73       163\n",
      "weighted avg       0.91      0.85      0.87       163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_6 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_6, y_train)\n",
    "y_lr = lr_model_6.predict(X_test_model_6)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7: Without Sentiment Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_7 = X_train_final.iloc[:,np.r_[3:10,12:1291]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1462, 1286)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_7 = X_test_final.iloc[:,np.r_[3:10,12:1291]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 1286)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.446\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.85      0.91       144\n",
      "           1       0.42      0.84      0.56        19\n",
      "\n",
      "    accuracy                           0.85       163\n",
      "   macro avg       0.70      0.84      0.73       163\n",
      "weighted avg       0.91      0.85      0.87       163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_7_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_7_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_7,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_7), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 122, False Positive: 22, False Negative: 3, True Positive: 16\n",
      "--------------------------------------------------------------------------------\n",
      "[[122  22]\n",
      " [  3  16]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.85      0.91       144\n",
      "           1       0.42      0.84      0.56        19\n",
      "\n",
      "    accuracy                           0.85       163\n",
      "   macro avg       0.70      0.84      0.73       163\n",
      "weighted avg       0.91      0.85      0.87       163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_7 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_7, y_train)\n",
    "y_lr = lr_model_7.predict(X_test_model_7)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 8: Without NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_8 = X_train_final.iloc[:,np.r_[3:12,13:1291]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1462, 1287)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_8 = X_test_final.iloc[:,np.r_[3:12,13:1291]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 1287)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.451\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.84      0.90       144\n",
      "           1       0.41      0.84      0.55        19\n",
      "\n",
      "    accuracy                           0.84       163\n",
      "   macro avg       0.69      0.84      0.73       163\n",
      "weighted avg       0.91      0.84      0.86       163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_8_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_8_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_8,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_8), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 121, False Positive: 23, False Negative: 3, True Positive: 16\n",
      "--------------------------------------------------------------------------------\n",
      "[[121  23]\n",
      " [  3  16]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.84      0.90       144\n",
      "           1       0.41      0.84      0.55        19\n",
      "\n",
      "    accuracy                           0.84       163\n",
      "   macro avg       0.69      0.84      0.73       163\n",
      "weighted avg       0.91      0.84      0.86       163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_8 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_8, y_train)\n",
    "y_lr = lr_model_8.predict(X_test_model_8)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Summary\n",
    "| Experiment | Model Number | Features Used                                                | Precision | Recall | Macro F1 |\n",
    "| :--------: | :----------: | :----------------------------------------------------------: | :-------: | :----: | :------: |\n",
    "| Baseline   | 1            | Unigrams, POS Tag Count, Sentiment Polarity and Subjectivity | 0.69      | 0.84   | 0.73     |\n",
    "| Baseline   | 2            | All features (baseline)                                      | 0.69      | 0.84   | 0.73     |\n",
    "| Baseline   | 3            | Without Unigrams                                             | 0.62      | 0.75   | 0.63     |\n",
    "| Baseline   | 4            | Without Embeddings                                           | 0.69      | 0.84   | 0.72     |\n",
    "| Baseline   | 5            | Without POS tag                                              | 0.7       | 0.81   | 0.73     |\n",
    "| Baseline   | 6            | Without STEM similarity (paper baseline)                     | 0.7       | 0.84   | 0.73     |\n",
    "| Baseline   | 7            | Without sentiment features                                   | 0.7       | 0.84   | 0.73     |\n",
    "| Baseline   | 8            | Without NER                                                  | 0.69      | 0.84   | 0.73     |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
