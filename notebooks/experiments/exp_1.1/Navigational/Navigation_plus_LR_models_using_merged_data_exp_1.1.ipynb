{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation Plus Logistic Regression Models Using Merged Data Experiment 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47752fc4a03c44fca5714cb576bd5fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 06:55:02 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-09-25 06:55:02 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-09-25 06:55:03 INFO: File exists: /Users/gbaldonado/stanza_resources/en/default.zip\n",
      "2024-09-25 06:55:06 INFO: Finished downloading models and saved to /Users/gbaldonado/stanza_resources\n",
      "2024-09-25 06:55:06 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "994d6b0ddae64ab3a2807fb076ee21bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 06:55:06 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-09-25 06:55:07 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-09-25 06:55:07 INFO: Using device: cpu\n",
      "2024-09-25 06:55:07 INFO: Loading: tokenize\n",
      "2024-09-25 06:55:07 INFO: Loading: mwt\n",
      "2024-09-25 06:55:07 INFO: Loading: pos\n",
      "2024-09-25 06:55:07 INFO: Loading: lemma\n",
      "2024-09-25 06:55:07 INFO: Loading: constituency\n",
      "2024-09-25 06:55:08 INFO: Loading: depparse\n",
      "2024-09-25 06:55:08 INFO: Loading: sentiment\n",
      "2024-09-25 06:55:08 INFO: Loading: ner\n",
      "2024-09-25 06:55:08 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import pickle\n",
    "import warnings\n",
    "import stanza\n",
    "\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, f1_score, r2_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Set random seed\n",
    "random.seed(18)\n",
    "seed = 18\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Initialize lemmatizer, stop words, and stanza\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline('en') # initialize English neural pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data and quick exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_navigational_df = pd.read_csv(\"../../../../data/processed_for_model/merged_themes_using_jaccard_method/merged_Navigational_sentence_level_batch_1_jaccard.csv\", encoding='utf-8')\n",
    "\n",
    "# Shuffle the merged dataset\n",
    "merged_navigational_df = shuffle(merged_navigational_df, random_state=seed)\n",
    "\n",
    "# Train-test split \n",
    "training_df, test_df = train_test_split(merged_navigational_df, test_size=0.2, random_state=18, stratify=merged_navigational_df['label'])\n",
    "\n",
    "training_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i also have to take physics 2 next semester so i would like to have a firm understanding of the basic concepts before they get even more complicated.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I am in this class because I am not good at math. I know physics is very math heavy and conceptually confusing, so I knew sci would be perfect for me this semester. I also have to take physics 2 next semester so I would like to have a firm understanding of the basic concepts before they get even more complicated.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>after graduating high school there are three paths a person might take: work immediately, enroll into the military, or go back to school.</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"I'm here at this institution becauase in order to do what I want to be when I grow up, I need a college degree. Planning to become a physical therapist requires a degree or some form of education. That is the main reason I'm here. After graduating high school there are three paths a person might take: work immediately, enroll into the military, or go back to school. So, I chose to go back to school.\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>these are valuable skills that will definitely help me later on in life and going through a rigorous class will only help foster these skills even more.</td>\n",
       "      <td>0</td>\n",
       "      <td>['If I am going to be completely honest Im here to fulfill my requirements in order to graduate.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>there are certain aspects of physics that i find to be very boring, for example: the math portion of it.</td>\n",
       "      <td>0</td>\n",
       "      <td>['it is a requirement for me to take Physics 111112 for my Biology Major']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i want to represent myself, my gender, my sexuality, and my latina heritage and prove the world wrong that we do have the potential to flourish.</td>\n",
       "      <td>0</td>\n",
       "      <td>['Im currently lost as to how it is that I want to make my print in this world currently out of bio chemistry major not sure if I want to do research or medicine or neither Ill just go with the flow.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                   sentence  \\\n",
       "0     i also have to take physics 2 next semester so i would like to have a firm understanding of the basic concepts before they get even more complicated.   \n",
       "1                 after graduating high school there are three paths a person might take: work immediately, enroll into the military, or go back to school.   \n",
       "2  these are valuable skills that will definitely help me later on in life and going through a rigorous class will only help foster these skills even more.   \n",
       "3                                                  there are certain aspects of physics that i find to be very boring, for example: the math portion of it.   \n",
       "4          i want to represent myself, my gender, my sexuality, and my latina heritage and prove the world wrong that we do have the potential to flourish.   \n",
       "\n",
       "   label  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      0   \n",
       "3      0   \n",
       "4      0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                   phrase  \n",
       "0                                                                                          ['I am in this class because I am not good at math. I know physics is very math heavy and conceptually confusing, so I knew sci would be perfect for me this semester. I also have to take physics 2 next semester so I would like to have a firm understanding of the basic concepts before they get even more complicated.']  \n",
       "1  [\"I'm here at this institution becauase in order to do what I want to be when I grow up, I need a college degree. Planning to become a physical therapist requires a degree or some form of education. That is the main reason I'm here. After graduating high school there are three paths a person might take: work immediately, enroll into the military, or go back to school. So, I chose to go back to school.\"]  \n",
       "2                                                                                                                                                                                                                                                                                                                      ['If I am going to be completely honest Im here to fulfill my requirements in order to graduate.']  \n",
       "3                                                                                                                                                                                                                                                                                                                                              ['it is a requirement for me to take Physics 111112 for my Biology Major']  \n",
       "4                                                                                                                                                                                                               ['Im currently lost as to how it is that I want to make my print in this world currently out of bio chemistry major not sure if I want to do research or medicine or neither Ill just go with the flow.']  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>why am i here?</td>\n",
       "      <td>0</td>\n",
       "      <td>['Im here because I need to take this class as a prerequisite for the future courses I am obligated to take for my major.', 'So to encapsulate everything I have just stated I am taking this physics 2 course in order to fulfill existing prerequisites for future classes that I need to take for my major']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i choose this school because it was close and i enjoy living in sf, other places aren't as nice and i felt i would receive the most benefit going to a school in my city.</td>\n",
       "      <td>0</td>\n",
       "      <td>['Currently I am taking this Astronomy 116 Lab for the lab credit to fulfill my general education requirements, I found my Astronomy 116 lecture to be more engaging then my Biology course.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i want to get a job in the film industry.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I am here because I want to fulfill all of my GE requirements.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>these just are the reasons why im in here.</td>\n",
       "      <td>0</td>\n",
       "      <td>['Beside getting the credit and meeting the GE requirement, I have the interest in it.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i can't say i am the biggest fan of physics, but i do enjoy solving problems so i hope to be able to understand the work i do as well as be able to apply what i learn in other areas.</td>\n",
       "      <td>0</td>\n",
       "      <td>['For this course, I expect to be able to relate the theory that I learn in 230 and be able to understand it in practical terms. I am a computer science major, so besides being required to take the course, I am looking to learn about the process that this course teaches you.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                 sentence  \\\n",
       "0                                                                                                                                                                          why am i here?   \n",
       "1               i choose this school because it was close and i enjoy living in sf, other places aren't as nice and i felt i would receive the most benefit going to a school in my city.   \n",
       "2                                                                                                                                               i want to get a job in the film industry.   \n",
       "3                                                                                                                                              these just are the reasons why im in here.   \n",
       "4  i can't say i am the biggest fan of physics, but i do enjoy solving problems so i hope to be able to understand the work i do as well as be able to apply what i learn in other areas.   \n",
       "\n",
       "   label  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      0   \n",
       "3      0   \n",
       "4      0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                            phrase  \n",
       "0  ['Im here because I need to take this class as a prerequisite for the future courses I am obligated to take for my major.', 'So to encapsulate everything I have just stated I am taking this physics 2 course in order to fulfill existing prerequisites for future classes that I need to take for my major']  \n",
       "1                                                                                                                   ['Currently I am taking this Astronomy 116 Lab for the lab credit to fulfill my general education requirements, I found my Astronomy 116 lecture to be more engaging then my Biology course.']  \n",
       "2                                                                                                                                                                                                                                               ['I am here because I want to fulfill all of my GE requirements.']  \n",
       "3                                                                                                                                                                                                                         ['Beside getting the credit and meeting the GE requirement, I have the interest in it.']  \n",
       "4                            ['For this course, I expect to be able to relate the theory that I learn in 230 and be able to understand it in practical terms. I am a computer science major, so besides being required to take the course, I am looking to learn about the process that this course teaches you.']  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (2556, 3) \n",
      "Test dataset shape: (640, 3)\n",
      "Positive labels present in the dataset : 227  out of 2556 or 8.881064162754305%\n",
      "Positive labels present in the test dataset : 57  out of 640 or 8.90625%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset shape: {training_df.shape} \\nTest dataset shape: {test_df.shape}\")\n",
    "pos_labels = len([n for n in training_df['label'] if n==1])\n",
    "print(\"Positive labels present in the dataset : {}  out of {} or {}%\".format(pos_labels, len(training_df['label']), (pos_labels/len(training_df['label']))*100))\n",
    "pos_labels = len([n for n in test_df['label'] if n==1])\n",
    "print(\"Positive labels present in the test dataset : {}  out of {} or {}%\".format(pos_labels, len(test_df['label']), (pos_labels/len(test_df['label']))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the data for the plots\n",
    "training_counts = training_df['label'].value_counts()\n",
    "test_counts = test_df['label'].value_counts()\n",
    "\n",
    "# Set up the subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot for the training set\n",
    "sns.barplot(x=training_counts.index, y=training_counts.values, ax=axes[0])\n",
    "axes[0].set_title('Distribution of labels in training set')\n",
    "axes[0].set_ylabel('Sentences')\n",
    "axes[0].set_xlabel('Label')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot for the test set\n",
    "sns.barplot(x=test_counts.index, y=test_counts.values, ax=axes[1])\n",
    "axes[1].set_title('Distribution of labels in test set')\n",
    "axes[1].set_ylabel('Sentences')\n",
    "axes[1].set_xlabel('Label')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack Obama']\n"
     ]
    }
   ],
   "source": [
    "def get_ner(text):\n",
    "    ner_list = []\n",
    "    # Annotate the text using stanza\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for entity in sentence.ents:\n",
    "            if entity.type == 'PERSON':\n",
    "                ner_list.append(entity.text)\n",
    "\n",
    "    return ner_list\n",
    "\n",
    "# Example usage\n",
    "text = \"Barack Obama was the 44th doctor of the United States.\"\n",
    "print(get_ner(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a named entity is present in the sentence\n",
    "def named_entity_present(sentence):\n",
    "    ner_list = get_ner(sentence)\n",
    "    if len(ner_list) > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Similarity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the similar words and similarity score\n",
    "# The function takes tokens of sentence as input and if its not a stop word, get its similarity with synsets of STEM.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words |= set([\"help\",\"try\", \"work\", \"process\", \"support\", \"job\"] )\n",
    "def word_similarity(tokens, syns, field):    \n",
    "    if field in ['engineering', 'technology']:\n",
    "        score_threshold = 0.5\n",
    "    else:\n",
    "        score_threshold = 0.2\n",
    "    sim_words = 0\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            try:\n",
    "                syns_word = wordnet.synsets(token) \n",
    "                score = syns_word[0].path_similarity(syns[0])\n",
    "                if score >= score_threshold:\n",
    "                    sim_words += 1\n",
    "            except: \n",
    "                score = 0\n",
    "    \n",
    "    return sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to create columns for similarity based on all STEM fields\n",
    "syns_bio = wordnet.synsets(lemmatizer.lemmatize(\"biology\"))\n",
    "syns_maths = wordnet.synsets(lemmatizer.lemmatize(\"mathematics\")) \n",
    "syns_tech = wordnet.synsets(lemmatizer.lemmatize(\"technology\"))\n",
    "syns_eng = wordnet.synsets(lemmatizer.lemmatize(\"engineering\"))\n",
    "syns_chem = wordnet.synsets(lemmatizer.lemmatize(\"chemistry\"))\n",
    "syns_phy = wordnet.synsets(lemmatizer.lemmatize(\"physics\"))\n",
    "syns_sci = wordnet.synsets(lemmatizer.lemmatize(\"science\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Medical Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['interventional', 'liaison', 'gastroenterology', 'immunology', 'cardiac', 'diagnostic', 'gastrointestinal', 'pediatrics', 'electrophysiology', 'neuroradiology', 'geriatric', 'oncology', 'infectious', 'medical', 'anesthesiology', 'rehabilitation', 'head', 'failure', 'behavioral', 'urology', 'transplant', 'dermatology', 'military', 'medicine', 'endocrinology', 'chemical', 'abuse', 'metabolism', 'interventional', 'orbit', 'ocular', 'adolescent', 'clinical', 'reproductive', 'sports', 'calculi', 'emergency', 'fetal', 'neonatal', 'perinatal', 'glaucoma', 'public', 'consultation', 'cardiothoracic', 'cytogenetics', 'reconstructive', 'hematology', 'maternal', 'reconstructive', 'cytopathology', 'critical', 'child', 'microbiology', 'neck', 'psychiatry', 'ophthalmology', 'forensic', 'molecular', 'cardiovascular', 'diabetes', 'pediatric', 'oncology', 'genetics', 'radiology', 'infectious', 'procedural', 'neuro', 'advanced', 'neuromuscular', 'uveitis', 'preventive', 'neurology', 'gynecologic', 'neuropathology', 'strabismus', 'brain', 'rheumatology', 'disabilities', 'endovascular', 'radiation', 'rheumatology', 'pelvic', 'cornea', 'sleep', 'disease', 'surgery', 'genitourinary', 'sports', 'plastic', 'retardation', 'heart', 'community', 'research', 'physical', 'hospice', 'administrative', 'hematology', 'anterior', 'neurology', 'obstetrics', 'toxicology', 'pain', 'pediatric', 'psychosomatic', 'transplant', 'gynecology', 'health', 'pathology', 'neurourology', 'musculoskeletal', 'anesthesiology', 'surgical', 'critical', 'dermatology', 'hepatology', 'cardiology', 'female', 'neurodevelopmental', 'pulmonology', 'endocrinologists', 'breast', 'infertility', 'imaging', 'internal', 'gastroenterology', 'allergy', 'psychiatry', 'and', 'nephrology', 'ophthalmic', 'pulmonary', 'palliative', 'banking', 'family', 'genetic', 'blood', 'biochemical', 'occupational', 'nuclear', 'pediatrics', 'dermatopathology', 'abdominal', 'vascular', 'nephrology', 'care', 'aerospace', 'psychiatric', 'retina', 'urology', 'immunopathology', 'neuroradiology', 'genetic', 'developmental', 'neurophysiology', 'male', 'renal', 'addiction', 'pathology', 'ophthalmology', 'surgery', 'chest', 'transfusion', 'endocrinology', 'urologic', 'internal', 'segment', 'oculoplastics', 'anatomical', 'diseases', 'mental', 'injury', 'adolescent']\n"
     ]
    }
   ],
   "source": [
    "# Load the medical specialization text file and create a list\n",
    "medical_list = []\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/medical_specialities.txt', 'r') as medical_fields:\n",
    "    for line in medical_fields.readlines():\n",
    "        special_field = line.rstrip('\\n')\n",
    "        special_field = re.sub(\"\\W\",\" \", special_field )\n",
    "#         print(special_field)\n",
    "        medical_list += special_field.split()\n",
    "medical_list = list(set(medical_list))  \n",
    "medical_list = [x.lower() for x in medical_list]\n",
    "print(medical_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get medical words\n",
    "def check_medical_words(tokens):\n",
    "    for token in tokens:\n",
    "        if token not in stop_words and token in [x.lower() for x in medical_list]:\n",
    "            return 1\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sentiment Polarity and Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get polarity and subjectivity of the sentence using TexBlob\n",
    "def get_sentiment(sentence):\n",
    "    sentiments =TextBlob(sentence).sentiment\n",
    "    polarity = sentiments.polarity\n",
    "    subjectivity = sentiments.subjectivity\n",
    "    return polarity, subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the count of POS tags of the sentence\n",
    "def count_pos_tags(tokens):\n",
    "    token_pos = pos_tag(tokens)\n",
    "    count = Counter(tag for word,tag in token_pos)\n",
    "    interjections =  count['UH']\n",
    "    nouns = count['NN'] + count['NNS'] + count['NNP'] + count['NNPS']\n",
    "    adverb = count['RB'] + count['RBS'] + count['RBR']\n",
    "    verb = count['VB'] + count['VBD'] + count['VBG'] + count['VBN']\n",
    "    determiner = count['DT']\n",
    "    pronoun = count['PRP']\n",
    "    adjetive = count['JJ'] + count['JJR'] + count['JJS']\n",
    "    preposition = count['IN']\n",
    "    return interjections, nouns, adverb, verb, determiner, pronoun, adjetive,preposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_extraction(dataframe, field, func, column_names):\n",
    "    return pd.concat((\n",
    "        dataframe,\n",
    "        dataframe[field].apply(\n",
    "            lambda cell: pd.Series(func(cell), index=column_names))), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the w2v dict from pickle file\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/pickle/embeddings06122024.pickle', 'rb') as w2v_file:\n",
    "    w2v_dict = pickle.load(w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of word embeddings:  4762\n"
     ]
    }
   ],
   "source": [
    "print(\"length of word embeddings: \", len(w2v_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vectors for the essay\n",
    "def vectorizer(sequence):\n",
    "    vect = []\n",
    "    numw = 0\n",
    "    for w in sequence: \n",
    "        try :\n",
    "            if numw == 0:\n",
    "                vect = w2v_dict[w]\n",
    "            else:\n",
    "                vect = np.add(vect, w2v_dict[w])\n",
    "            numw += 1\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    return vect/ numw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into words\n",
    "def split_into_words(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vectorizer\n",
    "unigram_vect = CountVectorizer(ngram_range=(1, 1), min_df=2, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Putting them all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for feature engineering\n",
    "def feature_engineering(original_dataset):\n",
    "\n",
    "    dataset = original_dataset.copy()\n",
    "    # create a new column with sentence tokens\n",
    "    dataset['tokens'] = dataset['sentence'].apply(word_tokenize)\n",
    "    # 1. Similarity features\n",
    "    # biology\n",
    "    dataset['bio_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_bio,'biology',)) \n",
    "    # chemistry\n",
    "    dataset['chem_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_chem,'chemistry',))\n",
    "    # physics\n",
    "    dataset['phy_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_phy,'physics',))\n",
    "    # mathematics\n",
    "    dataset['math_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_maths,'mathematics',))\n",
    "    # technology\n",
    "    dataset['tech_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_tech,'technology',))\n",
    "    # engineering\n",
    "    dataset['eng_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_eng,'engineering',))\n",
    "    \n",
    "    # medical terms\n",
    "    dataset['medical_terms'] = dataset['tokens'].apply(check_medical_words)\n",
    "    \n",
    "    # polarity and subjectivity\n",
    "    dataset['polarity'], dataset['subjectivity'] = zip(*dataset['sentence'].apply(get_sentiment))\n",
    "    \n",
    "    # named entity recognition\n",
    "    dataset['ner'] = dataset['sentence'].apply(named_entity_present)\n",
    "    \n",
    "    # pos tag count\n",
    "    dataset = pos_tag_extraction(dataset, 'tokens', count_pos_tags, ['interjections', 'nouns', 'adverb', 'verb', 'determiner', 'pronoun', 'adjetive','preposition'])\n",
    "    \n",
    "    # labels\n",
    "    data_labels = dataset['label']\n",
    "    # X\n",
    "    data_x = dataset.drop(columns='label')\n",
    "\n",
    "    \n",
    "    # vectorize all the essays\n",
    "    vect_arr = data_x.tokens.apply(vectorizer)\n",
    "    for index in range(0, len(vect_arr)):\n",
    "        i = 0\n",
    "        for item in vect_arr[index]:\n",
    "            column_name= \"embedding\" + str(i)\n",
    "            data_x.loc[index, column_name] = item\n",
    "            i +=1\n",
    "    \n",
    "    return data_x,data_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = feature_engineering(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2556, 121)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = feature_engineering(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 121)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Unigram features for both train and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2556, 121)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 121)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Navigational/saved_features/X_test_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Navigational/saved_features/X_train_final.csv\", index=False)\n",
    "X_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Navigational/saved_features/X_test_final.csv\", index=False)\n",
    "y_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Navigational/saved_features/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Navigational/saved_features/y_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the unigram df for train :  (2556, 1600)\n"
     ]
    }
   ],
   "source": [
    "# Unigrams for training set\n",
    "unigram_matrix = unigram_vect.fit_transform(X_train['sentence'])\n",
    "unigrams = pd.DataFrame(unigram_matrix.toarray())\n",
    "print(\"Shape of the unigram df for train : \",unigrams.shape)\n",
    "unigrams = unigrams.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = pd.concat([X_train, unigrams], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final.columns = X_train_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2556, 1721)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test unigram df shape :  (640, 1600)\n"
     ]
    }
   ],
   "source": [
    "unigram_matrix_test = unigram_vect.transform(X_test['sentence'])\n",
    "unigrams_test = pd.DataFrame(unigram_matrix_test.toarray())\n",
    "unigrams_test = unigrams_test.reset_index(drop=True)\n",
    "print(\"Test unigram df shape : \",unigrams_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 1721)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final = pd.concat([X_test, unigrams_test], axis = 1)\n",
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final.columns = X_test_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 1721)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ---- sentence\n",
      "1 ---- phrase\n",
      "2 ---- tokens\n",
      "3 ---- bio_sim_words\n",
      "4 ---- chem_sim_words\n",
      "5 ---- phy_sim_words\n",
      "6 ---- math_sim_words\n",
      "7 ---- tech_sim_words\n",
      "8 ---- eng_sim_words\n",
      "9 ---- medical_terms\n",
      "10 ---- polarity\n",
      "11 ---- subjectivity\n",
      "12 ---- ner\n",
      "13 ---- interjections\n",
      "14 ---- nouns\n",
      "15 ---- adverb\n",
      "16 ---- verb\n",
      "17 ---- determiner\n",
      "18 ---- pronoun\n",
      "19 ---- adjetive\n",
      "20 ---- preposition\n",
      "21 ---- embedding0\n",
      "22 ---- embedding1\n",
      "23 ---- embedding2\n",
      "24 ---- embedding3\n",
      "25 ---- embedding4\n",
      "26 ---- embedding5\n",
      "27 ---- embedding6\n",
      "28 ---- embedding7\n",
      "29 ---- embedding8\n",
      "30 ---- embedding9\n",
      "31 ---- embedding10\n",
      "32 ---- embedding11\n",
      "33 ---- embedding12\n",
      "34 ---- embedding13\n",
      "35 ---- embedding14\n",
      "36 ---- embedding15\n",
      "37 ---- embedding16\n",
      "38 ---- embedding17\n",
      "39 ---- embedding18\n",
      "40 ---- embedding19\n",
      "41 ---- embedding20\n",
      "42 ---- embedding21\n",
      "43 ---- embedding22\n",
      "44 ---- embedding23\n",
      "45 ---- embedding24\n",
      "46 ---- embedding25\n",
      "47 ---- embedding26\n",
      "48 ---- embedding27\n",
      "49 ---- embedding28\n",
      "50 ---- embedding29\n",
      "51 ---- embedding30\n",
      "52 ---- embedding31\n",
      "53 ---- embedding32\n",
      "54 ---- embedding33\n",
      "55 ---- embedding34\n",
      "56 ---- embedding35\n",
      "57 ---- embedding36\n",
      "58 ---- embedding37\n",
      "59 ---- embedding38\n",
      "60 ---- embedding39\n",
      "61 ---- embedding40\n",
      "62 ---- embedding41\n",
      "63 ---- embedding42\n",
      "64 ---- embedding43\n",
      "65 ---- embedding44\n",
      "66 ---- embedding45\n",
      "67 ---- embedding46\n",
      "68 ---- embedding47\n",
      "69 ---- embedding48\n",
      "70 ---- embedding49\n",
      "71 ---- embedding50\n",
      "72 ---- embedding51\n",
      "73 ---- embedding52\n",
      "74 ---- embedding53\n",
      "75 ---- embedding54\n",
      "76 ---- embedding55\n",
      "77 ---- embedding56\n",
      "78 ---- embedding57\n",
      "79 ---- embedding58\n",
      "80 ---- embedding59\n",
      "81 ---- embedding60\n",
      "82 ---- embedding61\n",
      "83 ---- embedding62\n",
      "84 ---- embedding63\n",
      "85 ---- embedding64\n",
      "86 ---- embedding65\n",
      "87 ---- embedding66\n",
      "88 ---- embedding67\n",
      "89 ---- embedding68\n",
      "90 ---- embedding69\n",
      "91 ---- embedding70\n",
      "92 ---- embedding71\n",
      "93 ---- embedding72\n",
      "94 ---- embedding73\n",
      "95 ---- embedding74\n",
      "96 ---- embedding75\n",
      "97 ---- embedding76\n",
      "98 ---- embedding77\n",
      "99 ---- embedding78\n",
      "100 ---- embedding79\n",
      "101 ---- embedding80\n",
      "102 ---- embedding81\n",
      "103 ---- embedding82\n",
      "104 ---- embedding83\n",
      "105 ---- embedding84\n",
      "106 ---- embedding85\n",
      "107 ---- embedding86\n",
      "108 ---- embedding87\n",
      "109 ---- embedding88\n",
      "110 ---- embedding89\n",
      "111 ---- embedding90\n",
      "112 ---- embedding91\n",
      "113 ---- embedding92\n",
      "114 ---- embedding93\n",
      "115 ---- embedding94\n",
      "116 ---- embedding95\n",
      "117 ---- embedding96\n",
      "118 ---- embedding97\n",
      "119 ---- embedding98\n",
      "120 ---- embedding99\n",
      "121 ---- 0\n",
      "122 ---- 1\n",
      "123 ---- 2\n",
      "124 ---- 3\n",
      "125 ---- 4\n",
      "126 ---- 5\n",
      "127 ---- 6\n",
      "128 ---- 7\n",
      "129 ---- 8\n",
      "130 ---- 9\n",
      "131 ---- 10\n",
      "132 ---- 11\n",
      "133 ---- 12\n",
      "134 ---- 13\n",
      "135 ---- 14\n",
      "136 ---- 15\n",
      "137 ---- 16\n",
      "138 ---- 17\n",
      "139 ---- 18\n",
      "140 ---- 19\n",
      "141 ---- 20\n",
      "142 ---- 21\n",
      "143 ---- 22\n",
      "144 ---- 23\n",
      "145 ---- 24\n",
      "146 ---- 25\n",
      "147 ---- 26\n",
      "148 ---- 27\n",
      "149 ---- 28\n",
      "150 ---- 29\n",
      "151 ---- 30\n",
      "152 ---- 31\n",
      "153 ---- 32\n",
      "154 ---- 33\n",
      "155 ---- 34\n",
      "156 ---- 35\n",
      "157 ---- 36\n",
      "158 ---- 37\n",
      "159 ---- 38\n",
      "160 ---- 39\n",
      "161 ---- 40\n",
      "162 ---- 41\n",
      "163 ---- 42\n",
      "164 ---- 43\n",
      "165 ---- 44\n",
      "166 ---- 45\n",
      "167 ---- 46\n",
      "168 ---- 47\n",
      "169 ---- 48\n",
      "170 ---- 49\n",
      "171 ---- 50\n",
      "172 ---- 51\n",
      "173 ---- 52\n",
      "174 ---- 53\n",
      "175 ---- 54\n",
      "176 ---- 55\n",
      "177 ---- 56\n",
      "178 ---- 57\n",
      "179 ---- 58\n",
      "180 ---- 59\n",
      "181 ---- 60\n",
      "182 ---- 61\n",
      "183 ---- 62\n",
      "184 ---- 63\n",
      "185 ---- 64\n",
      "186 ---- 65\n",
      "187 ---- 66\n",
      "188 ---- 67\n",
      "189 ---- 68\n",
      "190 ---- 69\n",
      "191 ---- 70\n",
      "192 ---- 71\n",
      "193 ---- 72\n",
      "194 ---- 73\n",
      "195 ---- 74\n",
      "196 ---- 75\n",
      "197 ---- 76\n",
      "198 ---- 77\n",
      "199 ---- 78\n",
      "200 ---- 79\n",
      "201 ---- 80\n",
      "202 ---- 81\n",
      "203 ---- 82\n",
      "204 ---- 83\n",
      "205 ---- 84\n",
      "206 ---- 85\n",
      "207 ---- 86\n",
      "208 ---- 87\n",
      "209 ---- 88\n",
      "210 ---- 89\n",
      "211 ---- 90\n",
      "212 ---- 91\n",
      "213 ---- 92\n",
      "214 ---- 93\n",
      "215 ---- 94\n",
      "216 ---- 95\n",
      "217 ---- 96\n",
      "218 ---- 97\n",
      "219 ---- 98\n",
      "220 ---- 99\n",
      "221 ---- 100\n",
      "222 ---- 101\n",
      "223 ---- 102\n",
      "224 ---- 103\n",
      "225 ---- 104\n",
      "226 ---- 105\n",
      "227 ---- 106\n",
      "228 ---- 107\n",
      "229 ---- 108\n",
      "230 ---- 109\n",
      "231 ---- 110\n",
      "232 ---- 111\n",
      "233 ---- 112\n",
      "234 ---- 113\n",
      "235 ---- 114\n",
      "236 ---- 115\n",
      "237 ---- 116\n",
      "238 ---- 117\n",
      "239 ---- 118\n",
      "240 ---- 119\n",
      "241 ---- 120\n",
      "242 ---- 121\n",
      "243 ---- 122\n",
      "244 ---- 123\n",
      "245 ---- 124\n",
      "246 ---- 125\n",
      "247 ---- 126\n",
      "248 ---- 127\n",
      "249 ---- 128\n",
      "250 ---- 129\n",
      "251 ---- 130\n",
      "252 ---- 131\n",
      "253 ---- 132\n",
      "254 ---- 133\n",
      "255 ---- 134\n",
      "256 ---- 135\n",
      "257 ---- 136\n",
      "258 ---- 137\n",
      "259 ---- 138\n",
      "260 ---- 139\n",
      "261 ---- 140\n",
      "262 ---- 141\n",
      "263 ---- 142\n",
      "264 ---- 143\n",
      "265 ---- 144\n",
      "266 ---- 145\n",
      "267 ---- 146\n",
      "268 ---- 147\n",
      "269 ---- 148\n",
      "270 ---- 149\n",
      "271 ---- 150\n",
      "272 ---- 151\n",
      "273 ---- 152\n",
      "274 ---- 153\n",
      "275 ---- 154\n",
      "276 ---- 155\n",
      "277 ---- 156\n",
      "278 ---- 157\n",
      "279 ---- 158\n",
      "280 ---- 159\n",
      "281 ---- 160\n",
      "282 ---- 161\n",
      "283 ---- 162\n",
      "284 ---- 163\n",
      "285 ---- 164\n",
      "286 ---- 165\n",
      "287 ---- 166\n",
      "288 ---- 167\n",
      "289 ---- 168\n",
      "290 ---- 169\n",
      "291 ---- 170\n",
      "292 ---- 171\n",
      "293 ---- 172\n",
      "294 ---- 173\n",
      "295 ---- 174\n",
      "296 ---- 175\n",
      "297 ---- 176\n",
      "298 ---- 177\n",
      "299 ---- 178\n",
      "300 ---- 179\n",
      "301 ---- 180\n",
      "302 ---- 181\n",
      "303 ---- 182\n",
      "304 ---- 183\n",
      "305 ---- 184\n",
      "306 ---- 185\n",
      "307 ---- 186\n",
      "308 ---- 187\n",
      "309 ---- 188\n",
      "310 ---- 189\n",
      "311 ---- 190\n",
      "312 ---- 191\n",
      "313 ---- 192\n",
      "314 ---- 193\n",
      "315 ---- 194\n",
      "316 ---- 195\n",
      "317 ---- 196\n",
      "318 ---- 197\n",
      "319 ---- 198\n",
      "320 ---- 199\n",
      "321 ---- 200\n",
      "322 ---- 201\n",
      "323 ---- 202\n",
      "324 ---- 203\n",
      "325 ---- 204\n",
      "326 ---- 205\n",
      "327 ---- 206\n",
      "328 ---- 207\n",
      "329 ---- 208\n",
      "330 ---- 209\n",
      "331 ---- 210\n",
      "332 ---- 211\n",
      "333 ---- 212\n",
      "334 ---- 213\n",
      "335 ---- 214\n",
      "336 ---- 215\n",
      "337 ---- 216\n",
      "338 ---- 217\n",
      "339 ---- 218\n",
      "340 ---- 219\n",
      "341 ---- 220\n",
      "342 ---- 221\n",
      "343 ---- 222\n",
      "344 ---- 223\n",
      "345 ---- 224\n",
      "346 ---- 225\n",
      "347 ---- 226\n",
      "348 ---- 227\n",
      "349 ---- 228\n",
      "350 ---- 229\n",
      "351 ---- 230\n",
      "352 ---- 231\n",
      "353 ---- 232\n",
      "354 ---- 233\n",
      "355 ---- 234\n",
      "356 ---- 235\n",
      "357 ---- 236\n",
      "358 ---- 237\n",
      "359 ---- 238\n",
      "360 ---- 239\n",
      "361 ---- 240\n",
      "362 ---- 241\n",
      "363 ---- 242\n",
      "364 ---- 243\n",
      "365 ---- 244\n",
      "366 ---- 245\n",
      "367 ---- 246\n",
      "368 ---- 247\n",
      "369 ---- 248\n",
      "370 ---- 249\n",
      "371 ---- 250\n",
      "372 ---- 251\n",
      "373 ---- 252\n",
      "374 ---- 253\n",
      "375 ---- 254\n",
      "376 ---- 255\n",
      "377 ---- 256\n",
      "378 ---- 257\n",
      "379 ---- 258\n",
      "380 ---- 259\n",
      "381 ---- 260\n",
      "382 ---- 261\n",
      "383 ---- 262\n",
      "384 ---- 263\n",
      "385 ---- 264\n",
      "386 ---- 265\n",
      "387 ---- 266\n",
      "388 ---- 267\n",
      "389 ---- 268\n",
      "390 ---- 269\n",
      "391 ---- 270\n",
      "392 ---- 271\n",
      "393 ---- 272\n",
      "394 ---- 273\n",
      "395 ---- 274\n",
      "396 ---- 275\n",
      "397 ---- 276\n",
      "398 ---- 277\n",
      "399 ---- 278\n",
      "400 ---- 279\n",
      "401 ---- 280\n",
      "402 ---- 281\n",
      "403 ---- 282\n",
      "404 ---- 283\n",
      "405 ---- 284\n",
      "406 ---- 285\n",
      "407 ---- 286\n",
      "408 ---- 287\n",
      "409 ---- 288\n",
      "410 ---- 289\n",
      "411 ---- 290\n",
      "412 ---- 291\n",
      "413 ---- 292\n",
      "414 ---- 293\n",
      "415 ---- 294\n",
      "416 ---- 295\n",
      "417 ---- 296\n",
      "418 ---- 297\n",
      "419 ---- 298\n",
      "420 ---- 299\n",
      "421 ---- 300\n",
      "422 ---- 301\n",
      "423 ---- 302\n",
      "424 ---- 303\n",
      "425 ---- 304\n",
      "426 ---- 305\n",
      "427 ---- 306\n",
      "428 ---- 307\n",
      "429 ---- 308\n",
      "430 ---- 309\n",
      "431 ---- 310\n",
      "432 ---- 311\n",
      "433 ---- 312\n",
      "434 ---- 313\n",
      "435 ---- 314\n",
      "436 ---- 315\n",
      "437 ---- 316\n",
      "438 ---- 317\n",
      "439 ---- 318\n",
      "440 ---- 319\n",
      "441 ---- 320\n",
      "442 ---- 321\n",
      "443 ---- 322\n",
      "444 ---- 323\n",
      "445 ---- 324\n",
      "446 ---- 325\n",
      "447 ---- 326\n",
      "448 ---- 327\n",
      "449 ---- 328\n",
      "450 ---- 329\n",
      "451 ---- 330\n",
      "452 ---- 331\n",
      "453 ---- 332\n",
      "454 ---- 333\n",
      "455 ---- 334\n",
      "456 ---- 335\n",
      "457 ---- 336\n",
      "458 ---- 337\n",
      "459 ---- 338\n",
      "460 ---- 339\n",
      "461 ---- 340\n",
      "462 ---- 341\n",
      "463 ---- 342\n",
      "464 ---- 343\n",
      "465 ---- 344\n",
      "466 ---- 345\n",
      "467 ---- 346\n",
      "468 ---- 347\n",
      "469 ---- 348\n",
      "470 ---- 349\n",
      "471 ---- 350\n",
      "472 ---- 351\n",
      "473 ---- 352\n",
      "474 ---- 353\n",
      "475 ---- 354\n",
      "476 ---- 355\n",
      "477 ---- 356\n",
      "478 ---- 357\n",
      "479 ---- 358\n",
      "480 ---- 359\n",
      "481 ---- 360\n",
      "482 ---- 361\n",
      "483 ---- 362\n",
      "484 ---- 363\n",
      "485 ---- 364\n",
      "486 ---- 365\n",
      "487 ---- 366\n",
      "488 ---- 367\n",
      "489 ---- 368\n",
      "490 ---- 369\n",
      "491 ---- 370\n",
      "492 ---- 371\n",
      "493 ---- 372\n",
      "494 ---- 373\n",
      "495 ---- 374\n",
      "496 ---- 375\n",
      "497 ---- 376\n",
      "498 ---- 377\n",
      "499 ---- 378\n",
      "500 ---- 379\n",
      "501 ---- 380\n",
      "502 ---- 381\n",
      "503 ---- 382\n",
      "504 ---- 383\n",
      "505 ---- 384\n",
      "506 ---- 385\n",
      "507 ---- 386\n",
      "508 ---- 387\n",
      "509 ---- 388\n",
      "510 ---- 389\n",
      "511 ---- 390\n",
      "512 ---- 391\n",
      "513 ---- 392\n",
      "514 ---- 393\n",
      "515 ---- 394\n",
      "516 ---- 395\n",
      "517 ---- 396\n",
      "518 ---- 397\n",
      "519 ---- 398\n",
      "520 ---- 399\n",
      "521 ---- 400\n",
      "522 ---- 401\n",
      "523 ---- 402\n",
      "524 ---- 403\n",
      "525 ---- 404\n",
      "526 ---- 405\n",
      "527 ---- 406\n",
      "528 ---- 407\n",
      "529 ---- 408\n",
      "530 ---- 409\n",
      "531 ---- 410\n",
      "532 ---- 411\n",
      "533 ---- 412\n",
      "534 ---- 413\n",
      "535 ---- 414\n",
      "536 ---- 415\n",
      "537 ---- 416\n",
      "538 ---- 417\n",
      "539 ---- 418\n",
      "540 ---- 419\n",
      "541 ---- 420\n",
      "542 ---- 421\n",
      "543 ---- 422\n",
      "544 ---- 423\n",
      "545 ---- 424\n",
      "546 ---- 425\n",
      "547 ---- 426\n",
      "548 ---- 427\n",
      "549 ---- 428\n",
      "550 ---- 429\n",
      "551 ---- 430\n",
      "552 ---- 431\n",
      "553 ---- 432\n",
      "554 ---- 433\n",
      "555 ---- 434\n",
      "556 ---- 435\n",
      "557 ---- 436\n",
      "558 ---- 437\n",
      "559 ---- 438\n",
      "560 ---- 439\n",
      "561 ---- 440\n",
      "562 ---- 441\n",
      "563 ---- 442\n",
      "564 ---- 443\n",
      "565 ---- 444\n",
      "566 ---- 445\n",
      "567 ---- 446\n",
      "568 ---- 447\n",
      "569 ---- 448\n",
      "570 ---- 449\n",
      "571 ---- 450\n",
      "572 ---- 451\n",
      "573 ---- 452\n",
      "574 ---- 453\n",
      "575 ---- 454\n",
      "576 ---- 455\n",
      "577 ---- 456\n",
      "578 ---- 457\n",
      "579 ---- 458\n",
      "580 ---- 459\n",
      "581 ---- 460\n",
      "582 ---- 461\n",
      "583 ---- 462\n",
      "584 ---- 463\n",
      "585 ---- 464\n",
      "586 ---- 465\n",
      "587 ---- 466\n",
      "588 ---- 467\n",
      "589 ---- 468\n",
      "590 ---- 469\n",
      "591 ---- 470\n",
      "592 ---- 471\n",
      "593 ---- 472\n",
      "594 ---- 473\n",
      "595 ---- 474\n",
      "596 ---- 475\n",
      "597 ---- 476\n",
      "598 ---- 477\n",
      "599 ---- 478\n",
      "600 ---- 479\n",
      "601 ---- 480\n",
      "602 ---- 481\n",
      "603 ---- 482\n",
      "604 ---- 483\n",
      "605 ---- 484\n",
      "606 ---- 485\n",
      "607 ---- 486\n",
      "608 ---- 487\n",
      "609 ---- 488\n",
      "610 ---- 489\n",
      "611 ---- 490\n",
      "612 ---- 491\n",
      "613 ---- 492\n",
      "614 ---- 493\n",
      "615 ---- 494\n",
      "616 ---- 495\n",
      "617 ---- 496\n",
      "618 ---- 497\n",
      "619 ---- 498\n",
      "620 ---- 499\n",
      "621 ---- 500\n",
      "622 ---- 501\n",
      "623 ---- 502\n",
      "624 ---- 503\n",
      "625 ---- 504\n",
      "626 ---- 505\n",
      "627 ---- 506\n",
      "628 ---- 507\n",
      "629 ---- 508\n",
      "630 ---- 509\n",
      "631 ---- 510\n",
      "632 ---- 511\n",
      "633 ---- 512\n",
      "634 ---- 513\n",
      "635 ---- 514\n",
      "636 ---- 515\n",
      "637 ---- 516\n",
      "638 ---- 517\n",
      "639 ---- 518\n",
      "640 ---- 519\n",
      "641 ---- 520\n",
      "642 ---- 521\n",
      "643 ---- 522\n",
      "644 ---- 523\n",
      "645 ---- 524\n",
      "646 ---- 525\n",
      "647 ---- 526\n",
      "648 ---- 527\n",
      "649 ---- 528\n",
      "650 ---- 529\n",
      "651 ---- 530\n",
      "652 ---- 531\n",
      "653 ---- 532\n",
      "654 ---- 533\n",
      "655 ---- 534\n",
      "656 ---- 535\n",
      "657 ---- 536\n",
      "658 ---- 537\n",
      "659 ---- 538\n",
      "660 ---- 539\n",
      "661 ---- 540\n",
      "662 ---- 541\n",
      "663 ---- 542\n",
      "664 ---- 543\n",
      "665 ---- 544\n",
      "666 ---- 545\n",
      "667 ---- 546\n",
      "668 ---- 547\n",
      "669 ---- 548\n",
      "670 ---- 549\n",
      "671 ---- 550\n",
      "672 ---- 551\n",
      "673 ---- 552\n",
      "674 ---- 553\n",
      "675 ---- 554\n",
      "676 ---- 555\n",
      "677 ---- 556\n",
      "678 ---- 557\n",
      "679 ---- 558\n",
      "680 ---- 559\n",
      "681 ---- 560\n",
      "682 ---- 561\n",
      "683 ---- 562\n",
      "684 ---- 563\n",
      "685 ---- 564\n",
      "686 ---- 565\n",
      "687 ---- 566\n",
      "688 ---- 567\n",
      "689 ---- 568\n",
      "690 ---- 569\n",
      "691 ---- 570\n",
      "692 ---- 571\n",
      "693 ---- 572\n",
      "694 ---- 573\n",
      "695 ---- 574\n",
      "696 ---- 575\n",
      "697 ---- 576\n",
      "698 ---- 577\n",
      "699 ---- 578\n",
      "700 ---- 579\n",
      "701 ---- 580\n",
      "702 ---- 581\n",
      "703 ---- 582\n",
      "704 ---- 583\n",
      "705 ---- 584\n",
      "706 ---- 585\n",
      "707 ---- 586\n",
      "708 ---- 587\n",
      "709 ---- 588\n",
      "710 ---- 589\n",
      "711 ---- 590\n",
      "712 ---- 591\n",
      "713 ---- 592\n",
      "714 ---- 593\n",
      "715 ---- 594\n",
      "716 ---- 595\n",
      "717 ---- 596\n",
      "718 ---- 597\n",
      "719 ---- 598\n",
      "720 ---- 599\n",
      "721 ---- 600\n",
      "722 ---- 601\n",
      "723 ---- 602\n",
      "724 ---- 603\n",
      "725 ---- 604\n",
      "726 ---- 605\n",
      "727 ---- 606\n",
      "728 ---- 607\n",
      "729 ---- 608\n",
      "730 ---- 609\n",
      "731 ---- 610\n",
      "732 ---- 611\n",
      "733 ---- 612\n",
      "734 ---- 613\n",
      "735 ---- 614\n",
      "736 ---- 615\n",
      "737 ---- 616\n",
      "738 ---- 617\n",
      "739 ---- 618\n",
      "740 ---- 619\n",
      "741 ---- 620\n",
      "742 ---- 621\n",
      "743 ---- 622\n",
      "744 ---- 623\n",
      "745 ---- 624\n",
      "746 ---- 625\n",
      "747 ---- 626\n",
      "748 ---- 627\n",
      "749 ---- 628\n",
      "750 ---- 629\n",
      "751 ---- 630\n",
      "752 ---- 631\n",
      "753 ---- 632\n",
      "754 ---- 633\n",
      "755 ---- 634\n",
      "756 ---- 635\n",
      "757 ---- 636\n",
      "758 ---- 637\n",
      "759 ---- 638\n",
      "760 ---- 639\n",
      "761 ---- 640\n",
      "762 ---- 641\n",
      "763 ---- 642\n",
      "764 ---- 643\n",
      "765 ---- 644\n",
      "766 ---- 645\n",
      "767 ---- 646\n",
      "768 ---- 647\n",
      "769 ---- 648\n",
      "770 ---- 649\n",
      "771 ---- 650\n",
      "772 ---- 651\n",
      "773 ---- 652\n",
      "774 ---- 653\n",
      "775 ---- 654\n",
      "776 ---- 655\n",
      "777 ---- 656\n",
      "778 ---- 657\n",
      "779 ---- 658\n",
      "780 ---- 659\n",
      "781 ---- 660\n",
      "782 ---- 661\n",
      "783 ---- 662\n",
      "784 ---- 663\n",
      "785 ---- 664\n",
      "786 ---- 665\n",
      "787 ---- 666\n",
      "788 ---- 667\n",
      "789 ---- 668\n",
      "790 ---- 669\n",
      "791 ---- 670\n",
      "792 ---- 671\n",
      "793 ---- 672\n",
      "794 ---- 673\n",
      "795 ---- 674\n",
      "796 ---- 675\n",
      "797 ---- 676\n",
      "798 ---- 677\n",
      "799 ---- 678\n",
      "800 ---- 679\n",
      "801 ---- 680\n",
      "802 ---- 681\n",
      "803 ---- 682\n",
      "804 ---- 683\n",
      "805 ---- 684\n",
      "806 ---- 685\n",
      "807 ---- 686\n",
      "808 ---- 687\n",
      "809 ---- 688\n",
      "810 ---- 689\n",
      "811 ---- 690\n",
      "812 ---- 691\n",
      "813 ---- 692\n",
      "814 ---- 693\n",
      "815 ---- 694\n",
      "816 ---- 695\n",
      "817 ---- 696\n",
      "818 ---- 697\n",
      "819 ---- 698\n",
      "820 ---- 699\n",
      "821 ---- 700\n",
      "822 ---- 701\n",
      "823 ---- 702\n",
      "824 ---- 703\n",
      "825 ---- 704\n",
      "826 ---- 705\n",
      "827 ---- 706\n",
      "828 ---- 707\n",
      "829 ---- 708\n",
      "830 ---- 709\n",
      "831 ---- 710\n",
      "832 ---- 711\n",
      "833 ---- 712\n",
      "834 ---- 713\n",
      "835 ---- 714\n",
      "836 ---- 715\n",
      "837 ---- 716\n",
      "838 ---- 717\n",
      "839 ---- 718\n",
      "840 ---- 719\n",
      "841 ---- 720\n",
      "842 ---- 721\n",
      "843 ---- 722\n",
      "844 ---- 723\n",
      "845 ---- 724\n",
      "846 ---- 725\n",
      "847 ---- 726\n",
      "848 ---- 727\n",
      "849 ---- 728\n",
      "850 ---- 729\n",
      "851 ---- 730\n",
      "852 ---- 731\n",
      "853 ---- 732\n",
      "854 ---- 733\n",
      "855 ---- 734\n",
      "856 ---- 735\n",
      "857 ---- 736\n",
      "858 ---- 737\n",
      "859 ---- 738\n",
      "860 ---- 739\n",
      "861 ---- 740\n",
      "862 ---- 741\n",
      "863 ---- 742\n",
      "864 ---- 743\n",
      "865 ---- 744\n",
      "866 ---- 745\n",
      "867 ---- 746\n",
      "868 ---- 747\n",
      "869 ---- 748\n",
      "870 ---- 749\n",
      "871 ---- 750\n",
      "872 ---- 751\n",
      "873 ---- 752\n",
      "874 ---- 753\n",
      "875 ---- 754\n",
      "876 ---- 755\n",
      "877 ---- 756\n",
      "878 ---- 757\n",
      "879 ---- 758\n",
      "880 ---- 759\n",
      "881 ---- 760\n",
      "882 ---- 761\n",
      "883 ---- 762\n",
      "884 ---- 763\n",
      "885 ---- 764\n",
      "886 ---- 765\n",
      "887 ---- 766\n",
      "888 ---- 767\n",
      "889 ---- 768\n",
      "890 ---- 769\n",
      "891 ---- 770\n",
      "892 ---- 771\n",
      "893 ---- 772\n",
      "894 ---- 773\n",
      "895 ---- 774\n",
      "896 ---- 775\n",
      "897 ---- 776\n",
      "898 ---- 777\n",
      "899 ---- 778\n",
      "900 ---- 779\n",
      "901 ---- 780\n",
      "902 ---- 781\n",
      "903 ---- 782\n",
      "904 ---- 783\n",
      "905 ---- 784\n",
      "906 ---- 785\n",
      "907 ---- 786\n",
      "908 ---- 787\n",
      "909 ---- 788\n",
      "910 ---- 789\n",
      "911 ---- 790\n",
      "912 ---- 791\n",
      "913 ---- 792\n",
      "914 ---- 793\n",
      "915 ---- 794\n",
      "916 ---- 795\n",
      "917 ---- 796\n",
      "918 ---- 797\n",
      "919 ---- 798\n",
      "920 ---- 799\n",
      "921 ---- 800\n",
      "922 ---- 801\n",
      "923 ---- 802\n",
      "924 ---- 803\n",
      "925 ---- 804\n",
      "926 ---- 805\n",
      "927 ---- 806\n",
      "928 ---- 807\n",
      "929 ---- 808\n",
      "930 ---- 809\n",
      "931 ---- 810\n",
      "932 ---- 811\n",
      "933 ---- 812\n",
      "934 ---- 813\n",
      "935 ---- 814\n",
      "936 ---- 815\n",
      "937 ---- 816\n",
      "938 ---- 817\n",
      "939 ---- 818\n",
      "940 ---- 819\n",
      "941 ---- 820\n",
      "942 ---- 821\n",
      "943 ---- 822\n",
      "944 ---- 823\n",
      "945 ---- 824\n",
      "946 ---- 825\n",
      "947 ---- 826\n",
      "948 ---- 827\n",
      "949 ---- 828\n",
      "950 ---- 829\n",
      "951 ---- 830\n",
      "952 ---- 831\n",
      "953 ---- 832\n",
      "954 ---- 833\n",
      "955 ---- 834\n",
      "956 ---- 835\n",
      "957 ---- 836\n",
      "958 ---- 837\n",
      "959 ---- 838\n",
      "960 ---- 839\n",
      "961 ---- 840\n",
      "962 ---- 841\n",
      "963 ---- 842\n",
      "964 ---- 843\n",
      "965 ---- 844\n",
      "966 ---- 845\n",
      "967 ---- 846\n",
      "968 ---- 847\n",
      "969 ---- 848\n",
      "970 ---- 849\n",
      "971 ---- 850\n",
      "972 ---- 851\n",
      "973 ---- 852\n",
      "974 ---- 853\n",
      "975 ---- 854\n",
      "976 ---- 855\n",
      "977 ---- 856\n",
      "978 ---- 857\n",
      "979 ---- 858\n",
      "980 ---- 859\n",
      "981 ---- 860\n",
      "982 ---- 861\n",
      "983 ---- 862\n",
      "984 ---- 863\n",
      "985 ---- 864\n",
      "986 ---- 865\n",
      "987 ---- 866\n",
      "988 ---- 867\n",
      "989 ---- 868\n",
      "990 ---- 869\n",
      "991 ---- 870\n",
      "992 ---- 871\n",
      "993 ---- 872\n",
      "994 ---- 873\n",
      "995 ---- 874\n",
      "996 ---- 875\n",
      "997 ---- 876\n",
      "998 ---- 877\n",
      "999 ---- 878\n",
      "1000 ---- 879\n",
      "1001 ---- 880\n",
      "1002 ---- 881\n",
      "1003 ---- 882\n",
      "1004 ---- 883\n",
      "1005 ---- 884\n",
      "1006 ---- 885\n",
      "1007 ---- 886\n",
      "1008 ---- 887\n",
      "1009 ---- 888\n",
      "1010 ---- 889\n",
      "1011 ---- 890\n",
      "1012 ---- 891\n",
      "1013 ---- 892\n",
      "1014 ---- 893\n",
      "1015 ---- 894\n",
      "1016 ---- 895\n",
      "1017 ---- 896\n",
      "1018 ---- 897\n",
      "1019 ---- 898\n",
      "1020 ---- 899\n",
      "1021 ---- 900\n",
      "1022 ---- 901\n",
      "1023 ---- 902\n",
      "1024 ---- 903\n",
      "1025 ---- 904\n",
      "1026 ---- 905\n",
      "1027 ---- 906\n",
      "1028 ---- 907\n",
      "1029 ---- 908\n",
      "1030 ---- 909\n",
      "1031 ---- 910\n",
      "1032 ---- 911\n",
      "1033 ---- 912\n",
      "1034 ---- 913\n",
      "1035 ---- 914\n",
      "1036 ---- 915\n",
      "1037 ---- 916\n",
      "1038 ---- 917\n",
      "1039 ---- 918\n",
      "1040 ---- 919\n",
      "1041 ---- 920\n",
      "1042 ---- 921\n",
      "1043 ---- 922\n",
      "1044 ---- 923\n",
      "1045 ---- 924\n",
      "1046 ---- 925\n",
      "1047 ---- 926\n",
      "1048 ---- 927\n",
      "1049 ---- 928\n",
      "1050 ---- 929\n",
      "1051 ---- 930\n",
      "1052 ---- 931\n",
      "1053 ---- 932\n",
      "1054 ---- 933\n",
      "1055 ---- 934\n",
      "1056 ---- 935\n",
      "1057 ---- 936\n",
      "1058 ---- 937\n",
      "1059 ---- 938\n",
      "1060 ---- 939\n",
      "1061 ---- 940\n",
      "1062 ---- 941\n",
      "1063 ---- 942\n",
      "1064 ---- 943\n",
      "1065 ---- 944\n",
      "1066 ---- 945\n",
      "1067 ---- 946\n",
      "1068 ---- 947\n",
      "1069 ---- 948\n",
      "1070 ---- 949\n",
      "1071 ---- 950\n",
      "1072 ---- 951\n",
      "1073 ---- 952\n",
      "1074 ---- 953\n",
      "1075 ---- 954\n",
      "1076 ---- 955\n",
      "1077 ---- 956\n",
      "1078 ---- 957\n",
      "1079 ---- 958\n",
      "1080 ---- 959\n",
      "1081 ---- 960\n",
      "1082 ---- 961\n",
      "1083 ---- 962\n",
      "1084 ---- 963\n",
      "1085 ---- 964\n",
      "1086 ---- 965\n",
      "1087 ---- 966\n",
      "1088 ---- 967\n",
      "1089 ---- 968\n",
      "1090 ---- 969\n",
      "1091 ---- 970\n",
      "1092 ---- 971\n",
      "1093 ---- 972\n",
      "1094 ---- 973\n",
      "1095 ---- 974\n",
      "1096 ---- 975\n",
      "1097 ---- 976\n",
      "1098 ---- 977\n",
      "1099 ---- 978\n",
      "1100 ---- 979\n",
      "1101 ---- 980\n",
      "1102 ---- 981\n",
      "1103 ---- 982\n",
      "1104 ---- 983\n",
      "1105 ---- 984\n",
      "1106 ---- 985\n",
      "1107 ---- 986\n",
      "1108 ---- 987\n",
      "1109 ---- 988\n",
      "1110 ---- 989\n",
      "1111 ---- 990\n",
      "1112 ---- 991\n",
      "1113 ---- 992\n",
      "1114 ---- 993\n",
      "1115 ---- 994\n",
      "1116 ---- 995\n",
      "1117 ---- 996\n",
      "1118 ---- 997\n",
      "1119 ---- 998\n",
      "1120 ---- 999\n",
      "1121 ---- 1000\n",
      "1122 ---- 1001\n",
      "1123 ---- 1002\n",
      "1124 ---- 1003\n",
      "1125 ---- 1004\n",
      "1126 ---- 1005\n",
      "1127 ---- 1006\n",
      "1128 ---- 1007\n",
      "1129 ---- 1008\n",
      "1130 ---- 1009\n",
      "1131 ---- 1010\n",
      "1132 ---- 1011\n",
      "1133 ---- 1012\n",
      "1134 ---- 1013\n",
      "1135 ---- 1014\n",
      "1136 ---- 1015\n",
      "1137 ---- 1016\n",
      "1138 ---- 1017\n",
      "1139 ---- 1018\n",
      "1140 ---- 1019\n",
      "1141 ---- 1020\n",
      "1142 ---- 1021\n",
      "1143 ---- 1022\n",
      "1144 ---- 1023\n",
      "1145 ---- 1024\n",
      "1146 ---- 1025\n",
      "1147 ---- 1026\n",
      "1148 ---- 1027\n",
      "1149 ---- 1028\n",
      "1150 ---- 1029\n",
      "1151 ---- 1030\n",
      "1152 ---- 1031\n",
      "1153 ---- 1032\n",
      "1154 ---- 1033\n",
      "1155 ---- 1034\n",
      "1156 ---- 1035\n",
      "1157 ---- 1036\n",
      "1158 ---- 1037\n",
      "1159 ---- 1038\n",
      "1160 ---- 1039\n",
      "1161 ---- 1040\n",
      "1162 ---- 1041\n",
      "1163 ---- 1042\n",
      "1164 ---- 1043\n",
      "1165 ---- 1044\n",
      "1166 ---- 1045\n",
      "1167 ---- 1046\n",
      "1168 ---- 1047\n",
      "1169 ---- 1048\n",
      "1170 ---- 1049\n",
      "1171 ---- 1050\n",
      "1172 ---- 1051\n",
      "1173 ---- 1052\n",
      "1174 ---- 1053\n",
      "1175 ---- 1054\n",
      "1176 ---- 1055\n",
      "1177 ---- 1056\n",
      "1178 ---- 1057\n",
      "1179 ---- 1058\n",
      "1180 ---- 1059\n",
      "1181 ---- 1060\n",
      "1182 ---- 1061\n",
      "1183 ---- 1062\n",
      "1184 ---- 1063\n",
      "1185 ---- 1064\n",
      "1186 ---- 1065\n",
      "1187 ---- 1066\n",
      "1188 ---- 1067\n",
      "1189 ---- 1068\n",
      "1190 ---- 1069\n",
      "1191 ---- 1070\n",
      "1192 ---- 1071\n",
      "1193 ---- 1072\n",
      "1194 ---- 1073\n",
      "1195 ---- 1074\n",
      "1196 ---- 1075\n",
      "1197 ---- 1076\n",
      "1198 ---- 1077\n",
      "1199 ---- 1078\n",
      "1200 ---- 1079\n",
      "1201 ---- 1080\n",
      "1202 ---- 1081\n",
      "1203 ---- 1082\n",
      "1204 ---- 1083\n",
      "1205 ---- 1084\n",
      "1206 ---- 1085\n",
      "1207 ---- 1086\n",
      "1208 ---- 1087\n",
      "1209 ---- 1088\n",
      "1210 ---- 1089\n",
      "1211 ---- 1090\n",
      "1212 ---- 1091\n",
      "1213 ---- 1092\n",
      "1214 ---- 1093\n",
      "1215 ---- 1094\n",
      "1216 ---- 1095\n",
      "1217 ---- 1096\n",
      "1218 ---- 1097\n",
      "1219 ---- 1098\n",
      "1220 ---- 1099\n",
      "1221 ---- 1100\n",
      "1222 ---- 1101\n",
      "1223 ---- 1102\n",
      "1224 ---- 1103\n",
      "1225 ---- 1104\n",
      "1226 ---- 1105\n",
      "1227 ---- 1106\n",
      "1228 ---- 1107\n",
      "1229 ---- 1108\n",
      "1230 ---- 1109\n",
      "1231 ---- 1110\n",
      "1232 ---- 1111\n",
      "1233 ---- 1112\n",
      "1234 ---- 1113\n",
      "1235 ---- 1114\n",
      "1236 ---- 1115\n",
      "1237 ---- 1116\n",
      "1238 ---- 1117\n",
      "1239 ---- 1118\n",
      "1240 ---- 1119\n",
      "1241 ---- 1120\n",
      "1242 ---- 1121\n",
      "1243 ---- 1122\n",
      "1244 ---- 1123\n",
      "1245 ---- 1124\n",
      "1246 ---- 1125\n",
      "1247 ---- 1126\n",
      "1248 ---- 1127\n",
      "1249 ---- 1128\n",
      "1250 ---- 1129\n",
      "1251 ---- 1130\n",
      "1252 ---- 1131\n",
      "1253 ---- 1132\n",
      "1254 ---- 1133\n",
      "1255 ---- 1134\n",
      "1256 ---- 1135\n",
      "1257 ---- 1136\n",
      "1258 ---- 1137\n",
      "1259 ---- 1138\n",
      "1260 ---- 1139\n",
      "1261 ---- 1140\n",
      "1262 ---- 1141\n",
      "1263 ---- 1142\n",
      "1264 ---- 1143\n",
      "1265 ---- 1144\n",
      "1266 ---- 1145\n",
      "1267 ---- 1146\n",
      "1268 ---- 1147\n",
      "1269 ---- 1148\n",
      "1270 ---- 1149\n",
      "1271 ---- 1150\n",
      "1272 ---- 1151\n",
      "1273 ---- 1152\n",
      "1274 ---- 1153\n",
      "1275 ---- 1154\n",
      "1276 ---- 1155\n",
      "1277 ---- 1156\n",
      "1278 ---- 1157\n",
      "1279 ---- 1158\n",
      "1280 ---- 1159\n",
      "1281 ---- 1160\n",
      "1282 ---- 1161\n",
      "1283 ---- 1162\n",
      "1284 ---- 1163\n",
      "1285 ---- 1164\n",
      "1286 ---- 1165\n",
      "1287 ---- 1166\n",
      "1288 ---- 1167\n",
      "1289 ---- 1168\n",
      "1290 ---- 1169\n",
      "1291 ---- 1170\n",
      "1292 ---- 1171\n",
      "1293 ---- 1172\n",
      "1294 ---- 1173\n",
      "1295 ---- 1174\n",
      "1296 ---- 1175\n",
      "1297 ---- 1176\n",
      "1298 ---- 1177\n",
      "1299 ---- 1178\n",
      "1300 ---- 1179\n",
      "1301 ---- 1180\n",
      "1302 ---- 1181\n",
      "1303 ---- 1182\n",
      "1304 ---- 1183\n",
      "1305 ---- 1184\n",
      "1306 ---- 1185\n",
      "1307 ---- 1186\n",
      "1308 ---- 1187\n",
      "1309 ---- 1188\n",
      "1310 ---- 1189\n",
      "1311 ---- 1190\n",
      "1312 ---- 1191\n",
      "1313 ---- 1192\n",
      "1314 ---- 1193\n",
      "1315 ---- 1194\n",
      "1316 ---- 1195\n",
      "1317 ---- 1196\n",
      "1318 ---- 1197\n",
      "1319 ---- 1198\n",
      "1320 ---- 1199\n",
      "1321 ---- 1200\n",
      "1322 ---- 1201\n",
      "1323 ---- 1202\n",
      "1324 ---- 1203\n",
      "1325 ---- 1204\n",
      "1326 ---- 1205\n",
      "1327 ---- 1206\n",
      "1328 ---- 1207\n",
      "1329 ---- 1208\n",
      "1330 ---- 1209\n",
      "1331 ---- 1210\n",
      "1332 ---- 1211\n",
      "1333 ---- 1212\n",
      "1334 ---- 1213\n",
      "1335 ---- 1214\n",
      "1336 ---- 1215\n",
      "1337 ---- 1216\n",
      "1338 ---- 1217\n",
      "1339 ---- 1218\n",
      "1340 ---- 1219\n",
      "1341 ---- 1220\n",
      "1342 ---- 1221\n",
      "1343 ---- 1222\n",
      "1344 ---- 1223\n",
      "1345 ---- 1224\n",
      "1346 ---- 1225\n",
      "1347 ---- 1226\n",
      "1348 ---- 1227\n",
      "1349 ---- 1228\n",
      "1350 ---- 1229\n",
      "1351 ---- 1230\n",
      "1352 ---- 1231\n",
      "1353 ---- 1232\n",
      "1354 ---- 1233\n",
      "1355 ---- 1234\n",
      "1356 ---- 1235\n",
      "1357 ---- 1236\n",
      "1358 ---- 1237\n",
      "1359 ---- 1238\n",
      "1360 ---- 1239\n",
      "1361 ---- 1240\n",
      "1362 ---- 1241\n",
      "1363 ---- 1242\n",
      "1364 ---- 1243\n",
      "1365 ---- 1244\n",
      "1366 ---- 1245\n",
      "1367 ---- 1246\n",
      "1368 ---- 1247\n",
      "1369 ---- 1248\n",
      "1370 ---- 1249\n",
      "1371 ---- 1250\n",
      "1372 ---- 1251\n",
      "1373 ---- 1252\n",
      "1374 ---- 1253\n",
      "1375 ---- 1254\n",
      "1376 ---- 1255\n",
      "1377 ---- 1256\n",
      "1378 ---- 1257\n",
      "1379 ---- 1258\n",
      "1380 ---- 1259\n",
      "1381 ---- 1260\n",
      "1382 ---- 1261\n",
      "1383 ---- 1262\n",
      "1384 ---- 1263\n",
      "1385 ---- 1264\n",
      "1386 ---- 1265\n",
      "1387 ---- 1266\n",
      "1388 ---- 1267\n",
      "1389 ---- 1268\n",
      "1390 ---- 1269\n",
      "1391 ---- 1270\n",
      "1392 ---- 1271\n",
      "1393 ---- 1272\n",
      "1394 ---- 1273\n",
      "1395 ---- 1274\n",
      "1396 ---- 1275\n",
      "1397 ---- 1276\n",
      "1398 ---- 1277\n",
      "1399 ---- 1278\n",
      "1400 ---- 1279\n",
      "1401 ---- 1280\n",
      "1402 ---- 1281\n",
      "1403 ---- 1282\n",
      "1404 ---- 1283\n",
      "1405 ---- 1284\n",
      "1406 ---- 1285\n",
      "1407 ---- 1286\n",
      "1408 ---- 1287\n",
      "1409 ---- 1288\n",
      "1410 ---- 1289\n",
      "1411 ---- 1290\n",
      "1412 ---- 1291\n",
      "1413 ---- 1292\n",
      "1414 ---- 1293\n",
      "1415 ---- 1294\n",
      "1416 ---- 1295\n",
      "1417 ---- 1296\n",
      "1418 ---- 1297\n",
      "1419 ---- 1298\n",
      "1420 ---- 1299\n",
      "1421 ---- 1300\n",
      "1422 ---- 1301\n",
      "1423 ---- 1302\n",
      "1424 ---- 1303\n",
      "1425 ---- 1304\n",
      "1426 ---- 1305\n",
      "1427 ---- 1306\n",
      "1428 ---- 1307\n",
      "1429 ---- 1308\n",
      "1430 ---- 1309\n",
      "1431 ---- 1310\n",
      "1432 ---- 1311\n",
      "1433 ---- 1312\n",
      "1434 ---- 1313\n",
      "1435 ---- 1314\n",
      "1436 ---- 1315\n",
      "1437 ---- 1316\n",
      "1438 ---- 1317\n",
      "1439 ---- 1318\n",
      "1440 ---- 1319\n",
      "1441 ---- 1320\n",
      "1442 ---- 1321\n",
      "1443 ---- 1322\n",
      "1444 ---- 1323\n",
      "1445 ---- 1324\n",
      "1446 ---- 1325\n",
      "1447 ---- 1326\n",
      "1448 ---- 1327\n",
      "1449 ---- 1328\n",
      "1450 ---- 1329\n",
      "1451 ---- 1330\n",
      "1452 ---- 1331\n",
      "1453 ---- 1332\n",
      "1454 ---- 1333\n",
      "1455 ---- 1334\n",
      "1456 ---- 1335\n",
      "1457 ---- 1336\n",
      "1458 ---- 1337\n",
      "1459 ---- 1338\n",
      "1460 ---- 1339\n",
      "1461 ---- 1340\n",
      "1462 ---- 1341\n",
      "1463 ---- 1342\n",
      "1464 ---- 1343\n",
      "1465 ---- 1344\n",
      "1466 ---- 1345\n",
      "1467 ---- 1346\n",
      "1468 ---- 1347\n",
      "1469 ---- 1348\n",
      "1470 ---- 1349\n",
      "1471 ---- 1350\n",
      "1472 ---- 1351\n",
      "1473 ---- 1352\n",
      "1474 ---- 1353\n",
      "1475 ---- 1354\n",
      "1476 ---- 1355\n",
      "1477 ---- 1356\n",
      "1478 ---- 1357\n",
      "1479 ---- 1358\n",
      "1480 ---- 1359\n",
      "1481 ---- 1360\n",
      "1482 ---- 1361\n",
      "1483 ---- 1362\n",
      "1484 ---- 1363\n",
      "1485 ---- 1364\n",
      "1486 ---- 1365\n",
      "1487 ---- 1366\n",
      "1488 ---- 1367\n",
      "1489 ---- 1368\n",
      "1490 ---- 1369\n",
      "1491 ---- 1370\n",
      "1492 ---- 1371\n",
      "1493 ---- 1372\n",
      "1494 ---- 1373\n",
      "1495 ---- 1374\n",
      "1496 ---- 1375\n",
      "1497 ---- 1376\n",
      "1498 ---- 1377\n",
      "1499 ---- 1378\n",
      "1500 ---- 1379\n",
      "1501 ---- 1380\n",
      "1502 ---- 1381\n",
      "1503 ---- 1382\n",
      "1504 ---- 1383\n",
      "1505 ---- 1384\n",
      "1506 ---- 1385\n",
      "1507 ---- 1386\n",
      "1508 ---- 1387\n",
      "1509 ---- 1388\n",
      "1510 ---- 1389\n",
      "1511 ---- 1390\n",
      "1512 ---- 1391\n",
      "1513 ---- 1392\n",
      "1514 ---- 1393\n",
      "1515 ---- 1394\n",
      "1516 ---- 1395\n",
      "1517 ---- 1396\n",
      "1518 ---- 1397\n",
      "1519 ---- 1398\n",
      "1520 ---- 1399\n",
      "1521 ---- 1400\n",
      "1522 ---- 1401\n",
      "1523 ---- 1402\n",
      "1524 ---- 1403\n",
      "1525 ---- 1404\n",
      "1526 ---- 1405\n",
      "1527 ---- 1406\n",
      "1528 ---- 1407\n",
      "1529 ---- 1408\n",
      "1530 ---- 1409\n",
      "1531 ---- 1410\n",
      "1532 ---- 1411\n",
      "1533 ---- 1412\n",
      "1534 ---- 1413\n",
      "1535 ---- 1414\n",
      "1536 ---- 1415\n",
      "1537 ---- 1416\n",
      "1538 ---- 1417\n",
      "1539 ---- 1418\n",
      "1540 ---- 1419\n",
      "1541 ---- 1420\n",
      "1542 ---- 1421\n",
      "1543 ---- 1422\n",
      "1544 ---- 1423\n",
      "1545 ---- 1424\n",
      "1546 ---- 1425\n",
      "1547 ---- 1426\n",
      "1548 ---- 1427\n",
      "1549 ---- 1428\n",
      "1550 ---- 1429\n",
      "1551 ---- 1430\n",
      "1552 ---- 1431\n",
      "1553 ---- 1432\n",
      "1554 ---- 1433\n",
      "1555 ---- 1434\n",
      "1556 ---- 1435\n",
      "1557 ---- 1436\n",
      "1558 ---- 1437\n",
      "1559 ---- 1438\n",
      "1560 ---- 1439\n",
      "1561 ---- 1440\n",
      "1562 ---- 1441\n",
      "1563 ---- 1442\n",
      "1564 ---- 1443\n",
      "1565 ---- 1444\n",
      "1566 ---- 1445\n",
      "1567 ---- 1446\n",
      "1568 ---- 1447\n",
      "1569 ---- 1448\n",
      "1570 ---- 1449\n",
      "1571 ---- 1450\n",
      "1572 ---- 1451\n",
      "1573 ---- 1452\n",
      "1574 ---- 1453\n",
      "1575 ---- 1454\n",
      "1576 ---- 1455\n",
      "1577 ---- 1456\n",
      "1578 ---- 1457\n",
      "1579 ---- 1458\n",
      "1580 ---- 1459\n",
      "1581 ---- 1460\n",
      "1582 ---- 1461\n",
      "1583 ---- 1462\n",
      "1584 ---- 1463\n",
      "1585 ---- 1464\n",
      "1586 ---- 1465\n",
      "1587 ---- 1466\n",
      "1588 ---- 1467\n",
      "1589 ---- 1468\n",
      "1590 ---- 1469\n",
      "1591 ---- 1470\n",
      "1592 ---- 1471\n",
      "1593 ---- 1472\n",
      "1594 ---- 1473\n",
      "1595 ---- 1474\n",
      "1596 ---- 1475\n",
      "1597 ---- 1476\n",
      "1598 ---- 1477\n",
      "1599 ---- 1478\n",
      "1600 ---- 1479\n",
      "1601 ---- 1480\n",
      "1602 ---- 1481\n",
      "1603 ---- 1482\n",
      "1604 ---- 1483\n",
      "1605 ---- 1484\n",
      "1606 ---- 1485\n",
      "1607 ---- 1486\n",
      "1608 ---- 1487\n",
      "1609 ---- 1488\n",
      "1610 ---- 1489\n",
      "1611 ---- 1490\n",
      "1612 ---- 1491\n",
      "1613 ---- 1492\n",
      "1614 ---- 1493\n",
      "1615 ---- 1494\n",
      "1616 ---- 1495\n",
      "1617 ---- 1496\n",
      "1618 ---- 1497\n",
      "1619 ---- 1498\n",
      "1620 ---- 1499\n",
      "1621 ---- 1500\n",
      "1622 ---- 1501\n",
      "1623 ---- 1502\n",
      "1624 ---- 1503\n",
      "1625 ---- 1504\n",
      "1626 ---- 1505\n",
      "1627 ---- 1506\n",
      "1628 ---- 1507\n",
      "1629 ---- 1508\n",
      "1630 ---- 1509\n",
      "1631 ---- 1510\n",
      "1632 ---- 1511\n",
      "1633 ---- 1512\n",
      "1634 ---- 1513\n",
      "1635 ---- 1514\n",
      "1636 ---- 1515\n",
      "1637 ---- 1516\n",
      "1638 ---- 1517\n",
      "1639 ---- 1518\n",
      "1640 ---- 1519\n",
      "1641 ---- 1520\n",
      "1642 ---- 1521\n",
      "1643 ---- 1522\n",
      "1644 ---- 1523\n",
      "1645 ---- 1524\n",
      "1646 ---- 1525\n",
      "1647 ---- 1526\n",
      "1648 ---- 1527\n",
      "1649 ---- 1528\n",
      "1650 ---- 1529\n",
      "1651 ---- 1530\n",
      "1652 ---- 1531\n",
      "1653 ---- 1532\n",
      "1654 ---- 1533\n",
      "1655 ---- 1534\n",
      "1656 ---- 1535\n",
      "1657 ---- 1536\n",
      "1658 ---- 1537\n",
      "1659 ---- 1538\n",
      "1660 ---- 1539\n",
      "1661 ---- 1540\n",
      "1662 ---- 1541\n",
      "1663 ---- 1542\n",
      "1664 ---- 1543\n",
      "1665 ---- 1544\n",
      "1666 ---- 1545\n",
      "1667 ---- 1546\n",
      "1668 ---- 1547\n",
      "1669 ---- 1548\n",
      "1670 ---- 1549\n",
      "1671 ---- 1550\n",
      "1672 ---- 1551\n",
      "1673 ---- 1552\n",
      "1674 ---- 1553\n",
      "1675 ---- 1554\n",
      "1676 ---- 1555\n",
      "1677 ---- 1556\n",
      "1678 ---- 1557\n",
      "1679 ---- 1558\n",
      "1680 ---- 1559\n",
      "1681 ---- 1560\n",
      "1682 ---- 1561\n",
      "1683 ---- 1562\n",
      "1684 ---- 1563\n",
      "1685 ---- 1564\n",
      "1686 ---- 1565\n",
      "1687 ---- 1566\n",
      "1688 ---- 1567\n",
      "1689 ---- 1568\n",
      "1690 ---- 1569\n",
      "1691 ---- 1570\n",
      "1692 ---- 1571\n",
      "1693 ---- 1572\n",
      "1694 ---- 1573\n",
      "1695 ---- 1574\n",
      "1696 ---- 1575\n",
      "1697 ---- 1576\n",
      "1698 ---- 1577\n",
      "1699 ---- 1578\n",
      "1700 ---- 1579\n",
      "1701 ---- 1580\n",
      "1702 ---- 1581\n",
      "1703 ---- 1582\n",
      "1704 ---- 1583\n",
      "1705 ---- 1584\n",
      "1706 ---- 1585\n",
      "1707 ---- 1586\n",
      "1708 ---- 1587\n",
      "1709 ---- 1588\n",
      "1710 ---- 1589\n",
      "1711 ---- 1590\n",
      "1712 ---- 1591\n",
      "1713 ---- 1592\n",
      "1714 ---- 1593\n",
      "1715 ---- 1594\n",
      "1716 ---- 1595\n",
      "1717 ---- 1596\n",
      "1718 ---- 1597\n",
      "1719 ---- 1598\n",
      "1720 ---- 1599\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(X_train_final.columns)):\n",
    "    print('{} ---- {}'.format(i, X_train_final.columns[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Unigrams, POS Tag Count, Sentiment Polarity, Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_1 = X_train_final.iloc[:,np.r_[10:12,13:21,121:1721]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2556, 1610)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_1 = X_test_final.iloc[:,np.r_[10:12,13:21,121:1721]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 1610)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.447\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9631    0.8508    0.9035       583\n",
      "           1     0.3040    0.6667    0.4176        57\n",
      "\n",
      "    accuracy                         0.8344       640\n",
      "   macro avg     0.6336    0.7587    0.6605       640\n",
      "weighted avg     0.9044    0.8344    0.8602       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_1_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_1_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_1,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_1), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 496, False Positive: 87, False Negative: 19, True Positive: 38\n",
      "--------------------------------------------------------------------------------\n",
      "[[496  87]\n",
      " [ 19  38]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.85      0.90       583\n",
      "           1       0.30      0.67      0.42        57\n",
      "\n",
      "    accuracy                           0.83       640\n",
      "   macro avg       0.63      0.76      0.66       640\n",
      "weighted avg       0.90      0.83      0.86       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_1 = LogisticRegression(random_state=18, \n",
    "                                solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], \n",
    "                                class_weight='balanced').fit(X_train_model_1, y_train)\n",
    "y_lr = lr_model_1.predict(X_test_model_1)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_model_2 = X_train_final.iloc[:,np.r_[3:1113]]\n",
    "X_train_model_2 = X_train_final.iloc[:, np.r_[3:1721]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2556, 1718)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_2 = X_test_final.iloc[:,np.r_[3:1721]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 1718)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.449\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9630    0.8491    0.9025       583\n",
      "           1     0.3016    0.6667    0.4153        57\n",
      "\n",
      "    accuracy                         0.8328       640\n",
      "   macro avg     0.6323    0.7579    0.6589       640\n",
      "weighted avg     0.9041    0.8328    0.8591       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_2_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_2_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_2,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_2), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 495, False Positive: 88, False Negative: 19, True Positive: 38\n",
      "--------------------------------------------------------------------------------\n",
      "[[495  88]\n",
      " [ 19  38]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.85      0.90       583\n",
      "           1       0.30      0.67      0.42        57\n",
      "\n",
      "    accuracy                           0.83       640\n",
      "   macro avg       0.63      0.76      0.66       640\n",
      "weighted avg       0.90      0.83      0.86       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_2 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_2, y_train)\n",
    "y_lr = lr_model_2.predict(X_test_model_2)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Without Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_3 = X_train_final.iloc[:,np.r_[3:121]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2556, 118)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_3 = X_test_final.iloc[:,np.r_[3:121]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 118)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n",
      "Best score: 0.379\n",
      "Best parameters set:\n",
      "\tclf__C: 5\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9806    0.7787    0.8681       583\n",
      "           1     0.2712    0.8421    0.4103        57\n",
      "\n",
      "    accuracy                         0.7844       640\n",
      "   macro avg     0.6259    0.8104    0.6392       640\n",
      "weighted avg     0.9174    0.7844    0.8273       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_3_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.0001, 0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\", \"elasticnet\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_3_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_3,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_3), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 454, False Positive: 129, False Negative: 9, True Positive: 48\n",
      "--------------------------------------------------------------------------------\n",
      "[[454 129]\n",
      " [  9  48]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.78      0.87       583\n",
      "           1       0.27      0.84      0.41        57\n",
      "\n",
      "    accuracy                           0.78       640\n",
      "   macro avg       0.63      0.81      0.64       640\n",
      "weighted avg       0.92      0.78      0.83       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_3 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_3, y_train)\n",
    "y_lr = lr_model_3.predict(X_test_model_3)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Without Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_4 = X_train_final.iloc[:,np.r_[3:21,121:1721]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2556, 1618)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_4 = X_test_final.iloc[:,np.r_[3:21,121:1721]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 1618)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.445\n",
      "Best parameters set:\n",
      "\tclf__C: 1\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9540    0.8902    0.9210       583\n",
      "           1     0.3333    0.5614    0.4183        57\n",
      "\n",
      "    accuracy                         0.8609       640\n",
      "   macro avg     0.6437    0.7258    0.6697       640\n",
      "weighted avg     0.8988    0.8609    0.8763       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_4_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_4_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_4,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_4), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 519, False Positive: 64, False Negative: 25, True Positive: 32\n",
      "--------------------------------------------------------------------------------\n",
      "[[519  64]\n",
      " [ 25  32]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.89      0.92       583\n",
      "           1       0.33      0.56      0.42        57\n",
      "\n",
      "    accuracy                           0.86       640\n",
      "   macro avg       0.64      0.73      0.67       640\n",
      "weighted avg       0.90      0.86      0.88       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_4 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_4, y_train)\n",
    "y_lr = lr_model_4.predict(X_test_model_4)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Without POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_5 = X_train_final.iloc[:,np.r_[3:13,21:1721]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2556, 1710)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_5 = X_test_final.iloc[:,np.r_[3:13,21:1721]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 1710)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.454\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.86      0.91       583\n",
      "           1       0.31      0.67      0.42        57\n",
      "\n",
      "    accuracy                           0.84       640\n",
      "   macro avg       0.64      0.76      0.67       640\n",
      "weighted avg       0.91      0.84      0.86       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_5_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_5_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_5,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_5), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 499, False Positive: 84, False Negative: 19, True Positive: 38\n",
      "--------------------------------------------------------------------------------\n",
      "[[499  84]\n",
      " [ 19  38]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.86      0.91       583\n",
      "           1       0.31      0.67      0.42        57\n",
      "\n",
      "    accuracy                           0.84       640\n",
      "   macro avg       0.64      0.76      0.67       640\n",
      "weighted avg       0.91      0.84      0.86       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_5 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_5, y_train)\n",
    "y_lr = lr_model_5.predict(X_test_model_5)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6: Without STEM Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_6 = X_train_final.iloc[:,np.r_[10:1721]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2556, 1711)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_6 = X_test_final.iloc[:,np.r_[10:1721]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 1711)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.447\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.85      0.90       583\n",
      "           1       0.30      0.65      0.41        57\n",
      "\n",
      "    accuracy                           0.83       640\n",
      "   macro avg       0.63      0.75      0.66       640\n",
      "weighted avg       0.90      0.83      0.86       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_6_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_6_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_6,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_6), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 497, False Positive: 86, False Negative: 20, True Positive: 37\n",
      "--------------------------------------------------------------------------------\n",
      "[[497  86]\n",
      " [ 20  37]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.85      0.90       583\n",
      "           1       0.30      0.65      0.41        57\n",
      "\n",
      "    accuracy                           0.83       640\n",
      "   macro avg       0.63      0.75      0.66       640\n",
      "weighted avg       0.90      0.83      0.86       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_6 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_6, y_train)\n",
    "y_lr = lr_model_6.predict(X_test_model_6)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7: Without Sentiment Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_7 = X_train_final.iloc[:,np.r_[3:10,12:1721]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2556, 1716)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_7 = X_test_final.iloc[:,np.r_[3:10,12:1721]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 1716)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.450\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.85      0.90       583\n",
      "           1       0.31      0.68      0.42        57\n",
      "\n",
      "    accuracy                           0.83       640\n",
      "   macro avg       0.64      0.77      0.66       640\n",
      "weighted avg       0.91      0.83      0.86       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_7_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_7_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_7,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_7), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 495, False Positive: 88, False Negative: 18, True Positive: 39\n",
      "--------------------------------------------------------------------------------\n",
      "[[495  88]\n",
      " [ 18  39]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.85      0.90       583\n",
      "           1       0.31      0.68      0.42        57\n",
      "\n",
      "    accuracy                           0.83       640\n",
      "   macro avg       0.64      0.77      0.66       640\n",
      "weighted avg       0.91      0.83      0.86       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_7 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_7, y_train)\n",
    "y_lr = lr_model_7.predict(X_test_model_7)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 8: Without NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_8 = X_train_final.iloc[:,np.r_[3:12,13:1721]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2556, 1717)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_8 = X_test_final.iloc[:,np.r_[3:12,13:1721]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 1717)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.449\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.85      0.90       583\n",
      "           1       0.30      0.67      0.42        57\n",
      "\n",
      "    accuracy                           0.83       640\n",
      "   macro avg       0.63      0.76      0.66       640\n",
      "weighted avg       0.90      0.83      0.86       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_8_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_8_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_8,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_8), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 495, False Positive: 88, False Negative: 19, True Positive: 38\n",
      "--------------------------------------------------------------------------------\n",
      "[[495  88]\n",
      " [ 19  38]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.85      0.90       583\n",
      "           1       0.30      0.67      0.42        57\n",
      "\n",
      "    accuracy                           0.83       640\n",
      "   macro avg       0.63      0.76      0.66       640\n",
      "weighted avg       0.90      0.83      0.86       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_8 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_8, y_train)\n",
    "y_lr = lr_model_8.predict(X_test_model_8)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Summary\n",
    "| Experiment | Model Number | Features Used                                                | Precision | Recall | Macro F1 |\n",
    "| :--------: | :----------: | :----------------------------------------------------------: | :-------: | :----: | :------: |\n",
    "| Baseline   | 1            | Unigrams, POS Tag Count, Sentiment Polarity and Subjectivity | 0.64      | 0.74   | 0.66     |\n",
    "| Baseline   | 2            | All features (baseline)                                      | 0.64      | 0.76   | 0.66     |\n",
    "| Baseline   | 3            | Without Unigrams                                             | 0.61      | 0.72   | 0.59     |\n",
    "| Baseline   | 4            | Without Embeddings                                           | 0.64      | 0.74   | 0.66     |\n",
    "| Baseline   | 5            | Without POS tag                                              | 0.64      | 0.75   | 0.66     |\n",
    "| Baseline   | 6            | Without STEM similarity (paper baseline)                     | 0.65      | 0.77   | 0.67     |\n",
    "| Baseline   | 7            | Without sentiment features                                   | 0.64      | 0.75   | 0.66     |\n",
    "| Baseline   | 8            | Without NER                                                  | 0.64      | 0.76   | 0.66     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
