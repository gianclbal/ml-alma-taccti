{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ive had a history of struggling in my math classes. Since physics is basically another math class',\n",
       " 'I knew I would need to take extra steps to ensure my success. When I heard the option to enroll in a supplemental course for physics. I knew I had to enroll for the extra help.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "# Add the 'utils' directory to the system path\n",
    "utils_path = '/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/utils'\n",
    "sys.path.insert(0, utils_path)\n",
    "\n",
    "from text_cleaner import clean_text\n",
    "\n",
    "\n",
    "ex = 'I decided to enroll into the physics supplemental course because I’ve had a history of struggling in my math classes. Since physics is basically another math class, I knew I would need to take extra steps to ensure my success. In the past I’ve had a packed schedule with classes and work, but this semester I allowed myself more room for school by cutting back my work hours. I originally planned on letting myself have time to attend office hours, but I realized that wouldn’t be enough.\\nWhen I heard the option to enroll in a supplemental course for physics, I knew I had to enroll for the extra help. Not only would I have more practice with physics problems, but I would be able to connect more with my fellow classmates. I figured that if I felt like I created a little community with the supplemental course, I would feel more comfortable with physics problems. With a community I would feel comfortable enough to ask questions without having to worry about sounding unintelligent.\\nAnother big reason for enrolling was when I heard I wouldn’t have homework. I will have the opportunity to have the extra practice with physics problems without the stress of having to worry about my grade. I will be able to do the problems at my own pace and truly understand the steps to get the solution'\n",
    "annotation = ' I’ve had a history of struggling in my math classes. Since physics is basically another math class /%/ I knew I would need to take extra steps to ensure my success. /-/ When I heard the option to enroll in a supplemental course for physics. /-/ I knew I had to enroll for the extra help.'\n",
    "\n",
    "annotation_list = re.split(r'/%/', annotation)\n",
    "annotation_list = [clean_text(text) for text in annotation_list]\n",
    "annotation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: No\n",
      "Snippets: [CLS]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load your pre-trained sequence classification model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "def classify_and_extract_snippets(text):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "    \n",
    "    # Get model outputs\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Determine the label\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    label = 'Yes' if predicted_class == 1 else 'No'\n",
    "    \n",
    "    # Calculate token importance using softmax on logits\n",
    "    token_importance = F.softmax(logits, dim=1).squeeze()\n",
    "    \n",
    "    # Convert input IDs back to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    # Determine threshold to select important tokens\n",
    "    threshold = 0.5\n",
    "    important_tokens = [token for token, importance in zip(tokens, token_importance) if importance > threshold]\n",
    "    \n",
    "    # Clean the tokens and join them to form snippets\n",
    "    clean_snippets = \" \".join(important_tokens).replace(\" ##\", \"\")\n",
    "    \n",
    "    return label, clean_snippets\n",
    "\n",
    "# Example usage\n",
    "ex = \"\"\"I decided to enroll into the physics supplemental course because I’ve had a history of struggling in my math classes. Since physics is basically another math class, I knew I would need to take extra steps to ensure my success. In the past I’ve had a packed schedule with classes and work, but this semester I allowed myself more room for school by cutting back my work hours. I originally planned on letting myself have time to attend office hours, but I realized that wouldn’t be enough.\n",
    "When I heard the option to enroll in a supplemental course for physics, I knew I had to enroll for the extra help. Not only would I have more practice with physics problems, but I would be able to connect more with my fellow classmates. I figured that if I felt like I created a little community with the supplemental course, I would feel more comfortable with physics problems. With a community I would feel comfortable enough to ask questions without having to worry about sounding unintelligent.\n",
    "Another big reason for enrolling was when I heard I wouldn’t have homework. I will have the opportunity to have the extra practice with physics problems without the stress of having to worry about my grade. I will be able to do the problems at my own pace and truly understand the steps to get the solution.\"\"\"\n",
    "\n",
    "label, snippets = classify_and_extract_snippets(ex)\n",
    "print(f'Label: {label}')\n",
    "print(f'Snippets: {snippets}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snippets: ['decided', 'physics', 'supplemental', 'history', 'math', '.', 'is', 'basically', ',', 'knew', 'would', '.', 'semester', 'allowed', '.', 'i', 'realized', 'wouldn', 't', '.', 'heard', 'the', 'option', 'supplemental', 'physics', 'knew', '.', 'physics', 'problems', ',', 'but', 'would', '.', 'figured', 'that', 'created', 'little', 'community', 'would', 'physics', 'problems', '.', 'community', 'would', '.', 'big', 'was', 'wouldn', 't', '.', 'physics', 'problems', '.', 'problems', 'solution', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model for token classification\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=3)  # 3 labels: B, I, O\n",
    "\n",
    "# Tokenize and encode the entire essay\n",
    "inputs = tokenizer(ex, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Get the token-level predictions\n",
    "logits = outputs.logits\n",
    "predicted_tokens = torch.argmax(logits, dim=2)\n",
    "\n",
    "# Decode the tokens and identify the snippets\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "labels = ['B', 'I', 'O']  # Assuming the model is trained with BIO scheme\n",
    "\n",
    "snippets = []\n",
    "current_snippet = []\n",
    "\n",
    "for token, label_id in zip(tokens, predicted_tokens[0].tolist()):\n",
    "    label = labels[label_id]\n",
    "    if label == 'B':\n",
    "        if current_snippet:\n",
    "            snippets.append(\" \".join(current_snippet))\n",
    "            current_snippet = []\n",
    "        current_snippet.append(token)\n",
    "    elif label == 'I' and current_snippet:\n",
    "        current_snippet.append(token)\n",
    "    elif label == 'O' and current_snippet:\n",
    "        snippets.append(\" \".join(current_snippet))\n",
    "        current_snippet = []\n",
    "\n",
    "# Add the last snippet if any\n",
    "if current_snippet:\n",
    "    snippets.append(\" \".join(current_snippet))\n",
    "\n",
    "# Clean up the snippets by removing special tokens like [CLS] and [SEP]\n",
    "clean_snippets = [\" \".join(snippet.replace(\"##\", \"\") for snippet in snippet.split()) for snippet in snippets]\n",
    "\n",
    "print(f'Snippets: {clean_snippets}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: I\n",
      "Snippets: ['[CLS] i', 'course', 'because i', '’', 've', 'history', 'in', 'my math', 'classes', '.', 'class', ', i', 'need', 'take', 'steps to', 'ensure my', 'success', '. in the past i ’', 've', 'had', 'a', 'packed', 'schedule', 'with classes', 'and', 'work', 'semester i', 'allowed', 'myself', 'more', 'room', 'for', 'school', 'by', 'cutting', 'my', 'work', 'hours', '. i', 'on', 'myself', 'have', 'time', 'attend', 'office', 'hours', 'that', 'wouldn ’', 'be', '.', 'course', 'for', 'physics', 'had', 'for the', 'help', '.', 'would', 'i', 'have', 'more', 'practice with physics', 'problems', ',', 'i', 'would', 'be', 'connect', 'more with', 'fellow', 'classmates', '.', 'course', 'i', 'would', 'feel', 'problems', '.', 'with', 'would', 'feel comfortable', 'enough', 'ask', 'questions', '.', 'another', 'big', 'reason', 'for enroll ing', 'i', 'wouldn ’', 'have', '.', 'i', 'will', 'have the opportunity', 'have', 'the', 'practice', 'with physics', 'problems', 'the', 'of', 'grade', '.', 'i', 'will', 'be', 'do the', 'problems', 'at', 'my', 'pace', 'and', 'truly', 'understand', 'the', 'steps', 'to', 'get', 'the', 'solution . [SEP]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, BertForTokenClassification\n",
    "import torch\n",
    "\n",
    "# Initialize the tokenizer and the sequence classification model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "sequence_classification_model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Initialize the token classification model (fine-tune it for your specific token classification task)\n",
    "token_classification_model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "\n",
    "def classify_and_extract_snippets(text):\n",
    "    # Step 1: Classify the essay\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "    classification_outputs = sequence_classification_model(**inputs)\n",
    "    logits = classification_outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    label = 'Yes' if predicted_class == 1 else 'No'\n",
    "\n",
    "    # Step 2: Extract snippets\n",
    "    token_outputs = token_classification_model(**inputs)\n",
    "    token_logits = token_outputs.logits\n",
    "    predicted_tokens = torch.argmax(token_logits, dim=2)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    labels = ['B', 'I', 'O']  # Assuming the model is trained with BIO scheme\n",
    "\n",
    "    snippets = []\n",
    "    current_snippet = []\n",
    "\n",
    "    for token, label_id in zip(tokens, predicted_tokens[0].tolist()):\n",
    "        label = labels[label_id]\n",
    "        if label == 'B':\n",
    "            if current_snippet:\n",
    "                snippets.append(\" \".join(current_snippet))\n",
    "                current_snippet = []\n",
    "            current_snippet.append(token)\n",
    "        elif label == 'I' and current_snippet:\n",
    "            current_snippet.append(token)\n",
    "        elif label == 'O' and current_snippet:\n",
    "            snippets.append(\" \".join(current_snippet))\n",
    "            current_snippet = []\n",
    "\n",
    "    if current_snippet:\n",
    "        snippets.append(\" \".join(current_snippet))\n",
    "\n",
    "    clean_snippets = [\" \".join(snippet.replace(\"##\", \"\") for snippet in snippet.split()) for snippet in snippets]\n",
    "\n",
    "    return label, clean_snippets\n",
    "\n",
    "# Example usage\n",
    "ex = \"\"\"I decided to enroll into the physics supplemental course because I’ve had a history of struggling in my math classes. Since physics is basically another math class, I knew I would need to take extra steps to ensure my success. In the past I’ve had a packed schedule with classes and work, but this semester I allowed myself more room for school by cutting back my work hours. I originally planned on letting myself have time to attend office hours, but I realized that wouldn’t be enough.\n",
    "When I heard the option to enroll in a supplemental course for physics, I knew I had to enroll for the extra help. Not only would I have more practice with physics problems, but I would be able to connect more with my fellow classmates. I figured that if I felt like I created a little community with the supplemental course, I would feel more comfortable with physics problems. With a community I would feel comfortable enough to ask questions without having to worry about sounding unintelligent.\n",
    "Another big reason for enrolling was when I heard I wouldn’t have homework. I will have the opportunity to have the extra practice with physics problems without the stress of having to worry about my grade. I will be able to do the problems at my own pace and truly understand the steps to get the solution.\"\"\"\n",
    "\n",
    "label, snippets = classify_and_extract_snippets(ex)\n",
    "print(f'Label: {label}')\n",
    "print(f'Snippets: {snippets}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] i',\n",
       " 'course',\n",
       " 'because i',\n",
       " '’',\n",
       " 've',\n",
       " 'history',\n",
       " 'in',\n",
       " 'my math',\n",
       " 'classes',\n",
       " '.',\n",
       " 'class',\n",
       " ', i',\n",
       " 'need',\n",
       " 'take',\n",
       " 'steps to',\n",
       " 'ensure my',\n",
       " 'success',\n",
       " '. in the past i ’',\n",
       " 've',\n",
       " 'had',\n",
       " 'a',\n",
       " 'packed',\n",
       " 'schedule',\n",
       " 'with classes',\n",
       " 'and',\n",
       " 'work',\n",
       " 'semester i',\n",
       " 'allowed',\n",
       " 'myself',\n",
       " 'more',\n",
       " 'room',\n",
       " 'for',\n",
       " 'school',\n",
       " 'by',\n",
       " 'cutting',\n",
       " 'my',\n",
       " 'work',\n",
       " 'hours',\n",
       " '. i',\n",
       " 'on',\n",
       " 'myself',\n",
       " 'have',\n",
       " 'time',\n",
       " 'attend',\n",
       " 'office',\n",
       " 'hours',\n",
       " 'that',\n",
       " 'wouldn ’',\n",
       " 'be',\n",
       " '.',\n",
       " 'course',\n",
       " 'for',\n",
       " 'physics',\n",
       " 'had',\n",
       " 'for the',\n",
       " 'help',\n",
       " '.',\n",
       " 'would',\n",
       " 'i',\n",
       " 'have',\n",
       " 'more',\n",
       " 'practice with physics',\n",
       " 'problems',\n",
       " ',',\n",
       " 'i',\n",
       " 'would',\n",
       " 'be',\n",
       " 'connect',\n",
       " 'more with',\n",
       " 'fellow',\n",
       " 'classmates',\n",
       " '.',\n",
       " 'course',\n",
       " 'i',\n",
       " 'would',\n",
       " 'feel',\n",
       " 'problems',\n",
       " '.',\n",
       " 'with',\n",
       " 'would',\n",
       " 'feel comfortable',\n",
       " 'enough',\n",
       " 'ask',\n",
       " 'questions',\n",
       " '.',\n",
       " 'another',\n",
       " 'big',\n",
       " 'reason',\n",
       " 'for enroll ing',\n",
       " 'i',\n",
       " 'wouldn ’',\n",
       " 'have',\n",
       " '.',\n",
       " 'i',\n",
       " 'will',\n",
       " 'have the opportunity',\n",
       " 'have',\n",
       " 'the',\n",
       " 'practice',\n",
       " 'with physics',\n",
       " 'problems',\n",
       " 'the',\n",
       " 'of',\n",
       " 'grade',\n",
       " '.',\n",
       " 'i',\n",
       " 'will',\n",
       " 'be',\n",
       " 'do the',\n",
       " 'problems',\n",
       " 'at',\n",
       " 'my',\n",
       " 'pace',\n",
       " 'and',\n",
       " 'truly',\n",
       " 'understand',\n",
       " 'the',\n",
       " 'steps',\n",
       " 'to',\n",
       " 'get',\n",
       " 'the',\n",
       " 'solution . [SEP]']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ive had a history of struggling in my math classes. Since physics is basically another math class',\n",
       " 'I knew I would need to take extra steps to ensure my success. When I heard the option to enroll in a supplemental course for physics. I knew I had to enroll for the extra help.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
