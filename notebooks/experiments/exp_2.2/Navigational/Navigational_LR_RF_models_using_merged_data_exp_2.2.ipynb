{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigational Logistic Regression and Random Forest Models Using Merged Data Experiment 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623d3d07f10d4d99a46cf1bd3b92b25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 08:58:48 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-10-16 08:58:48 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-10-16 08:58:49 INFO: File exists: /Users/gbaldonado/stanza_resources/en/default.zip\n",
      "2024-10-16 08:58:52 INFO: Finished downloading models and saved to /Users/gbaldonado/stanza_resources\n",
      "2024-10-16 08:58:52 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb142ca1a604e9eb2abb6d66db38e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 08:58:52 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-10-16 08:58:53 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-10-16 08:58:53 INFO: Using device: cpu\n",
      "2024-10-16 08:58:53 INFO: Loading: tokenize\n",
      "2024-10-16 08:58:53 INFO: Loading: mwt\n",
      "2024-10-16 08:58:53 INFO: Loading: pos\n",
      "2024-10-16 08:58:54 INFO: Loading: lemma\n",
      "2024-10-16 08:58:54 INFO: Loading: constituency\n",
      "2024-10-16 08:58:54 INFO: Loading: depparse\n",
      "2024-10-16 08:58:54 INFO: Loading: sentiment\n",
      "2024-10-16 08:58:54 INFO: Loading: ner\n",
      "2024-10-16 08:58:55 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import pickle\n",
    "import warnings\n",
    "import stanza\n",
    "\n",
    "from random import shuffle\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, f1_score, r2_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Set random seed\n",
    "random.seed(18)\n",
    "seed = 18\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Initialize lemmatizer, stop words, and stanza\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline('en') # initialize English neural pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data and quick exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and test sets loaded.\n"
     ]
    }
   ],
   "source": [
    "merged_navigational_df_batch_1 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/merged_Navigational_sentence_level_batch_1_jaccard.csv\", encoding='utf-8')\n",
    "merged_navigational_df_batch_2 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/Navigational Plus_sentence_level_batch_2_jaccard.csv\", encoding='utf-8')\n",
    "\n",
    "merged_navigational_df = pd.concat([merged_navigational_df_batch_1, merged_navigational_df_batch_2])\n",
    "\n",
    "\n",
    "seed = 18\n",
    "# Shuffle the merged dataset\n",
    "merged_navigational_df = shuffle(merged_navigational_df, random_state=seed)\n",
    "\n",
    "# Function for undersampling or oversampling\n",
    "def resample_data(X, y, strategy='oversample', random_state=seed):\n",
    "    \"\"\"\n",
    "    Resample the data using either undersampling or oversampling.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Features\n",
    "    - y: Labels\n",
    "    - strategy: 'oversample' or 'undersample'\n",
    "    - random_state: Seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    - X_resampled, y_resampled: Resampled data and labels\n",
    "    \"\"\"\n",
    "    if strategy == 'oversample':\n",
    "        sampler = RandomOverSampler(random_state=random_state)\n",
    "    elif strategy == 'undersample':\n",
    "        sampler = RandomUnderSampler(random_state=random_state)\n",
    "    else:\n",
    "        raise ValueError(\"Strategy must be 'oversample' or 'undersample'\")\n",
    "\n",
    "    X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Separate features and labels\n",
    "X = merged_navigational_df.drop(columns=['label'])  # Replace 'label' with your target column name\n",
    "y = merged_navigational_df['label']\n",
    "\n",
    "# Toggle resampling\n",
    "resample = True  # Set this to False to turn off resampling\n",
    "\n",
    "if resample:\n",
    "    # Apply resampling (choose 'oversample' or 'undersample')\n",
    "    X_resampled, y_resampled = resample_data(X, y, strategy='oversample', random_state=seed)\n",
    "\n",
    "    # Combine resampled data into a single DataFrame\n",
    "    resampled_df = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "else:\n",
    "    # No resampling, use original dataset\n",
    "    resampled_df = merged_navigational_df\n",
    "\n",
    "# Train-test split\n",
    "training_df, test_df = train_test_split(resampled_df, test_size=0.1, random_state=18, stratify=resampled_df['label'])\n",
    "\n",
    "\n",
    "# # Train-test split\n",
    "training_df, test_df = train_test_split(resampled_df, test_size=0.1, random_state=18, stratify=resampled_df['label'])\n",
    "\n",
    "training_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Training and test sets loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (13903, 3) \n",
      "Test dataset shape: (1545, 3)\n",
      "Positive labels present in the dataset : 6952  out of 13903 or 50.003596346112346%\n",
      "Positive labels present in the test dataset : 772  out of 1545 or 49.967637540453076%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset shape: {training_df.shape} \\nTest dataset shape: {test_df.shape}\")\n",
    "pos_labels = len([n for n in training_df['label'] if n==1])\n",
    "print(\"Positive labels present in the dataset : {}  out of {} or {}%\".format(pos_labels, len(training_df['label']), (pos_labels/len(training_df['label']))*100))\n",
    "pos_labels = len([n for n in test_df['label'] if n==1])\n",
    "print(\"Positive labels present in the test dataset : {}  out of {} or {}%\".format(pos_labels, len(test_df['label']), (pos_labels/len(test_df['label']))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the data for the plots\n",
    "training_counts = training_df['label'].value_counts()\n",
    "test_counts = test_df['label'].value_counts()\n",
    "\n",
    "# Set up the subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot for the training set\n",
    "sns.barplot(x=training_counts.index, y=training_counts.values, ax=axes[0])\n",
    "axes[0].set_title('Distribution of labels in training set')\n",
    "axes[0].set_ylabel('Sentences')\n",
    "axes[0].set_xlabel('Label')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot for the test set\n",
    "sns.barplot(x=test_counts.index, y=test_counts.values, ax=axes[1])\n",
    "axes[1].set_title('Distribution of labels in test set')\n",
    "axes[1].set_ylabel('Sentences')\n",
    "axes[1].set_xlabel('Label')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack Obama']\n"
     ]
    }
   ],
   "source": [
    "def get_ner(text):\n",
    "    ner_list = []\n",
    "    # Annotate the text using stanza\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for entity in sentence.ents:\n",
    "            if entity.type == 'PERSON':\n",
    "                ner_list.append(entity.text)\n",
    "\n",
    "    return ner_list\n",
    "\n",
    "# Example usage\n",
    "text = \"Barack Obama was the 44th doctor of the United States.\"\n",
    "print(get_ner(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a named entity is present in the sentence\n",
    "def named_entity_present(sentence):\n",
    "    ner_list = get_ner(sentence)\n",
    "    if len(ner_list) > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Similarity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the similar words and similarity score\n",
    "# The function takes tokens of sentence as input and if its not a stop word, get its similarity with synsets of STEM.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words |= set([\"help\",\"try\", \"work\", \"process\", \"support\", \"job\"] )\n",
    "def word_similarity(tokens, syns, field):    \n",
    "    if field in ['engineering', 'technology']:\n",
    "        score_threshold = 0.5\n",
    "    else:\n",
    "        score_threshold = 0.2\n",
    "    sim_words = 0\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            try:\n",
    "                syns_word = wordnet.synsets(token) \n",
    "                score = syns_word[0].path_similarity(syns[0])\n",
    "                if score >= score_threshold:\n",
    "                    sim_words += 1\n",
    "            except: \n",
    "                score = 0\n",
    "    \n",
    "    return sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to create columns for similarity based on all STEM fields\n",
    "syns_bio = wordnet.synsets(lemmatizer.lemmatize(\"biology\"))\n",
    "syns_maths = wordnet.synsets(lemmatizer.lemmatize(\"mathematics\")) \n",
    "syns_tech = wordnet.synsets(lemmatizer.lemmatize(\"technology\"))\n",
    "syns_eng = wordnet.synsets(lemmatizer.lemmatize(\"engineering\"))\n",
    "syns_chem = wordnet.synsets(lemmatizer.lemmatize(\"chemistry\"))\n",
    "syns_phy = wordnet.synsets(lemmatizer.lemmatize(\"physics\"))\n",
    "syns_sci = wordnet.synsets(lemmatizer.lemmatize(\"science\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Medical Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['endovascular', 'internal', 'urology', 'vascular', 'psychiatry', 'radiology', 'health', 'brain', 'and', 'strabismus', 'pediatric', 'nephrology', 'maternal', 'critical', 'procedural', 'neurology', 'pelvic', 'military', 'neuroradiology', 'liaison', 'gastrointestinal', 'neuroradiology', 'neurodevelopmental', 'hospice', 'genetics', 'gastroenterology', 'endocrinologists', 'cytopathology', 'rheumatology', 'hepatology', 'microbiology', 'gynecologic', 'orbit', 'neurophysiology', 'disease', 'preventive', 'forensic', 'psychosomatic', 'diagnostic', 'pediatric', 'abuse', 'genitourinary', 'metabolism', 'developmental', 'pain', 'anterior', 'ocular', 'gastroenterology', 'failure', 'addiction', 'medicine', 'internal', 'pathology', 'imaging', 'toxicology', 'ophthalmology', 'dermatology', 'ophthalmology', 'uveitis', 'community', 'nuclear', 'renal', 'sleep', 'banking', 'glaucoma', 'perinatal', 'neuropathology', 'female', 'psychiatry', 'administrative', 'interventional', 'cardiovascular', 'allergy', 'anesthesiology', 'oculoplastics', 'advanced', 'aerospace', 'pulmonary', 'gynecology', 'endocrinology', 'geriatric', 'abdominal', 'mental', 'diabetes', 'cytogenetics', 'immunopathology', 'hematology', 'infertility', 'genetic', 'radiation', 'chest', 'neurology', 'chemical', 'reconstructive', 'infectious', 'oncology', 'neurourology', 'cardiology', 'heart', 'psychiatric', 'infectious', 'emergency', 'musculoskeletal', 'adolescent', 'dermatopathology', 'clinical', 'genetic', 'retardation', 'neonatal', 'diseases', 'care', 'anatomical', 'transplant', 'segment', 'dermatology', 'injury', 'reconstructive', 'hematology', 'oncology', 'plastic', 'rehabilitation', 'sports', 'ophthalmic', 'interventional', 'research', 'neck', 'pediatrics', 'pathology', 'urologic', 'consultation', 'occupational', 'critical', 'adolescent', 'cardiothoracic', 'behavioral', 'calculi', 'sports', 'breast', 'head', 'child', 'nephrology', 'surgical', 'cardiac', 'rheumatology', 'disabilities', 'male', 'blood', 'pediatrics', 'pulmonology', 'public', 'physical', 'surgery', 'immunology', 'biochemical', 'transplant', 'endocrinology', 'palliative', 'electrophysiology', 'neuromuscular', 'molecular', 'anesthesiology', 'medical', 'fetal', 'reproductive', 'cornea', 'neuro', 'obstetrics', 'retina', 'family', 'transfusion', 'urology', 'surgery']\n"
     ]
    }
   ],
   "source": [
    "# Load the medical specialization text file and create a list\n",
    "medical_list = []\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/medical_specialities.txt', 'r') as medical_fields:\n",
    "    for line in medical_fields.readlines():\n",
    "        special_field = line.rstrip('\\n')\n",
    "        special_field = re.sub(\"\\W\",\" \", special_field )\n",
    "#         print(special_field)\n",
    "        medical_list += special_field.split()\n",
    "medical_list = list(set(medical_list))  \n",
    "medical_list = [x.lower() for x in medical_list]\n",
    "print(medical_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get medical words\n",
    "def check_medical_words(tokens):\n",
    "    for token in tokens:\n",
    "        if token not in stop_words and token in [x.lower() for x in medical_list]:\n",
    "            return 1\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sentiment Polarity and Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get polarity and subjectivity of the sentence using TexBlob\n",
    "def get_sentiment(sentence):\n",
    "    sentiments =TextBlob(sentence).sentiment\n",
    "    polarity = sentiments.polarity\n",
    "    subjectivity = sentiments.subjectivity\n",
    "    return polarity, subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the count of POS tags of the sentence\n",
    "def count_pos_tags(tokens):\n",
    "    token_pos = pos_tag(tokens)\n",
    "    count = Counter(tag for word,tag in token_pos)\n",
    "    interjections =  count['UH']\n",
    "    nouns = count['NN'] + count['NNS'] + count['NNP'] + count['NNPS']\n",
    "    adverb = count['RB'] + count['RBS'] + count['RBR']\n",
    "    verb = count['VB'] + count['VBD'] + count['VBG'] + count['VBN']\n",
    "    determiner = count['DT']\n",
    "    pronoun = count['PRP']\n",
    "    adjetive = count['JJ'] + count['JJR'] + count['JJS']\n",
    "    preposition = count['IN']\n",
    "    return interjections, nouns, adverb, verb, determiner, pronoun, adjetive,preposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_extraction(dataframe, field, func, column_names):\n",
    "    return pd.concat((\n",
    "        dataframe,\n",
    "        dataframe[field].apply(\n",
    "            lambda cell: pd.Series(func(cell), index=column_names))), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the w2v dict from pickle file\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/pickle/embeddings06122024.pickle', 'rb') as w2v_file:\n",
    "    w2v_dict = pickle.load(w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of word embeddings:  4762\n"
     ]
    }
   ],
   "source": [
    "print(\"length of word embeddings: \", len(w2v_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vectors for the essay\n",
    "def vectorizer(sequence):\n",
    "    vect = []\n",
    "    numw = 0\n",
    "    for w in sequence: \n",
    "        try :\n",
    "            if numw == 0:\n",
    "                vect = w2v_dict[w]\n",
    "            else:\n",
    "                vect = np.add(vect, w2v_dict[w])\n",
    "            numw += 1\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    return vect/ numw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into words\n",
    "def split_into_words(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vectorizer\n",
    "unigram_vect = CountVectorizer(ngram_range=(1, 1), min_df=2, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Putting them all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for feature engineering\n",
    "def feature_engineering(original_dataset):\n",
    "\n",
    "    dataset = original_dataset.copy()\n",
    "    # create a new column with sentence tokens\n",
    "    dataset['tokens'] = dataset['sentence'].apply(word_tokenize)\n",
    "    # 1. Similarity features\n",
    "    # biology\n",
    "    dataset['bio_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_bio,'biology',)) \n",
    "    # chemistry\n",
    "    dataset['chem_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_chem,'chemistry',))\n",
    "    # physics\n",
    "    dataset['phy_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_phy,'physics',))\n",
    "    # mathematics\n",
    "    dataset['math_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_maths,'mathematics',))\n",
    "    # technology\n",
    "    dataset['tech_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_tech,'technology',))\n",
    "    # engineering\n",
    "    dataset['eng_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_eng,'engineering',))\n",
    "    \n",
    "    # medical terms\n",
    "    dataset['medical_terms'] = dataset['tokens'].apply(check_medical_words)\n",
    "    \n",
    "    # polarity and subjectivity\n",
    "    dataset['polarity'], dataset['subjectivity'] = zip(*dataset['sentence'].apply(get_sentiment))\n",
    "    \n",
    "    # named entity recognition\n",
    "    dataset['ner'] = dataset['sentence'].apply(named_entity_present)\n",
    "    \n",
    "    # pos tag count\n",
    "    dataset = pos_tag_extraction(dataset, 'tokens', count_pos_tags, ['interjections', 'nouns', 'adverb', 'verb', 'determiner', 'pronoun', 'adjetive','preposition'])\n",
    "    \n",
    "    # labels\n",
    "    data_labels = dataset['label']\n",
    "    # X\n",
    "    data_x = dataset.drop(columns='label')\n",
    "\n",
    "    \n",
    "    # vectorize all the essays\n",
    "    vect_arr = data_x.tokens.apply(vectorizer)\n",
    "    for index in range(0, len(vect_arr)):\n",
    "        i = 0\n",
    "        for item in vect_arr[index]:\n",
    "            column_name= \"embedding\" + str(i)\n",
    "            data_x.loc[index, column_name] = item\n",
    "            i +=1\n",
    "    \n",
    "    return data_x,data_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = feature_engineering(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(860, 121)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = feature_engineering(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 121)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Unigram features for both train and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(860, 121)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 121)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2.1/Navigational/saved_features/X_train_final.csv\", index=False)\n",
    "X_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2.1/Navigational/saved_features/X_test_final.csv\", index=False)\n",
    "y_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2.1/Navigational/saved_features/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2.1/Navigational/saved_features/y_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the unigram df for train :  (860, 743)\n"
     ]
    }
   ],
   "source": [
    "# Unigrams for training set\n",
    "unigram_matrix = unigram_vect.fit_transform(X_train['sentence'])\n",
    "unigrams = pd.DataFrame(unigram_matrix.toarray())\n",
    "print(\"Shape of the unigram df for train : \",unigrams.shape)\n",
    "unigrams = unigrams.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = pd.concat([X_train, unigrams], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final.columns = X_train_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(860, 864)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test unigram df shape :  (96, 743)\n"
     ]
    }
   ],
   "source": [
    "unigram_matrix_test = unigram_vect.transform(X_test['sentence'])\n",
    "unigrams_test = pd.DataFrame(unigram_matrix_test.toarray())\n",
    "unigrams_test = unigrams_test.reset_index(drop=True)\n",
    "print(\"Test unigram df shape : \",unigrams_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 864)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final = pd.concat([X_test, unigrams_test], axis = 1)\n",
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final.columns = X_test_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 864)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ---- sentence\n",
      "1 ---- phrase\n",
      "2 ---- tokens\n",
      "3 ---- bio_sim_words\n",
      "4 ---- chem_sim_words\n",
      "5 ---- phy_sim_words\n",
      "6 ---- math_sim_words\n",
      "7 ---- tech_sim_words\n",
      "8 ---- eng_sim_words\n",
      "9 ---- medical_terms\n",
      "10 ---- polarity\n",
      "11 ---- subjectivity\n",
      "12 ---- ner\n",
      "13 ---- interjections\n",
      "14 ---- nouns\n",
      "15 ---- adverb\n",
      "16 ---- verb\n",
      "17 ---- determiner\n",
      "18 ---- pronoun\n",
      "19 ---- adjetive\n",
      "20 ---- preposition\n",
      "21 ---- embedding0\n",
      "22 ---- embedding1\n",
      "23 ---- embedding2\n",
      "24 ---- embedding3\n",
      "25 ---- embedding4\n",
      "26 ---- embedding5\n",
      "27 ---- embedding6\n",
      "28 ---- embedding7\n",
      "29 ---- embedding8\n",
      "30 ---- embedding9\n",
      "31 ---- embedding10\n",
      "32 ---- embedding11\n",
      "33 ---- embedding12\n",
      "34 ---- embedding13\n",
      "35 ---- embedding14\n",
      "36 ---- embedding15\n",
      "37 ---- embedding16\n",
      "38 ---- embedding17\n",
      "39 ---- embedding18\n",
      "40 ---- embedding19\n",
      "41 ---- embedding20\n",
      "42 ---- embedding21\n",
      "43 ---- embedding22\n",
      "44 ---- embedding23\n",
      "45 ---- embedding24\n",
      "46 ---- embedding25\n",
      "47 ---- embedding26\n",
      "48 ---- embedding27\n",
      "49 ---- embedding28\n",
      "50 ---- embedding29\n",
      "51 ---- embedding30\n",
      "52 ---- embedding31\n",
      "53 ---- embedding32\n",
      "54 ---- embedding33\n",
      "55 ---- embedding34\n",
      "56 ---- embedding35\n",
      "57 ---- embedding36\n",
      "58 ---- embedding37\n",
      "59 ---- embedding38\n",
      "60 ---- embedding39\n",
      "61 ---- embedding40\n",
      "62 ---- embedding41\n",
      "63 ---- embedding42\n",
      "64 ---- embedding43\n",
      "65 ---- embedding44\n",
      "66 ---- embedding45\n",
      "67 ---- embedding46\n",
      "68 ---- embedding47\n",
      "69 ---- embedding48\n",
      "70 ---- embedding49\n",
      "71 ---- embedding50\n",
      "72 ---- embedding51\n",
      "73 ---- embedding52\n",
      "74 ---- embedding53\n",
      "75 ---- embedding54\n",
      "76 ---- embedding55\n",
      "77 ---- embedding56\n",
      "78 ---- embedding57\n",
      "79 ---- embedding58\n",
      "80 ---- embedding59\n",
      "81 ---- embedding60\n",
      "82 ---- embedding61\n",
      "83 ---- embedding62\n",
      "84 ---- embedding63\n",
      "85 ---- embedding64\n",
      "86 ---- embedding65\n",
      "87 ---- embedding66\n",
      "88 ---- embedding67\n",
      "89 ---- embedding68\n",
      "90 ---- embedding69\n",
      "91 ---- embedding70\n",
      "92 ---- embedding71\n",
      "93 ---- embedding72\n",
      "94 ---- embedding73\n",
      "95 ---- embedding74\n",
      "96 ---- embedding75\n",
      "97 ---- embedding76\n",
      "98 ---- embedding77\n",
      "99 ---- embedding78\n",
      "100 ---- embedding79\n",
      "101 ---- embedding80\n",
      "102 ---- embedding81\n",
      "103 ---- embedding82\n",
      "104 ---- embedding83\n",
      "105 ---- embedding84\n",
      "106 ---- embedding85\n",
      "107 ---- embedding86\n",
      "108 ---- embedding87\n",
      "109 ---- embedding88\n",
      "110 ---- embedding89\n",
      "111 ---- embedding90\n",
      "112 ---- embedding91\n",
      "113 ---- embedding92\n",
      "114 ---- embedding93\n",
      "115 ---- embedding94\n",
      "116 ---- embedding95\n",
      "117 ---- embedding96\n",
      "118 ---- embedding97\n",
      "119 ---- embedding98\n",
      "120 ---- embedding99\n",
      "121 ---- 0\n",
      "122 ---- 1\n",
      "123 ---- 2\n",
      "124 ---- 3\n",
      "125 ---- 4\n",
      "126 ---- 5\n",
      "127 ---- 6\n",
      "128 ---- 7\n",
      "129 ---- 8\n",
      "130 ---- 9\n",
      "131 ---- 10\n",
      "132 ---- 11\n",
      "133 ---- 12\n",
      "134 ---- 13\n",
      "135 ---- 14\n",
      "136 ---- 15\n",
      "137 ---- 16\n",
      "138 ---- 17\n",
      "139 ---- 18\n",
      "140 ---- 19\n",
      "141 ---- 20\n",
      "142 ---- 21\n",
      "143 ---- 22\n",
      "144 ---- 23\n",
      "145 ---- 24\n",
      "146 ---- 25\n",
      "147 ---- 26\n",
      "148 ---- 27\n",
      "149 ---- 28\n",
      "150 ---- 29\n",
      "151 ---- 30\n",
      "152 ---- 31\n",
      "153 ---- 32\n",
      "154 ---- 33\n",
      "155 ---- 34\n",
      "156 ---- 35\n",
      "157 ---- 36\n",
      "158 ---- 37\n",
      "159 ---- 38\n",
      "160 ---- 39\n",
      "161 ---- 40\n",
      "162 ---- 41\n",
      "163 ---- 42\n",
      "164 ---- 43\n",
      "165 ---- 44\n",
      "166 ---- 45\n",
      "167 ---- 46\n",
      "168 ---- 47\n",
      "169 ---- 48\n",
      "170 ---- 49\n",
      "171 ---- 50\n",
      "172 ---- 51\n",
      "173 ---- 52\n",
      "174 ---- 53\n",
      "175 ---- 54\n",
      "176 ---- 55\n",
      "177 ---- 56\n",
      "178 ---- 57\n",
      "179 ---- 58\n",
      "180 ---- 59\n",
      "181 ---- 60\n",
      "182 ---- 61\n",
      "183 ---- 62\n",
      "184 ---- 63\n",
      "185 ---- 64\n",
      "186 ---- 65\n",
      "187 ---- 66\n",
      "188 ---- 67\n",
      "189 ---- 68\n",
      "190 ---- 69\n",
      "191 ---- 70\n",
      "192 ---- 71\n",
      "193 ---- 72\n",
      "194 ---- 73\n",
      "195 ---- 74\n",
      "196 ---- 75\n",
      "197 ---- 76\n",
      "198 ---- 77\n",
      "199 ---- 78\n",
      "200 ---- 79\n",
      "201 ---- 80\n",
      "202 ---- 81\n",
      "203 ---- 82\n",
      "204 ---- 83\n",
      "205 ---- 84\n",
      "206 ---- 85\n",
      "207 ---- 86\n",
      "208 ---- 87\n",
      "209 ---- 88\n",
      "210 ---- 89\n",
      "211 ---- 90\n",
      "212 ---- 91\n",
      "213 ---- 92\n",
      "214 ---- 93\n",
      "215 ---- 94\n",
      "216 ---- 95\n",
      "217 ---- 96\n",
      "218 ---- 97\n",
      "219 ---- 98\n",
      "220 ---- 99\n",
      "221 ---- 100\n",
      "222 ---- 101\n",
      "223 ---- 102\n",
      "224 ---- 103\n",
      "225 ---- 104\n",
      "226 ---- 105\n",
      "227 ---- 106\n",
      "228 ---- 107\n",
      "229 ---- 108\n",
      "230 ---- 109\n",
      "231 ---- 110\n",
      "232 ---- 111\n",
      "233 ---- 112\n",
      "234 ---- 113\n",
      "235 ---- 114\n",
      "236 ---- 115\n",
      "237 ---- 116\n",
      "238 ---- 117\n",
      "239 ---- 118\n",
      "240 ---- 119\n",
      "241 ---- 120\n",
      "242 ---- 121\n",
      "243 ---- 122\n",
      "244 ---- 123\n",
      "245 ---- 124\n",
      "246 ---- 125\n",
      "247 ---- 126\n",
      "248 ---- 127\n",
      "249 ---- 128\n",
      "250 ---- 129\n",
      "251 ---- 130\n",
      "252 ---- 131\n",
      "253 ---- 132\n",
      "254 ---- 133\n",
      "255 ---- 134\n",
      "256 ---- 135\n",
      "257 ---- 136\n",
      "258 ---- 137\n",
      "259 ---- 138\n",
      "260 ---- 139\n",
      "261 ---- 140\n",
      "262 ---- 141\n",
      "263 ---- 142\n",
      "264 ---- 143\n",
      "265 ---- 144\n",
      "266 ---- 145\n",
      "267 ---- 146\n",
      "268 ---- 147\n",
      "269 ---- 148\n",
      "270 ---- 149\n",
      "271 ---- 150\n",
      "272 ---- 151\n",
      "273 ---- 152\n",
      "274 ---- 153\n",
      "275 ---- 154\n",
      "276 ---- 155\n",
      "277 ---- 156\n",
      "278 ---- 157\n",
      "279 ---- 158\n",
      "280 ---- 159\n",
      "281 ---- 160\n",
      "282 ---- 161\n",
      "283 ---- 162\n",
      "284 ---- 163\n",
      "285 ---- 164\n",
      "286 ---- 165\n",
      "287 ---- 166\n",
      "288 ---- 167\n",
      "289 ---- 168\n",
      "290 ---- 169\n",
      "291 ---- 170\n",
      "292 ---- 171\n",
      "293 ---- 172\n",
      "294 ---- 173\n",
      "295 ---- 174\n",
      "296 ---- 175\n",
      "297 ---- 176\n",
      "298 ---- 177\n",
      "299 ---- 178\n",
      "300 ---- 179\n",
      "301 ---- 180\n",
      "302 ---- 181\n",
      "303 ---- 182\n",
      "304 ---- 183\n",
      "305 ---- 184\n",
      "306 ---- 185\n",
      "307 ---- 186\n",
      "308 ---- 187\n",
      "309 ---- 188\n",
      "310 ---- 189\n",
      "311 ---- 190\n",
      "312 ---- 191\n",
      "313 ---- 192\n",
      "314 ---- 193\n",
      "315 ---- 194\n",
      "316 ---- 195\n",
      "317 ---- 196\n",
      "318 ---- 197\n",
      "319 ---- 198\n",
      "320 ---- 199\n",
      "321 ---- 200\n",
      "322 ---- 201\n",
      "323 ---- 202\n",
      "324 ---- 203\n",
      "325 ---- 204\n",
      "326 ---- 205\n",
      "327 ---- 206\n",
      "328 ---- 207\n",
      "329 ---- 208\n",
      "330 ---- 209\n",
      "331 ---- 210\n",
      "332 ---- 211\n",
      "333 ---- 212\n",
      "334 ---- 213\n",
      "335 ---- 214\n",
      "336 ---- 215\n",
      "337 ---- 216\n",
      "338 ---- 217\n",
      "339 ---- 218\n",
      "340 ---- 219\n",
      "341 ---- 220\n",
      "342 ---- 221\n",
      "343 ---- 222\n",
      "344 ---- 223\n",
      "345 ---- 224\n",
      "346 ---- 225\n",
      "347 ---- 226\n",
      "348 ---- 227\n",
      "349 ---- 228\n",
      "350 ---- 229\n",
      "351 ---- 230\n",
      "352 ---- 231\n",
      "353 ---- 232\n",
      "354 ---- 233\n",
      "355 ---- 234\n",
      "356 ---- 235\n",
      "357 ---- 236\n",
      "358 ---- 237\n",
      "359 ---- 238\n",
      "360 ---- 239\n",
      "361 ---- 240\n",
      "362 ---- 241\n",
      "363 ---- 242\n",
      "364 ---- 243\n",
      "365 ---- 244\n",
      "366 ---- 245\n",
      "367 ---- 246\n",
      "368 ---- 247\n",
      "369 ---- 248\n",
      "370 ---- 249\n",
      "371 ---- 250\n",
      "372 ---- 251\n",
      "373 ---- 252\n",
      "374 ---- 253\n",
      "375 ---- 254\n",
      "376 ---- 255\n",
      "377 ---- 256\n",
      "378 ---- 257\n",
      "379 ---- 258\n",
      "380 ---- 259\n",
      "381 ---- 260\n",
      "382 ---- 261\n",
      "383 ---- 262\n",
      "384 ---- 263\n",
      "385 ---- 264\n",
      "386 ---- 265\n",
      "387 ---- 266\n",
      "388 ---- 267\n",
      "389 ---- 268\n",
      "390 ---- 269\n",
      "391 ---- 270\n",
      "392 ---- 271\n",
      "393 ---- 272\n",
      "394 ---- 273\n",
      "395 ---- 274\n",
      "396 ---- 275\n",
      "397 ---- 276\n",
      "398 ---- 277\n",
      "399 ---- 278\n",
      "400 ---- 279\n",
      "401 ---- 280\n",
      "402 ---- 281\n",
      "403 ---- 282\n",
      "404 ---- 283\n",
      "405 ---- 284\n",
      "406 ---- 285\n",
      "407 ---- 286\n",
      "408 ---- 287\n",
      "409 ---- 288\n",
      "410 ---- 289\n",
      "411 ---- 290\n",
      "412 ---- 291\n",
      "413 ---- 292\n",
      "414 ---- 293\n",
      "415 ---- 294\n",
      "416 ---- 295\n",
      "417 ---- 296\n",
      "418 ---- 297\n",
      "419 ---- 298\n",
      "420 ---- 299\n",
      "421 ---- 300\n",
      "422 ---- 301\n",
      "423 ---- 302\n",
      "424 ---- 303\n",
      "425 ---- 304\n",
      "426 ---- 305\n",
      "427 ---- 306\n",
      "428 ---- 307\n",
      "429 ---- 308\n",
      "430 ---- 309\n",
      "431 ---- 310\n",
      "432 ---- 311\n",
      "433 ---- 312\n",
      "434 ---- 313\n",
      "435 ---- 314\n",
      "436 ---- 315\n",
      "437 ---- 316\n",
      "438 ---- 317\n",
      "439 ---- 318\n",
      "440 ---- 319\n",
      "441 ---- 320\n",
      "442 ---- 321\n",
      "443 ---- 322\n",
      "444 ---- 323\n",
      "445 ---- 324\n",
      "446 ---- 325\n",
      "447 ---- 326\n",
      "448 ---- 327\n",
      "449 ---- 328\n",
      "450 ---- 329\n",
      "451 ---- 330\n",
      "452 ---- 331\n",
      "453 ---- 332\n",
      "454 ---- 333\n",
      "455 ---- 334\n",
      "456 ---- 335\n",
      "457 ---- 336\n",
      "458 ---- 337\n",
      "459 ---- 338\n",
      "460 ---- 339\n",
      "461 ---- 340\n",
      "462 ---- 341\n",
      "463 ---- 342\n",
      "464 ---- 343\n",
      "465 ---- 344\n",
      "466 ---- 345\n",
      "467 ---- 346\n",
      "468 ---- 347\n",
      "469 ---- 348\n",
      "470 ---- 349\n",
      "471 ---- 350\n",
      "472 ---- 351\n",
      "473 ---- 352\n",
      "474 ---- 353\n",
      "475 ---- 354\n",
      "476 ---- 355\n",
      "477 ---- 356\n",
      "478 ---- 357\n",
      "479 ---- 358\n",
      "480 ---- 359\n",
      "481 ---- 360\n",
      "482 ---- 361\n",
      "483 ---- 362\n",
      "484 ---- 363\n",
      "485 ---- 364\n",
      "486 ---- 365\n",
      "487 ---- 366\n",
      "488 ---- 367\n",
      "489 ---- 368\n",
      "490 ---- 369\n",
      "491 ---- 370\n",
      "492 ---- 371\n",
      "493 ---- 372\n",
      "494 ---- 373\n",
      "495 ---- 374\n",
      "496 ---- 375\n",
      "497 ---- 376\n",
      "498 ---- 377\n",
      "499 ---- 378\n",
      "500 ---- 379\n",
      "501 ---- 380\n",
      "502 ---- 381\n",
      "503 ---- 382\n",
      "504 ---- 383\n",
      "505 ---- 384\n",
      "506 ---- 385\n",
      "507 ---- 386\n",
      "508 ---- 387\n",
      "509 ---- 388\n",
      "510 ---- 389\n",
      "511 ---- 390\n",
      "512 ---- 391\n",
      "513 ---- 392\n",
      "514 ---- 393\n",
      "515 ---- 394\n",
      "516 ---- 395\n",
      "517 ---- 396\n",
      "518 ---- 397\n",
      "519 ---- 398\n",
      "520 ---- 399\n",
      "521 ---- 400\n",
      "522 ---- 401\n",
      "523 ---- 402\n",
      "524 ---- 403\n",
      "525 ---- 404\n",
      "526 ---- 405\n",
      "527 ---- 406\n",
      "528 ---- 407\n",
      "529 ---- 408\n",
      "530 ---- 409\n",
      "531 ---- 410\n",
      "532 ---- 411\n",
      "533 ---- 412\n",
      "534 ---- 413\n",
      "535 ---- 414\n",
      "536 ---- 415\n",
      "537 ---- 416\n",
      "538 ---- 417\n",
      "539 ---- 418\n",
      "540 ---- 419\n",
      "541 ---- 420\n",
      "542 ---- 421\n",
      "543 ---- 422\n",
      "544 ---- 423\n",
      "545 ---- 424\n",
      "546 ---- 425\n",
      "547 ---- 426\n",
      "548 ---- 427\n",
      "549 ---- 428\n",
      "550 ---- 429\n",
      "551 ---- 430\n",
      "552 ---- 431\n",
      "553 ---- 432\n",
      "554 ---- 433\n",
      "555 ---- 434\n",
      "556 ---- 435\n",
      "557 ---- 436\n",
      "558 ---- 437\n",
      "559 ---- 438\n",
      "560 ---- 439\n",
      "561 ---- 440\n",
      "562 ---- 441\n",
      "563 ---- 442\n",
      "564 ---- 443\n",
      "565 ---- 444\n",
      "566 ---- 445\n",
      "567 ---- 446\n",
      "568 ---- 447\n",
      "569 ---- 448\n",
      "570 ---- 449\n",
      "571 ---- 450\n",
      "572 ---- 451\n",
      "573 ---- 452\n",
      "574 ---- 453\n",
      "575 ---- 454\n",
      "576 ---- 455\n",
      "577 ---- 456\n",
      "578 ---- 457\n",
      "579 ---- 458\n",
      "580 ---- 459\n",
      "581 ---- 460\n",
      "582 ---- 461\n",
      "583 ---- 462\n",
      "584 ---- 463\n",
      "585 ---- 464\n",
      "586 ---- 465\n",
      "587 ---- 466\n",
      "588 ---- 467\n",
      "589 ---- 468\n",
      "590 ---- 469\n",
      "591 ---- 470\n",
      "592 ---- 471\n",
      "593 ---- 472\n",
      "594 ---- 473\n",
      "595 ---- 474\n",
      "596 ---- 475\n",
      "597 ---- 476\n",
      "598 ---- 477\n",
      "599 ---- 478\n",
      "600 ---- 479\n",
      "601 ---- 480\n",
      "602 ---- 481\n",
      "603 ---- 482\n",
      "604 ---- 483\n",
      "605 ---- 484\n",
      "606 ---- 485\n",
      "607 ---- 486\n",
      "608 ---- 487\n",
      "609 ---- 488\n",
      "610 ---- 489\n",
      "611 ---- 490\n",
      "612 ---- 491\n",
      "613 ---- 492\n",
      "614 ---- 493\n",
      "615 ---- 494\n",
      "616 ---- 495\n",
      "617 ---- 496\n",
      "618 ---- 497\n",
      "619 ---- 498\n",
      "620 ---- 499\n",
      "621 ---- 500\n",
      "622 ---- 501\n",
      "623 ---- 502\n",
      "624 ---- 503\n",
      "625 ---- 504\n",
      "626 ---- 505\n",
      "627 ---- 506\n",
      "628 ---- 507\n",
      "629 ---- 508\n",
      "630 ---- 509\n",
      "631 ---- 510\n",
      "632 ---- 511\n",
      "633 ---- 512\n",
      "634 ---- 513\n",
      "635 ---- 514\n",
      "636 ---- 515\n",
      "637 ---- 516\n",
      "638 ---- 517\n",
      "639 ---- 518\n",
      "640 ---- 519\n",
      "641 ---- 520\n",
      "642 ---- 521\n",
      "643 ---- 522\n",
      "644 ---- 523\n",
      "645 ---- 524\n",
      "646 ---- 525\n",
      "647 ---- 526\n",
      "648 ---- 527\n",
      "649 ---- 528\n",
      "650 ---- 529\n",
      "651 ---- 530\n",
      "652 ---- 531\n",
      "653 ---- 532\n",
      "654 ---- 533\n",
      "655 ---- 534\n",
      "656 ---- 535\n",
      "657 ---- 536\n",
      "658 ---- 537\n",
      "659 ---- 538\n",
      "660 ---- 539\n",
      "661 ---- 540\n",
      "662 ---- 541\n",
      "663 ---- 542\n",
      "664 ---- 543\n",
      "665 ---- 544\n",
      "666 ---- 545\n",
      "667 ---- 546\n",
      "668 ---- 547\n",
      "669 ---- 548\n",
      "670 ---- 549\n",
      "671 ---- 550\n",
      "672 ---- 551\n",
      "673 ---- 552\n",
      "674 ---- 553\n",
      "675 ---- 554\n",
      "676 ---- 555\n",
      "677 ---- 556\n",
      "678 ---- 557\n",
      "679 ---- 558\n",
      "680 ---- 559\n",
      "681 ---- 560\n",
      "682 ---- 561\n",
      "683 ---- 562\n",
      "684 ---- 563\n",
      "685 ---- 564\n",
      "686 ---- 565\n",
      "687 ---- 566\n",
      "688 ---- 567\n",
      "689 ---- 568\n",
      "690 ---- 569\n",
      "691 ---- 570\n",
      "692 ---- 571\n",
      "693 ---- 572\n",
      "694 ---- 573\n",
      "695 ---- 574\n",
      "696 ---- 575\n",
      "697 ---- 576\n",
      "698 ---- 577\n",
      "699 ---- 578\n",
      "700 ---- 579\n",
      "701 ---- 580\n",
      "702 ---- 581\n",
      "703 ---- 582\n",
      "704 ---- 583\n",
      "705 ---- 584\n",
      "706 ---- 585\n",
      "707 ---- 586\n",
      "708 ---- 587\n",
      "709 ---- 588\n",
      "710 ---- 589\n",
      "711 ---- 590\n",
      "712 ---- 591\n",
      "713 ---- 592\n",
      "714 ---- 593\n",
      "715 ---- 594\n",
      "716 ---- 595\n",
      "717 ---- 596\n",
      "718 ---- 597\n",
      "719 ---- 598\n",
      "720 ---- 599\n",
      "721 ---- 600\n",
      "722 ---- 601\n",
      "723 ---- 602\n",
      "724 ---- 603\n",
      "725 ---- 604\n",
      "726 ---- 605\n",
      "727 ---- 606\n",
      "728 ---- 607\n",
      "729 ---- 608\n",
      "730 ---- 609\n",
      "731 ---- 610\n",
      "732 ---- 611\n",
      "733 ---- 612\n",
      "734 ---- 613\n",
      "735 ---- 614\n",
      "736 ---- 615\n",
      "737 ---- 616\n",
      "738 ---- 617\n",
      "739 ---- 618\n",
      "740 ---- 619\n",
      "741 ---- 620\n",
      "742 ---- 621\n",
      "743 ---- 622\n",
      "744 ---- 623\n",
      "745 ---- 624\n",
      "746 ---- 625\n",
      "747 ---- 626\n",
      "748 ---- 627\n",
      "749 ---- 628\n",
      "750 ---- 629\n",
      "751 ---- 630\n",
      "752 ---- 631\n",
      "753 ---- 632\n",
      "754 ---- 633\n",
      "755 ---- 634\n",
      "756 ---- 635\n",
      "757 ---- 636\n",
      "758 ---- 637\n",
      "759 ---- 638\n",
      "760 ---- 639\n",
      "761 ---- 640\n",
      "762 ---- 641\n",
      "763 ---- 642\n",
      "764 ---- 643\n",
      "765 ---- 644\n",
      "766 ---- 645\n",
      "767 ---- 646\n",
      "768 ---- 647\n",
      "769 ---- 648\n",
      "770 ---- 649\n",
      "771 ---- 650\n",
      "772 ---- 651\n",
      "773 ---- 652\n",
      "774 ---- 653\n",
      "775 ---- 654\n",
      "776 ---- 655\n",
      "777 ---- 656\n",
      "778 ---- 657\n",
      "779 ---- 658\n",
      "780 ---- 659\n",
      "781 ---- 660\n",
      "782 ---- 661\n",
      "783 ---- 662\n",
      "784 ---- 663\n",
      "785 ---- 664\n",
      "786 ---- 665\n",
      "787 ---- 666\n",
      "788 ---- 667\n",
      "789 ---- 668\n",
      "790 ---- 669\n",
      "791 ---- 670\n",
      "792 ---- 671\n",
      "793 ---- 672\n",
      "794 ---- 673\n",
      "795 ---- 674\n",
      "796 ---- 675\n",
      "797 ---- 676\n",
      "798 ---- 677\n",
      "799 ---- 678\n",
      "800 ---- 679\n",
      "801 ---- 680\n",
      "802 ---- 681\n",
      "803 ---- 682\n",
      "804 ---- 683\n",
      "805 ---- 684\n",
      "806 ---- 685\n",
      "807 ---- 686\n",
      "808 ---- 687\n",
      "809 ---- 688\n",
      "810 ---- 689\n",
      "811 ---- 690\n",
      "812 ---- 691\n",
      "813 ---- 692\n",
      "814 ---- 693\n",
      "815 ---- 694\n",
      "816 ---- 695\n",
      "817 ---- 696\n",
      "818 ---- 697\n",
      "819 ---- 698\n",
      "820 ---- 699\n",
      "821 ---- 700\n",
      "822 ---- 701\n",
      "823 ---- 702\n",
      "824 ---- 703\n",
      "825 ---- 704\n",
      "826 ---- 705\n",
      "827 ---- 706\n",
      "828 ---- 707\n",
      "829 ---- 708\n",
      "830 ---- 709\n",
      "831 ---- 710\n",
      "832 ---- 711\n",
      "833 ---- 712\n",
      "834 ---- 713\n",
      "835 ---- 714\n",
      "836 ---- 715\n",
      "837 ---- 716\n",
      "838 ---- 717\n",
      "839 ---- 718\n",
      "840 ---- 719\n",
      "841 ---- 720\n",
      "842 ---- 721\n",
      "843 ---- 722\n",
      "844 ---- 723\n",
      "845 ---- 724\n",
      "846 ---- 725\n",
      "847 ---- 726\n",
      "848 ---- 727\n",
      "849 ---- 728\n",
      "850 ---- 729\n",
      "851 ---- 730\n",
      "852 ---- 731\n",
      "853 ---- 732\n",
      "854 ---- 733\n",
      "855 ---- 734\n",
      "856 ---- 735\n",
      "857 ---- 736\n",
      "858 ---- 737\n",
      "859 ---- 738\n",
      "860 ---- 739\n",
      "861 ---- 740\n",
      "862 ---- 741\n",
      "863 ---- 742\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(X_train_final.columns)):\n",
    "    print('{} ---- {}'.format(i, X_train_final.columns[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Model 5: Without POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_5 = X_train_final.iloc[:,np.r_[3:13,21:863]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(860, 852)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_5 = X_test_final.iloc[:,np.r_[3:13,21:863]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 852)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.857\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.75      0.71        48\n",
      "           1       0.71      0.62      0.67        48\n",
      "\n",
      "    accuracy                           0.69        96\n",
      "   macro avg       0.69      0.69      0.69        96\n",
      "weighted avg       0.69      0.69      0.69        96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_5_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_5_pipeline, parameters, scoring=\"average_precision\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_5,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_5), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 36, False Positive: 12, False Negative: 18, True Positive: 30\n",
      "--------------------------------------------------------------------------------\n",
      "[[36 12]\n",
      " [18 30]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.75      0.71        48\n",
      "           1       0.71      0.62      0.67        48\n",
      "\n",
      "    accuracy                           0.69        96\n",
      "   macro avg       0.69      0.69      0.69        96\n",
      "weighted avg       0.69      0.69      0.69        96\n",
      "\n",
      "Average Precision: 0.6339\n"
     ]
    }
   ],
   "source": [
    "lr_model_5 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_5, y_train)\n",
    "y_lr = lr_model_5.predict(X_test_model_5)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))\n",
    "\n",
    "# Calculate and print the average precision score\n",
    "avg_precision = average_precision_score(y_test, y_lr)\n",
    "print(f'Average Precision: {avg_precision:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF Model 4: Without Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [10,20,50,100,200,300]\n",
    "max_depth = [2,5,8,10,15,20]\n",
    "min_samples_split = [2, 5, 10, 15, 20]\n",
    "min_samples_leaf = [1, 2, 5, 10,20]\n",
    "rf_parameters = dict(n_estimators = n_estimators, max_depth = max_depth,  \n",
    "              min_samples_split = min_samples_split, \n",
    "              min_samples_leaf = min_samples_leaf)\n",
    "\n",
    "## reduced parameters\n",
    "# n_estimators = [50, 100, 200]       # Reduced options\n",
    "# max_depth = [5, 10, 15]             # Reduced options\n",
    "# min_samples_split = [2, 10]         # Reduced options\n",
    "# min_samples_leaf = [1, 5]           # Reduced options\n",
    "\n",
    "# rf_parameters = dict(\n",
    "#     n_estimators = n_estimators, \n",
    "#     max_depth = max_depth,  \n",
    "#     min_samples_split = min_samples_split, \n",
    "#     min_samples_leaf = min_samples_leaf\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_5 = X_train_final.iloc[:,np.r_[3:13,21:863]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(860, 852)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_5 = X_test_final.iloc[:,np.r_[3:13,21:863]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 852)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 900 candidates, totalling 9000 fits\n",
      "Best score: 0.874\n",
      "Best parameters set:\n",
      "\tmax_depth: 20\n",
      "\tmin_samples_leaf: 1\n",
      "\tmin_samples_split: 10\n",
      "\tn_estimators: 50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.71      0.70        48\n",
      "           1       0.70      0.69      0.69        48\n",
      "\n",
      "    accuracy                           0.70        96\n",
      "   macro avg       0.70      0.70      0.70        96\n",
      "weighted avg       0.70      0.70      0.70        96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_model_5 = RandomForestClassifier(random_state=18,class_weight='balanced')\n",
    "grid_search = GridSearchCV(rf_model_5, rf_parameters, scoring=\"average_precision\", cv = 10, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_model_5,y_train)\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(rf_parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_5), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 34, False Positive: 14, False Negative: 15, True Positive: 33\n",
      "--------------------------------------------------------------------------------\n",
      "[[34 14]\n",
      " [15 33]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.71      0.70        48\n",
      "           1       0.70      0.69      0.69        48\n",
      "\n",
      "    accuracy                           0.70        96\n",
      "   macro avg       0.70      0.70      0.70        96\n",
      "weighted avg       0.70      0.70      0.70        96\n",
      "\n",
      "Average Precision: 0.6390\n"
     ]
    }
   ],
   "source": [
    "randomForest_5 = RandomForestClassifier(random_state=18,\n",
    "                                        class_weight=best_parameters['class_weight'],\n",
    "                                        max_depth=best_parameters['max_depth'],\n",
    "                                        min_samples_leaf=best_parameters['min_samples_leaf'],\n",
    "                                        min_samples_split=best_parameters['min_samples_split'],\n",
    "                                        n_estimators=best_parameters['n_estimators']).fit(X_train_model_5, y_train)\n",
    "\n",
    "y_lr = randomForest_5.predict(X_test_model_5)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))\n",
    "\n",
    "# Calculate and print the average precision score\n",
    "avg_precision = average_precision_score(y_test, y_lr)\n",
    "print(f'Average Precision: {avg_precision:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
