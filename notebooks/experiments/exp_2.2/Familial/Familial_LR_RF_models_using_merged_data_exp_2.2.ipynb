{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Familial Logistic Regression and Random Forest Models Using Merged Data Experiment 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f74696a5df0d4dbc80479f5d829d0270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 08:57:44 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-10-16 08:57:44 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-10-16 08:57:46 INFO: File exists: /Users/gbaldonado/stanza_resources/en/default.zip\n",
      "2024-10-16 08:57:48 INFO: Finished downloading models and saved to /Users/gbaldonado/stanza_resources\n",
      "2024-10-16 08:57:48 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a721412a31941ef807164d516caa1a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 08:57:48 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-10-16 08:57:49 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-10-16 08:57:49 INFO: Using device: cpu\n",
      "2024-10-16 08:57:49 INFO: Loading: tokenize\n",
      "2024-10-16 08:57:49 INFO: Loading: mwt\n",
      "2024-10-16 08:57:49 INFO: Loading: pos\n",
      "2024-10-16 08:57:50 INFO: Loading: lemma\n",
      "2024-10-16 08:57:50 INFO: Loading: constituency\n",
      "2024-10-16 08:57:50 INFO: Loading: depparse\n",
      "2024-10-16 08:57:50 INFO: Loading: sentiment\n",
      "2024-10-16 08:57:51 INFO: Loading: ner\n",
      "2024-10-16 08:57:51 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import pickle\n",
    "import warnings\n",
    "import stanza\n",
    "\n",
    "from random import shuffle\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, f1_score, r2_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Set random seed\n",
    "random.seed(18)\n",
    "seed = 18\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Initialize lemmatizer, stop words, and stanza\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline('en') # initialize English neural pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data and quick exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and test sets loaded.\n"
     ]
    }
   ],
   "source": [
    "merged_familial_df_batch_1 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/merged_Familial_sentence_level_batch_1_jaccard.csv\", encoding='utf-8')\n",
    "merged_familial_df_batch_2 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/Familial Plus_sentence_level_batch_2_jaccard.csv\", encoding='utf-8')\n",
    "\n",
    "merged_familial_df = pd.concat([merged_familial_df_batch_1, merged_familial_df_batch_2])\n",
    "\n",
    "\n",
    "seed = 18\n",
    "# Shuffle the merged dataset\n",
    "merged_familial_df = shuffle(merged_familial_df, random_state=seed)\n",
    "\n",
    "# Function for undersampling or oversampling\n",
    "def resample_data(X, y, strategy='oversample', random_state=seed):\n",
    "    \"\"\"\n",
    "    Resample the data using either undersampling or oversampling.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Features\n",
    "    - y: Labels\n",
    "    - strategy: 'oversample' or 'undersample'\n",
    "    - random_state: Seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    - X_resampled, y_resampled: Resampled data and labels\n",
    "    \"\"\"\n",
    "    if strategy == 'oversample':\n",
    "        sampler = RandomOverSampler(random_state=random_state)\n",
    "    elif strategy == 'undersample':\n",
    "        sampler = RandomUnderSampler(random_state=random_state)\n",
    "    else:\n",
    "        raise ValueError(\"Strategy must be 'oversample' or 'undersample'\")\n",
    "\n",
    "    X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Separate features and labels\n",
    "X = merged_familial_df.drop(columns=['label'])  # Replace 'label' with your target column name\n",
    "y = merged_familial_df['label']\n",
    "\n",
    "# Toggle resampling\n",
    "resample = True  # Set this to False to turn off resampling\n",
    "\n",
    "if resample:\n",
    "    # Apply resampling (choose 'oversample' or 'undersample')\n",
    "    X_resampled, y_resampled = resample_data(X, y, strategy='oversample', random_state=seed)\n",
    "\n",
    "    # Combine resampled data into a single DataFrame\n",
    "    resampled_df = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "else:\n",
    "    # No resampling, use original dataset\n",
    "    resampled_df = merged_familial_df\n",
    "\n",
    "# Train-test split\n",
    "training_df, test_df = train_test_split(resampled_df, test_size=0.1, random_state=18, stratify=resampled_df['label'])\n",
    "\n",
    "\n",
    "# # Train-test split\n",
    "training_df, test_df = train_test_split(resampled_df, test_size=0.1, random_state=18, stratify=resampled_df['label'])\n",
    "\n",
    "training_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Training and test sets loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (4305, 3) \n",
      "Test dataset shape: (479, 3)\n",
      "Positive labels present in the dataset : 2153  out of 4305 or 50.01161440185831%\n",
      "Positive labels present in the test dataset : 239  out of 479 or 49.89561586638831%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset shape: {training_df.shape} \\nTest dataset shape: {test_df.shape}\")\n",
    "pos_labels = len([n for n in training_df['label'] if n==1])\n",
    "print(\"Positive labels present in the dataset : {}  out of {} or {}%\".format(pos_labels, len(training_df['label']), (pos_labels/len(training_df['label']))*100))\n",
    "pos_labels = len([n for n in test_df['label'] if n==1])\n",
    "print(\"Positive labels present in the test dataset : {}  out of {} or {}%\".format(pos_labels, len(test_df['label']), (pos_labels/len(test_df['label']))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABWcAAAJICAYAAAANc1ZxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAA9hAAAPYQGoP6dpAABGtklEQVR4nO3deZhXdd0//tfAwACzIAwKGosYtxZCZiKIpiKGC6m5lQaoSCouaUDRLSqClqLe3okmfpVbLuNWUkLTfu4iiyaSSwvZaKVkggsKCMzQ6LCd3x9efu5GQBiYD+8BHo/rOld+3md7nc/Gq+eczzkFWZZlAQAAAADANtUodQEAAAAAADsj4SwAAAAAQALCWQAAAACABISzAAAAAAAJCGcBAAAAABIQzgIAAAAAJCCcBQAAAABIQDgLAAAAAJCAcBbYKlmWpS6hQdRA/fF61o3nCwDyryH8e9sQaqD+5Ov13FbvE+9HqD/CWdiB9enTJwoKCnJTo0aNorS0NA444ID4+c9/HmvXrq21/J577hmDBw/e7O3/f//f/xdnnXXWJpcbPHhw7Lnnnlu8n42pqamJESNGxC9/+cuN7qshuPTSS6O8vDyKi4vjf//3f9ebP3v27CgoKIjZs2dv9ja3ZJ2N6dOnT/Tp02eL1//nP/8ZBQUF8Ytf/GKr6nj77bfjuOOOi7feemurtvOpgoKCGDt2bN7XSenVV1+NQw45JHUZAJCUnrdh0PNunvrueT+1fPnyOOuss+K3v/1tvW53Q/SgUL+Es7CD23///WPu3Lkxd+7c+O1vfxu//OUvo2fPnjFs2LAYMGBArb94PvjggzF69OjN3vbPfvazWLBgwSaXGz16dDz44INbVP/nee+99+Kmm26K1atX531fW+ovf/lLXH/99XHKKafEE088Eccee2zqkurd7rvvHnPnzo1vfvObW7Wdp59+Oh599NF6qipi7ty5cc455+R9nZR+9atfxdy5c1OXAQDJ6XnT0vNuvvrueT/1pz/9Kf73f/831q1bV+/b/iw9KNSvwtQFAPlVVlYWBx10UK2x448/Pvbee+8YMWJEnHDCCTFw4MCI+KSpzYcvfvGLedlu6n1tjqVLl0ZExHe/+9049NBDE1eTH0VFReu9xxqCLampIR4HALBpet609LwAW86Zs7CTuuSSS2KPPfaI22+/PTf22Z9eTZ06Nfbbb79o3rx57LrrrjFo0KB47733IuKTnwU988wz8cwzz+R+avTpz47uuOOO6NSpU7Rt2zaeeuqpDf7savXq1XHJJZdEq1atolWrVnHWWWfF4sWLc/M3tM6//5Ton//8Z3Tu3DkiIs4+++zcsp9db+3atXHbbbdF9+7do3nz5tGxY8e49NJL4+OPP661r2984xtx1113xd577x1FRUWx3377xWOPPbbJ53Hq1KnRo0ePKCkpiXbt2sX5558fy5Yti4iIsWPH5n461bdv3zr99Oyhhx6KQw89NEpLS6OoqCi+9KUvxa233rrecq+++moceuih0axZs+jSpUv8/Oc/rzV/3bp1cd1110WXLl2iqKgo9t577/WW+aynn346evfuHSUlJdGqVas48cQT429/+9tGl//sT7x+8YtfRGFhYbzwwgvRu3fvaNasWXTs2DFuuOGGjW7jF7/4RZx99tkREdG5c+fc+3DPPfeM4cOHx5FHHhllZWVx/vnnR0TEn//85zj55JNj1113jSZNmsQXvvCFuOSSS+Kjjz7KbfPfL1Hw6XtzxowZcdRRR0WLFi2ibdu2MXLkyFizZs1WrVNVVRVDhw6N3XbbLUpKSuL000+P8ePHR0FBwec+z5/3+frUnXfeGfvuu28UFRVFx44dY+zYsbl9jx07Nq666qr16gYA/o+eV8+7MQ2p5434/L4vImLJkiUxaNCgaNeuXTRr1iy++tWvxt133x0Rn/StRxxxREREHHHEEZ97+QY9KDRAGbDDOvzww7PDDz98o/PPOOOMrEmTJtnq1auzLMuyTp06ZWeddVaWZVn23HPPZY0bN86uuuqqbNasWdndd9+dtWvXLre9ioqKbP/998/233//bO7cudmKFSuyWbNmZRGRtW7dOps2bVp29913Z5WVldlZZ52VderUKbffTp06ZY0bN8569+6d/eY3v8n+53/+JysvL88OPvjg3DKfXSfLsuzNN9/MIiK76667so8//jj79a9/nUVEdsUVV2R/+MMfNrje9773vaywsDC7/PLLs6eeeiq7/vrrsxYtWmRHHXVUtm7dutw6LVu2zL785S9n9957b/bYY49lBxxwQNa8efPsww8/3Ojz95Of/CSLiOzCCy/Mnnjiiey2227LysvLs6985StZdXV1tnDhwmzChAlZRGQTJkzI1fhZnz5vs2bNyrIsyx555JEsIrIf/OAH2YwZM7KHH344O/roo7OIyObMmVNrnSZNmmQ/+tGPsieeeCK76KKLsojIJk6cmNv2eeedlzVp0iQbM2ZM9uSTT2aXXXZZ1qhRo+zqq6/OLfPv75P58+dnzZs3zy666KJs5syZ2f3335/ts88+2V577ZWtXbt2g/X/++uSZVl21113ZQUFBVnHjh2z8ePHZzNmzMgGDBiQRUT2xBNPbHAbH3zwQXbFFVdkEZH9+te/zt54440syz55rxQWFmbDhg3Lnnrqqey5557L3n333aysrCw76qijskceeSSbPn16NmzYsCwismuuuSa3zYjIxowZU+v5atu2bXb11VdnM2bMyIYPH55FRHb77bdv1Tp9+/bNdtlll+y2227LHnnkkax///5ZUVFR9nn/xG7q85VlWXbttddmBQUF2SWXXJI9+eST2fXXX581a9YsGzJkSJZlWbZw4cLse9/7XhYR2dy5c7OFCxdudH8AsCPT8+p5t/eed1N9X5Zl2VFHHZV99atfzR588MFsxowZ2eDBg3PP54oVK2q9BhUVFRvcvx4UGibhLOzANtWojhw5MouIbNGiRVmW1W5Ux40bl5WUlGQfffRRbvnHHnssGzt2bK7B++z2P22eLr/88lr72VCj2qZNm6yysjI39tBDD2URkT355JMbXCfL1m+IPvv4s+tVVFRkEZH99Kc/rbWdu+++O4uI7LHHHsutExG55ijLsuyZZ57JIiK7//77N/jcffjhh1lRUVF2zjnn1Bp/9tlns4jIbrvttlrPyadN6IZ8dpkbbrghO/PMM2sts3Tp0iwismuvvbbWOkOHDq213Iknnpi1b98+W7t2bfa3v/0tKygoyK677rpay1xxxRVZs2bNsiVLlmRZVvt1vPfee7OIyN5+++3c8i+88EJ22WWXZStWrNhg/RtqVCMiu/POO3PLfPzxx1mzZs2y73//+xt9Hj5d780338yNderUKevYsWOtJvnJJ5/MDjvssPXq6d69e3bUUUflHm8oaL3iiitqrdO5c+fsuOOO2+J1ZsyYkUVE9sADD+Tmr127NuvatevnhrOb+nwtX748a9GiRXb++efXWu/OO+/MIiL7y1/+kmVZlo0ZM+Zz9wMAOwM9r553e+55N7fvKyoqqvUar127NvvhD3+Y/fa3v631XH3ea6AHhYbJZQ2ADf78+vDDD4/q6uro3r17XH755TFnzpw46qijYsyYMZv8uXb37t03uc/+/ftHaWlp7vHxxx8fTZo0iaeffrruB7ARzzzzTERE7vpinzr99NOjcePGMWvWrNzYrrvuWuvaXe3bt4+IiH/9618b3Pbvfve7qKmpWW/bhx56aHTq1KnWtutq5MiRMXny5PjXv/4V8+bNi2nTpsV1110XERGrVq2qtexpp51W6/HJJ58cb7/9dvz1r3+NmTNnRpZlcfzxx8eaNWty0wknnBAff/zxBu/ketBBB0WzZs2iZ8+eMWLEiHj66afjq1/9alxzzTVRVlZWp+Po3bt37r+Liopi11133ejz+Xm6du0ajRr93z9XRx11VDzzzDPRvHnz+Pvf/x6PPPJIXHvttfHBBx+s9/x8Xk0Rn7zOm6rp89aZOXNmNGnSJE488cTc/EaNGsV3vvOdz93mpj5fc+fOjerq6jjhhBNqvXbHH398RERMnz79c7cPAKxPz6vn/VRD6nk3t+874ogjYsyYMfGd73wnfvGLX8TixYvjxhtvjK9//eubvS89KDRMwlnYib3zzjvRvHnzKC8vX29e796947HHHou99tor949++/bt4+abb97kdtu2bbvJZdq1a1frcaNGjaK8vDx37ar68OGHH25wX4WFhdGmTZtYvnx5bqxFixbr1RMRG73b6ca2/enYv2+7rpYsWRKnnHJKlJWVxQEHHBBXXnll7nnJ/u1Owxva/2677RYREcuWLcvdmGHfffeNJk2a5KaePXtGRMS777673r733HPPeOaZZ6JXr14xceLE6NevX7Rt2zYuv/zyOt/5dUPP6ZbcPfaz76d169bFpZdeGq1bt4599tknLrzwwvjDH/4QzZs3X+/5qY+aPm+dxYsXR3l5ea3wOGLD74t/t6nP16evXf/+/Wu9dp8+Fxt67QCADdPzLs+N6Xk/0ZB63s3t++6777744Q9/GC+++GKcffbZsccee8QxxxwTb7755mbvSw8KDVNh6gKANNauXRuzZ8+OQw45JBo3brzBZY4++ug4+uijo7q6OmbOnBk333xzDBs2LA466KDo1avXVu3/sw3p2rVrY8mSJblGq6CgINauXVtrmZUrV9ZpH61bt46IiEWLFtW6McHq1atjyZIl0aZNmy2ofP1tf+lLX6o177333ou99tpri7c9YMCAeO211+Lpp5+Ogw8+OIqKiqK6ujruvPPO9Zb97PO4aNGiiPikYd1ll10i4pOzO//9jI1PdezYcYP779mzZ/z617+OVatWxXPPPRd33HFHXHvttfGVr3xlvbMWUrjuuuviZz/7Wdx+++1xyimnRMuWLSMicg34ttS+fftYsmRJrFu3rlZA+8EHH2xy3c/7fH362k2ZMiX23nvv9dbdnP8zCADoefW8Db/n3dy+r2XLlnH99dfH9ddfH3/729/iN7/5TVx99dVx4YUXxuOPP77Z+9ODQsPjzFnYSd1+++3x7rvvxgUXXLDB+T/60Y+iZ8+ekWVZtGjRIo477ri48cYbIyJi4cKFEREbbXA3x9NPP13r7qP3339/rFmzJneX0bKysliyZEmtO8zOmTOn1jY2tf/DDz88Ij5pLv7dfffdF2vXrq3TT4A+q1evXlFUVLTetp977rlYsGDBVm37ueeei1NPPTWOOOKIKCoqiojINVyf/Sv8E088UevxfffdFx06dIguXbrkjn/JkiXRo0eP3LR06dK44oorcn8Z/3fjx4+PPffcM2pqaqJp06bRt2/fmDhxYkT83+ueL5v7fnruuedi3333jSFDhuSC2XfeeSdeeeWVLTozd2scfvjhsWbNmnj44YdrjT/44IOfu96mPl8HHXRQNG3aNN55551ar13Tpk3j0ksvzZ0hsTWfQQDYGeh59bwNvefdnL7vrbfeig4dOsT9998fERH77LNP/PjHP45+/frV6X2qB4WGyZmzsIOrrKyM3/3udxHxSZOzZMmSePLJJ+OOO+6IQYMGxcknn7zB9b7xjW/Ez372sxg8eHAMGjQoVq1aFTfccEO0bt06+vbtGxGf/JV37ty5MXPmzNh///3rVNeiRYvilFNOiYsvvjhef/31GDVqVPTr1y+OPPLIiIg47rjj4pZbbokhQ4bEueeeG3/5y1/ixhtvrNUIfBrMzZgxI7785S+vd2ZD165d46yzzoqxY8fGRx99FH369Ik//elPMXbs2DjiiCPimGOOqVPN/65169Zx6aWXxlVXXRVNmzaNb33rW/Hmm2/G6NGjo2vXrjF48OAt3nbPnj1jypQpccABB0T79u3j+eefj2uvvTYKCgrWu37VLbfcEqWlpbH//vvHfffdF0888UTcfffdUVBQEN26dYtBgwbFueeeG//85z+jR48e8be//S0uu+yy6Ny58wb/Gt63b9/4z//8zzjppJPi+9//fhQWFsbtt98eRUVFuWtN5cunf6n/9a9/Hf3791/v7IxP9ezZM37yk5/EddddF71794433ngjrr322qipqdmia9pujcMOOyz69esXQ4YMiWuvvTY6deoUkyZNinnz5n3udeo29flq3bp1/PjHP47Ro0dHZWVl9OnTJ955550YPXp0FBQUxH777RcR//ec3XvvvXHQQQdF586dt8VhA0CDo+fV827PPe+m+r6WLVtG+/bt45JLLonKysr44he/GC+//HI89thjMWrUqFrbffTRR6NVq1a5fvHf6UGhgUp2KzIg7w4//PAsInJTo0aNsnbt2mV9+vTJ7rnnntwdaD/173euzbIs++Uvf5l97Wtfy0pKSrLS0tLs2GOPzf785z/n5s+cOTPr2LFj1rRp02zKlCkbvUPohu5c+4Mf/CA799xzs5KSkqx169bZhRdemK1cubLWejfeeGPWsWPHrKioKDv44IOz3//+91lRUVGtO9WOGDEiKy4uznbZZZespqZmvX2tWbMm++lPf5rttddeWZMmTbI999wzGzVqVK07lG7OXXI35v/9v/+Xde3aNWvatGm2++67ZxdeeGH24Ycf5uZvyZ1r//nPf2bHHXdc1rJly6xly5bZgQcemN1zzz3ZMccckx144IG11rnvvvuyAw88MGvatGn2pS99Kbv33ntrbXv16tXZ1VdfnTv+9u3bZxdccEG2dOnS3DKfvQPxk08+mR1yyCFZWVlZ1qJFi+ywww7LnnnmmY3Wv7E71356B9pPffb99VlVVVXZN77xjaxp06ZZ//79N7rOxx9/nF100UVZu3btsubNm2f77LNPNmbMmOyqq67KioqKcs9/RGRjxozZ4HO8sWPfknU+/PDDbPDgwdkuu+ySFRcXZwMHDswuuuiirLS0dKPHmmWb/nxlWZZNmDAh9/5q27ZtNnDgwOytt97KzX/nnXeyAw88MGvSpEl2wQUXfO7+AGBHpefV827vPW+Wbbrve++997LBgwdne+yxR9a0adPsi1/8YnbNNddka9euzbIsy9auXZt997vfzZo1a5btu+++G92/HhQanoIs28TdUwCADXrrrbdi7ty58a1vfSuaN2+eG//2t78d8+fPjz/84Q8JqwMAAKChc1kDANhCjRo1isGDB8e3vvWt+N73vheFhYXx2GOPxQMPPBB33XVX6vIAAABo4Jw5CwBbYdasWXH11VfHH//4x1i9enV07do1RowYEd/97ndTlwYAAEADJ5wFAAAAAEigUeoCAAAAAAB2RsJZAAAAAIAEhLMAAAAAAAkUpi5gW1u3bl28++67UVpaGgUFBanLAQBgM2VZFlVVVbHHHntEo0Y7xzkGelcAgO3T5vauO104++6770aHDh1SlwEAwBZauHBhtG/fPnUZ24TeFQBg+7ap3nWnC2dLS0sj4pMnpqysLHE1AABsrsrKyujQoUOun9sZ6F0BALZPm9u77nTh7Kc/BysrK9PgAgBsh3amn/frXQEAtm+b6l13jot1AQAAAAA0MMJZAAAAAIAEhLMAAAAAAAkIZwEAAAAAEhDOAgAAAAAkIJwFAAAAAEhAOAsAAAAAkIBwFgAAAAAgAeEsAAAAAEACwlkAAAAAgASEswAAAAAACQhnAQAAAAASEM4CAAAAACQgnAUAAAAASEA4CwAAAACQgHAWAAAAACAB4SwAAAAAQALCWQAAAACABISzAAAAAAAJCGcBAAAAABIQzgIAAAAAJFCYugAAdhwHjPzf1CUA28Dv/+vM1CWwjaxdty4aN3I+B+zofNYB0hHOAgAAG9S4UaO44pe/jTc/WJG6FCBPOu/WMn464NDUZQDstISzAADARr35wYr46zsfpi4DAGCHJJzdxvzkF3Z8fu4LAAAAbA4XlQEAAAAASEA4CwAAAACQgHAWAAAAACAB4SwAAAAAQALCWQAAAACABISzAAAAAAAJCGcBAAAAABIQzgIAAAAAJCCcBQAAAABIQDgLAAAAAJCAcBYAALbCvHnzol+/ftG6deto165dnHnmmbFkyZKIiLjggguiqKgoSkpKctPEiRMTVwwAQEMhnAUAgC300UcfxbHHHhsHH3xwLFq0KCoqKmLp0qVx9tlnR0TESy+9FBMnToyVK1fmpvPOOy9x1QAANBTCWQAA2EILFiyI/fbbL6688spo2rRplJeXx9ChQ+PZZ5+NmpqaeOWVV6JHjx6pywQAoIESzgIAwBbaZ5994vHHH4/GjRvnxu6///444IADYt68ebF69eq48soro23btrH33nvH9ddfH+vWrUtYMQAADUlh6gIAAGBHkGVZjB49Oh5++OF49tlnY9GiRdGnT5+45JJL4r777os//vGPcdJJJ0WjRo1i5MiRG9xGTU1N1NTU5B5XVlZuq/IBAEjAmbMAALCVKisr49RTT4177rknnn322ejevXv069cvZs6cGYcffng0adIkevbsGcOGDYupU6dudDvjxo2Lli1b5qYOHTpsw6MA2Lms9UsG2Ck09M+6M2cBAGArzJ8/P/r37x8dO3aMl19+Odq0aRMREQ899FC8//77MXTo0NyyNTU10bx5841ua9SoUTFixIjc48rKSgEtQJ40btQorvjlb+PND1akLgXIk867tYyfDjg0dRmfSzgLAABbaNmyZdG3b9/o27dvTJo0KRo1+r8fpmVZFsOHD48uXbpE375943e/+13cfPPNcdNNN210e0VFRVFUVLQtSgcgIt78YEX89Z0PU5cB7MSEswAAsIXuuuuuWLBgQfzqV7+KadOm1Zq3cuXKuOmmm+LCCy+Mt99+O9q1axdXXXVVDBo0KFG1AAA0NMJZAADYQiNGjKh1GYLPGjp0aK3LGgAAwL9zQzAAAAAAgASEswAAAAAACQhnAQAAAAASEM4CAAAAACQgnAUAAAAASEA4CwAAAACQgHAWAAAAACAB4SwAAAAAQALCWQAAAACABISzAAAAAAAJCGcBAAAAABIQzgIAAAAAJCCcBQAAAABIQDgLAAAAAJCAcBYAAAAAIAHhLAAAAABAAsJZAAAAAIAEhLMAAAAAAAkIZwEAAAAAEhDOAgAAAAAkIJwFAAAAAEhAOAsAAAAAkIBwFgAAAAAgAeEsAAAAAEACwlkAAAAAgASEswAAAAAACQhnAQAAAAASEM4CAAAAACQgnAUAAAAASEA4CwAAAACQgHAWAAAAACCBJOHsvHnzol+/ftG6deto165dnHnmmbFkyZKIiHjhhReiV69eUVJSEp07d45JkybVWnfy5MnRpUuXKC4ujh49esTcuXNTHAIAAAAAwFbZ5uHsRx99FMcee2wcfPDBsWjRoqioqIilS5fG2WefHcuWLYv+/fvHmWeeGcuXL49JkybF8OHD48UXX4yIiNmzZ8fFF18ckydPjuXLl8fAgQPjhBNOiOrq6m19GAAAAAAAW2Wbh7MLFiyI/fbbL6688spo2rRplJeXx9ChQ+PZZ5+NBx54IMrLy+Oiiy6KwsLC6Nu3bwwcODAmTJgQERF33nlnnH766XHIIYdEkyZNYvjw4dGmTZuYOnXqtj4MAAAAAICtss3D2X322Scef/zxaNy4cW7s/vvvjwMOOCAqKiqie/futZbv2rVrzJs3LyJik/M3pKamJiorK2tNAAAAAACpJb0hWJZlccUVV8TDDz8cN998c1RVVUVxcXGtZVq0aBErV66MiNjk/A0ZN25ctGzZMjd16NCh/g8EAAAAAKCOkoWzlZWVceqpp8Y999wTzz77bHTv3j2Ki4vXu35sdXV1lJaWRkRscv6GjBo1KlasWJGbFi5cWP8HAwAAAABQR0nC2fnz58eBBx4YlZWV8fLLL+cuVdCtW7eoqKioteyrr74a3bp126z5G1JUVBRlZWW1JgAAAACA1LZ5OLts2bLo27dvHHzwwfHkk09GmzZtcvNOPvnkWLRoUYwfPz5Wr14ds2bNiilTpsSQIUMiImLIkCExZcqUmDVrVqxevTrGjx8f77//fpx00knb+jAAAAAAALbKNg9n77rrrliwYEH86le/irKysigpKclN5eXlMX369Jg2bVqUl5fHOeecE7fcckscccQRERFx5JFHxm233RYXXHBBtGrVKu699954/PHHo3Xr1tv6MAAAAAAAtkrhtt7hiBEjYsSIERud36NHj5gzZ85G5w8aNCgGDRqUj9IAAAAAALaZZDcEAwAAAADYmQlnAQAAAAASEM4CAAAAACQgnAUAAAAASEA4CwAAAACQgHAWAAAAACAB4SwAAAAAQALCWQAAAACABISzAAAAAAAJCGcBAAAAABIQzgIAAAAAJCCcBQAAAABIQDgLAAAAAJCAcBYAAAAAIAHhLAAAAABAAsJZAAAAAIAEhLMAAAAAAAkIZwEAAAAAEhDOAgAAAAAkIJwFAAAAAEhAOAsAAAAAkIBwFgAAAAAgAeEsAAAAAEACwlkAAAAAgASEswAAAAAACQhnAQAAAAASEM4CAAAAACQgnAUAAAAASEA4CwAAAACQgHAWAAAAACAB4SwAAAAAQALCWQAAAACABISzAAAAAAAJCGcBAAAAABIQzgIAAAAAJCCcBQAAAABIQDgLAAAAAJCAcBYAAAAAIAHhLAAAAABAAsJZAAAAAIAEhLMAAAAAAAkIZwEAAAAAEhDOAgDAVpg3b17069cvWrduHe3atYszzzwzlixZEhERL7zwQvTq1StKSkqic+fOMWnSpMTVAgDQkAhnAQBgC3300Udx7LHHxsEHHxyLFi2KioqKWLp0aZx99tmxbNmy6N+/f5x55pmxfPnymDRpUgwfPjxefPHF1GUDANBACGcBAGALLViwIPbbb7+48soro2nTplFeXh5Dhw6NZ599Nh544IEoLy+Piy66KAoLC6Nv374xcODAmDBhQuqyAQBoIISzAACwhfbZZ594/PHHo3Hjxrmx+++/Pw444ICoqKiI7t2711q+a9euMW/evI1ur6amJiorK2tNAADsuISzAABQD7IsiyuuuCIefvjhuPnmm6OqqiqKi4trLdOiRYtYuXLlRrcxbty4aNmyZW7q0KFDvssGACAh4SwAAGylysrKOPXUU+Oee+6JZ599Nrp37x7FxcVRXV1da7nq6uooLS3d6HZGjRoVK1asyE0LFy7Md+kAACRUmLoAAADYns2fPz/69+8fHTt2jJdffjnatGkTERHdunWLp556qtayr776anTr1m2j2yoqKoqioqK81gsAQMPhzFkAANhCy5Yti759+8bBBx8cTz75ZC6YjYg4+eSTY9GiRTF+/PhYvXp1zJo1K6ZMmRJDhgxJWDEAAA2JcBYAALbQXXfdFQsWLIhf/epXUVZWFiUlJbmpvLw8pk+fHtOmTYvy8vI455xz4pZbbokjjjgiddkAADQQLmsAAABbaMSIETFixIiNzu/Ro0fMmTNnG1YEAMD2xJmzAAAAAAAJCGcBAAAAABIQzgIAAAAAJCCcBQAAAABIQDgLAAAAAJCAcBYAAAAAIAHhLAAAAABAAsJZAAAAAIAEhLMAAAAAAAkIZwEAAAAAEhDOAgAAAAAkIJwFAAAAAEhAOAsAAAAAkIBwFgAAAAAgAeEsAAAAAEACwlkAAAAAgASEswAAAAAACQhnAQAAAAASEM4CAAAAACQgnAUAAAAASEA4CwAAAACQgHAWAAAAACAB4SwAAAAAQALCWQAAAACABISzAAAAAAAJCGcBAAAAABIQzgIAAAAAJCCcBQAAAABIQDgLAAAAAJCAcBYAAAAAIAHhLAAAAABAAsJZAAAAAIAEhLMAAAAAAAkIZwEAAAAAEhDOAgAAAAAkIJwFAAAAAEhAOAsAAAAAkIBwFgAAAAAgAeEsAAAAAEACwlkAAAAAgASEswAAAAAACQhnAQAAAAASEM4CAAAAACSQNJxdvHhxdOnSJWbPnp0bu+CCC6KoqChKSkpy08SJE3PzJ0+eHF26dIni4uLo0aNHzJ07N0HlAAAAAABbJ1k4O2fOnOjdu3fMnz+/1vhLL70UEydOjJUrV+am8847LyIiZs+eHRdffHFMnjw5li9fHgMHDowTTjghqqurUxwCAAAAAMAWSxLOTp48OQYMGBDXXHNNrfGampp45ZVXokePHhtc784774zTTz89DjnkkGjSpEkMHz482rRpE1OnTt0WZQMAAAAA1Jsk4ezRRx8d8+fPj9NOO63W+Lx582L16tVx5ZVXRtu2bWPvvfeO66+/PtatWxcRERUVFdG9e/da63Tt2jXmzZu3zWoHAAAAAKgPhSl22q5duw2Or1ixIvr06ROXXHJJ3HffffHHP/4xTjrppGjUqFGMHDkyqqqqori4uNY6LVq0iJUrV250XzU1NVFTU5N7XFlZWT8HAQAAAACwFZLeEOyz+vXrFzNnzozDDz88mjRpEj179oxhw4blLltQXFy83vVlq6uro7S0dKPbHDduXLRs2TI3dejQIa/HAAAAAACwORpUOPvQQw/FHXfcUWuspqYmmjdvHhER3bp1i4qKilrzX3311ejWrdtGtzlq1KhYsWJFblq4cGH9Fw4AAAAAUEcNKpzNsiyGDx8eM2bMiCzLYu7cuXHzzTfH0KFDIyJiyJAhMWXKlJg1a1asXr06xo8fH++//36cdNJJG91mUVFRlJWV1ZoAAAAAAFJLcs3ZjTnppJPipptuigsvvDDefvvtaNeuXVx11VUxaNCgiIg48sgj47bbbosLLrgg3n777dh3333j8ccfj9atWyeuHAAAAACgbpKHs1mW1Xo8dOjQ3JmyGzJo0KBcWAsAAAAAsL1qUJc1AAAAAADYWQhnAQAAAAASEM4CAAAAACQgnAUAAAAASEA4CwAAAACQgHAWAAAAACAB4SwAAAAAQALCWQAAAACABISzAAAAAAAJCGcBAAAAABIQzgIAAAAAJCCcBQAAAABIQDgLAAAAAJCAcBYAAAAAIAHhLAAAAABAAsJZAAAAAIAEhLMAAAAAAAkIZwEAAAAAEhDOAgAAAAAkIJwFAAAAAEhAOAsAAAAAkIBwFgAAAAAgAeEsAAAAAEACwlkAAAAAgASEswAAAAAACQhnAQAAAAASEM4CAAAAACSw1eFsVVVVrFq1qj5qAQCAZPS1AABsa3UOZ//617/GSSedFBERDz74YJSXl8fuu+8ec+bMqffiAAAgX/S1AACkVudwdtiwYdGqVavIsiwuu+yyuPrqq+MnP/lJjBgxIh/1AQBAXtR3X7t48eLo0qVLzJ49Ozd2wQUXRFFRUZSUlOSmiRMn1tMRAACwvSus6wp//vOf4+GHH4633nor3njjjbjooouipKQkLr300nzUBwAAeVGffe2cOXPirLPOivnz59caf+mll2LixIlx1lln1VfZAADsQOp85uzq1asjy7J46qmn4oADDojS0tJYsmRJNGvWLB/1AQBAXtRXXzt58uQYMGBAXHPNNbXGa2pq4pVXXokePXrUZ9kAAOxA6hzOfuMb34iTTz45fvKTn8SAAQPiH//4R5x00knxzW9+Mx/1AQBAXtRXX3v00UfH/Pnz47TTTqs1Pm/evFi9enVceeWV0bZt29h7773j+uuvj3Xr1tXnYQAAsB2rczj7P//zP9GjR4/4/ve/H5dcckmsXLkyvva1r8WECRPyUR8AAORFffW17dq1i8LC9a8WtmLFiujTp09ccskl8fbbb8c999wTt9xyS/z3f//3RrdVU1MTlZWVtSYAAHZcdb7mbElJSYwdOzYiIpYsWRJf+cpX4pZbbqnvugAAIK/y3df269cv+vXrl3vcs2fPGDZsWEydOjVGjhy5wXXGjRsXV111Vb3VAABAw7ZF15y9/PLLo2XLltGpU6f4xz/+EQceeGC89957+agPAADyIt997UMPPRR33HFHrbGamppo3rz5RtcZNWpUrFixIjctXLiwXmoBAKBhqnM4e9VVV8XMmTNj2rRp0bRp02jbtm20b98+fvCDH+SjPgAAyIt897VZlsXw4cNjxowZkWVZzJ07N26++eYYOnToRtcpKiqKsrKyWhMAADuuOl/WYMqUKfHcc8/FF77whSgoKIji4uK46667okuXLvmoDwAA8iLffe1JJ50UN910U1x44YXx9ttvR7t27eKqq66KQYMG1cv2AQDY/tU5nF25cmXstttuEfHJ2QARES1atIhGjep8Ei4AACSTj7720+18aujQoZ97piwAADu3OneevXv3zt2koKCgICIibrnlljjwwAPrtzIAAMgjfS0AAKnV+czZ8ePHx5FHHhm/+MUvoqqqKrp27RpVVVXx9NNP56M+AADIC30tAACp1Tmc3WuvvaKioiIeffTR+Oc//xnt27eP4447LkpLS/NRHwAA5IW+FgCA1Op8WYNVq1bFNddcEz169IiRI0fGBx98EDfccEOsW7cuH/UBAEBe6GsBAEitzuHs8OHD4/HHH4/GjRtHRMQBBxwQTz75ZFx66aX1XhwAAOSLvhYAgNTqHM4+8MAD8dRTT0XHjh0jIuLrX/96PPzww3HPPffUe3EAAJAv+loAAFKrczj78ccfR3Fxca2xsrKyWL16db0VBQAA+aavBQAgtTqHs4cddliMGDEiampqIuKTpnbkyJFxyCGH1HtxAACQL/paAABSK6zrCjfffHMcffTRUVZWFm3atIklS5bE3nvvHY888kg+6gMAgLzQ1wIAkFqdw9nOnTvHa6+9Fs8991wsWrQoOnToED179ozCwjpvCgAAktHXAgCQ2hZ1nmvXro0vfvGL0blz54iIePfddyMicjdTAACA7YG+FgCAlOoczk6bNi3OO++8qKyszI1lWRYFBQWxdu3aei0OAADyRV8LAEBqdQ5nx4wZE9///vfjrLPOiiZNmuSjJgAAyDt9LQAAqdU5nF24cGGMGTPGtbgAANiu6WsBAEitUV1X+NrXvhavvvpqPmoBAIBtRl8LAEBqdT5N4JBDDokjjzwyvv3tb0e7du1qzbvyyivrrTAAAMgnfS0AAKnVOZydO3dudOvWLV577bV47bXXcuMFBQWaWAAAthv6WgAAUqtzODtr1qx81AEAANuUvhYAgNTqfM3ZiIjXXnstfvCDH8TJJ58cS5cujVtvvbW+6wIAgLzT1wIAkFKdw9np06dHr169YsmSJfH0009HdXV1XH311XH99dfnoz4AAMgLfS0AAKnVOZy97LLL4r777ospU6ZE48aNo0OHDvHYY4/FHXfckY/6AAAgL/S1AACkVudw9vXXX49jjz02Ij65WUJERI8ePeLDDz+s38oAACCP9LUAAKRW53C2U6dO8fzzz9cae/nll6NDhw71VhQAAOSbvhYAgNTqHM6OGjUqjj/++Lj88stj1apVccMNN8SJJ54YI0eOzEd9AACQF/paAABSK6zrCqeffnqUlZXFhAkTolOnTjFjxoy4+eab45RTTslHfQAAkBf6WgAAUqtzODtt2rT49re/Hf379681PnHixDjvvPPqrTAAAMgnfS0AAKltVjhbXV0dS5YsiYiIIUOGxEEHHRRZluXmr1ixIkaMGKGJBQCgQdPXAgDQkGxWOFtZWRn77rtvVFdXR0TEnnvuGVmWRUFBQe5/TzzxxHzWCQAAW01fCwBAQ7JZ4Wy7du1i/vz5UV1dHd26dYuKiopa85s1axZt27bNS4EAAFBf9LUAADQkm33N2d122y0iPjnboFGjRnkrCAAA8klfCwBAQ1HnG4ItWrQofvrTn8bf//73WLduXa15M2fOrLfCAAAgn/S1AACkVudwdvDgwfH+++/H8ccfH02aNMlHTQAAkHf6WgAAUqtzOPvSSy/F3//+99h1113zUQ8AAGwT+loAAFKr80W2dtlll2jWrFk+agEAgG1GXwsAQGp1DmdHjx4dgwcPjpdeeikWLFhQawIAgO2FvhYAgNTqfFmDc845JyIiHnzwwYiIKCgoiCzLoqCgINauXVu/1QEAQJ7oawEASK3O4eybb76ZjzoAAGCb0tcCAJBanS9r0KlTp+jUqVN8+OGH8fvf/z523333aN68eXTq1Ckf9QEAQF7oawEASK3O4ewHH3wQhxxySPTq1SvOPPPMmD9/fnzxi1+MuXPn5qM+AADIC30tAACp1TmcHTZsWHTv3j2WL18eTZo0iS9/+ctx6aWXxsiRI/NRHwAA5IW+FgCA1Op8zdmZM2fGP/7xj2jRokUUFBRERMSPf/zjuPHGG+u9OAAAyBd9LQAAqdX5zNmmTZvGRx99FBERWZZFRERVVVWUlpbWb2UAAJBH+loAAFKrczh7wgknxKBBg+L111+PgoKC+OCDD+LCCy+Mb37zm/moDwAA8kJfCwBAanUOZ6+77rooKSmJffbZJ5YvXx677757VFdXx3XXXZeP+gAAIC/0tQAApFana86uW7cuampqYtq0abF48eK46667YtWqVfHtb387WrZsma8aAQCgXulrAQBoCDb7zNl33nknunfvnrt77fTp0+Oyyy6Lhx56KHr16hUvv/xy3ooEAID6oq8FAKCh2Oxw9vLLL4+vfOUruZ95jRkzJv7zP/8zXn755ZgwYUKMGTMmb0UCAEB90dcCANBQbPZlDaZPnx5/+tOfYtddd40FCxbE/Pnz44wzzoiIiG9961tx8cUX561IAACoL/paAAAais0+c7aysjJ23XXXiIh44YUXYpdddokvfelLERHRrFmzWLVqVX4qBACAeqSvBQCgodjscLZVq1axePHiiIiYPXt2fP3rX8/N++tf/5prcAEAoCHT1wIA0FBsdjh7/PHHx8UXXxxTp06NKVOmxOmnnx4REcuXL4/Ro0fHMccck7ciAQCgvuhrAQBoKDY7nL3mmmviww8/jCFDhsSpp54aAwYMiIiIDh06xF/+8pcYO3ZsvmoEAIB6o68FAKCh2Owbgu2yyy7x1FNPrTf+wAMPxGGHHRbNmjWr18IAACAf9LUAADQUmx3ObsxRRx1VH3UAAEBS+loAALa1zb6sAQAAAAAA9Uc4CwAAAACQgHAWAAAAACCBpOHs4sWLo0uXLjF79uzc2AsvvBC9evWKkpKS6Ny5c0yaNKnWOpMnT44uXbpEcXFx9OjRI+bOnbuNqwYAAAAA2HrJwtk5c+ZE7969Y/78+bmxZcuWRf/+/ePMM8+M5cuXx6RJk2L48OHx4osvRkTE7Nmz4+KLL47JkyfH8uXLY+DAgXHCCSdEdXV1qsMAAAAAANgiScLZyZMnx4ABA+Kaa66pNf7AAw9EeXl5XHTRRVFYWBh9+/aNgQMHxoQJEyIi4s4774zTTz89DjnkkGjSpEkMHz482rRpE1OnTk1xGAAAAAAAWyxJOHv00UfH/Pnz47TTTqs1XlFREd27d6811rVr15g3b95mzd+QmpqaqKysrDUBAAAAAKSWJJxt165dFBYWrjdeVVUVxcXFtcZatGgRK1eu3Kz5GzJu3Lho2bJlburQoUM9HAEAAAAAwNZJekOwzyouLl7v+rHV1dVRWlq6WfM3ZNSoUbFixYrctHDhwvovHAAAAACgjhpUONutW7eoqKioNfbqq69Gt27dNmv+hhQVFUVZWVmtCQAAAAAgtQYVzp588smxaNGiGD9+fKxevTpmzZoVU6ZMiSFDhkRExJAhQ2LKlCkxa9asWL16dYwfPz7ef//9OOmkkxJXDgAAAABQNw0qnC0vL4/p06fHtGnTory8PM4555y45ZZb4ogjjoiIiCOPPDJuu+22uOCCC6JVq1Zx7733xuOPPx6tW7dOXDkAAAAAQN2sf1eubSzLslqPe/ToEXPmzNno8oMGDYpBgwbluywAAAAAgLxqUGfOAgAAAADsLISzAAAAAAAJCGcBAAAAABIQzgIAAAAAJCCcBQAAAABIQDgLAAAAAJCAcBYAAAAAIAHhLAAAAABAAsJZAAAAAIAEhLMAAAAAAAkIZwEAAAAAEhDOAgAAAAAkIJwFAAAAAEhAOAsAAAAAkIBwFgAAAAAgAeEsAADUg8WLF0eXLl1i9uzZubEXXnghevXqFSUlJdG5c+eYNGlSugIBAGhwhLMAALCV5syZE71794758+fnxpYtWxb9+/ePM888M5YvXx6TJk2K4cOHx4svvpiwUgAAGhLhLAAAbIXJkyfHgAED4pprrqk1/sADD0R5eXlcdNFFUVhYGH379o2BAwfGhAkTElUKAEBDI5wFAICtcPTRR8f8+fPjtNNOqzVeUVER3bt3rzXWtWvXmDdv3rYsDwCABqwwdQEAALA9a9eu3QbHq6qqori4uNZYixYtYuXKlRvdVk1NTdTU1OQeV1ZW1k+RAAA0SM6cBQCAPCguLo7q6upaY9XV1VFaWrrRdcaNGxctW7bMTR06dMh3mQAAJCScBQCAPOjWrVtUVFTUGnv11VejW7duG11n1KhRsWLFity0cOHCfJcJAEBCwlkAAMiDk08+ORYtWhTjx4+P1atXx6xZs2LKlCkxZMiQja5TVFQUZWVltSYAAHZcwlkAAMiD8vLymD59ekybNi3Ky8vjnHPOiVtuuSWOOOKI1KUBANBAuCEYAADUkyzLaj3u0aNHzJkzJ1E1AAA0dM6cBQAAAABIQDgLAAAAAJCAcBYAAAAAIAHhLAAAAABAAsJZAAAAAIAEhLMAAAAAAAkIZwEAAAAAEhDOAgAAAAAkIJwFAAAAAEhAOAsAAAAAkIBwFgAAAAAgAeEsAAAAAEACwlkAAAAAgASEswAAAAAACQhnAQAAAAASEM4CAAAAACQgnAUAAAAASEA4CwAAAACQgHAWAAAAACAB4SwAAAAAQALCWQAAAACABISzAAAAAAAJCGcBAAAAABIQzgIAAAAAJCCcBQAAAABIQDgLAAAAAJCAcBYAAAAAIAHhLAAAAABAAsJZAAAAAIAEhLMAAAAAAAkIZwEAAAAAEhDOAgAAAAAkIJwFAAAAAEhAOAsAAAAAkIBwFgAAAAAgAeEsAAAAAEACwlkAAAAAgASEswAAAAAACQhnAQAAAAASEM4CAAAAACQgnAUAAAAASEA4CwAAAACQgHAWAAAAACAB4SwAAAAAQALCWQAAAACABISzAAAAAAAJCGcBAAAAABIQzgIAAAAAJCCcBQAAAABIQDgLAAAAAJCAcBYAAAAAIAHhLAAAAABAAsJZAAAAAIAEhLMAAAAAAAkIZwEAAAAAEhDOAgAAAAAkIJwFAAAAAEhAOAsAAAAAkIBwFgAAAAAgAeEsAAAAAEACwlkAAAAAgASEswAAAAAACQhnAQAAAAASEM4CAAAAACQgnAUAAAAASEA4CwAAAACQgHAWAAAAACAB4SwAAAAAQALCWQAAAACABISzAAAAAAAJNMhwdurUqVFYWBglJSW56YwzzoiIiBdeeCF69eoVJSUl0blz55g0aVLiagEAAAAA6q5BhrMvvfRSnHHGGbFy5crcdPfdd8eyZcuif//+ceaZZ8by5ctj0qRJMXz48HjxxRdTlwwAAAAAUCcNNpzt0aPHeuMPPPBAlJeXx0UXXRSFhYXRt2/fGDhwYEyYMCFBlQAAAAAAW67BhbPr1q2LP/zhD/Hoo49Gp06don379nHeeefFsmXLoqKiIrp3715r+a5du8a8efM2ur2ampqorKysNQEAAAAApNbgwtnFixfH/vvvH6eeemq89tpr8fzzz8frr78egwYNiqqqqiguLq61fIsWLWLlypUb3d64ceOiZcuWualDhw75PgQAAAAAgE1qcOFs27Zt49lnn40hQ4ZEixYtomPHjnHDDTfE448/HlmWRXV1da3lq6uro7S0dKPbGzVqVKxYsSI3LVy4MN+HAAAAAACwSQ0unP3zn/8cl156aWRZlhurqamJRo0aRc+ePaOioqLW8q+++mp069Zto9srKiqKsrKyWhMAAAAAQGoNLpxt3bp13HrrrfFf//VfsWbNmliwYEGMHDkyBg8eHKeeemosWrQoxo8fH6tXr45Zs2bFlClTYsiQIanLBgAAAACokwYXzrZv3z4effTReOihh6J169bRo0ePOPDAA+PWW2+N8vLymD59ekybNi3Ky8vjnHPOiVtuuSWOOOKI1GUDAAAAANRJYeoCNuTwww+P559/foPzevToEXPmzNnGFQEAAAAA1K8Gd+YsAAAAAMDOQDgLAAAAAJCAcBYAAAAAIAHhLAAA5NHUqVOjsLAwSkpKctMZZ5yRuiwAABqABnlDMAAA2FG89NJLccYZZ8Rdd92VuhQAABoYZ84CAEAevfTSS9GjR4/UZQAA0AAJZwEAIE/WrVsXf/jDH+LRRx+NTp06Rfv27eO8886LZcuWbXD5mpqaqKysrDUBALDjEs4CAECeLF68OPbff/849dRT47XXXovnn38+Xn/99Rg0aNAGlx83bly0bNkyN3Xo0GEbVwwAwLbkmrMAAJAnbdu2jWeffTb3uGPHjnHDDTdEr169oqqqKkpLS2stP2rUqBgxYkTucWVlpYAWAGAH5sxZAADIkz//+c9x6aWXRpZlubGamppo1KhRNG3adL3li4qKoqysrNYEAMCOSzgLAAB50rp167j11lvjv/7rv2LNmjWxYMGCGDlyZAwePDiKiopSlwcAQGLCWQAAyJP27dvHo48+Gg899FC0bt06evToEQceeGDceuutqUsDAKABcM1ZAADIo8MPPzyef/751GUAANAAOXMWAAAAACAB4SwAAAAAQALCWQAAAACABISzAAAAAAAJCGcBAAAAABIQzgIAAAAAJCCcBQAAAABIQDgLAAAAAJCAcBYAAAAAIAHhLAAAAABAAsJZAAAAAIAEhLMAAAAAAAkIZwEAAAAAEhDOAgAAAAAkIJwFAAAAAEhAOAsAAAAAkIBwFgAAAAAgAeEsAAAAAEACwlkAAAAAgASEswAAAAAACQhnAQAAAAASEM4CAAAAACQgnAUAAAAASEA4CwAAAACQgHAWAAAAACAB4SwAAAAAQALCWQAAAACABISzAAAAAAAJCGcBAAAAABIQzgIAAAAAJCCcBQAAAABIQDgLAAAAAJCAcBYAAAAAIAHhLAAAAABAAsJZAAAAAIAEhLMAAAAAAAkIZwEAAAAAEhDOAgAAAAAkIJwFAAAAAEhAOAsAAAAAkIBwFgAAAAAgAeEsAAAAAEACwlkAAAAAgASEswAAAAAACQhnAQAAAAASEM4CAAAAACQgnAUAAAAASEA4CwAAAACQgHAWAAAAACAB4SwAAAAAQALCWQAAAACABISzAAAAAAAJCGcBAAAAABIQzgIAAAAAJCCcBQAAAABIQDgLAAAAAJCAcBYAAAAAIAHhLAAAAABAAsJZAAAAAIAEhLMAAAAAAAkIZwEAAAAAEhDOAgAAAAAkIJwFAAAAAEhAOAsAAAAAkIBwFgAAAAAgAeEsAAAAAEACwlkAAAAAgASEswAAAAAACQhnAQAAAAASEM4CAAAAACQgnAUAAAAASEA4CwAAAACQgHAWAAAAACAB4SwAAAAAQALCWQAAAACABISzAAAAAAAJCGcBAAAAABIQzgIAAAAAJCCcBQAAAABIQDgLAAAAAJCAcBYAAAAAIAHhLAAAAABAAttlOPvBBx/EiSeeGLvssku0adMmhg0bFmvWrEldFgAArEfvCgDAxmyX4expp50WJSUl8e6778aLL74YTz/9dNx0002pywIAgPXoXQEA2JjtLpx94403Yvbs2XHDDTdEixYtYq+99orRo0fHrbfemro0AACoRe8KAMDn2e7C2YqKimjdunXsscceubGuXbvGggULYvny5ekKAwCAz9C7AgDweQpTF1BXVVVVUVxcXGusRYsWERGxcuXK2GWXXWrNq6mpiZqamtzjFStWREREZWVlfgvdiLU1HyXZL7DtpPp+aQh8x8HOIdX33Kf7zbIsyf63xPbeu0ZE7FFSGKvLmyXbP5Bfe5QU7tT9q+842LGl/I7b3N51uwtni4uLo7q6utbYp49LS0vXW37cuHFx1VVXrTfeoUOH/BQI7PRa/vz81CUA5FXq77mqqqpo2bJl0ho2l94V2B7ceG7qCgDyJ/V33KZ614Jsezr1ICJef/312HvvvWPRokXRtm3biIiYOnVq/OhHP4qFCxeut/xnzz5Yt25dfPjhh1FeXh4FBQXbrG52TpWVldGhQ4dYuHBhlJWVpS4HoF75jmNby7IsqqqqYo899ohGjbaPq3PpXdme+F4HdmS+49jWNrd33e7C2YiIQw89NNq3bx8TJ06MJUuWxPHHHx+nnnpqjB07NnVpUEtlZWW0bNkyVqxY4csf2OH4joPNo3dle+F7HdiR+Y6jodo+Tjn4jPvvvz/WrFkTnTt3jl69esUxxxwTo0ePTl0WAACsR+8KAMDGbHfXnI2IaNu2bUybNi11GQAAsEl6VwAANma7PHMWthdFRUUxZsyYKCoqSl0KQL3zHQewY/G9DuzIfMfRUG2X15wFAAAAANjeOXMWAAAAACAB4SwAAAAAQALCWciTDz74IE488cTYZZddok2bNjFs2LBYs2ZN6rIA6tXixYujS5cuMXv27NSlALAV9K7AzkDvSkMknIU8Oe2006KkpCTefffdePHFF+Ppp5+Om266KXVZAPVmzpw50bt375g/f37qUgDYSnpXYEend6WhEs5CHrzxxhsxe/bsuOGGG6JFixax1157xejRo+PWW29NXRpAvZg8eXIMGDAgrrnmmtSlALCV9K7Ajk7vSkMmnIU8qKioiNatW8cee+yRG+vatWssWLAgli9fnq4wgHpy9NFHx/z58+O0005LXQoAW0nvCuzo9K40ZMJZyIOqqqooLi6uNdaiRYuIiFi5cmWKkgDqVbt27aKwsDB1GQDUA70rsKPTu9KQCWchD4qLi6O6urrW2KePS0tLU5QEAAAbpHcFgHSEs5AH3bp1i6VLl8b777+fG3v11Vejffv20bJly4SVAQBAbXpXAEhHOAt58B//8R/x9a9/PYYNGxZVVVXx5ptvxk9+8pP43ve+l7o0AACoRe8KAOkIZyFP7r///lizZk107tw5evXqFcccc0yMHj06dVkAALAevSsApFGQZVmWuggAAAAAgJ2NM2cBAAAAABIQzgIAAAAAJCCcBQAAAABIQDgLAAAAAJCAcBYAAAAAIAHhLAAAAABAAsJZAAAAAIAEhLMAAAAAAAkIZwEamIKCgpg9e/YWrdunT58YO3bsFq07e/bsKCgo2KJ1AQDYOeldAbaOcBYAAAAAIAHhLMB2ZNWqVTFy5Mj48pe/HKWlpbHbbrvFxRdfHFmW5ZaZP39+9OnTJ1q1ahWHHHJIvPTSS7l577//fgwaNCjatWsXe+yxR5x//vlRVVWV4lAAANjB6V0BNk04C7AdGT9+fDz++OMxc+bMqKqqit/85jdx++23x8yZM3PL/OY3v4mrr746Pvjgg+jfv38cc8wxsXz58li3bl1861vfikaNGsXrr78er7zySrzzzjtx3nnnJTwiAAB2VHpXgE0TzgJsR84999yYMWNGtGvXLt5777346KOPorS0NN55553cMt/73vfisMMOiyZNmsRll10WzZs3j8ceeyxefvnl+P3vfx+33XZblJaWRnl5efz3f/933HfffbF06dKERwUAwI5I7wqwaYWpCwBg8/3rX/+K73//+/HMM89E+/bt42tf+1pkWRbr1q3LLdO5c+fcfxcUFET79u3jnXfeicLCwli7dm20b9++1jaLioriH//4xzY7BgAAdg56V4BNE84CbEfOPffcaN26dbz33nvRrFmzWLduXbRq1arWMu+++27uv9etWxdvvfVW7LnnnvGFL3whmjdvHkuXLo3GjRtHRERNTU28+eab0aVLl3juuee26bEAALBj07sCbJrLGgA0QIsXL46333671rRmzZpYsWJFNGvWLBo3bhxVVVUxcuTIqKysjFWrVuXWnTRpUrzwwguxatWqGDt2bDRp0iT69+8fPXv2jP/4j/+IH/7wh7Fy5cr46KOPYvjw4XHkkUfGmjVrEh4tAADbM70rwJYTzgI0QN/5zneiQ4cOtaY33ngjfv7zn8ef/vSnaNWqVeyzzz5RWVkZxxxzTLzyyiu5dU855ZQ4//zzo02bNvHcc8/Fk08+GcXFxVFYWBiPPPJILFq0KLp06RK77757vPHGGzF9+vRo1qxZwqMFAGB7pncF2HIFWZZlqYsAAAAAANjZOHMWAAAAACAB4SwAAAAAQALCWQAAAACABISzAAAAAAAJCGcBAAAAABIQzgIAAAAAJCCcBQAAAABIQDgLAAAAAJCAcBYAAAAAIAHhLAAAAABAAsJZAAAAAIAEhLMAAAAAAAn8/9rf8TY0pwc/AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the data for the plots\n",
    "training_counts = training_df['label'].value_counts()\n",
    "test_counts = test_df['label'].value_counts()\n",
    "\n",
    "# Set up the subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot for the training set\n",
    "sns.barplot(x=training_counts.index, y=training_counts.values, ax=axes[0])\n",
    "axes[0].set_title('Distribution of labels in training set')\n",
    "axes[0].set_ylabel('Sentences')\n",
    "axes[0].set_xlabel('Label')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot for the test set\n",
    "sns.barplot(x=test_counts.index, y=test_counts.values, ax=axes[1])\n",
    "axes[1].set_title('Distribution of labels in test set')\n",
    "axes[1].set_ylabel('Sentences')\n",
    "axes[1].set_xlabel('Label')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack Obama']\n"
     ]
    }
   ],
   "source": [
    "def get_ner(text):\n",
    "    ner_list = []\n",
    "    # Annotate the text using stanza\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for entity in sentence.ents:\n",
    "            if entity.type == 'PERSON':\n",
    "                ner_list.append(entity.text)\n",
    "\n",
    "    return ner_list\n",
    "\n",
    "# Example usage\n",
    "text = \"Barack Obama was the 44th doctor of the United States.\"\n",
    "print(get_ner(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a named entity is present in the sentence\n",
    "def named_entity_present(sentence):\n",
    "    ner_list = get_ner(sentence)\n",
    "    if len(ner_list) > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Similarity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the similar words and similarity score\n",
    "# The function takes tokens of sentence as input and if its not a stop word, get its similarity with synsets of STEM.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words |= set([\"help\",\"try\", \"work\", \"process\", \"support\", \"job\"] )\n",
    "def word_similarity(tokens, syns, field):    \n",
    "    if field in ['engineering', 'technology']:\n",
    "        score_threshold = 0.5\n",
    "    else:\n",
    "        score_threshold = 0.2\n",
    "    sim_words = 0\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            try:\n",
    "                syns_word = wordnet.synsets(token) \n",
    "                score = syns_word[0].path_similarity(syns[0])\n",
    "                if score >= score_threshold:\n",
    "                    sim_words += 1\n",
    "            except: \n",
    "                score = 0\n",
    "    \n",
    "    return sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to create columns for similarity based on all STEM fields\n",
    "syns_bio = wordnet.synsets(lemmatizer.lemmatize(\"biology\"))\n",
    "syns_maths = wordnet.synsets(lemmatizer.lemmatize(\"mathematics\")) \n",
    "syns_tech = wordnet.synsets(lemmatizer.lemmatize(\"technology\"))\n",
    "syns_eng = wordnet.synsets(lemmatizer.lemmatize(\"engineering\"))\n",
    "syns_chem = wordnet.synsets(lemmatizer.lemmatize(\"chemistry\"))\n",
    "syns_phy = wordnet.synsets(lemmatizer.lemmatize(\"physics\"))\n",
    "syns_sci = wordnet.synsets(lemmatizer.lemmatize(\"science\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Medical Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neurodevelopmental', 'surgery', 'neonatal', 'gastroenterology', 'military', 'anesthesiology', 'transplant', 'internal', 'genitourinary', 'neurophysiology', 'maternal', 'procedural', 'oncology', 'pediatric', 'consultation', 'imaging', 'retardation', 'neurourology', 'calculi', 'critical', 'reconstructive', 'immunology', 'cardiology', 'critical', 'diabetes', 'metabolism', 'pediatrics', 'radiation', 'pathology', 'ophthalmology', 'community', 'emergency', 'endocrinology', 'anterior', 'chemical', 'breast', 'neuroradiology', 'interventional', 'biochemical', 'pulmonology', 'reproductive', 'research', 'sleep', 'preventive', 'neuroradiology', 'vascular', 'hematology', 'transplant', 'oncology', 'perinatal', 'diseases', 'endocrinology', 'toxicology', 'genetics', 'gynecologic', 'genetic', 'cardiac', 'urology', 'blood', 'psychiatric', 'pediatrics', 'renal', 'family', 'infectious', 'uveitis', 'dermatology', 'public', 'hematology', 'neuromuscular', 'neurology', 'female', 'segment', 'pediatric', 'nuclear', 'anatomical', 'advanced', 'abuse', 'urologic', 'urology', 'musculoskeletal', 'mental', 'dermatology', 'ophthalmology', 'obstetrics', 'glaucoma', 'neuropathology', 'cytogenetics', 'medical', 'neck', 'psychosomatic', 'endocrinologists', 'orbit', 'administrative', 'hospice', 'plastic', 'reconstructive', 'geriatric', 'pelvic', 'dermatopathology', 'allergy', 'gastrointestinal', 'cardiothoracic', 'immunopathology', 'neuro', 'occupational', 'palliative', 'psychiatry', 'hepatology', 'radiology', 'gastroenterology', 'psychiatry', 'chest', 'microbiology', 'physical', 'health', 'internal', 'adolescent', 'genetic', 'liaison', 'infertility', 'oculoplastics', 'rheumatology', 'pain', 'disease', 'banking', 'gynecology', 'neurology', 'male', 'infectious', 'rehabilitation', 'strabismus', 'adolescent', 'nephrology', 'head', 'care', 'clinical', 'brain', 'transfusion', 'diagnostic', 'surgery', 'disabilities', 'rheumatology', 'ocular', 'developmental', 'abdominal', 'and', 'addiction', 'medicine', 'cornea', 'pathology', 'molecular', 'failure', 'behavioral', 'surgical', 'fetal', 'anesthesiology', 'electrophysiology', 'sports', 'retina', 'child', 'interventional', 'heart', 'cardiovascular', 'forensic', 'sports', 'endovascular', 'injury', 'cytopathology', 'aerospace', 'nephrology', 'pulmonary', 'ophthalmic']\n"
     ]
    }
   ],
   "source": [
    "# Load the medical specialization text file and create a list\n",
    "medical_list = []\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/medical_specialities.txt', 'r') as medical_fields:\n",
    "    for line in medical_fields.readlines():\n",
    "        special_field = line.rstrip('\\n')\n",
    "        special_field = re.sub(\"\\W\",\" \", special_field )\n",
    "#         print(special_field)\n",
    "        medical_list += special_field.split()\n",
    "medical_list = list(set(medical_list))  \n",
    "medical_list = [x.lower() for x in medical_list]\n",
    "print(medical_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get medical words\n",
    "def check_medical_words(tokens):\n",
    "    for token in tokens:\n",
    "        if token not in stop_words and token in [x.lower() for x in medical_list]:\n",
    "            return 1\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sentiment Polarity and Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get polarity and subjectivity of the sentence using TexBlob\n",
    "def get_sentiment(sentence):\n",
    "    sentiments =TextBlob(sentence).sentiment\n",
    "    polarity = sentiments.polarity\n",
    "    subjectivity = sentiments.subjectivity\n",
    "    return polarity, subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the count of POS tags of the sentence\n",
    "def count_pos_tags(tokens):\n",
    "    token_pos = pos_tag(tokens)\n",
    "    count = Counter(tag for word,tag in token_pos)\n",
    "    interjections =  count['UH']\n",
    "    nouns = count['NN'] + count['NNS'] + count['NNP'] + count['NNPS']\n",
    "    adverb = count['RB'] + count['RBS'] + count['RBR']\n",
    "    verb = count['VB'] + count['VBD'] + count['VBG'] + count['VBN']\n",
    "    determiner = count['DT']\n",
    "    pronoun = count['PRP']\n",
    "    adjetive = count['JJ'] + count['JJR'] + count['JJS']\n",
    "    preposition = count['IN']\n",
    "    return interjections, nouns, adverb, verb, determiner, pronoun, adjetive,preposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_extraction(dataframe, field, func, column_names):\n",
    "    return pd.concat((\n",
    "        dataframe,\n",
    "        dataframe[field].apply(\n",
    "            lambda cell: pd.Series(func(cell), index=column_names))), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the w2v dict from pickle file\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/pickle/embeddings06122024.pickle', 'rb') as w2v_file:\n",
    "    w2v_dict = pickle.load(w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of word embeddings:  4762\n"
     ]
    }
   ],
   "source": [
    "print(\"length of word embeddings: \", len(w2v_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vectors for the essay\n",
    "def vectorizer(sequence):\n",
    "    vect = []\n",
    "    numw = 0\n",
    "    for w in sequence: \n",
    "        try :\n",
    "            if numw == 0:\n",
    "                vect = w2v_dict[w]\n",
    "            else:\n",
    "                vect = np.add(vect, w2v_dict[w])\n",
    "            numw += 1\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    return vect/ numw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into words\n",
    "def split_into_words(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vectorizer\n",
    "unigram_vect = CountVectorizer(ngram_range=(1, 1), min_df=2, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Putting them all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for feature engineering\n",
    "def feature_engineering(original_dataset):\n",
    "\n",
    "    dataset = original_dataset.copy()\n",
    "    # create a new column with sentence tokens\n",
    "    dataset['tokens'] = dataset['sentence'].apply(word_tokenize)\n",
    "    # 1. Similarity features\n",
    "    # biology\n",
    "    dataset['bio_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_bio,'biology',)) \n",
    "    # chemistry\n",
    "    dataset['chem_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_chem,'chemistry',))\n",
    "    # physics\n",
    "    dataset['phy_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_phy,'physics',))\n",
    "    # mathematics\n",
    "    dataset['math_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_maths,'mathematics',))\n",
    "    # technology\n",
    "    dataset['tech_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_tech,'technology',))\n",
    "    # engineering\n",
    "    dataset['eng_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_eng,'engineering',))\n",
    "    \n",
    "    # medical terms\n",
    "    dataset['medical_terms'] = dataset['tokens'].apply(check_medical_words)\n",
    "    \n",
    "    # polarity and subjectivity\n",
    "    dataset['polarity'], dataset['subjectivity'] = zip(*dataset['sentence'].apply(get_sentiment))\n",
    "    \n",
    "    # named entity recognition\n",
    "    dataset['ner'] = dataset['sentence'].apply(named_entity_present)\n",
    "    \n",
    "    # pos tag count\n",
    "    dataset = pos_tag_extraction(dataset, 'tokens', count_pos_tags, ['interjections', 'nouns', 'adverb', 'verb', 'determiner', 'pronoun', 'adjetive','preposition'])\n",
    "    \n",
    "    # labels\n",
    "    data_labels = dataset['label']\n",
    "    # X\n",
    "    data_x = dataset.drop(columns='label')\n",
    "\n",
    "    \n",
    "    # vectorize all the essays\n",
    "    vect_arr = data_x.tokens.apply(vectorizer)\n",
    "    for index in range(0, len(vect_arr)):\n",
    "        i = 0\n",
    "        for item in vect_arr[index]:\n",
    "            column_name= \"embedding\" + str(i)\n",
    "            data_x.loc[index, column_name] = item\n",
    "            i +=1\n",
    "    \n",
    "    return data_x,data_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = feature_engineering(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455, 121)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = feature_engineering(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 121)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Unigram features for both train and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455, 121)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 121)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2.1/Familial/saved_features/X_train_final.csv\", index=False)\n",
    "X_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2.1/Familial/saved_features/X_test_final.csv\", index=False)\n",
    "y_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2.1/Familial/saved_features/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2.1/Familial/saved_features/y_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the unigram df for train :  (455, 513)\n"
     ]
    }
   ],
   "source": [
    "# Unigrams for training set\n",
    "unigram_matrix = unigram_vect.fit_transform(X_train['sentence'])\n",
    "unigrams = pd.DataFrame(unigram_matrix.toarray())\n",
    "print(\"Shape of the unigram df for train : \",unigrams.shape)\n",
    "unigrams = unigrams.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = pd.concat([X_train, unigrams], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final.columns = X_train_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455, 634)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test unigram df shape :  (51, 513)\n"
     ]
    }
   ],
   "source": [
    "unigram_matrix_test = unigram_vect.transform(X_test['sentence'])\n",
    "unigrams_test = pd.DataFrame(unigram_matrix_test.toarray())\n",
    "unigrams_test = unigrams_test.reset_index(drop=True)\n",
    "print(\"Test unigram df shape : \",unigrams_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 634)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final = pd.concat([X_test, unigrams_test], axis = 1)\n",
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final.columns = X_test_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 634)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ---- sentence\n",
      "1 ---- phrase\n",
      "2 ---- tokens\n",
      "3 ---- bio_sim_words\n",
      "4 ---- chem_sim_words\n",
      "5 ---- phy_sim_words\n",
      "6 ---- math_sim_words\n",
      "7 ---- tech_sim_words\n",
      "8 ---- eng_sim_words\n",
      "9 ---- medical_terms\n",
      "10 ---- polarity\n",
      "11 ---- subjectivity\n",
      "12 ---- ner\n",
      "13 ---- interjections\n",
      "14 ---- nouns\n",
      "15 ---- adverb\n",
      "16 ---- verb\n",
      "17 ---- determiner\n",
      "18 ---- pronoun\n",
      "19 ---- adjetive\n",
      "20 ---- preposition\n",
      "21 ---- embedding0\n",
      "22 ---- embedding1\n",
      "23 ---- embedding2\n",
      "24 ---- embedding3\n",
      "25 ---- embedding4\n",
      "26 ---- embedding5\n",
      "27 ---- embedding6\n",
      "28 ---- embedding7\n",
      "29 ---- embedding8\n",
      "30 ---- embedding9\n",
      "31 ---- embedding10\n",
      "32 ---- embedding11\n",
      "33 ---- embedding12\n",
      "34 ---- embedding13\n",
      "35 ---- embedding14\n",
      "36 ---- embedding15\n",
      "37 ---- embedding16\n",
      "38 ---- embedding17\n",
      "39 ---- embedding18\n",
      "40 ---- embedding19\n",
      "41 ---- embedding20\n",
      "42 ---- embedding21\n",
      "43 ---- embedding22\n",
      "44 ---- embedding23\n",
      "45 ---- embedding24\n",
      "46 ---- embedding25\n",
      "47 ---- embedding26\n",
      "48 ---- embedding27\n",
      "49 ---- embedding28\n",
      "50 ---- embedding29\n",
      "51 ---- embedding30\n",
      "52 ---- embedding31\n",
      "53 ---- embedding32\n",
      "54 ---- embedding33\n",
      "55 ---- embedding34\n",
      "56 ---- embedding35\n",
      "57 ---- embedding36\n",
      "58 ---- embedding37\n",
      "59 ---- embedding38\n",
      "60 ---- embedding39\n",
      "61 ---- embedding40\n",
      "62 ---- embedding41\n",
      "63 ---- embedding42\n",
      "64 ---- embedding43\n",
      "65 ---- embedding44\n",
      "66 ---- embedding45\n",
      "67 ---- embedding46\n",
      "68 ---- embedding47\n",
      "69 ---- embedding48\n",
      "70 ---- embedding49\n",
      "71 ---- embedding50\n",
      "72 ---- embedding51\n",
      "73 ---- embedding52\n",
      "74 ---- embedding53\n",
      "75 ---- embedding54\n",
      "76 ---- embedding55\n",
      "77 ---- embedding56\n",
      "78 ---- embedding57\n",
      "79 ---- embedding58\n",
      "80 ---- embedding59\n",
      "81 ---- embedding60\n",
      "82 ---- embedding61\n",
      "83 ---- embedding62\n",
      "84 ---- embedding63\n",
      "85 ---- embedding64\n",
      "86 ---- embedding65\n",
      "87 ---- embedding66\n",
      "88 ---- embedding67\n",
      "89 ---- embedding68\n",
      "90 ---- embedding69\n",
      "91 ---- embedding70\n",
      "92 ---- embedding71\n",
      "93 ---- embedding72\n",
      "94 ---- embedding73\n",
      "95 ---- embedding74\n",
      "96 ---- embedding75\n",
      "97 ---- embedding76\n",
      "98 ---- embedding77\n",
      "99 ---- embedding78\n",
      "100 ---- embedding79\n",
      "101 ---- embedding80\n",
      "102 ---- embedding81\n",
      "103 ---- embedding82\n",
      "104 ---- embedding83\n",
      "105 ---- embedding84\n",
      "106 ---- embedding85\n",
      "107 ---- embedding86\n",
      "108 ---- embedding87\n",
      "109 ---- embedding88\n",
      "110 ---- embedding89\n",
      "111 ---- embedding90\n",
      "112 ---- embedding91\n",
      "113 ---- embedding92\n",
      "114 ---- embedding93\n",
      "115 ---- embedding94\n",
      "116 ---- embedding95\n",
      "117 ---- embedding96\n",
      "118 ---- embedding97\n",
      "119 ---- embedding98\n",
      "120 ---- embedding99\n",
      "121 ---- 0\n",
      "122 ---- 1\n",
      "123 ---- 2\n",
      "124 ---- 3\n",
      "125 ---- 4\n",
      "126 ---- 5\n",
      "127 ---- 6\n",
      "128 ---- 7\n",
      "129 ---- 8\n",
      "130 ---- 9\n",
      "131 ---- 10\n",
      "132 ---- 11\n",
      "133 ---- 12\n",
      "134 ---- 13\n",
      "135 ---- 14\n",
      "136 ---- 15\n",
      "137 ---- 16\n",
      "138 ---- 17\n",
      "139 ---- 18\n",
      "140 ---- 19\n",
      "141 ---- 20\n",
      "142 ---- 21\n",
      "143 ---- 22\n",
      "144 ---- 23\n",
      "145 ---- 24\n",
      "146 ---- 25\n",
      "147 ---- 26\n",
      "148 ---- 27\n",
      "149 ---- 28\n",
      "150 ---- 29\n",
      "151 ---- 30\n",
      "152 ---- 31\n",
      "153 ---- 32\n",
      "154 ---- 33\n",
      "155 ---- 34\n",
      "156 ---- 35\n",
      "157 ---- 36\n",
      "158 ---- 37\n",
      "159 ---- 38\n",
      "160 ---- 39\n",
      "161 ---- 40\n",
      "162 ---- 41\n",
      "163 ---- 42\n",
      "164 ---- 43\n",
      "165 ---- 44\n",
      "166 ---- 45\n",
      "167 ---- 46\n",
      "168 ---- 47\n",
      "169 ---- 48\n",
      "170 ---- 49\n",
      "171 ---- 50\n",
      "172 ---- 51\n",
      "173 ---- 52\n",
      "174 ---- 53\n",
      "175 ---- 54\n",
      "176 ---- 55\n",
      "177 ---- 56\n",
      "178 ---- 57\n",
      "179 ---- 58\n",
      "180 ---- 59\n",
      "181 ---- 60\n",
      "182 ---- 61\n",
      "183 ---- 62\n",
      "184 ---- 63\n",
      "185 ---- 64\n",
      "186 ---- 65\n",
      "187 ---- 66\n",
      "188 ---- 67\n",
      "189 ---- 68\n",
      "190 ---- 69\n",
      "191 ---- 70\n",
      "192 ---- 71\n",
      "193 ---- 72\n",
      "194 ---- 73\n",
      "195 ---- 74\n",
      "196 ---- 75\n",
      "197 ---- 76\n",
      "198 ---- 77\n",
      "199 ---- 78\n",
      "200 ---- 79\n",
      "201 ---- 80\n",
      "202 ---- 81\n",
      "203 ---- 82\n",
      "204 ---- 83\n",
      "205 ---- 84\n",
      "206 ---- 85\n",
      "207 ---- 86\n",
      "208 ---- 87\n",
      "209 ---- 88\n",
      "210 ---- 89\n",
      "211 ---- 90\n",
      "212 ---- 91\n",
      "213 ---- 92\n",
      "214 ---- 93\n",
      "215 ---- 94\n",
      "216 ---- 95\n",
      "217 ---- 96\n",
      "218 ---- 97\n",
      "219 ---- 98\n",
      "220 ---- 99\n",
      "221 ---- 100\n",
      "222 ---- 101\n",
      "223 ---- 102\n",
      "224 ---- 103\n",
      "225 ---- 104\n",
      "226 ---- 105\n",
      "227 ---- 106\n",
      "228 ---- 107\n",
      "229 ---- 108\n",
      "230 ---- 109\n",
      "231 ---- 110\n",
      "232 ---- 111\n",
      "233 ---- 112\n",
      "234 ---- 113\n",
      "235 ---- 114\n",
      "236 ---- 115\n",
      "237 ---- 116\n",
      "238 ---- 117\n",
      "239 ---- 118\n",
      "240 ---- 119\n",
      "241 ---- 120\n",
      "242 ---- 121\n",
      "243 ---- 122\n",
      "244 ---- 123\n",
      "245 ---- 124\n",
      "246 ---- 125\n",
      "247 ---- 126\n",
      "248 ---- 127\n",
      "249 ---- 128\n",
      "250 ---- 129\n",
      "251 ---- 130\n",
      "252 ---- 131\n",
      "253 ---- 132\n",
      "254 ---- 133\n",
      "255 ---- 134\n",
      "256 ---- 135\n",
      "257 ---- 136\n",
      "258 ---- 137\n",
      "259 ---- 138\n",
      "260 ---- 139\n",
      "261 ---- 140\n",
      "262 ---- 141\n",
      "263 ---- 142\n",
      "264 ---- 143\n",
      "265 ---- 144\n",
      "266 ---- 145\n",
      "267 ---- 146\n",
      "268 ---- 147\n",
      "269 ---- 148\n",
      "270 ---- 149\n",
      "271 ---- 150\n",
      "272 ---- 151\n",
      "273 ---- 152\n",
      "274 ---- 153\n",
      "275 ---- 154\n",
      "276 ---- 155\n",
      "277 ---- 156\n",
      "278 ---- 157\n",
      "279 ---- 158\n",
      "280 ---- 159\n",
      "281 ---- 160\n",
      "282 ---- 161\n",
      "283 ---- 162\n",
      "284 ---- 163\n",
      "285 ---- 164\n",
      "286 ---- 165\n",
      "287 ---- 166\n",
      "288 ---- 167\n",
      "289 ---- 168\n",
      "290 ---- 169\n",
      "291 ---- 170\n",
      "292 ---- 171\n",
      "293 ---- 172\n",
      "294 ---- 173\n",
      "295 ---- 174\n",
      "296 ---- 175\n",
      "297 ---- 176\n",
      "298 ---- 177\n",
      "299 ---- 178\n",
      "300 ---- 179\n",
      "301 ---- 180\n",
      "302 ---- 181\n",
      "303 ---- 182\n",
      "304 ---- 183\n",
      "305 ---- 184\n",
      "306 ---- 185\n",
      "307 ---- 186\n",
      "308 ---- 187\n",
      "309 ---- 188\n",
      "310 ---- 189\n",
      "311 ---- 190\n",
      "312 ---- 191\n",
      "313 ---- 192\n",
      "314 ---- 193\n",
      "315 ---- 194\n",
      "316 ---- 195\n",
      "317 ---- 196\n",
      "318 ---- 197\n",
      "319 ---- 198\n",
      "320 ---- 199\n",
      "321 ---- 200\n",
      "322 ---- 201\n",
      "323 ---- 202\n",
      "324 ---- 203\n",
      "325 ---- 204\n",
      "326 ---- 205\n",
      "327 ---- 206\n",
      "328 ---- 207\n",
      "329 ---- 208\n",
      "330 ---- 209\n",
      "331 ---- 210\n",
      "332 ---- 211\n",
      "333 ---- 212\n",
      "334 ---- 213\n",
      "335 ---- 214\n",
      "336 ---- 215\n",
      "337 ---- 216\n",
      "338 ---- 217\n",
      "339 ---- 218\n",
      "340 ---- 219\n",
      "341 ---- 220\n",
      "342 ---- 221\n",
      "343 ---- 222\n",
      "344 ---- 223\n",
      "345 ---- 224\n",
      "346 ---- 225\n",
      "347 ---- 226\n",
      "348 ---- 227\n",
      "349 ---- 228\n",
      "350 ---- 229\n",
      "351 ---- 230\n",
      "352 ---- 231\n",
      "353 ---- 232\n",
      "354 ---- 233\n",
      "355 ---- 234\n",
      "356 ---- 235\n",
      "357 ---- 236\n",
      "358 ---- 237\n",
      "359 ---- 238\n",
      "360 ---- 239\n",
      "361 ---- 240\n",
      "362 ---- 241\n",
      "363 ---- 242\n",
      "364 ---- 243\n",
      "365 ---- 244\n",
      "366 ---- 245\n",
      "367 ---- 246\n",
      "368 ---- 247\n",
      "369 ---- 248\n",
      "370 ---- 249\n",
      "371 ---- 250\n",
      "372 ---- 251\n",
      "373 ---- 252\n",
      "374 ---- 253\n",
      "375 ---- 254\n",
      "376 ---- 255\n",
      "377 ---- 256\n",
      "378 ---- 257\n",
      "379 ---- 258\n",
      "380 ---- 259\n",
      "381 ---- 260\n",
      "382 ---- 261\n",
      "383 ---- 262\n",
      "384 ---- 263\n",
      "385 ---- 264\n",
      "386 ---- 265\n",
      "387 ---- 266\n",
      "388 ---- 267\n",
      "389 ---- 268\n",
      "390 ---- 269\n",
      "391 ---- 270\n",
      "392 ---- 271\n",
      "393 ---- 272\n",
      "394 ---- 273\n",
      "395 ---- 274\n",
      "396 ---- 275\n",
      "397 ---- 276\n",
      "398 ---- 277\n",
      "399 ---- 278\n",
      "400 ---- 279\n",
      "401 ---- 280\n",
      "402 ---- 281\n",
      "403 ---- 282\n",
      "404 ---- 283\n",
      "405 ---- 284\n",
      "406 ---- 285\n",
      "407 ---- 286\n",
      "408 ---- 287\n",
      "409 ---- 288\n",
      "410 ---- 289\n",
      "411 ---- 290\n",
      "412 ---- 291\n",
      "413 ---- 292\n",
      "414 ---- 293\n",
      "415 ---- 294\n",
      "416 ---- 295\n",
      "417 ---- 296\n",
      "418 ---- 297\n",
      "419 ---- 298\n",
      "420 ---- 299\n",
      "421 ---- 300\n",
      "422 ---- 301\n",
      "423 ---- 302\n",
      "424 ---- 303\n",
      "425 ---- 304\n",
      "426 ---- 305\n",
      "427 ---- 306\n",
      "428 ---- 307\n",
      "429 ---- 308\n",
      "430 ---- 309\n",
      "431 ---- 310\n",
      "432 ---- 311\n",
      "433 ---- 312\n",
      "434 ---- 313\n",
      "435 ---- 314\n",
      "436 ---- 315\n",
      "437 ---- 316\n",
      "438 ---- 317\n",
      "439 ---- 318\n",
      "440 ---- 319\n",
      "441 ---- 320\n",
      "442 ---- 321\n",
      "443 ---- 322\n",
      "444 ---- 323\n",
      "445 ---- 324\n",
      "446 ---- 325\n",
      "447 ---- 326\n",
      "448 ---- 327\n",
      "449 ---- 328\n",
      "450 ---- 329\n",
      "451 ---- 330\n",
      "452 ---- 331\n",
      "453 ---- 332\n",
      "454 ---- 333\n",
      "455 ---- 334\n",
      "456 ---- 335\n",
      "457 ---- 336\n",
      "458 ---- 337\n",
      "459 ---- 338\n",
      "460 ---- 339\n",
      "461 ---- 340\n",
      "462 ---- 341\n",
      "463 ---- 342\n",
      "464 ---- 343\n",
      "465 ---- 344\n",
      "466 ---- 345\n",
      "467 ---- 346\n",
      "468 ---- 347\n",
      "469 ---- 348\n",
      "470 ---- 349\n",
      "471 ---- 350\n",
      "472 ---- 351\n",
      "473 ---- 352\n",
      "474 ---- 353\n",
      "475 ---- 354\n",
      "476 ---- 355\n",
      "477 ---- 356\n",
      "478 ---- 357\n",
      "479 ---- 358\n",
      "480 ---- 359\n",
      "481 ---- 360\n",
      "482 ---- 361\n",
      "483 ---- 362\n",
      "484 ---- 363\n",
      "485 ---- 364\n",
      "486 ---- 365\n",
      "487 ---- 366\n",
      "488 ---- 367\n",
      "489 ---- 368\n",
      "490 ---- 369\n",
      "491 ---- 370\n",
      "492 ---- 371\n",
      "493 ---- 372\n",
      "494 ---- 373\n",
      "495 ---- 374\n",
      "496 ---- 375\n",
      "497 ---- 376\n",
      "498 ---- 377\n",
      "499 ---- 378\n",
      "500 ---- 379\n",
      "501 ---- 380\n",
      "502 ---- 381\n",
      "503 ---- 382\n",
      "504 ---- 383\n",
      "505 ---- 384\n",
      "506 ---- 385\n",
      "507 ---- 386\n",
      "508 ---- 387\n",
      "509 ---- 388\n",
      "510 ---- 389\n",
      "511 ---- 390\n",
      "512 ---- 391\n",
      "513 ---- 392\n",
      "514 ---- 393\n",
      "515 ---- 394\n",
      "516 ---- 395\n",
      "517 ---- 396\n",
      "518 ---- 397\n",
      "519 ---- 398\n",
      "520 ---- 399\n",
      "521 ---- 400\n",
      "522 ---- 401\n",
      "523 ---- 402\n",
      "524 ---- 403\n",
      "525 ---- 404\n",
      "526 ---- 405\n",
      "527 ---- 406\n",
      "528 ---- 407\n",
      "529 ---- 408\n",
      "530 ---- 409\n",
      "531 ---- 410\n",
      "532 ---- 411\n",
      "533 ---- 412\n",
      "534 ---- 413\n",
      "535 ---- 414\n",
      "536 ---- 415\n",
      "537 ---- 416\n",
      "538 ---- 417\n",
      "539 ---- 418\n",
      "540 ---- 419\n",
      "541 ---- 420\n",
      "542 ---- 421\n",
      "543 ---- 422\n",
      "544 ---- 423\n",
      "545 ---- 424\n",
      "546 ---- 425\n",
      "547 ---- 426\n",
      "548 ---- 427\n",
      "549 ---- 428\n",
      "550 ---- 429\n",
      "551 ---- 430\n",
      "552 ---- 431\n",
      "553 ---- 432\n",
      "554 ---- 433\n",
      "555 ---- 434\n",
      "556 ---- 435\n",
      "557 ---- 436\n",
      "558 ---- 437\n",
      "559 ---- 438\n",
      "560 ---- 439\n",
      "561 ---- 440\n",
      "562 ---- 441\n",
      "563 ---- 442\n",
      "564 ---- 443\n",
      "565 ---- 444\n",
      "566 ---- 445\n",
      "567 ---- 446\n",
      "568 ---- 447\n",
      "569 ---- 448\n",
      "570 ---- 449\n",
      "571 ---- 450\n",
      "572 ---- 451\n",
      "573 ---- 452\n",
      "574 ---- 453\n",
      "575 ---- 454\n",
      "576 ---- 455\n",
      "577 ---- 456\n",
      "578 ---- 457\n",
      "579 ---- 458\n",
      "580 ---- 459\n",
      "581 ---- 460\n",
      "582 ---- 461\n",
      "583 ---- 462\n",
      "584 ---- 463\n",
      "585 ---- 464\n",
      "586 ---- 465\n",
      "587 ---- 466\n",
      "588 ---- 467\n",
      "589 ---- 468\n",
      "590 ---- 469\n",
      "591 ---- 470\n",
      "592 ---- 471\n",
      "593 ---- 472\n",
      "594 ---- 473\n",
      "595 ---- 474\n",
      "596 ---- 475\n",
      "597 ---- 476\n",
      "598 ---- 477\n",
      "599 ---- 478\n",
      "600 ---- 479\n",
      "601 ---- 480\n",
      "602 ---- 481\n",
      "603 ---- 482\n",
      "604 ---- 483\n",
      "605 ---- 484\n",
      "606 ---- 485\n",
      "607 ---- 486\n",
      "608 ---- 487\n",
      "609 ---- 488\n",
      "610 ---- 489\n",
      "611 ---- 490\n",
      "612 ---- 491\n",
      "613 ---- 492\n",
      "614 ---- 493\n",
      "615 ---- 494\n",
      "616 ---- 495\n",
      "617 ---- 496\n",
      "618 ---- 497\n",
      "619 ---- 498\n",
      "620 ---- 499\n",
      "621 ---- 500\n",
      "622 ---- 501\n",
      "623 ---- 502\n",
      "624 ---- 503\n",
      "625 ---- 504\n",
      "626 ---- 505\n",
      "627 ---- 506\n",
      "628 ---- 507\n",
      "629 ---- 508\n",
      "630 ---- 509\n",
      "631 ---- 510\n",
      "632 ---- 511\n",
      "633 ---- 512\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(X_train_final.columns)):\n",
    "    print('{} ---- {}'.format(i, X_train_final.columns[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Model 5: Without POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_5 = X_train_final.iloc[:,np.r_[3:13,21:633]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455, 622)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_5 = X_test_final.iloc[:,np.r_[3:13,21:633]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 622)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.787\n",
      "Best parameters set:\n",
      "\tclf__C: 1\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.77      0.78        26\n",
      "           1       0.77      0.80      0.78        25\n",
      "\n",
      "    accuracy                           0.78        51\n",
      "   macro avg       0.78      0.78      0.78        51\n",
      "weighted avg       0.78      0.78      0.78        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_5_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_5_pipeline, parameters, scoring=\"average_precision\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_5,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_5), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 20, False Positive: 6, False Negative: 5, True Positive: 20\n",
      "--------------------------------------------------------------------------------\n",
      "[[20  6]\n",
      " [ 5 20]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.77      0.78        26\n",
      "           1       0.77      0.80      0.78        25\n",
      "\n",
      "    accuracy                           0.78        51\n",
      "   macro avg       0.78      0.78      0.78        51\n",
      "weighted avg       0.78      0.78      0.78        51\n",
      "\n",
      "Average Precision: 0.7134\n"
     ]
    }
   ],
   "source": [
    "lr_model_5 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_5, y_train)\n",
    "y_lr = lr_model_5.predict(X_test_model_5)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))\n",
    "\n",
    "# Calculate and print the average precision score\n",
    "avg_precision = average_precision_score(y_test, y_lr)\n",
    "print(f'Average Precision: {avg_precision:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF Model 4: Without Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [10,20,50,100,200,300]\n",
    "max_depth = [2,5,8,10,15,20]\n",
    "min_samples_split = [2, 5, 10, 15, 20]\n",
    "min_samples_leaf = [1, 2, 5, 10,20]\n",
    "rf_parameters = dict(n_estimators = n_estimators, max_depth = max_depth,  \n",
    "              min_samples_split = min_samples_split, \n",
    "              min_samples_leaf = min_samples_leaf)\n",
    "\n",
    "## reduced parameters\n",
    "# n_estimators = [50, 100, 200]       # Reduced options\n",
    "# max_depth = [5, 10, 15]             # Reduced options\n",
    "# min_samples_split = [2, 10]         # Reduced options\n",
    "# min_samples_leaf = [1, 5]           # Reduced options\n",
    "\n",
    "# rf_parameters = dict(\n",
    "#     n_estimators = n_estimators, \n",
    "#     max_depth = max_depth,  \n",
    "#     min_samples_split = min_samples_split, \n",
    "#     min_samples_leaf = min_samples_leaf\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_4 = X_train_final.iloc[:,np.r_[3:21,121:633]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455, 530)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_4 = X_test_final.iloc[:,np.r_[3:21,121:633]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 530)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 900 candidates, totalling 9000 fits\n",
      "Best score: 0.876\n",
      "Best parameters set:\n",
      "\tmax_depth: 15\n",
      "\tmin_samples_leaf: 1\n",
      "\tmin_samples_split: 20\n",
      "\tn_estimators: 50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.73      0.72        26\n",
      "           1       0.71      0.68      0.69        25\n",
      "\n",
      "    accuracy                           0.71        51\n",
      "   macro avg       0.71      0.71      0.71        51\n",
      "weighted avg       0.71      0.71      0.71        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_model_4 = RandomForestClassifier(random_state=18,class_weight='balanced')\n",
    "grid_search = GridSearchCV(rf_model_4, rf_parameters, scoring=\"average_precision\", cv = 10, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_model_4,y_train)\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(rf_parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_4), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 19, False Positive: 7, False Negative: 8, True Positive: 17\n",
      "--------------------------------------------------------------------------------\n",
      "[[19  7]\n",
      " [ 8 17]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.73      0.72        26\n",
      "           1       0.71      0.68      0.69        25\n",
      "\n",
      "    accuracy                           0.71        51\n",
      "   macro avg       0.71      0.71      0.71        51\n",
      "weighted avg       0.71      0.71      0.71        51\n",
      "\n",
      "Average Precision: 0.6385\n"
     ]
    }
   ],
   "source": [
    "randomForest_4 = RandomForestClassifier(random_state=18,\n",
    "                                        class_weight=best_parameters['class_weight'],\n",
    "                                        max_depth=best_parameters['max_depth'],\n",
    "                                        min_samples_leaf=best_parameters['min_samples_leaf'],\n",
    "                                        min_samples_split=best_parameters['min_samples_split'],\n",
    "                                        n_estimators=best_parameters['n_estimators']).fit(X_train_model_4, y_train)\n",
    "\n",
    "y_lr = randomForest_4.predict(X_test_model_4)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))\n",
    "\n",
    "# Calculate and print the average precision score\n",
    "avg_precision = average_precision_score(y_test, y_lr)\n",
    "print(f'Average Precision: {avg_precision:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
