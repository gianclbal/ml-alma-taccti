{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Logistic Regression and Random Forest Models Using Merged Data Experiment 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb13978356b4e38a14406c6d10ac689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 09:01:25 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-10-16 09:01:25 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-10-16 09:01:27 INFO: File exists: /Users/gbaldonado/stanza_resources/en/default.zip\n",
      "2024-10-16 09:01:29 INFO: Finished downloading models and saved to /Users/gbaldonado/stanza_resources\n",
      "2024-10-16 09:01:29 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "723a47fc482546b686fe76466932b8a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 09:01:29 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-10-16 09:01:30 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-10-16 09:01:30 INFO: Using device: cpu\n",
      "2024-10-16 09:01:30 INFO: Loading: tokenize\n",
      "2024-10-16 09:01:30 INFO: Loading: mwt\n",
      "2024-10-16 09:01:30 INFO: Loading: pos\n",
      "2024-10-16 09:01:31 INFO: Loading: lemma\n",
      "2024-10-16 09:01:31 INFO: Loading: constituency\n",
      "2024-10-16 09:01:31 INFO: Loading: depparse\n",
      "2024-10-16 09:01:31 INFO: Loading: sentiment\n",
      "2024-10-16 09:01:31 INFO: Loading: ner\n",
      "2024-10-16 09:01:32 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import pickle\n",
    "import warnings\n",
    "import stanza\n",
    "\n",
    "from random import shuffle\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, f1_score, r2_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Set random seed\n",
    "random.seed(18)\n",
    "seed = 18\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Initialize lemmatizer, stop words, and stanza\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline('en') # initialize English neural pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data and quick exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and test sets loaded.\n"
     ]
    }
   ],
   "source": [
    "merged_social_df_batch_1 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/merged_Social_sentence_level_batch_1_jaccard.csv\", encoding='utf-8')\n",
    "merged_social_df_batch_2 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/Social Plus_sentence_level_batch_2_jaccard.csv\", encoding='utf-8')\n",
    "\n",
    "merged_social_df = pd.concat([merged_social_df_batch_1, merged_social_df_batch_2])\n",
    "\n",
    "\n",
    "seed = 18\n",
    "# Shuffle the merged dataset\n",
    "merged_social_df = shuffle(merged_social_df, random_state=seed)\n",
    "\n",
    "# Function for undersampling or oversampling\n",
    "def resample_data(X, y, strategy='oversample', random_state=seed):\n",
    "    \"\"\"\n",
    "    Resample the data using either undersampling or oversampling.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Features\n",
    "    - y: Labels\n",
    "    - strategy: 'oversample' or 'undersample'\n",
    "    - random_state: Seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    - X_resampled, y_resampled: Resampled data and labels\n",
    "    \"\"\"\n",
    "    if strategy == 'oversample':\n",
    "        sampler = RandomOverSampler(random_state=random_state)\n",
    "    elif strategy == 'undersample':\n",
    "        sampler = RandomUnderSampler(random_state=random_state)\n",
    "    else:\n",
    "        raise ValueError(\"Strategy must be 'oversample' or 'undersample'\")\n",
    "\n",
    "    X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Separate features and labels\n",
    "X = merged_social_df.drop(columns=['label'])  # Replace 'label' with your target column name\n",
    "y = merged_social_df['label']\n",
    "\n",
    "# Toggle resampling\n",
    "resample = True  # Set this to False to turn off resampling\n",
    "\n",
    "if resample:\n",
    "    # Apply resampling (choose 'oversample' or 'undersample')\n",
    "    X_resampled, y_resampled = resample_data(X, y, strategy='oversample', random_state=seed)\n",
    "\n",
    "    # Combine resampled data into a single DataFrame\n",
    "    resampled_df = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "else:\n",
    "    # No resampling, use original dataset\n",
    "    resampled_df = merged_social_df\n",
    "\n",
    "# Train-test split\n",
    "training_df, test_df = train_test_split(resampled_df, test_size=0.1, random_state=18, stratify=resampled_df['label'])\n",
    "\n",
    "\n",
    "# # Train-test split\n",
    "training_df, test_df = train_test_split(resampled_df, test_size=0.1, random_state=18, stratify=resampled_df['label'])\n",
    "\n",
    "training_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Training and test sets loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (5464, 3) \n",
      "Test dataset shape: (608, 3)\n",
      "Positive labels present in the dataset : 2732  out of 5464 or 50.0%\n",
      "Positive labels present in the test dataset : 304  out of 608 or 50.0%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset shape: {training_df.shape} \\nTest dataset shape: {test_df.shape}\")\n",
    "pos_labels = len([n for n in training_df['label'] if n==1])\n",
    "print(\"Positive labels present in the dataset : {}  out of {} or {}%\".format(pos_labels, len(training_df['label']), (pos_labels/len(training_df['label']))*100))\n",
    "pos_labels = len([n for n in test_df['label'] if n==1])\n",
    "print(\"Positive labels present in the test dataset : {}  out of {} or {}%\".format(pos_labels, len(test_df['label']), (pos_labels/len(test_df['label']))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the data for the plots\n",
    "training_counts = training_df['label'].value_counts()\n",
    "test_counts = test_df['label'].value_counts()\n",
    "\n",
    "# Set up the subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot for the training set\n",
    "sns.barplot(x=training_counts.index, y=training_counts.values, ax=axes[0])\n",
    "axes[0].set_title('Distribution of labels in training set')\n",
    "axes[0].set_ylabel('Sentences')\n",
    "axes[0].set_xlabel('Label')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot for the test set\n",
    "sns.barplot(x=test_counts.index, y=test_counts.values, ax=axes[1])\n",
    "axes[1].set_title('Distribution of labels in test set')\n",
    "axes[1].set_ylabel('Sentences')\n",
    "axes[1].set_xlabel('Label')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack Obama']\n"
     ]
    }
   ],
   "source": [
    "def get_ner(text):\n",
    "    ner_list = []\n",
    "    # Annotate the text using stanza\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for entity in sentence.ents:\n",
    "            if entity.type == 'PERSON':\n",
    "                ner_list.append(entity.text)\n",
    "\n",
    "    return ner_list\n",
    "\n",
    "# Example usage\n",
    "text = \"Barack Obama was the 44th doctor of the United States.\"\n",
    "print(get_ner(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a named entity is present in the sentence\n",
    "def named_entity_present(sentence):\n",
    "    ner_list = get_ner(sentence)\n",
    "    if len(ner_list) > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Similarity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the similar words and similarity score\n",
    "# The function takes tokens of sentence as input and if its not a stop word, get its similarity with synsets of STEM.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words |= set([\"help\",\"try\", \"work\", \"process\", \"support\", \"job\"] )\n",
    "def word_similarity(tokens, syns, field):    \n",
    "    if field in ['engineering', 'technology']:\n",
    "        score_threshold = 0.5\n",
    "    else:\n",
    "        score_threshold = 0.2\n",
    "    sim_words = 0\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            try:\n",
    "                syns_word = wordnet.synsets(token) \n",
    "                score = syns_word[0].path_similarity(syns[0])\n",
    "                if score >= score_threshold:\n",
    "                    sim_words += 1\n",
    "            except: \n",
    "                score = 0\n",
    "    \n",
    "    return sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to create columns for similarity based on all STEM fields\n",
    "syns_bio = wordnet.synsets(lemmatizer.lemmatize(\"biology\"))\n",
    "syns_maths = wordnet.synsets(lemmatizer.lemmatize(\"mathematics\")) \n",
    "syns_tech = wordnet.synsets(lemmatizer.lemmatize(\"technology\"))\n",
    "syns_eng = wordnet.synsets(lemmatizer.lemmatize(\"engineering\"))\n",
    "syns_chem = wordnet.synsets(lemmatizer.lemmatize(\"chemistry\"))\n",
    "syns_phy = wordnet.synsets(lemmatizer.lemmatize(\"physics\"))\n",
    "syns_sci = wordnet.synsets(lemmatizer.lemmatize(\"science\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Medical Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male', 'blood', 'microbiology', 'ophthalmology', 'and', 'neurology', 'chemical', 'family', 'preventive', 'pulmonology', 'cardiothoracic', 'dermatopathology', 'neurophysiology', 'brain', 'ocular', 'gynecology', 'chest', 'gastroenterology', 'military', 'injury', 'diagnostic', 'consultation', 'neurourology', 'metabolism', 'breast', 'head', 'failure', 'health', 'anatomical', 'rehabilitation', 'palliative', 'radiology', 'reconstructive', 'disease', 'interventional', 'disabilities', 'uveitis', 'pediatrics', 'neuroradiology', 'neonatal', 'sports', 'advanced', 'addiction', 'clinical', 'aerospace', 'neuro', 'diseases', 'endocrinology', 'surgery', 'critical', 'mental', 'adolescent', 'maternal', 'procedural', 'dermatology', 'endovascular', 'physical', 'urology', 'neuropathology', 'occupational', 'orbit', 'anesthesiology', 'endocrinology', 'research', 'pulmonary', 'ophthalmology', 'internal', 'transplant', 'hepatology', 'critical', 'electrophysiology', 'neurodevelopmental', 'cytopathology', 'nephrology', 'endocrinologists', 'liaison', 'glaucoma', 'obstetrics', 'gynecologic', 'neurology', 'pathology', 'cardiovascular', 'oncology', 'sleep', 'pain', 'cytogenetics', 'psychosomatic', 'renal', 'sports', 'genetic', 'fetal', 'plastic', 'psychiatric', 'behavioral', 'imaging', 'infertility', 'ophthalmic', 'allergy', 'hematology', 'pathology', 'radiation', 'anterior', 'immunopathology', 'rheumatology', 'gastrointestinal', 'hematology', 'toxicology', 'pediatric', 'pediatric', 'forensic', 'administrative', 'surgical', 'transfusion', 'anesthesiology', 'strabismus', 'infectious', 'oculoplastics', 'neck', 'genetics', 'neuromuscular', 'child', 'emergency', 'developmental', 'care', 'surgery', 'immunology', 'segment', 'diabetes', 'internal', 'adolescent', 'retina', 'vascular', 'infectious', 'cardiac', 'medicine', 'nuclear', 'cornea', 'abuse', 'urologic', 'calculi', 'cardiology', 'reproductive', 'heart', 'pelvic', 'genitourinary', 'geriatric', 'rheumatology', 'banking', 'biochemical', 'psychiatry', 'pediatrics', 'psychiatry', 'abdominal', 'female', 'transplant', 'nephrology', 'community', 'oncology', 'dermatology', 'gastroenterology', 'public', 'neuroradiology', 'interventional', 'perinatal', 'medical', 'musculoskeletal', 'retardation', 'urology', 'hospice', 'reconstructive', 'molecular', 'genetic']\n"
     ]
    }
   ],
   "source": [
    "# Load the medical specialization text file and create a list\n",
    "medical_list = []\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/medical_specialities.txt', 'r') as medical_fields:\n",
    "    for line in medical_fields.readlines():\n",
    "        special_field = line.rstrip('\\n')\n",
    "        special_field = re.sub(\"\\W\",\" \", special_field )\n",
    "#         print(special_field)\n",
    "        medical_list += special_field.split()\n",
    "medical_list = list(set(medical_list))  \n",
    "medical_list = [x.lower() for x in medical_list]\n",
    "print(medical_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get medical words\n",
    "def check_medical_words(tokens):\n",
    "    for token in tokens:\n",
    "        if token not in stop_words and token in [x.lower() for x in medical_list]:\n",
    "            return 1\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sentiment Polarity and Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get polarity and subjectivity of the sentence using TexBlob\n",
    "def get_sentiment(sentence):\n",
    "    sentiments =TextBlob(sentence).sentiment\n",
    "    polarity = sentiments.polarity\n",
    "    subjectivity = sentiments.subjectivity\n",
    "    return polarity, subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the count of POS tags of the sentence\n",
    "def count_pos_tags(tokens):\n",
    "    token_pos = pos_tag(tokens)\n",
    "    count = Counter(tag for word,tag in token_pos)\n",
    "    interjections =  count['UH']\n",
    "    nouns = count['NN'] + count['NNS'] + count['NNP'] + count['NNPS']\n",
    "    adverb = count['RB'] + count['RBS'] + count['RBR']\n",
    "    verb = count['VB'] + count['VBD'] + count['VBG'] + count['VBN']\n",
    "    determiner = count['DT']\n",
    "    pronoun = count['PRP']\n",
    "    adjetive = count['JJ'] + count['JJR'] + count['JJS']\n",
    "    preposition = count['IN']\n",
    "    return interjections, nouns, adverb, verb, determiner, pronoun, adjetive,preposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_extraction(dataframe, field, func, column_names):\n",
    "    return pd.concat((\n",
    "        dataframe,\n",
    "        dataframe[field].apply(\n",
    "            lambda cell: pd.Series(func(cell), index=column_names))), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the w2v dict from pickle file\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/pickle/embeddings06122024.pickle', 'rb') as w2v_file:\n",
    "    w2v_dict = pickle.load(w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of word embeddings:  4762\n"
     ]
    }
   ],
   "source": [
    "print(\"length of word embeddings: \", len(w2v_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vectors for the essay\n",
    "def vectorizer(sequence):\n",
    "    vect = []\n",
    "    numw = 0\n",
    "    for w in sequence: \n",
    "        try :\n",
    "            if numw == 0:\n",
    "                vect = w2v_dict[w]\n",
    "            else:\n",
    "                vect = np.add(vect, w2v_dict[w])\n",
    "            numw += 1\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    return vect/ numw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into words\n",
    "def split_into_words(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vectorizer\n",
    "unigram_vect = CountVectorizer(ngram_range=(1, 1), min_df=2, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Putting them all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for feature engineering\n",
    "def feature_engineering(original_dataset):\n",
    "\n",
    "    dataset = original_dataset.copy()\n",
    "    # create a new column with sentence tokens\n",
    "    dataset['tokens'] = dataset['sentence'].apply(word_tokenize)\n",
    "    # 1. Similarity features\n",
    "    # biology\n",
    "    dataset['bio_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_bio,'biology',)) \n",
    "    # chemistry\n",
    "    dataset['chem_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_chem,'chemistry',))\n",
    "    # physics\n",
    "    dataset['phy_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_phy,'physics',))\n",
    "    # mathematics\n",
    "    dataset['math_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_maths,'mathematics',))\n",
    "    # technology\n",
    "    dataset['tech_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_tech,'technology',))\n",
    "    # engineering\n",
    "    dataset['eng_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_eng,'engineering',))\n",
    "    \n",
    "    # medical terms\n",
    "    dataset['medical_terms'] = dataset['tokens'].apply(check_medical_words)\n",
    "    \n",
    "    # polarity and subjectivity\n",
    "    dataset['polarity'], dataset['subjectivity'] = zip(*dataset['sentence'].apply(get_sentiment))\n",
    "    \n",
    "    # named entity recognition\n",
    "    dataset['ner'] = dataset['sentence'].apply(named_entity_present)\n",
    "    \n",
    "    # pos tag count\n",
    "    dataset = pos_tag_extraction(dataset, 'tokens', count_pos_tags, ['interjections', 'nouns', 'adverb', 'verb', 'determiner', 'pronoun', 'adjetive','preposition'])\n",
    "    \n",
    "    # labels\n",
    "    data_labels = dataset['label']\n",
    "    # X\n",
    "    data_x = dataset.drop(columns='label')\n",
    "\n",
    "    \n",
    "    # vectorize all the essays\n",
    "    vect_arr = data_x.tokens.apply(vectorizer)\n",
    "    for index in range(0, len(vect_arr)):\n",
    "        i = 0\n",
    "        for item in vect_arr[index]:\n",
    "            column_name= \"embedding\" + str(i)\n",
    "            data_x.loc[index, column_name] = item\n",
    "            i +=1\n",
    "    \n",
    "    return data_x,data_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = feature_engineering(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(563, 121)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = feature_engineering(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63, 121)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Unigram features for both train and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(563, 121)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63, 121)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2.1/Social/saved_features/X_train_final.csv\", index=False)\n",
    "X_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2.1/Social/saved_features/X_test_final.csv\", index=False)\n",
    "y_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2.1/Social/saved_features/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2.1/Social/saved_features/y_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the unigram df for train :  (563, 592)\n"
     ]
    }
   ],
   "source": [
    "# Unigrams for training set\n",
    "unigram_matrix = unigram_vect.fit_transform(X_train['sentence'])\n",
    "unigrams = pd.DataFrame(unigram_matrix.toarray())\n",
    "print(\"Shape of the unigram df for train : \",unigrams.shape)\n",
    "unigrams = unigrams.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = pd.concat([X_train, unigrams], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final.columns = X_train_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(563, 713)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test unigram df shape :  (63, 592)\n"
     ]
    }
   ],
   "source": [
    "unigram_matrix_test = unigram_vect.transform(X_test['sentence'])\n",
    "unigrams_test = pd.DataFrame(unigram_matrix_test.toarray())\n",
    "unigrams_test = unigrams_test.reset_index(drop=True)\n",
    "print(\"Test unigram df shape : \",unigrams_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63, 713)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final = pd.concat([X_test, unigrams_test], axis = 1)\n",
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final.columns = X_test_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63, 713)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ---- sentence\n",
      "1 ---- phrase\n",
      "2 ---- tokens\n",
      "3 ---- bio_sim_words\n",
      "4 ---- chem_sim_words\n",
      "5 ---- phy_sim_words\n",
      "6 ---- math_sim_words\n",
      "7 ---- tech_sim_words\n",
      "8 ---- eng_sim_words\n",
      "9 ---- medical_terms\n",
      "10 ---- polarity\n",
      "11 ---- subjectivity\n",
      "12 ---- ner\n",
      "13 ---- interjections\n",
      "14 ---- nouns\n",
      "15 ---- adverb\n",
      "16 ---- verb\n",
      "17 ---- determiner\n",
      "18 ---- pronoun\n",
      "19 ---- adjetive\n",
      "20 ---- preposition\n",
      "21 ---- embedding0\n",
      "22 ---- embedding1\n",
      "23 ---- embedding2\n",
      "24 ---- embedding3\n",
      "25 ---- embedding4\n",
      "26 ---- embedding5\n",
      "27 ---- embedding6\n",
      "28 ---- embedding7\n",
      "29 ---- embedding8\n",
      "30 ---- embedding9\n",
      "31 ---- embedding10\n",
      "32 ---- embedding11\n",
      "33 ---- embedding12\n",
      "34 ---- embedding13\n",
      "35 ---- embedding14\n",
      "36 ---- embedding15\n",
      "37 ---- embedding16\n",
      "38 ---- embedding17\n",
      "39 ---- embedding18\n",
      "40 ---- embedding19\n",
      "41 ---- embedding20\n",
      "42 ---- embedding21\n",
      "43 ---- embedding22\n",
      "44 ---- embedding23\n",
      "45 ---- embedding24\n",
      "46 ---- embedding25\n",
      "47 ---- embedding26\n",
      "48 ---- embedding27\n",
      "49 ---- embedding28\n",
      "50 ---- embedding29\n",
      "51 ---- embedding30\n",
      "52 ---- embedding31\n",
      "53 ---- embedding32\n",
      "54 ---- embedding33\n",
      "55 ---- embedding34\n",
      "56 ---- embedding35\n",
      "57 ---- embedding36\n",
      "58 ---- embedding37\n",
      "59 ---- embedding38\n",
      "60 ---- embedding39\n",
      "61 ---- embedding40\n",
      "62 ---- embedding41\n",
      "63 ---- embedding42\n",
      "64 ---- embedding43\n",
      "65 ---- embedding44\n",
      "66 ---- embedding45\n",
      "67 ---- embedding46\n",
      "68 ---- embedding47\n",
      "69 ---- embedding48\n",
      "70 ---- embedding49\n",
      "71 ---- embedding50\n",
      "72 ---- embedding51\n",
      "73 ---- embedding52\n",
      "74 ---- embedding53\n",
      "75 ---- embedding54\n",
      "76 ---- embedding55\n",
      "77 ---- embedding56\n",
      "78 ---- embedding57\n",
      "79 ---- embedding58\n",
      "80 ---- embedding59\n",
      "81 ---- embedding60\n",
      "82 ---- embedding61\n",
      "83 ---- embedding62\n",
      "84 ---- embedding63\n",
      "85 ---- embedding64\n",
      "86 ---- embedding65\n",
      "87 ---- embedding66\n",
      "88 ---- embedding67\n",
      "89 ---- embedding68\n",
      "90 ---- embedding69\n",
      "91 ---- embedding70\n",
      "92 ---- embedding71\n",
      "93 ---- embedding72\n",
      "94 ---- embedding73\n",
      "95 ---- embedding74\n",
      "96 ---- embedding75\n",
      "97 ---- embedding76\n",
      "98 ---- embedding77\n",
      "99 ---- embedding78\n",
      "100 ---- embedding79\n",
      "101 ---- embedding80\n",
      "102 ---- embedding81\n",
      "103 ---- embedding82\n",
      "104 ---- embedding83\n",
      "105 ---- embedding84\n",
      "106 ---- embedding85\n",
      "107 ---- embedding86\n",
      "108 ---- embedding87\n",
      "109 ---- embedding88\n",
      "110 ---- embedding89\n",
      "111 ---- embedding90\n",
      "112 ---- embedding91\n",
      "113 ---- embedding92\n",
      "114 ---- embedding93\n",
      "115 ---- embedding94\n",
      "116 ---- embedding95\n",
      "117 ---- embedding96\n",
      "118 ---- embedding97\n",
      "119 ---- embedding98\n",
      "120 ---- embedding99\n",
      "121 ---- 0\n",
      "122 ---- 1\n",
      "123 ---- 2\n",
      "124 ---- 3\n",
      "125 ---- 4\n",
      "126 ---- 5\n",
      "127 ---- 6\n",
      "128 ---- 7\n",
      "129 ---- 8\n",
      "130 ---- 9\n",
      "131 ---- 10\n",
      "132 ---- 11\n",
      "133 ---- 12\n",
      "134 ---- 13\n",
      "135 ---- 14\n",
      "136 ---- 15\n",
      "137 ---- 16\n",
      "138 ---- 17\n",
      "139 ---- 18\n",
      "140 ---- 19\n",
      "141 ---- 20\n",
      "142 ---- 21\n",
      "143 ---- 22\n",
      "144 ---- 23\n",
      "145 ---- 24\n",
      "146 ---- 25\n",
      "147 ---- 26\n",
      "148 ---- 27\n",
      "149 ---- 28\n",
      "150 ---- 29\n",
      "151 ---- 30\n",
      "152 ---- 31\n",
      "153 ---- 32\n",
      "154 ---- 33\n",
      "155 ---- 34\n",
      "156 ---- 35\n",
      "157 ---- 36\n",
      "158 ---- 37\n",
      "159 ---- 38\n",
      "160 ---- 39\n",
      "161 ---- 40\n",
      "162 ---- 41\n",
      "163 ---- 42\n",
      "164 ---- 43\n",
      "165 ---- 44\n",
      "166 ---- 45\n",
      "167 ---- 46\n",
      "168 ---- 47\n",
      "169 ---- 48\n",
      "170 ---- 49\n",
      "171 ---- 50\n",
      "172 ---- 51\n",
      "173 ---- 52\n",
      "174 ---- 53\n",
      "175 ---- 54\n",
      "176 ---- 55\n",
      "177 ---- 56\n",
      "178 ---- 57\n",
      "179 ---- 58\n",
      "180 ---- 59\n",
      "181 ---- 60\n",
      "182 ---- 61\n",
      "183 ---- 62\n",
      "184 ---- 63\n",
      "185 ---- 64\n",
      "186 ---- 65\n",
      "187 ---- 66\n",
      "188 ---- 67\n",
      "189 ---- 68\n",
      "190 ---- 69\n",
      "191 ---- 70\n",
      "192 ---- 71\n",
      "193 ---- 72\n",
      "194 ---- 73\n",
      "195 ---- 74\n",
      "196 ---- 75\n",
      "197 ---- 76\n",
      "198 ---- 77\n",
      "199 ---- 78\n",
      "200 ---- 79\n",
      "201 ---- 80\n",
      "202 ---- 81\n",
      "203 ---- 82\n",
      "204 ---- 83\n",
      "205 ---- 84\n",
      "206 ---- 85\n",
      "207 ---- 86\n",
      "208 ---- 87\n",
      "209 ---- 88\n",
      "210 ---- 89\n",
      "211 ---- 90\n",
      "212 ---- 91\n",
      "213 ---- 92\n",
      "214 ---- 93\n",
      "215 ---- 94\n",
      "216 ---- 95\n",
      "217 ---- 96\n",
      "218 ---- 97\n",
      "219 ---- 98\n",
      "220 ---- 99\n",
      "221 ---- 100\n",
      "222 ---- 101\n",
      "223 ---- 102\n",
      "224 ---- 103\n",
      "225 ---- 104\n",
      "226 ---- 105\n",
      "227 ---- 106\n",
      "228 ---- 107\n",
      "229 ---- 108\n",
      "230 ---- 109\n",
      "231 ---- 110\n",
      "232 ---- 111\n",
      "233 ---- 112\n",
      "234 ---- 113\n",
      "235 ---- 114\n",
      "236 ---- 115\n",
      "237 ---- 116\n",
      "238 ---- 117\n",
      "239 ---- 118\n",
      "240 ---- 119\n",
      "241 ---- 120\n",
      "242 ---- 121\n",
      "243 ---- 122\n",
      "244 ---- 123\n",
      "245 ---- 124\n",
      "246 ---- 125\n",
      "247 ---- 126\n",
      "248 ---- 127\n",
      "249 ---- 128\n",
      "250 ---- 129\n",
      "251 ---- 130\n",
      "252 ---- 131\n",
      "253 ---- 132\n",
      "254 ---- 133\n",
      "255 ---- 134\n",
      "256 ---- 135\n",
      "257 ---- 136\n",
      "258 ---- 137\n",
      "259 ---- 138\n",
      "260 ---- 139\n",
      "261 ---- 140\n",
      "262 ---- 141\n",
      "263 ---- 142\n",
      "264 ---- 143\n",
      "265 ---- 144\n",
      "266 ---- 145\n",
      "267 ---- 146\n",
      "268 ---- 147\n",
      "269 ---- 148\n",
      "270 ---- 149\n",
      "271 ---- 150\n",
      "272 ---- 151\n",
      "273 ---- 152\n",
      "274 ---- 153\n",
      "275 ---- 154\n",
      "276 ---- 155\n",
      "277 ---- 156\n",
      "278 ---- 157\n",
      "279 ---- 158\n",
      "280 ---- 159\n",
      "281 ---- 160\n",
      "282 ---- 161\n",
      "283 ---- 162\n",
      "284 ---- 163\n",
      "285 ---- 164\n",
      "286 ---- 165\n",
      "287 ---- 166\n",
      "288 ---- 167\n",
      "289 ---- 168\n",
      "290 ---- 169\n",
      "291 ---- 170\n",
      "292 ---- 171\n",
      "293 ---- 172\n",
      "294 ---- 173\n",
      "295 ---- 174\n",
      "296 ---- 175\n",
      "297 ---- 176\n",
      "298 ---- 177\n",
      "299 ---- 178\n",
      "300 ---- 179\n",
      "301 ---- 180\n",
      "302 ---- 181\n",
      "303 ---- 182\n",
      "304 ---- 183\n",
      "305 ---- 184\n",
      "306 ---- 185\n",
      "307 ---- 186\n",
      "308 ---- 187\n",
      "309 ---- 188\n",
      "310 ---- 189\n",
      "311 ---- 190\n",
      "312 ---- 191\n",
      "313 ---- 192\n",
      "314 ---- 193\n",
      "315 ---- 194\n",
      "316 ---- 195\n",
      "317 ---- 196\n",
      "318 ---- 197\n",
      "319 ---- 198\n",
      "320 ---- 199\n",
      "321 ---- 200\n",
      "322 ---- 201\n",
      "323 ---- 202\n",
      "324 ---- 203\n",
      "325 ---- 204\n",
      "326 ---- 205\n",
      "327 ---- 206\n",
      "328 ---- 207\n",
      "329 ---- 208\n",
      "330 ---- 209\n",
      "331 ---- 210\n",
      "332 ---- 211\n",
      "333 ---- 212\n",
      "334 ---- 213\n",
      "335 ---- 214\n",
      "336 ---- 215\n",
      "337 ---- 216\n",
      "338 ---- 217\n",
      "339 ---- 218\n",
      "340 ---- 219\n",
      "341 ---- 220\n",
      "342 ---- 221\n",
      "343 ---- 222\n",
      "344 ---- 223\n",
      "345 ---- 224\n",
      "346 ---- 225\n",
      "347 ---- 226\n",
      "348 ---- 227\n",
      "349 ---- 228\n",
      "350 ---- 229\n",
      "351 ---- 230\n",
      "352 ---- 231\n",
      "353 ---- 232\n",
      "354 ---- 233\n",
      "355 ---- 234\n",
      "356 ---- 235\n",
      "357 ---- 236\n",
      "358 ---- 237\n",
      "359 ---- 238\n",
      "360 ---- 239\n",
      "361 ---- 240\n",
      "362 ---- 241\n",
      "363 ---- 242\n",
      "364 ---- 243\n",
      "365 ---- 244\n",
      "366 ---- 245\n",
      "367 ---- 246\n",
      "368 ---- 247\n",
      "369 ---- 248\n",
      "370 ---- 249\n",
      "371 ---- 250\n",
      "372 ---- 251\n",
      "373 ---- 252\n",
      "374 ---- 253\n",
      "375 ---- 254\n",
      "376 ---- 255\n",
      "377 ---- 256\n",
      "378 ---- 257\n",
      "379 ---- 258\n",
      "380 ---- 259\n",
      "381 ---- 260\n",
      "382 ---- 261\n",
      "383 ---- 262\n",
      "384 ---- 263\n",
      "385 ---- 264\n",
      "386 ---- 265\n",
      "387 ---- 266\n",
      "388 ---- 267\n",
      "389 ---- 268\n",
      "390 ---- 269\n",
      "391 ---- 270\n",
      "392 ---- 271\n",
      "393 ---- 272\n",
      "394 ---- 273\n",
      "395 ---- 274\n",
      "396 ---- 275\n",
      "397 ---- 276\n",
      "398 ---- 277\n",
      "399 ---- 278\n",
      "400 ---- 279\n",
      "401 ---- 280\n",
      "402 ---- 281\n",
      "403 ---- 282\n",
      "404 ---- 283\n",
      "405 ---- 284\n",
      "406 ---- 285\n",
      "407 ---- 286\n",
      "408 ---- 287\n",
      "409 ---- 288\n",
      "410 ---- 289\n",
      "411 ---- 290\n",
      "412 ---- 291\n",
      "413 ---- 292\n",
      "414 ---- 293\n",
      "415 ---- 294\n",
      "416 ---- 295\n",
      "417 ---- 296\n",
      "418 ---- 297\n",
      "419 ---- 298\n",
      "420 ---- 299\n",
      "421 ---- 300\n",
      "422 ---- 301\n",
      "423 ---- 302\n",
      "424 ---- 303\n",
      "425 ---- 304\n",
      "426 ---- 305\n",
      "427 ---- 306\n",
      "428 ---- 307\n",
      "429 ---- 308\n",
      "430 ---- 309\n",
      "431 ---- 310\n",
      "432 ---- 311\n",
      "433 ---- 312\n",
      "434 ---- 313\n",
      "435 ---- 314\n",
      "436 ---- 315\n",
      "437 ---- 316\n",
      "438 ---- 317\n",
      "439 ---- 318\n",
      "440 ---- 319\n",
      "441 ---- 320\n",
      "442 ---- 321\n",
      "443 ---- 322\n",
      "444 ---- 323\n",
      "445 ---- 324\n",
      "446 ---- 325\n",
      "447 ---- 326\n",
      "448 ---- 327\n",
      "449 ---- 328\n",
      "450 ---- 329\n",
      "451 ---- 330\n",
      "452 ---- 331\n",
      "453 ---- 332\n",
      "454 ---- 333\n",
      "455 ---- 334\n",
      "456 ---- 335\n",
      "457 ---- 336\n",
      "458 ---- 337\n",
      "459 ---- 338\n",
      "460 ---- 339\n",
      "461 ---- 340\n",
      "462 ---- 341\n",
      "463 ---- 342\n",
      "464 ---- 343\n",
      "465 ---- 344\n",
      "466 ---- 345\n",
      "467 ---- 346\n",
      "468 ---- 347\n",
      "469 ---- 348\n",
      "470 ---- 349\n",
      "471 ---- 350\n",
      "472 ---- 351\n",
      "473 ---- 352\n",
      "474 ---- 353\n",
      "475 ---- 354\n",
      "476 ---- 355\n",
      "477 ---- 356\n",
      "478 ---- 357\n",
      "479 ---- 358\n",
      "480 ---- 359\n",
      "481 ---- 360\n",
      "482 ---- 361\n",
      "483 ---- 362\n",
      "484 ---- 363\n",
      "485 ---- 364\n",
      "486 ---- 365\n",
      "487 ---- 366\n",
      "488 ---- 367\n",
      "489 ---- 368\n",
      "490 ---- 369\n",
      "491 ---- 370\n",
      "492 ---- 371\n",
      "493 ---- 372\n",
      "494 ---- 373\n",
      "495 ---- 374\n",
      "496 ---- 375\n",
      "497 ---- 376\n",
      "498 ---- 377\n",
      "499 ---- 378\n",
      "500 ---- 379\n",
      "501 ---- 380\n",
      "502 ---- 381\n",
      "503 ---- 382\n",
      "504 ---- 383\n",
      "505 ---- 384\n",
      "506 ---- 385\n",
      "507 ---- 386\n",
      "508 ---- 387\n",
      "509 ---- 388\n",
      "510 ---- 389\n",
      "511 ---- 390\n",
      "512 ---- 391\n",
      "513 ---- 392\n",
      "514 ---- 393\n",
      "515 ---- 394\n",
      "516 ---- 395\n",
      "517 ---- 396\n",
      "518 ---- 397\n",
      "519 ---- 398\n",
      "520 ---- 399\n",
      "521 ---- 400\n",
      "522 ---- 401\n",
      "523 ---- 402\n",
      "524 ---- 403\n",
      "525 ---- 404\n",
      "526 ---- 405\n",
      "527 ---- 406\n",
      "528 ---- 407\n",
      "529 ---- 408\n",
      "530 ---- 409\n",
      "531 ---- 410\n",
      "532 ---- 411\n",
      "533 ---- 412\n",
      "534 ---- 413\n",
      "535 ---- 414\n",
      "536 ---- 415\n",
      "537 ---- 416\n",
      "538 ---- 417\n",
      "539 ---- 418\n",
      "540 ---- 419\n",
      "541 ---- 420\n",
      "542 ---- 421\n",
      "543 ---- 422\n",
      "544 ---- 423\n",
      "545 ---- 424\n",
      "546 ---- 425\n",
      "547 ---- 426\n",
      "548 ---- 427\n",
      "549 ---- 428\n",
      "550 ---- 429\n",
      "551 ---- 430\n",
      "552 ---- 431\n",
      "553 ---- 432\n",
      "554 ---- 433\n",
      "555 ---- 434\n",
      "556 ---- 435\n",
      "557 ---- 436\n",
      "558 ---- 437\n",
      "559 ---- 438\n",
      "560 ---- 439\n",
      "561 ---- 440\n",
      "562 ---- 441\n",
      "563 ---- 442\n",
      "564 ---- 443\n",
      "565 ---- 444\n",
      "566 ---- 445\n",
      "567 ---- 446\n",
      "568 ---- 447\n",
      "569 ---- 448\n",
      "570 ---- 449\n",
      "571 ---- 450\n",
      "572 ---- 451\n",
      "573 ---- 452\n",
      "574 ---- 453\n",
      "575 ---- 454\n",
      "576 ---- 455\n",
      "577 ---- 456\n",
      "578 ---- 457\n",
      "579 ---- 458\n",
      "580 ---- 459\n",
      "581 ---- 460\n",
      "582 ---- 461\n",
      "583 ---- 462\n",
      "584 ---- 463\n",
      "585 ---- 464\n",
      "586 ---- 465\n",
      "587 ---- 466\n",
      "588 ---- 467\n",
      "589 ---- 468\n",
      "590 ---- 469\n",
      "591 ---- 470\n",
      "592 ---- 471\n",
      "593 ---- 472\n",
      "594 ---- 473\n",
      "595 ---- 474\n",
      "596 ---- 475\n",
      "597 ---- 476\n",
      "598 ---- 477\n",
      "599 ---- 478\n",
      "600 ---- 479\n",
      "601 ---- 480\n",
      "602 ---- 481\n",
      "603 ---- 482\n",
      "604 ---- 483\n",
      "605 ---- 484\n",
      "606 ---- 485\n",
      "607 ---- 486\n",
      "608 ---- 487\n",
      "609 ---- 488\n",
      "610 ---- 489\n",
      "611 ---- 490\n",
      "612 ---- 491\n",
      "613 ---- 492\n",
      "614 ---- 493\n",
      "615 ---- 494\n",
      "616 ---- 495\n",
      "617 ---- 496\n",
      "618 ---- 497\n",
      "619 ---- 498\n",
      "620 ---- 499\n",
      "621 ---- 500\n",
      "622 ---- 501\n",
      "623 ---- 502\n",
      "624 ---- 503\n",
      "625 ---- 504\n",
      "626 ---- 505\n",
      "627 ---- 506\n",
      "628 ---- 507\n",
      "629 ---- 508\n",
      "630 ---- 509\n",
      "631 ---- 510\n",
      "632 ---- 511\n",
      "633 ---- 512\n",
      "634 ---- 513\n",
      "635 ---- 514\n",
      "636 ---- 515\n",
      "637 ---- 516\n",
      "638 ---- 517\n",
      "639 ---- 518\n",
      "640 ---- 519\n",
      "641 ---- 520\n",
      "642 ---- 521\n",
      "643 ---- 522\n",
      "644 ---- 523\n",
      "645 ---- 524\n",
      "646 ---- 525\n",
      "647 ---- 526\n",
      "648 ---- 527\n",
      "649 ---- 528\n",
      "650 ---- 529\n",
      "651 ---- 530\n",
      "652 ---- 531\n",
      "653 ---- 532\n",
      "654 ---- 533\n",
      "655 ---- 534\n",
      "656 ---- 535\n",
      "657 ---- 536\n",
      "658 ---- 537\n",
      "659 ---- 538\n",
      "660 ---- 539\n",
      "661 ---- 540\n",
      "662 ---- 541\n",
      "663 ---- 542\n",
      "664 ---- 543\n",
      "665 ---- 544\n",
      "666 ---- 545\n",
      "667 ---- 546\n",
      "668 ---- 547\n",
      "669 ---- 548\n",
      "670 ---- 549\n",
      "671 ---- 550\n",
      "672 ---- 551\n",
      "673 ---- 552\n",
      "674 ---- 553\n",
      "675 ---- 554\n",
      "676 ---- 555\n",
      "677 ---- 556\n",
      "678 ---- 557\n",
      "679 ---- 558\n",
      "680 ---- 559\n",
      "681 ---- 560\n",
      "682 ---- 561\n",
      "683 ---- 562\n",
      "684 ---- 563\n",
      "685 ---- 564\n",
      "686 ---- 565\n",
      "687 ---- 566\n",
      "688 ---- 567\n",
      "689 ---- 568\n",
      "690 ---- 569\n",
      "691 ---- 570\n",
      "692 ---- 571\n",
      "693 ---- 572\n",
      "694 ---- 573\n",
      "695 ---- 574\n",
      "696 ---- 575\n",
      "697 ---- 576\n",
      "698 ---- 577\n",
      "699 ---- 578\n",
      "700 ---- 579\n",
      "701 ---- 580\n",
      "702 ---- 581\n",
      "703 ---- 582\n",
      "704 ---- 583\n",
      "705 ---- 584\n",
      "706 ---- 585\n",
      "707 ---- 586\n",
      "708 ---- 587\n",
      "709 ---- 588\n",
      "710 ---- 589\n",
      "711 ---- 590\n",
      "712 ---- 591\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(X_train_final.columns)):\n",
    "    print('{} ---- {}'.format(i, X_train_final.columns[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Model 1: Unigrams, POS Tag Count, Sentiment Polarity, Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_1 = X_train_final.iloc[:,np.r_[10:12,13:21,121:712]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(563, 601)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_1 = X_test_final.iloc[:,np.r_[10:12,13:21,121:712]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63, 601)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.806\n",
      "Best parameters set:\n",
      "\tclf__C: 1\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6765    0.7188    0.6970        32\n",
      "           1     0.6897    0.6452    0.6667        31\n",
      "\n",
      "    accuracy                         0.6825        63\n",
      "   macro avg     0.6831    0.6820    0.6818        63\n",
      "weighted avg     0.6830    0.6825    0.6821        63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_1_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_1_pipeline, parameters, scoring=\"average_precision\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_1,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_1), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 23, False Positive: 9, False Negative: 11, True Positive: 20\n",
      "--------------------------------------------------------------------------------\n",
      "[[23  9]\n",
      " [11 20]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.72      0.70        32\n",
      "           1       0.69      0.65      0.67        31\n",
      "\n",
      "    accuracy                           0.68        63\n",
      "   macro avg       0.68      0.68      0.68        63\n",
      "weighted avg       0.68      0.68      0.68        63\n",
      "\n",
      "Average Precision: 0.6195\n"
     ]
    }
   ],
   "source": [
    "lr_model_1 = LogisticRegression(random_state=18, \n",
    "                                solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], \n",
    "                                class_weight='balanced').fit(X_train_model_1, y_train)\n",
    "y_lr = lr_model_1.predict(X_test_model_1)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))\n",
    "\n",
    "# Calculate and print the average precision score\n",
    "avg_precision = average_precision_score(y_test, y_lr)\n",
    "print(f'Average Precision: {avg_precision:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF Model 1: Unigrams, POS Tag Count, Sentiment Polarity, Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [10,20,50,100,200,300]\n",
    "max_depth = [2,5,8,10,15,20]\n",
    "min_samples_split = [2, 5, 10, 15, 20]\n",
    "min_samples_leaf = [1, 2, 5, 10,20]\n",
    "rf_parameters = dict(n_estimators = n_estimators, max_depth = max_depth,  \n",
    "              min_samples_split = min_samples_split, \n",
    "              min_samples_leaf = min_samples_leaf)\n",
    "\n",
    "## reduced parameters\n",
    "# n_estimators = [50, 100, 200]       # Reduced options\n",
    "# max_depth = [5, 10, 15]             # Reduced options\n",
    "# min_samples_split = [2, 10]         # Reduced options\n",
    "# min_samples_leaf = [1, 5]           # Reduced options\n",
    "\n",
    "# rf_parameters = dict(\n",
    "#     n_estimators = n_estimators, \n",
    "#     max_depth = max_depth,  \n",
    "#     min_samples_split = min_samples_split, \n",
    "#     min_samples_leaf = min_samples_leaf\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_1 = X_train_final.iloc[:,np.r_[10:12,13:21,121:712]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(563, 601)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_1 = X_test_final.iloc[:,np.r_[10:12,13:21,121:712]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 900 candidates, totalling 9000 fits\n",
      "Best score: 0.828\n",
      "Best parameters set:\n",
      "\tmax_depth: 15\n",
      "\tmin_samples_leaf: 1\n",
      "\tmin_samples_split: 20\n",
      "\tn_estimators: 200\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.72      0.73        32\n",
      "           1       0.72      0.74      0.73        31\n",
      "\n",
      "    accuracy                           0.73        63\n",
      "   macro avg       0.73      0.73      0.73        63\n",
      "weighted avg       0.73      0.73      0.73        63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_model_1 = RandomForestClassifier(random_state=18,class_weight='balanced')\n",
    "grid_search = GridSearchCV(rf_model_1, rf_parameters, scoring=\"average_precision\", cv = 10, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_model_1,y_train)\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(rf_parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_1), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 23, False Positive: 9, False Negative: 8, True Positive: 23\n",
      "--------------------------------------------------------------------------------\n",
      "[[23  9]\n",
      " [ 8 23]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.72      0.73        32\n",
      "           1       0.72      0.74      0.73        31\n",
      "\n",
      "    accuracy                           0.73        63\n",
      "   macro avg       0.73      0.73      0.73        63\n",
      "weighted avg       0.73      0.73      0.73        63\n",
      "\n",
      "Average Precision: 0.6603\n"
     ]
    }
   ],
   "source": [
    "randomForest_1 = RandomForestClassifier(random_state=18,\n",
    "                                        class_weight=best_parameters['class_weight'],\n",
    "                                        max_depth=best_parameters['max_depth'],\n",
    "                                        min_samples_leaf=best_parameters['min_samples_leaf'],\n",
    "                                        min_samples_split=best_parameters['min_samples_split'],\n",
    "                                        n_estimators=best_parameters['n_estimators']).fit(X_train_model_1, y_train)\n",
    "\n",
    "y_lr = randomForest_1.predict(X_test_model_1)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))\n",
    "\n",
    "# Calculate and print the average precision score\n",
    "avg_precision = average_precision_score(y_test, y_lr)\n",
    "print(f'Average Precision: {avg_precision:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
