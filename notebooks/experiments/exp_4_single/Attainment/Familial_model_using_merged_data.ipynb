{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Familial Model Using Single Batch 1 + Batch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ktrain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from ktrain import text\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Set random seed\n",
    "random.seed(18)\n",
    "seed = 18\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data and quick exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Aspirational_sentence_level_batch_1_jaccard.csv\n",
      "Loaded Attainment_sentence_level_batch_1_jaccard.csv\n",
      "Loaded Community Consciousness_sentence_level_batch_1_jaccard.csv\n",
      "Loaded Familial_sentence_level_batch_1_jaccard.csv\n",
      "Loaded Filial Piety_sentence_level_batch_1_jaccard.csv\n",
      "Loaded First Gen_sentence_level_batch_1_jaccard.csv\n",
      "Loaded Navigational_sentence_level_batch_1_jaccard.csv\n",
      "Loaded Perseverance_sentence_level_batch_1_jaccard.csv\n",
      "Loaded Resistance_sentence_level_batch_1_jaccard.csv\n",
      "Loaded Social_sentence_level_batch_1_jaccard.csv\n",
      "Loaded Spiritual_sentence_level_batch_1_jaccard.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the folder path and themes\n",
    "folder_path = '/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/single_theme_using_jaccard_method'\n",
    "themes = [\n",
    "    'Aspirational', 'Attainment', 'Community Consciousness', 'Familial', 'Filial Piety', \n",
    "    'First Gen', 'Navigational', 'Perseverance', 'Resistance', 'Social', 'Spiritual'\n",
    "]\n",
    "\n",
    "# Initialize an empty dictionary to store DataFrames\n",
    "theme_dataframes = {}\n",
    "# Loop through each theme and load its corresponding file\n",
    "for theme in themes:\n",
    "    # Construct the filename without modifying the theme name\n",
    "    file_name = f\"{theme}_sentence_level_batch_1_jaccard.csv\"\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Check if the file exists before attempting to load\n",
    "    if os.path.exists(file_path):\n",
    "        theme_dataframes[theme] = pd.read_csv(file_path)\n",
    "        print(f\"Loaded {file_name}\")\n",
    "    else:\n",
    "        print(f\"File not found for theme: {theme}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1299, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "familial_df = theme_dataframes[\"Familial\"]\n",
    "filial_piety_df = theme_dataframes[\"Filial Piety\"]\n",
    "\n",
    "familial_df = pd.concat([familial_df, filial_piety_df])\n",
    "\n",
    "familial_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# familial_df = theme_dataframes[\"Familial\"]\n",
    "\n",
    "# # Train-test split \n",
    "training_df, test_df = train_test_split(familial_df, test_size=0.2, random_state=42, stratify=familial_df['label'])\n",
    "\n",
    "training_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_familial_df_batch_1 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/merged_Familial_sentence_level_batch_1_jaccard.csv\", encoding='utf-8')\n",
    "merged_familial_df_batch_2 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/Familial Plus_sentence_level_batch_2_jaccard.csv\", encoding='utf-8')\n",
    "\n",
    "merged_familial_df = pd.concat([merged_familial_df_batch_1, merged_familial_df_batch_2])\n",
    "\n",
    "# Shuffle the merged dataset\n",
    "merged_familial_df = shuffle(merged_familial_df, random_state=seed)\n",
    "\n",
    "# Train-test split \n",
    "training_df, test_df = train_test_split(merged_familial_df, test_size=0.3, random_state=42, stratify=merged_familial_df['label'])\n",
    "\n",
    "training_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (1039, 3) \n",
      "Test dataset shape: (260, 3)\n",
      "Positive labels present in the dataset : 109  out of 1039 or 10.490856592877767%\n",
      "Positive labels present in the test dataset : 27  out of 260 or 10.384615384615385%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset shape: {training_df.shape} \\nTest dataset shape: {test_df.shape}\")\n",
    "pos_labels = len([n for n in training_df['label'] if n==1])\n",
    "print(\"Positive labels present in the dataset : {}  out of {} or {}%\".format(pos_labels, len(training_df['label']), (pos_labels/len(training_df['label']))*100))\n",
    "pos_labels = len([n for n in test_df['label'] if n==1])\n",
    "print(\"Positive labels present in the test dataset : {}  out of {} or {}%\".format(pos_labels, len(test_df['label']), (pos_labels/len(test_df['label']))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1039, 3)\n",
      "(260, 3)\n"
     ]
    }
   ],
   "source": [
    "print(training_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experimental Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_df['sentence']\n",
    "y = training_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 18, stratify=y)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing train...\n",
      "language: en\n",
      "train sequence lengths:\n",
      "\tmean : 20\n",
      "\t95percentile : 41\n",
      "\t99percentile : 58\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 20\n",
      "\t95percentile : 40\n",
      "\t99percentile : 53\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distillbert_transformer = text.Transformer(model_name, maxlen=MAXLEN, class_names=[0,1])\n",
    "training_set = distillbert_transformer.preprocess_train(X_train.tolist(), y_train.tolist())\n",
    "validation_set = distillbert_transformer.preprocess_test(X_test.tolist(), y_test.tolist())\n",
    "distillbert_base_model = distillbert_transformer.get_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55860215 4.76605505]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Define classes and class labels\n",
    "classes = np.array([0, 1])\n",
    "class_labels = list(training_df.label)\n",
    "\n",
    "# Compute class weights\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=class_labels)\n",
    "\n",
    "# Print class weights\n",
    "print(class_weights)\n",
    "\n",
    "class_weights = dict(zip(classes, class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using triangular learning rate policy with max lr of 4.96e-05...\n",
      "Epoch 1/11\n",
      "156/156 [==============================] - 86s 429ms/step - loss: 0.6576 - accuracy: 0.6920 - val_loss: 0.4729 - val_accuracy: 0.9038\n",
      "Epoch 2/11\n",
      "156/156 [==============================] - 54s 340ms/step - loss: 0.3988 - accuracy: 0.8107 - val_loss: 0.2343 - val_accuracy: 0.8942\n",
      "Epoch 3/11\n",
      "156/156 [==============================] - 51s 321ms/step - loss: 0.3638 - accuracy: 0.8086 - val_loss: 0.3050 - val_accuracy: 0.8846\n",
      "Epoch 4/11\n",
      "156/156 [==============================] - 52s 327ms/step - loss: 0.3191 - accuracy: 0.8374 - val_loss: 0.2263 - val_accuracy: 0.8942\n",
      "Epoch 5/11\n",
      "156/156 [==============================] - 52s 332ms/step - loss: 0.3149 - accuracy: 0.8588 - val_loss: 0.2656 - val_accuracy: 0.8942\n",
      "Epoch 6/11\n",
      "156/156 [==============================] - 53s 333ms/step - loss: 0.2882 - accuracy: 0.8642 - val_loss: 0.3246 - val_accuracy: 0.8750\n",
      "Epoch 7/11\n",
      "156/156 [==============================] - 53s 335ms/step - loss: 0.2897 - accuracy: 0.8727 - val_loss: 0.3082 - val_accuracy: 0.8750\n",
      "Epoch 8/11\n",
      "156/156 [==============================] - 51s 321ms/step - loss: 0.2428 - accuracy: 0.8620 - val_loss: 0.2194 - val_accuracy: 0.9231\n",
      "Epoch 9/11\n",
      "156/156 [==============================] - 52s 327ms/step - loss: 0.2085 - accuracy: 0.9016 - val_loss: 0.2605 - val_accuracy: 0.9135\n",
      "Epoch 10/11\n",
      "156/156 [==============================] - 52s 328ms/step - loss: 0.2062 - accuracy: 0.9016 - val_loss: 0.3483 - val_accuracy: 0.8846\n",
      "Epoch 11/11\n",
      "156/156 [==============================] - 53s 337ms/step - loss: 0.1771 - accuracy: 0.9198 - val_loss: 0.3452 - val_accuracy: 0.8942\n",
      "Weights from best epoch have been loaded into model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3059b8f10>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build BERT model\n",
    "# model = text.text_classifier('distilbert', train_data=(X_train, y_train), preproc=distillbert_transformer)\n",
    "distillbert_learner = ktrain.get_learner(distillbert_base_model, train_data=training_set, val_data=validation_set, batch_size=6)\n",
    "# learner.fit_onecycle(2e-5, 4, class_weight=class_weights)\n",
    "# learner.autofit(2.27E-06, early_stopping=4)\n",
    "distillbert_learner.set_weight_decay(0.001)\n",
    "distillbert_learner.autofit(4.96E-05, epochs=11, early_stopping=4, class_weight=class_weights)\n",
    "# distillbert_learner.set_weight_decay(0.001)\n",
    "# distillbert_learner.autofit(2.27E-06, early_stopping=4, class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 10s 1s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94        93\n",
      "           1       0.50      0.73      0.59        11\n",
      "\n",
      "    accuracy                           0.89       104\n",
      "   macro avg       0.73      0.82      0.77       104\n",
      "weighted avg       0.92      0.89      0.90       104\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[85,  8],\n",
       "       [ 3,  8]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distillbert_learner.validate(class_names=distillbert_transformer.get_classes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_151 (Dropout)       multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109483778 (417.65 MB)\n",
      "Trainable params: 109483778 (417.65 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "distillbert_learner.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "distillbert_predictor = ktrain.get_predictor(distillbert_learner.model, preproc=distillbert_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "distillbert_test_data = test_df['sentence'].tolist()\n",
    "distillbert_test_label = test_df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_distillbert = distillbert_predictor.predict(distillbert_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_distillbert = [int(x) for x in y_pred_distillbert]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negative: 203, False Positive: 30, False Negative: 7, True Positive: 20\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(distillbert_test_label, y_pred_distillbert).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.87      0.92       233\n",
      "           1       0.40      0.74      0.52        27\n",
      "\n",
      "    accuracy                           0.86       260\n",
      "   macro avg       0.68      0.81      0.72       260\n",
      "weighted avg       0.91      0.86      0.88       260\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('  Classification Report:\\n',classification_report(distillbert_test_label,y_pred_distillbert),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distillbert_predictor.save('../../model/first_generation_distilbert_base_uncased_model_10102020') # 256 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC roc score for distillbert model:  0.6427102188579892\n"
     ]
    }
   ],
   "source": [
    "print(\"AUC roc score for distillbert model: \", roc_auc_score(distillbert_test_label,y_pred_distillbert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
