{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aspirational Logistic Regression Models Using Merged Data Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4250cdd4763842d5abda83653f02bbb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 00:56:55 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-10-02 00:56:55 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-10-02 00:56:56 INFO: File exists: /Users/gbaldonado/stanza_resources/en/default.zip\n",
      "2024-10-02 00:56:59 INFO: Finished downloading models and saved to /Users/gbaldonado/stanza_resources\n",
      "2024-10-02 00:56:59 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb120c29f2a4e27a6be319886bcb129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 00:56:59 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-10-02 00:57:01 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-10-02 00:57:01 INFO: Using device: cpu\n",
      "2024-10-02 00:57:01 INFO: Loading: tokenize\n",
      "2024-10-02 00:57:02 INFO: Loading: mwt\n",
      "2024-10-02 00:57:02 INFO: Loading: pos\n",
      "2024-10-02 00:57:02 INFO: Loading: lemma\n",
      "2024-10-02 00:57:02 INFO: Loading: constituency\n",
      "2024-10-02 00:57:02 INFO: Loading: depparse\n",
      "2024-10-02 00:57:03 INFO: Loading: sentiment\n",
      "2024-10-02 00:57:03 INFO: Loading: ner\n",
      "2024-10-02 00:57:04 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import pickle\n",
    "import warnings\n",
    "import stanza\n",
    "\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, f1_score, r2_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Set random seed\n",
    "random.seed(18)\n",
    "seed = 18\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Initialize lemmatizer, stop words, and stanza\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline('en') # initialize English neural pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data and quick exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_aspirational_df_batch_1 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/merged_Aspirational_sentence_level_batch_1_jaccard.csv\", encoding='utf-8')\n",
    "merged_aspirational_df_batch_2 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/Aspirational Plus_sentence_level_batch_2_jaccard.csv\", encoding='utf-8')\n",
    "\n",
    "merged_aspirational_df = pd.concat([merged_aspirational_df_batch_1, merged_aspirational_df_batch_2])\n",
    "\n",
    "# Shuffle the merged dataset\n",
    "merged_aspirational_df = shuffle(merged_aspirational_df, random_state=seed)\n",
    "\n",
    "# Train-test split \n",
    "training_df, test_df = train_test_split(merged_aspirational_df, test_size=0.1, random_state=42, stratify=merged_aspirational_df['label'])\n",
    "\n",
    "training_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am excited to see what this virtual lab will bring, even though i know it will be a little more challenging to get the same experience as face to face classes.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I am here to receive my education.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cristian garcia 02062020lab 1why am i here?the reason i am taking physics 220 and physics 222 is because i want to advance in my education.</td>\n",
       "      <td>0</td>\n",
       "      <td>['The reason I am taking physics 220 and physics 222 is because I want to advance in my education. I want to receive my bachelor degree in Civil Engineering but in order to do that I have to pass my classes.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i want to set an example for my siblings, i want them to do better than what i am doing, i want them to achieve way more than what i had.</td>\n",
       "      <td>0</td>\n",
       "      <td>['The reason why I am here is that I want to be someone in this life. My goal is to graduate from this institution with a degree. I am here because I want to continue learning and I want to become a pediatrician and work with little children. I am here for a reason and I am here to figure it out.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i want to show the world my own ideas and make it into a film.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I want to get a job in the film industry. Im not really sure what part of it though. I was thinking a director, editor, or producer.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my dream goal is to have an animal sanctuary or rehabilitation center of my own.</td>\n",
       "      <td>0</td>\n",
       "      <td>['The reason for why I am at San Francisco State University is to reach my goal of working in a wildlife rehabilitation or conservation center. These shows further made me realize that I wanted to pursue a career that involved helping animals of any size. My dream goal is to have an animal sanctuary or rehabilitation center of my own.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                            sentence  \\\n",
       "0  i am excited to see what this virtual lab will bring, even though i know it will be a little more challenging to get the same experience as face to face classes.   \n",
       "1                        cristian garcia 02062020lab 1why am i here?the reason i am taking physics 220 and physics 222 is because i want to advance in my education.   \n",
       "2                          i want to set an example for my siblings, i want them to do better than what i am doing, i want them to achieve way more than what i had.   \n",
       "3                                                                                                     i want to show the world my own ideas and make it into a film.   \n",
       "4                                                                                   my dream goal is to have an animal sanctuary or rehabilitation center of my own.   \n",
       "\n",
       "   label  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      0   \n",
       "3      0   \n",
       "4      0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                               phrase  \n",
       "0                                                                                                                                                                                                                                                                                                              ['I am here to receive my education.']  \n",
       "1                                                                                                                                   ['The reason I am taking physics 220 and physics 222 is because I want to advance in my education. I want to receive my bachelor degree in Civil Engineering but in order to do that I have to pass my classes.']  \n",
       "2                                         ['The reason why I am here is that I want to be someone in this life. My goal is to graduate from this institution with a degree. I am here because I want to continue learning and I want to become a pediatrician and work with little children. I am here for a reason and I am here to figure it out.']  \n",
       "3                                                                                                                                                                                                            ['I want to get a job in the film industry. Im not really sure what part of it though. I was thinking a director, editor, or producer.']  \n",
       "4  ['The reason for why I am at San Francisco State University is to reach my goal of working in a wildlife rehabilitation or conservation center. These shows further made me realize that I wanted to pursue a career that involved helping animals of any size. My dream goal is to have an animal sanctuary or rehabilitation center of my own.']  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of my classes was talking about binary numbers and computers.</td>\n",
       "      <td>0</td>\n",
       "      <td>['Im here for one reason which is to learn something new every day. I hope to expand my knowledge about the type of work I want to do in the future.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i believe that education is best gained by practical experience.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I am grateful that physics is required because it allows me to have a deeper understanding of how the everyday experiences works.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i have to do both physics 111112 and 121122.</td>\n",
       "      <td>0</td>\n",
       "      <td>['However, I am also here to improve myself on both physics knowledge and practical problems solving skills.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the reason i am here is to make an impact in society and to make my parents proud of securing a future.</td>\n",
       "      <td>1</td>\n",
       "      <td>['I am here to help secure a future that I want to have.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>there can only be more that can be learned and can benefit me.</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"I am here to understand the material taught in class better. I'm here to learn and understand the class topics better.\"]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  sentence  \\\n",
       "0                                        one of my classes was talking about binary numbers and computers.   \n",
       "1                                         i believe that education is best gained by practical experience.   \n",
       "2                                                             i have to do both physics 111112 and 121122.   \n",
       "3  the reason i am here is to make an impact in society and to make my parents proud of securing a future.   \n",
       "4                                           there can only be more that can be learned and can benefit me.   \n",
       "\n",
       "   label  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      0   \n",
       "3      1   \n",
       "4      0   \n",
       "\n",
       "                                                                                                                                                   phrase  \n",
       "0  ['Im here for one reason which is to learn something new every day. I hope to expand my knowledge about the type of work I want to do in the future.']  \n",
       "1                   ['I am grateful that physics is required because it allows me to have a deeper understanding of how the everyday experiences works.']  \n",
       "2                                          ['However, I am also here to improve myself on both physics knowledge and practical problems solving skills.']  \n",
       "3                                                                                              ['I am here to help secure a future that I want to have.']  \n",
       "4                              [\"I am here to understand the material taught in class better. I'm here to learn and understand the class topics better.\"]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (8856, 3) \n",
      "Test dataset shape: (985, 3)\n",
      "Positive labels present in the dataset : 805  out of 8856 or 9.08988256549232%\n",
      "Positive labels present in the test dataset : 89  out of 985 or 9.035532994923857%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset shape: {training_df.shape} \\nTest dataset shape: {test_df.shape}\")\n",
    "pos_labels = len([n for n in training_df['label'] if n==1])\n",
    "print(\"Positive labels present in the dataset : {}  out of {} or {}%\".format(pos_labels, len(training_df['label']), (pos_labels/len(training_df['label']))*100))\n",
    "pos_labels = len([n for n in test_df['label'] if n==1])\n",
    "print(\"Positive labels present in the test dataset : {}  out of {} or {}%\".format(pos_labels, len(test_df['label']), (pos_labels/len(test_df['label']))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the data for the plots\n",
    "training_counts = training_df['label'].value_counts()\n",
    "test_counts = test_df['label'].value_counts()\n",
    "\n",
    "# Set up the subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot for the training set\n",
    "sns.barplot(x=training_counts.index, y=training_counts.values, ax=axes[0])\n",
    "axes[0].set_title('Distribution of labels in training set')\n",
    "axes[0].set_ylabel('Sentences')\n",
    "axes[0].set_xlabel('Label')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot for the test set\n",
    "sns.barplot(x=test_counts.index, y=test_counts.values, ax=axes[1])\n",
    "axes[1].set_title('Distribution of labels in test set')\n",
    "axes[1].set_ylabel('Sentences')\n",
    "axes[1].set_xlabel('Label')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack Obama']\n"
     ]
    }
   ],
   "source": [
    "def get_ner(text):\n",
    "    ner_list = []\n",
    "    # Annotate the text using stanza\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for entity in sentence.ents:\n",
    "            if entity.type == 'PERSON':\n",
    "                ner_list.append(entity.text)\n",
    "\n",
    "    return ner_list\n",
    "\n",
    "# Example usage\n",
    "text = \"Barack Obama was the 44th doctor of the United States.\"\n",
    "print(get_ner(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a named entity is present in the sentence\n",
    "def named_entity_present(sentence):\n",
    "    ner_list = get_ner(sentence)\n",
    "    if len(ner_list) > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Similarity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the similar words and similarity score\n",
    "# The function takes tokens of sentence as input and if its not a stop word, get its similarity with synsets of STEM.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words |= set([\"help\",\"try\", \"work\", \"process\", \"support\", \"job\"] )\n",
    "def word_similarity(tokens, syns, field):    \n",
    "    if field in ['engineering', 'technology']:\n",
    "        score_threshold = 0.5\n",
    "    else:\n",
    "        score_threshold = 0.2\n",
    "    sim_words = 0\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            try:\n",
    "                syns_word = wordnet.synsets(token) \n",
    "                score = syns_word[0].path_similarity(syns[0])\n",
    "                if score >= score_threshold:\n",
    "                    sim_words += 1\n",
    "            except: \n",
    "                score = 0\n",
    "    \n",
    "    return sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to create columns for similarity based on all STEM fields\n",
    "syns_bio = wordnet.synsets(lemmatizer.lemmatize(\"biology\"))\n",
    "syns_maths = wordnet.synsets(lemmatizer.lemmatize(\"mathematics\")) \n",
    "syns_tech = wordnet.synsets(lemmatizer.lemmatize(\"technology\"))\n",
    "syns_eng = wordnet.synsets(lemmatizer.lemmatize(\"engineering\"))\n",
    "syns_chem = wordnet.synsets(lemmatizer.lemmatize(\"chemistry\"))\n",
    "syns_phy = wordnet.synsets(lemmatizer.lemmatize(\"physics\"))\n",
    "syns_sci = wordnet.synsets(lemmatizer.lemmatize(\"science\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Medical Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['allergy', 'pain', 'internal', 'genitourinary', 'surgery', 'rheumatology', 'plastic', 'rheumatology', 'retardation', 'glaucoma', 'reconstructive', 'emergency', 'mental', 'genetics', 'hematology', 'abdominal', 'interventional', 'neurology', 'neurology', 'endocrinologists', 'interventional', 'vascular', 'cardiovascular', 'surgical', 'psychiatry', 'oncology', 'perinatal', 'nephrology', 'gastroenterology', 'military', 'chemical', 'diagnostic', 'chest', 'electrophysiology', 'cornea', 'segment', 'research', 'endocrinology', 'medicine', 'rehabilitation', 'gastroenterology', 'pulmonology', 'psychiatric', 'cytopathology', 'musculoskeletal', 'transplant', 'genetic', 'nephrology', 'biochemical', 'renal', 'anesthesiology', 'molecular', 'gynecology', 'critical', 'endovascular', 'internal', 'ocular', 'neck', 'pediatrics', 'reproductive', 'infectious', 'calculi', 'dermatology', 'cardiothoracic', 'transplant', 'genetic', 'occupational', 'radiology', 'public', 'liaison', 'sleep', 'heart', 'ophthalmology', 'diabetes', 'dermatopathology', 'geriatric', 'maternal', 'anesthesiology', 'palliative', 'brain', 'reconstructive', 'neurodevelopmental', 'adolescent', 'hospice', 'cardiology', 'obstetrics', 'pediatric', 'anatomical', 'retina', 'oculoplastics', 'critical', 'neurophysiology', 'anterior', 'transfusion', 'male', 'neuropathology', 'microbiology', 'ophthalmology', 'dermatology', 'metabolism', 'cytogenetics', 'pathology', 'ophthalmic', 'care', 'breast', 'failure', 'pediatrics', 'endocrinology', 'orbit', 'pulmonary', 'and', 'forensic', 'aerospace', 'consultation', 'urology', 'disease', 'neurourology', 'radiation', 'administrative', 'neuroradiology', 'neuromuscular', 'hematology', 'community', 'hepatology', 'pelvic', 'family', 'infectious', 'advanced', 'physical', 'injury', 'infertility', 'neuro', 'immunopathology', 'head', 'cardiac', 'toxicology', 'gastrointestinal', 'preventive', 'disabilities', 'neonatal', 'sports', 'diseases', 'addiction', 'neuroradiology', 'behavioral', 'oncology', 'abuse', 'adolescent', 'procedural', 'psychiatry', 'clinical', 'blood', 'sports', 'developmental', 'pathology', 'medical', 'uveitis', 'surgery', 'imaging', 'female', 'gynecologic', 'urology', 'fetal', 'strabismus', 'nuclear', 'health', 'urologic', 'child', 'immunology', 'psychosomatic', 'pediatric', 'banking']\n"
     ]
    }
   ],
   "source": [
    "# Load the medical specialization text file and create a list\n",
    "medical_list = []\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/medical_specialities.txt', 'r') as medical_fields:\n",
    "    for line in medical_fields.readlines():\n",
    "        special_field = line.rstrip('\\n')\n",
    "        special_field = re.sub(\"\\W\",\" \", special_field )\n",
    "#         print(special_field)\n",
    "        medical_list += special_field.split()\n",
    "medical_list = list(set(medical_list))  \n",
    "medical_list = [x.lower() for x in medical_list]\n",
    "print(medical_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get medical words\n",
    "def check_medical_words(tokens):\n",
    "    for token in tokens:\n",
    "        if token not in stop_words and token in [x.lower() for x in medical_list]:\n",
    "            return 1\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sentiment Polarity and Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get polarity and subjectivity of the sentence using TexBlob\n",
    "def get_sentiment(sentence):\n",
    "    sentiments =TextBlob(sentence).sentiment\n",
    "    polarity = sentiments.polarity\n",
    "    subjectivity = sentiments.subjectivity\n",
    "    return polarity, subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the count of POS tags of the sentence\n",
    "def count_pos_tags(tokens):\n",
    "    token_pos = pos_tag(tokens)\n",
    "    count = Counter(tag for word,tag in token_pos)\n",
    "    interjections =  count['UH']\n",
    "    nouns = count['NN'] + count['NNS'] + count['NNP'] + count['NNPS']\n",
    "    adverb = count['RB'] + count['RBS'] + count['RBR']\n",
    "    verb = count['VB'] + count['VBD'] + count['VBG'] + count['VBN']\n",
    "    determiner = count['DT']\n",
    "    pronoun = count['PRP']\n",
    "    adjetive = count['JJ'] + count['JJR'] + count['JJS']\n",
    "    preposition = count['IN']\n",
    "    return interjections, nouns, adverb, verb, determiner, pronoun, adjetive,preposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_extraction(dataframe, field, func, column_names):\n",
    "    return pd.concat((\n",
    "        dataframe,\n",
    "        dataframe[field].apply(\n",
    "            lambda cell: pd.Series(func(cell), index=column_names))), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the w2v dict from pickle file\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/pickle/embeddings10022024.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m w2v_file:\n\u001b[0;32m----> 3\u001b[0m     w2v_dict \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241m.\u001b[39mload(w2v_file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the w2v dict from pickle file\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/pickle/embeddings10022024.pickle', 'rb') as w2v_file:\n",
    "    w2v_dict = pickle.load(w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of word embeddings:  4762\n"
     ]
    }
   ],
   "source": [
    "print(\"length of word embeddings: \", len(w2v_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vectors for the essay\n",
    "def vectorizer(sequence):\n",
    "    vect = []\n",
    "    numw = 0\n",
    "    for w in sequence: \n",
    "        try :\n",
    "            if numw == 0:\n",
    "                vect = w2v_dict[w]\n",
    "            else:\n",
    "                vect = np.add(vect, w2v_dict[w])\n",
    "            numw += 1\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    return vect/ numw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into words\n",
    "def split_into_words(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vectorizer\n",
    "unigram_vect = CountVectorizer(ngram_range=(1, 1), min_df=2, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Putting them all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for feature engineering\n",
    "def feature_engineering(original_dataset):\n",
    "\n",
    "    dataset = original_dataset.copy()\n",
    "    # create a new column with sentence tokens\n",
    "    dataset['tokens'] = dataset['sentence'].apply(word_tokenize)\n",
    "    # 1. Similarity features\n",
    "    # biology\n",
    "    dataset['bio_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_bio,'biology',)) \n",
    "    # chemistry\n",
    "    dataset['chem_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_chem,'chemistry',))\n",
    "    # physics\n",
    "    dataset['phy_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_phy,'physics',))\n",
    "    # mathematics\n",
    "    dataset['math_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_maths,'mathematics',))\n",
    "    # technology\n",
    "    dataset['tech_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_tech,'technology',))\n",
    "    # engineering\n",
    "    dataset['eng_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_eng,'engineering',))\n",
    "    \n",
    "    # medical terms\n",
    "    dataset['medical_terms'] = dataset['tokens'].apply(check_medical_words)\n",
    "    \n",
    "    # polarity and subjectivity\n",
    "    dataset['polarity'], dataset['subjectivity'] = zip(*dataset['sentence'].apply(get_sentiment))\n",
    "    \n",
    "    # named entity recognition\n",
    "    dataset['ner'] = dataset['sentence'].apply(named_entity_present)\n",
    "    \n",
    "    # pos tag count\n",
    "    dataset = pos_tag_extraction(dataset, 'tokens', count_pos_tags, ['interjections', 'nouns', 'adverb', 'verb', 'determiner', 'pronoun', 'adjetive','preposition'])\n",
    "    \n",
    "    # labels\n",
    "    data_labels = dataset['label']\n",
    "    # X\n",
    "    data_x = dataset.drop(columns='label')\n",
    "\n",
    "    \n",
    "    # vectorize all the essays\n",
    "    vect_arr = data_x.tokens.apply(vectorizer)\n",
    "    for index in range(0, len(vect_arr)):\n",
    "        i = 0\n",
    "        for item in vect_arr[index]:\n",
    "            column_name= \"embedding\" + str(i)\n",
    "            data_x.loc[index, column_name] = item\n",
    "            i +=1\n",
    "    \n",
    "    return data_x,data_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = feature_engineering(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4233, 121)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = feature_engineering(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 121)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Unigram features for both train and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4233, 121)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 121)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Aspirational/saved_features/X_test_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mX_train\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Aspirational/saved_features/X_train_final.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m X_test\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Aspirational/saved_features/X_test_final.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m y_train\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Aspirational/saved_features/y_train.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "X_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Aspirational/saved_features/X_train_final.csv\", index=False)\n",
    "X_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Aspirational/saved_features/X_test_final.csv\", index=False)\n",
    "y_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Aspirational/saved_features/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Aspirational/saved_features/y_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the unigram df for train :  (4233, 2308)\n"
     ]
    }
   ],
   "source": [
    "# Unigrams for training set\n",
    "unigram_matrix = unigram_vect.fit_transform(X_train['sentence'])\n",
    "unigrams = pd.DataFrame(unigram_matrix.toarray())\n",
    "print(\"Shape of the unigram df for train : \",unigrams.shape)\n",
    "unigrams = unigrams.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = pd.concat([X_train, unigrams], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final.columns = X_train_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mX_train_final\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_final' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test unigram df shape :  (471, 2308)\n"
     ]
    }
   ],
   "source": [
    "unigram_matrix_test = unigram_vect.transform(X_test['sentence'])\n",
    "unigrams_test = pd.DataFrame(unigram_matrix_test.toarray())\n",
    "unigrams_test = unigrams_test.reset_index(drop=True)\n",
    "print(\"Test unigram df shape : \",unigrams_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 2429)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final = pd.concat([X_test, unigrams_test], axis = 1)\n",
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final.columns = X_test_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 2429)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ---- sentence\n",
      "1 ---- phrase\n",
      "2 ---- tokens\n",
      "3 ---- bio_sim_words\n",
      "4 ---- chem_sim_words\n",
      "5 ---- phy_sim_words\n",
      "6 ---- math_sim_words\n",
      "7 ---- tech_sim_words\n",
      "8 ---- eng_sim_words\n",
      "9 ---- medical_terms\n",
      "10 ---- polarity\n",
      "11 ---- subjectivity\n",
      "12 ---- ner\n",
      "13 ---- interjections\n",
      "14 ---- nouns\n",
      "15 ---- adverb\n",
      "16 ---- verb\n",
      "17 ---- determiner\n",
      "18 ---- pronoun\n",
      "19 ---- adjetive\n",
      "20 ---- preposition\n",
      "21 ---- embedding0\n",
      "22 ---- embedding1\n",
      "23 ---- embedding2\n",
      "24 ---- embedding3\n",
      "25 ---- embedding4\n",
      "26 ---- embedding5\n",
      "27 ---- embedding6\n",
      "28 ---- embedding7\n",
      "29 ---- embedding8\n",
      "30 ---- embedding9\n",
      "31 ---- embedding10\n",
      "32 ---- embedding11\n",
      "33 ---- embedding12\n",
      "34 ---- embedding13\n",
      "35 ---- embedding14\n",
      "36 ---- embedding15\n",
      "37 ---- embedding16\n",
      "38 ---- embedding17\n",
      "39 ---- embedding18\n",
      "40 ---- embedding19\n",
      "41 ---- embedding20\n",
      "42 ---- embedding21\n",
      "43 ---- embedding22\n",
      "44 ---- embedding23\n",
      "45 ---- embedding24\n",
      "46 ---- embedding25\n",
      "47 ---- embedding26\n",
      "48 ---- embedding27\n",
      "49 ---- embedding28\n",
      "50 ---- embedding29\n",
      "51 ---- embedding30\n",
      "52 ---- embedding31\n",
      "53 ---- embedding32\n",
      "54 ---- embedding33\n",
      "55 ---- embedding34\n",
      "56 ---- embedding35\n",
      "57 ---- embedding36\n",
      "58 ---- embedding37\n",
      "59 ---- embedding38\n",
      "60 ---- embedding39\n",
      "61 ---- embedding40\n",
      "62 ---- embedding41\n",
      "63 ---- embedding42\n",
      "64 ---- embedding43\n",
      "65 ---- embedding44\n",
      "66 ---- embedding45\n",
      "67 ---- embedding46\n",
      "68 ---- embedding47\n",
      "69 ---- embedding48\n",
      "70 ---- embedding49\n",
      "71 ---- embedding50\n",
      "72 ---- embedding51\n",
      "73 ---- embedding52\n",
      "74 ---- embedding53\n",
      "75 ---- embedding54\n",
      "76 ---- embedding55\n",
      "77 ---- embedding56\n",
      "78 ---- embedding57\n",
      "79 ---- embedding58\n",
      "80 ---- embedding59\n",
      "81 ---- embedding60\n",
      "82 ---- embedding61\n",
      "83 ---- embedding62\n",
      "84 ---- embedding63\n",
      "85 ---- embedding64\n",
      "86 ---- embedding65\n",
      "87 ---- embedding66\n",
      "88 ---- embedding67\n",
      "89 ---- embedding68\n",
      "90 ---- embedding69\n",
      "91 ---- embedding70\n",
      "92 ---- embedding71\n",
      "93 ---- embedding72\n",
      "94 ---- embedding73\n",
      "95 ---- embedding74\n",
      "96 ---- embedding75\n",
      "97 ---- embedding76\n",
      "98 ---- embedding77\n",
      "99 ---- embedding78\n",
      "100 ---- embedding79\n",
      "101 ---- embedding80\n",
      "102 ---- embedding81\n",
      "103 ---- embedding82\n",
      "104 ---- embedding83\n",
      "105 ---- embedding84\n",
      "106 ---- embedding85\n",
      "107 ---- embedding86\n",
      "108 ---- embedding87\n",
      "109 ---- embedding88\n",
      "110 ---- embedding89\n",
      "111 ---- embedding90\n",
      "112 ---- embedding91\n",
      "113 ---- embedding92\n",
      "114 ---- embedding93\n",
      "115 ---- embedding94\n",
      "116 ---- embedding95\n",
      "117 ---- embedding96\n",
      "118 ---- embedding97\n",
      "119 ---- embedding98\n",
      "120 ---- embedding99\n",
      "121 ---- 0\n",
      "122 ---- 1\n",
      "123 ---- 2\n",
      "124 ---- 3\n",
      "125 ---- 4\n",
      "126 ---- 5\n",
      "127 ---- 6\n",
      "128 ---- 7\n",
      "129 ---- 8\n",
      "130 ---- 9\n",
      "131 ---- 10\n",
      "132 ---- 11\n",
      "133 ---- 12\n",
      "134 ---- 13\n",
      "135 ---- 14\n",
      "136 ---- 15\n",
      "137 ---- 16\n",
      "138 ---- 17\n",
      "139 ---- 18\n",
      "140 ---- 19\n",
      "141 ---- 20\n",
      "142 ---- 21\n",
      "143 ---- 22\n",
      "144 ---- 23\n",
      "145 ---- 24\n",
      "146 ---- 25\n",
      "147 ---- 26\n",
      "148 ---- 27\n",
      "149 ---- 28\n",
      "150 ---- 29\n",
      "151 ---- 30\n",
      "152 ---- 31\n",
      "153 ---- 32\n",
      "154 ---- 33\n",
      "155 ---- 34\n",
      "156 ---- 35\n",
      "157 ---- 36\n",
      "158 ---- 37\n",
      "159 ---- 38\n",
      "160 ---- 39\n",
      "161 ---- 40\n",
      "162 ---- 41\n",
      "163 ---- 42\n",
      "164 ---- 43\n",
      "165 ---- 44\n",
      "166 ---- 45\n",
      "167 ---- 46\n",
      "168 ---- 47\n",
      "169 ---- 48\n",
      "170 ---- 49\n",
      "171 ---- 50\n",
      "172 ---- 51\n",
      "173 ---- 52\n",
      "174 ---- 53\n",
      "175 ---- 54\n",
      "176 ---- 55\n",
      "177 ---- 56\n",
      "178 ---- 57\n",
      "179 ---- 58\n",
      "180 ---- 59\n",
      "181 ---- 60\n",
      "182 ---- 61\n",
      "183 ---- 62\n",
      "184 ---- 63\n",
      "185 ---- 64\n",
      "186 ---- 65\n",
      "187 ---- 66\n",
      "188 ---- 67\n",
      "189 ---- 68\n",
      "190 ---- 69\n",
      "191 ---- 70\n",
      "192 ---- 71\n",
      "193 ---- 72\n",
      "194 ---- 73\n",
      "195 ---- 74\n",
      "196 ---- 75\n",
      "197 ---- 76\n",
      "198 ---- 77\n",
      "199 ---- 78\n",
      "200 ---- 79\n",
      "201 ---- 80\n",
      "202 ---- 81\n",
      "203 ---- 82\n",
      "204 ---- 83\n",
      "205 ---- 84\n",
      "206 ---- 85\n",
      "207 ---- 86\n",
      "208 ---- 87\n",
      "209 ---- 88\n",
      "210 ---- 89\n",
      "211 ---- 90\n",
      "212 ---- 91\n",
      "213 ---- 92\n",
      "214 ---- 93\n",
      "215 ---- 94\n",
      "216 ---- 95\n",
      "217 ---- 96\n",
      "218 ---- 97\n",
      "219 ---- 98\n",
      "220 ---- 99\n",
      "221 ---- 100\n",
      "222 ---- 101\n",
      "223 ---- 102\n",
      "224 ---- 103\n",
      "225 ---- 104\n",
      "226 ---- 105\n",
      "227 ---- 106\n",
      "228 ---- 107\n",
      "229 ---- 108\n",
      "230 ---- 109\n",
      "231 ---- 110\n",
      "232 ---- 111\n",
      "233 ---- 112\n",
      "234 ---- 113\n",
      "235 ---- 114\n",
      "236 ---- 115\n",
      "237 ---- 116\n",
      "238 ---- 117\n",
      "239 ---- 118\n",
      "240 ---- 119\n",
      "241 ---- 120\n",
      "242 ---- 121\n",
      "243 ---- 122\n",
      "244 ---- 123\n",
      "245 ---- 124\n",
      "246 ---- 125\n",
      "247 ---- 126\n",
      "248 ---- 127\n",
      "249 ---- 128\n",
      "250 ---- 129\n",
      "251 ---- 130\n",
      "252 ---- 131\n",
      "253 ---- 132\n",
      "254 ---- 133\n",
      "255 ---- 134\n",
      "256 ---- 135\n",
      "257 ---- 136\n",
      "258 ---- 137\n",
      "259 ---- 138\n",
      "260 ---- 139\n",
      "261 ---- 140\n",
      "262 ---- 141\n",
      "263 ---- 142\n",
      "264 ---- 143\n",
      "265 ---- 144\n",
      "266 ---- 145\n",
      "267 ---- 146\n",
      "268 ---- 147\n",
      "269 ---- 148\n",
      "270 ---- 149\n",
      "271 ---- 150\n",
      "272 ---- 151\n",
      "273 ---- 152\n",
      "274 ---- 153\n",
      "275 ---- 154\n",
      "276 ---- 155\n",
      "277 ---- 156\n",
      "278 ---- 157\n",
      "279 ---- 158\n",
      "280 ---- 159\n",
      "281 ---- 160\n",
      "282 ---- 161\n",
      "283 ---- 162\n",
      "284 ---- 163\n",
      "285 ---- 164\n",
      "286 ---- 165\n",
      "287 ---- 166\n",
      "288 ---- 167\n",
      "289 ---- 168\n",
      "290 ---- 169\n",
      "291 ---- 170\n",
      "292 ---- 171\n",
      "293 ---- 172\n",
      "294 ---- 173\n",
      "295 ---- 174\n",
      "296 ---- 175\n",
      "297 ---- 176\n",
      "298 ---- 177\n",
      "299 ---- 178\n",
      "300 ---- 179\n",
      "301 ---- 180\n",
      "302 ---- 181\n",
      "303 ---- 182\n",
      "304 ---- 183\n",
      "305 ---- 184\n",
      "306 ---- 185\n",
      "307 ---- 186\n",
      "308 ---- 187\n",
      "309 ---- 188\n",
      "310 ---- 189\n",
      "311 ---- 190\n",
      "312 ---- 191\n",
      "313 ---- 192\n",
      "314 ---- 193\n",
      "315 ---- 194\n",
      "316 ---- 195\n",
      "317 ---- 196\n",
      "318 ---- 197\n",
      "319 ---- 198\n",
      "320 ---- 199\n",
      "321 ---- 200\n",
      "322 ---- 201\n",
      "323 ---- 202\n",
      "324 ---- 203\n",
      "325 ---- 204\n",
      "326 ---- 205\n",
      "327 ---- 206\n",
      "328 ---- 207\n",
      "329 ---- 208\n",
      "330 ---- 209\n",
      "331 ---- 210\n",
      "332 ---- 211\n",
      "333 ---- 212\n",
      "334 ---- 213\n",
      "335 ---- 214\n",
      "336 ---- 215\n",
      "337 ---- 216\n",
      "338 ---- 217\n",
      "339 ---- 218\n",
      "340 ---- 219\n",
      "341 ---- 220\n",
      "342 ---- 221\n",
      "343 ---- 222\n",
      "344 ---- 223\n",
      "345 ---- 224\n",
      "346 ---- 225\n",
      "347 ---- 226\n",
      "348 ---- 227\n",
      "349 ---- 228\n",
      "350 ---- 229\n",
      "351 ---- 230\n",
      "352 ---- 231\n",
      "353 ---- 232\n",
      "354 ---- 233\n",
      "355 ---- 234\n",
      "356 ---- 235\n",
      "357 ---- 236\n",
      "358 ---- 237\n",
      "359 ---- 238\n",
      "360 ---- 239\n",
      "361 ---- 240\n",
      "362 ---- 241\n",
      "363 ---- 242\n",
      "364 ---- 243\n",
      "365 ---- 244\n",
      "366 ---- 245\n",
      "367 ---- 246\n",
      "368 ---- 247\n",
      "369 ---- 248\n",
      "370 ---- 249\n",
      "371 ---- 250\n",
      "372 ---- 251\n",
      "373 ---- 252\n",
      "374 ---- 253\n",
      "375 ---- 254\n",
      "376 ---- 255\n",
      "377 ---- 256\n",
      "378 ---- 257\n",
      "379 ---- 258\n",
      "380 ---- 259\n",
      "381 ---- 260\n",
      "382 ---- 261\n",
      "383 ---- 262\n",
      "384 ---- 263\n",
      "385 ---- 264\n",
      "386 ---- 265\n",
      "387 ---- 266\n",
      "388 ---- 267\n",
      "389 ---- 268\n",
      "390 ---- 269\n",
      "391 ---- 270\n",
      "392 ---- 271\n",
      "393 ---- 272\n",
      "394 ---- 273\n",
      "395 ---- 274\n",
      "396 ---- 275\n",
      "397 ---- 276\n",
      "398 ---- 277\n",
      "399 ---- 278\n",
      "400 ---- 279\n",
      "401 ---- 280\n",
      "402 ---- 281\n",
      "403 ---- 282\n",
      "404 ---- 283\n",
      "405 ---- 284\n",
      "406 ---- 285\n",
      "407 ---- 286\n",
      "408 ---- 287\n",
      "409 ---- 288\n",
      "410 ---- 289\n",
      "411 ---- 290\n",
      "412 ---- 291\n",
      "413 ---- 292\n",
      "414 ---- 293\n",
      "415 ---- 294\n",
      "416 ---- 295\n",
      "417 ---- 296\n",
      "418 ---- 297\n",
      "419 ---- 298\n",
      "420 ---- 299\n",
      "421 ---- 300\n",
      "422 ---- 301\n",
      "423 ---- 302\n",
      "424 ---- 303\n",
      "425 ---- 304\n",
      "426 ---- 305\n",
      "427 ---- 306\n",
      "428 ---- 307\n",
      "429 ---- 308\n",
      "430 ---- 309\n",
      "431 ---- 310\n",
      "432 ---- 311\n",
      "433 ---- 312\n",
      "434 ---- 313\n",
      "435 ---- 314\n",
      "436 ---- 315\n",
      "437 ---- 316\n",
      "438 ---- 317\n",
      "439 ---- 318\n",
      "440 ---- 319\n",
      "441 ---- 320\n",
      "442 ---- 321\n",
      "443 ---- 322\n",
      "444 ---- 323\n",
      "445 ---- 324\n",
      "446 ---- 325\n",
      "447 ---- 326\n",
      "448 ---- 327\n",
      "449 ---- 328\n",
      "450 ---- 329\n",
      "451 ---- 330\n",
      "452 ---- 331\n",
      "453 ---- 332\n",
      "454 ---- 333\n",
      "455 ---- 334\n",
      "456 ---- 335\n",
      "457 ---- 336\n",
      "458 ---- 337\n",
      "459 ---- 338\n",
      "460 ---- 339\n",
      "461 ---- 340\n",
      "462 ---- 341\n",
      "463 ---- 342\n",
      "464 ---- 343\n",
      "465 ---- 344\n",
      "466 ---- 345\n",
      "467 ---- 346\n",
      "468 ---- 347\n",
      "469 ---- 348\n",
      "470 ---- 349\n",
      "471 ---- 350\n",
      "472 ---- 351\n",
      "473 ---- 352\n",
      "474 ---- 353\n",
      "475 ---- 354\n",
      "476 ---- 355\n",
      "477 ---- 356\n",
      "478 ---- 357\n",
      "479 ---- 358\n",
      "480 ---- 359\n",
      "481 ---- 360\n",
      "482 ---- 361\n",
      "483 ---- 362\n",
      "484 ---- 363\n",
      "485 ---- 364\n",
      "486 ---- 365\n",
      "487 ---- 366\n",
      "488 ---- 367\n",
      "489 ---- 368\n",
      "490 ---- 369\n",
      "491 ---- 370\n",
      "492 ---- 371\n",
      "493 ---- 372\n",
      "494 ---- 373\n",
      "495 ---- 374\n",
      "496 ---- 375\n",
      "497 ---- 376\n",
      "498 ---- 377\n",
      "499 ---- 378\n",
      "500 ---- 379\n",
      "501 ---- 380\n",
      "502 ---- 381\n",
      "503 ---- 382\n",
      "504 ---- 383\n",
      "505 ---- 384\n",
      "506 ---- 385\n",
      "507 ---- 386\n",
      "508 ---- 387\n",
      "509 ---- 388\n",
      "510 ---- 389\n",
      "511 ---- 390\n",
      "512 ---- 391\n",
      "513 ---- 392\n",
      "514 ---- 393\n",
      "515 ---- 394\n",
      "516 ---- 395\n",
      "517 ---- 396\n",
      "518 ---- 397\n",
      "519 ---- 398\n",
      "520 ---- 399\n",
      "521 ---- 400\n",
      "522 ---- 401\n",
      "523 ---- 402\n",
      "524 ---- 403\n",
      "525 ---- 404\n",
      "526 ---- 405\n",
      "527 ---- 406\n",
      "528 ---- 407\n",
      "529 ---- 408\n",
      "530 ---- 409\n",
      "531 ---- 410\n",
      "532 ---- 411\n",
      "533 ---- 412\n",
      "534 ---- 413\n",
      "535 ---- 414\n",
      "536 ---- 415\n",
      "537 ---- 416\n",
      "538 ---- 417\n",
      "539 ---- 418\n",
      "540 ---- 419\n",
      "541 ---- 420\n",
      "542 ---- 421\n",
      "543 ---- 422\n",
      "544 ---- 423\n",
      "545 ---- 424\n",
      "546 ---- 425\n",
      "547 ---- 426\n",
      "548 ---- 427\n",
      "549 ---- 428\n",
      "550 ---- 429\n",
      "551 ---- 430\n",
      "552 ---- 431\n",
      "553 ---- 432\n",
      "554 ---- 433\n",
      "555 ---- 434\n",
      "556 ---- 435\n",
      "557 ---- 436\n",
      "558 ---- 437\n",
      "559 ---- 438\n",
      "560 ---- 439\n",
      "561 ---- 440\n",
      "562 ---- 441\n",
      "563 ---- 442\n",
      "564 ---- 443\n",
      "565 ---- 444\n",
      "566 ---- 445\n",
      "567 ---- 446\n",
      "568 ---- 447\n",
      "569 ---- 448\n",
      "570 ---- 449\n",
      "571 ---- 450\n",
      "572 ---- 451\n",
      "573 ---- 452\n",
      "574 ---- 453\n",
      "575 ---- 454\n",
      "576 ---- 455\n",
      "577 ---- 456\n",
      "578 ---- 457\n",
      "579 ---- 458\n",
      "580 ---- 459\n",
      "581 ---- 460\n",
      "582 ---- 461\n",
      "583 ---- 462\n",
      "584 ---- 463\n",
      "585 ---- 464\n",
      "586 ---- 465\n",
      "587 ---- 466\n",
      "588 ---- 467\n",
      "589 ---- 468\n",
      "590 ---- 469\n",
      "591 ---- 470\n",
      "592 ---- 471\n",
      "593 ---- 472\n",
      "594 ---- 473\n",
      "595 ---- 474\n",
      "596 ---- 475\n",
      "597 ---- 476\n",
      "598 ---- 477\n",
      "599 ---- 478\n",
      "600 ---- 479\n",
      "601 ---- 480\n",
      "602 ---- 481\n",
      "603 ---- 482\n",
      "604 ---- 483\n",
      "605 ---- 484\n",
      "606 ---- 485\n",
      "607 ---- 486\n",
      "608 ---- 487\n",
      "609 ---- 488\n",
      "610 ---- 489\n",
      "611 ---- 490\n",
      "612 ---- 491\n",
      "613 ---- 492\n",
      "614 ---- 493\n",
      "615 ---- 494\n",
      "616 ---- 495\n",
      "617 ---- 496\n",
      "618 ---- 497\n",
      "619 ---- 498\n",
      "620 ---- 499\n",
      "621 ---- 500\n",
      "622 ---- 501\n",
      "623 ---- 502\n",
      "624 ---- 503\n",
      "625 ---- 504\n",
      "626 ---- 505\n",
      "627 ---- 506\n",
      "628 ---- 507\n",
      "629 ---- 508\n",
      "630 ---- 509\n",
      "631 ---- 510\n",
      "632 ---- 511\n",
      "633 ---- 512\n",
      "634 ---- 513\n",
      "635 ---- 514\n",
      "636 ---- 515\n",
      "637 ---- 516\n",
      "638 ---- 517\n",
      "639 ---- 518\n",
      "640 ---- 519\n",
      "641 ---- 520\n",
      "642 ---- 521\n",
      "643 ---- 522\n",
      "644 ---- 523\n",
      "645 ---- 524\n",
      "646 ---- 525\n",
      "647 ---- 526\n",
      "648 ---- 527\n",
      "649 ---- 528\n",
      "650 ---- 529\n",
      "651 ---- 530\n",
      "652 ---- 531\n",
      "653 ---- 532\n",
      "654 ---- 533\n",
      "655 ---- 534\n",
      "656 ---- 535\n",
      "657 ---- 536\n",
      "658 ---- 537\n",
      "659 ---- 538\n",
      "660 ---- 539\n",
      "661 ---- 540\n",
      "662 ---- 541\n",
      "663 ---- 542\n",
      "664 ---- 543\n",
      "665 ---- 544\n",
      "666 ---- 545\n",
      "667 ---- 546\n",
      "668 ---- 547\n",
      "669 ---- 548\n",
      "670 ---- 549\n",
      "671 ---- 550\n",
      "672 ---- 551\n",
      "673 ---- 552\n",
      "674 ---- 553\n",
      "675 ---- 554\n",
      "676 ---- 555\n",
      "677 ---- 556\n",
      "678 ---- 557\n",
      "679 ---- 558\n",
      "680 ---- 559\n",
      "681 ---- 560\n",
      "682 ---- 561\n",
      "683 ---- 562\n",
      "684 ---- 563\n",
      "685 ---- 564\n",
      "686 ---- 565\n",
      "687 ---- 566\n",
      "688 ---- 567\n",
      "689 ---- 568\n",
      "690 ---- 569\n",
      "691 ---- 570\n",
      "692 ---- 571\n",
      "693 ---- 572\n",
      "694 ---- 573\n",
      "695 ---- 574\n",
      "696 ---- 575\n",
      "697 ---- 576\n",
      "698 ---- 577\n",
      "699 ---- 578\n",
      "700 ---- 579\n",
      "701 ---- 580\n",
      "702 ---- 581\n",
      "703 ---- 582\n",
      "704 ---- 583\n",
      "705 ---- 584\n",
      "706 ---- 585\n",
      "707 ---- 586\n",
      "708 ---- 587\n",
      "709 ---- 588\n",
      "710 ---- 589\n",
      "711 ---- 590\n",
      "712 ---- 591\n",
      "713 ---- 592\n",
      "714 ---- 593\n",
      "715 ---- 594\n",
      "716 ---- 595\n",
      "717 ---- 596\n",
      "718 ---- 597\n",
      "719 ---- 598\n",
      "720 ---- 599\n",
      "721 ---- 600\n",
      "722 ---- 601\n",
      "723 ---- 602\n",
      "724 ---- 603\n",
      "725 ---- 604\n",
      "726 ---- 605\n",
      "727 ---- 606\n",
      "728 ---- 607\n",
      "729 ---- 608\n",
      "730 ---- 609\n",
      "731 ---- 610\n",
      "732 ---- 611\n",
      "733 ---- 612\n",
      "734 ---- 613\n",
      "735 ---- 614\n",
      "736 ---- 615\n",
      "737 ---- 616\n",
      "738 ---- 617\n",
      "739 ---- 618\n",
      "740 ---- 619\n",
      "741 ---- 620\n",
      "742 ---- 621\n",
      "743 ---- 622\n",
      "744 ---- 623\n",
      "745 ---- 624\n",
      "746 ---- 625\n",
      "747 ---- 626\n",
      "748 ---- 627\n",
      "749 ---- 628\n",
      "750 ---- 629\n",
      "751 ---- 630\n",
      "752 ---- 631\n",
      "753 ---- 632\n",
      "754 ---- 633\n",
      "755 ---- 634\n",
      "756 ---- 635\n",
      "757 ---- 636\n",
      "758 ---- 637\n",
      "759 ---- 638\n",
      "760 ---- 639\n",
      "761 ---- 640\n",
      "762 ---- 641\n",
      "763 ---- 642\n",
      "764 ---- 643\n",
      "765 ---- 644\n",
      "766 ---- 645\n",
      "767 ---- 646\n",
      "768 ---- 647\n",
      "769 ---- 648\n",
      "770 ---- 649\n",
      "771 ---- 650\n",
      "772 ---- 651\n",
      "773 ---- 652\n",
      "774 ---- 653\n",
      "775 ---- 654\n",
      "776 ---- 655\n",
      "777 ---- 656\n",
      "778 ---- 657\n",
      "779 ---- 658\n",
      "780 ---- 659\n",
      "781 ---- 660\n",
      "782 ---- 661\n",
      "783 ---- 662\n",
      "784 ---- 663\n",
      "785 ---- 664\n",
      "786 ---- 665\n",
      "787 ---- 666\n",
      "788 ---- 667\n",
      "789 ---- 668\n",
      "790 ---- 669\n",
      "791 ---- 670\n",
      "792 ---- 671\n",
      "793 ---- 672\n",
      "794 ---- 673\n",
      "795 ---- 674\n",
      "796 ---- 675\n",
      "797 ---- 676\n",
      "798 ---- 677\n",
      "799 ---- 678\n",
      "800 ---- 679\n",
      "801 ---- 680\n",
      "802 ---- 681\n",
      "803 ---- 682\n",
      "804 ---- 683\n",
      "805 ---- 684\n",
      "806 ---- 685\n",
      "807 ---- 686\n",
      "808 ---- 687\n",
      "809 ---- 688\n",
      "810 ---- 689\n",
      "811 ---- 690\n",
      "812 ---- 691\n",
      "813 ---- 692\n",
      "814 ---- 693\n",
      "815 ---- 694\n",
      "816 ---- 695\n",
      "817 ---- 696\n",
      "818 ---- 697\n",
      "819 ---- 698\n",
      "820 ---- 699\n",
      "821 ---- 700\n",
      "822 ---- 701\n",
      "823 ---- 702\n",
      "824 ---- 703\n",
      "825 ---- 704\n",
      "826 ---- 705\n",
      "827 ---- 706\n",
      "828 ---- 707\n",
      "829 ---- 708\n",
      "830 ---- 709\n",
      "831 ---- 710\n",
      "832 ---- 711\n",
      "833 ---- 712\n",
      "834 ---- 713\n",
      "835 ---- 714\n",
      "836 ---- 715\n",
      "837 ---- 716\n",
      "838 ---- 717\n",
      "839 ---- 718\n",
      "840 ---- 719\n",
      "841 ---- 720\n",
      "842 ---- 721\n",
      "843 ---- 722\n",
      "844 ---- 723\n",
      "845 ---- 724\n",
      "846 ---- 725\n",
      "847 ---- 726\n",
      "848 ---- 727\n",
      "849 ---- 728\n",
      "850 ---- 729\n",
      "851 ---- 730\n",
      "852 ---- 731\n",
      "853 ---- 732\n",
      "854 ---- 733\n",
      "855 ---- 734\n",
      "856 ---- 735\n",
      "857 ---- 736\n",
      "858 ---- 737\n",
      "859 ---- 738\n",
      "860 ---- 739\n",
      "861 ---- 740\n",
      "862 ---- 741\n",
      "863 ---- 742\n",
      "864 ---- 743\n",
      "865 ---- 744\n",
      "866 ---- 745\n",
      "867 ---- 746\n",
      "868 ---- 747\n",
      "869 ---- 748\n",
      "870 ---- 749\n",
      "871 ---- 750\n",
      "872 ---- 751\n",
      "873 ---- 752\n",
      "874 ---- 753\n",
      "875 ---- 754\n",
      "876 ---- 755\n",
      "877 ---- 756\n",
      "878 ---- 757\n",
      "879 ---- 758\n",
      "880 ---- 759\n",
      "881 ---- 760\n",
      "882 ---- 761\n",
      "883 ---- 762\n",
      "884 ---- 763\n",
      "885 ---- 764\n",
      "886 ---- 765\n",
      "887 ---- 766\n",
      "888 ---- 767\n",
      "889 ---- 768\n",
      "890 ---- 769\n",
      "891 ---- 770\n",
      "892 ---- 771\n",
      "893 ---- 772\n",
      "894 ---- 773\n",
      "895 ---- 774\n",
      "896 ---- 775\n",
      "897 ---- 776\n",
      "898 ---- 777\n",
      "899 ---- 778\n",
      "900 ---- 779\n",
      "901 ---- 780\n",
      "902 ---- 781\n",
      "903 ---- 782\n",
      "904 ---- 783\n",
      "905 ---- 784\n",
      "906 ---- 785\n",
      "907 ---- 786\n",
      "908 ---- 787\n",
      "909 ---- 788\n",
      "910 ---- 789\n",
      "911 ---- 790\n",
      "912 ---- 791\n",
      "913 ---- 792\n",
      "914 ---- 793\n",
      "915 ---- 794\n",
      "916 ---- 795\n",
      "917 ---- 796\n",
      "918 ---- 797\n",
      "919 ---- 798\n",
      "920 ---- 799\n",
      "921 ---- 800\n",
      "922 ---- 801\n",
      "923 ---- 802\n",
      "924 ---- 803\n",
      "925 ---- 804\n",
      "926 ---- 805\n",
      "927 ---- 806\n",
      "928 ---- 807\n",
      "929 ---- 808\n",
      "930 ---- 809\n",
      "931 ---- 810\n",
      "932 ---- 811\n",
      "933 ---- 812\n",
      "934 ---- 813\n",
      "935 ---- 814\n",
      "936 ---- 815\n",
      "937 ---- 816\n",
      "938 ---- 817\n",
      "939 ---- 818\n",
      "940 ---- 819\n",
      "941 ---- 820\n",
      "942 ---- 821\n",
      "943 ---- 822\n",
      "944 ---- 823\n",
      "945 ---- 824\n",
      "946 ---- 825\n",
      "947 ---- 826\n",
      "948 ---- 827\n",
      "949 ---- 828\n",
      "950 ---- 829\n",
      "951 ---- 830\n",
      "952 ---- 831\n",
      "953 ---- 832\n",
      "954 ---- 833\n",
      "955 ---- 834\n",
      "956 ---- 835\n",
      "957 ---- 836\n",
      "958 ---- 837\n",
      "959 ---- 838\n",
      "960 ---- 839\n",
      "961 ---- 840\n",
      "962 ---- 841\n",
      "963 ---- 842\n",
      "964 ---- 843\n",
      "965 ---- 844\n",
      "966 ---- 845\n",
      "967 ---- 846\n",
      "968 ---- 847\n",
      "969 ---- 848\n",
      "970 ---- 849\n",
      "971 ---- 850\n",
      "972 ---- 851\n",
      "973 ---- 852\n",
      "974 ---- 853\n",
      "975 ---- 854\n",
      "976 ---- 855\n",
      "977 ---- 856\n",
      "978 ---- 857\n",
      "979 ---- 858\n",
      "980 ---- 859\n",
      "981 ---- 860\n",
      "982 ---- 861\n",
      "983 ---- 862\n",
      "984 ---- 863\n",
      "985 ---- 864\n",
      "986 ---- 865\n",
      "987 ---- 866\n",
      "988 ---- 867\n",
      "989 ---- 868\n",
      "990 ---- 869\n",
      "991 ---- 870\n",
      "992 ---- 871\n",
      "993 ---- 872\n",
      "994 ---- 873\n",
      "995 ---- 874\n",
      "996 ---- 875\n",
      "997 ---- 876\n",
      "998 ---- 877\n",
      "999 ---- 878\n",
      "1000 ---- 879\n",
      "1001 ---- 880\n",
      "1002 ---- 881\n",
      "1003 ---- 882\n",
      "1004 ---- 883\n",
      "1005 ---- 884\n",
      "1006 ---- 885\n",
      "1007 ---- 886\n",
      "1008 ---- 887\n",
      "1009 ---- 888\n",
      "1010 ---- 889\n",
      "1011 ---- 890\n",
      "1012 ---- 891\n",
      "1013 ---- 892\n",
      "1014 ---- 893\n",
      "1015 ---- 894\n",
      "1016 ---- 895\n",
      "1017 ---- 896\n",
      "1018 ---- 897\n",
      "1019 ---- 898\n",
      "1020 ---- 899\n",
      "1021 ---- 900\n",
      "1022 ---- 901\n",
      "1023 ---- 902\n",
      "1024 ---- 903\n",
      "1025 ---- 904\n",
      "1026 ---- 905\n",
      "1027 ---- 906\n",
      "1028 ---- 907\n",
      "1029 ---- 908\n",
      "1030 ---- 909\n",
      "1031 ---- 910\n",
      "1032 ---- 911\n",
      "1033 ---- 912\n",
      "1034 ---- 913\n",
      "1035 ---- 914\n",
      "1036 ---- 915\n",
      "1037 ---- 916\n",
      "1038 ---- 917\n",
      "1039 ---- 918\n",
      "1040 ---- 919\n",
      "1041 ---- 920\n",
      "1042 ---- 921\n",
      "1043 ---- 922\n",
      "1044 ---- 923\n",
      "1045 ---- 924\n",
      "1046 ---- 925\n",
      "1047 ---- 926\n",
      "1048 ---- 927\n",
      "1049 ---- 928\n",
      "1050 ---- 929\n",
      "1051 ---- 930\n",
      "1052 ---- 931\n",
      "1053 ---- 932\n",
      "1054 ---- 933\n",
      "1055 ---- 934\n",
      "1056 ---- 935\n",
      "1057 ---- 936\n",
      "1058 ---- 937\n",
      "1059 ---- 938\n",
      "1060 ---- 939\n",
      "1061 ---- 940\n",
      "1062 ---- 941\n",
      "1063 ---- 942\n",
      "1064 ---- 943\n",
      "1065 ---- 944\n",
      "1066 ---- 945\n",
      "1067 ---- 946\n",
      "1068 ---- 947\n",
      "1069 ---- 948\n",
      "1070 ---- 949\n",
      "1071 ---- 950\n",
      "1072 ---- 951\n",
      "1073 ---- 952\n",
      "1074 ---- 953\n",
      "1075 ---- 954\n",
      "1076 ---- 955\n",
      "1077 ---- 956\n",
      "1078 ---- 957\n",
      "1079 ---- 958\n",
      "1080 ---- 959\n",
      "1081 ---- 960\n",
      "1082 ---- 961\n",
      "1083 ---- 962\n",
      "1084 ---- 963\n",
      "1085 ---- 964\n",
      "1086 ---- 965\n",
      "1087 ---- 966\n",
      "1088 ---- 967\n",
      "1089 ---- 968\n",
      "1090 ---- 969\n",
      "1091 ---- 970\n",
      "1092 ---- 971\n",
      "1093 ---- 972\n",
      "1094 ---- 973\n",
      "1095 ---- 974\n",
      "1096 ---- 975\n",
      "1097 ---- 976\n",
      "1098 ---- 977\n",
      "1099 ---- 978\n",
      "1100 ---- 979\n",
      "1101 ---- 980\n",
      "1102 ---- 981\n",
      "1103 ---- 982\n",
      "1104 ---- 983\n",
      "1105 ---- 984\n",
      "1106 ---- 985\n",
      "1107 ---- 986\n",
      "1108 ---- 987\n",
      "1109 ---- 988\n",
      "1110 ---- 989\n",
      "1111 ---- 990\n",
      "1112 ---- 991\n",
      "1113 ---- 992\n",
      "1114 ---- 993\n",
      "1115 ---- 994\n",
      "1116 ---- 995\n",
      "1117 ---- 996\n",
      "1118 ---- 997\n",
      "1119 ---- 998\n",
      "1120 ---- 999\n",
      "1121 ---- 1000\n",
      "1122 ---- 1001\n",
      "1123 ---- 1002\n",
      "1124 ---- 1003\n",
      "1125 ---- 1004\n",
      "1126 ---- 1005\n",
      "1127 ---- 1006\n",
      "1128 ---- 1007\n",
      "1129 ---- 1008\n",
      "1130 ---- 1009\n",
      "1131 ---- 1010\n",
      "1132 ---- 1011\n",
      "1133 ---- 1012\n",
      "1134 ---- 1013\n",
      "1135 ---- 1014\n",
      "1136 ---- 1015\n",
      "1137 ---- 1016\n",
      "1138 ---- 1017\n",
      "1139 ---- 1018\n",
      "1140 ---- 1019\n",
      "1141 ---- 1020\n",
      "1142 ---- 1021\n",
      "1143 ---- 1022\n",
      "1144 ---- 1023\n",
      "1145 ---- 1024\n",
      "1146 ---- 1025\n",
      "1147 ---- 1026\n",
      "1148 ---- 1027\n",
      "1149 ---- 1028\n",
      "1150 ---- 1029\n",
      "1151 ---- 1030\n",
      "1152 ---- 1031\n",
      "1153 ---- 1032\n",
      "1154 ---- 1033\n",
      "1155 ---- 1034\n",
      "1156 ---- 1035\n",
      "1157 ---- 1036\n",
      "1158 ---- 1037\n",
      "1159 ---- 1038\n",
      "1160 ---- 1039\n",
      "1161 ---- 1040\n",
      "1162 ---- 1041\n",
      "1163 ---- 1042\n",
      "1164 ---- 1043\n",
      "1165 ---- 1044\n",
      "1166 ---- 1045\n",
      "1167 ---- 1046\n",
      "1168 ---- 1047\n",
      "1169 ---- 1048\n",
      "1170 ---- 1049\n",
      "1171 ---- 1050\n",
      "1172 ---- 1051\n",
      "1173 ---- 1052\n",
      "1174 ---- 1053\n",
      "1175 ---- 1054\n",
      "1176 ---- 1055\n",
      "1177 ---- 1056\n",
      "1178 ---- 1057\n",
      "1179 ---- 1058\n",
      "1180 ---- 1059\n",
      "1181 ---- 1060\n",
      "1182 ---- 1061\n",
      "1183 ---- 1062\n",
      "1184 ---- 1063\n",
      "1185 ---- 1064\n",
      "1186 ---- 1065\n",
      "1187 ---- 1066\n",
      "1188 ---- 1067\n",
      "1189 ---- 1068\n",
      "1190 ---- 1069\n",
      "1191 ---- 1070\n",
      "1192 ---- 1071\n",
      "1193 ---- 1072\n",
      "1194 ---- 1073\n",
      "1195 ---- 1074\n",
      "1196 ---- 1075\n",
      "1197 ---- 1076\n",
      "1198 ---- 1077\n",
      "1199 ---- 1078\n",
      "1200 ---- 1079\n",
      "1201 ---- 1080\n",
      "1202 ---- 1081\n",
      "1203 ---- 1082\n",
      "1204 ---- 1083\n",
      "1205 ---- 1084\n",
      "1206 ---- 1085\n",
      "1207 ---- 1086\n",
      "1208 ---- 1087\n",
      "1209 ---- 1088\n",
      "1210 ---- 1089\n",
      "1211 ---- 1090\n",
      "1212 ---- 1091\n",
      "1213 ---- 1092\n",
      "1214 ---- 1093\n",
      "1215 ---- 1094\n",
      "1216 ---- 1095\n",
      "1217 ---- 1096\n",
      "1218 ---- 1097\n",
      "1219 ---- 1098\n",
      "1220 ---- 1099\n",
      "1221 ---- 1100\n",
      "1222 ---- 1101\n",
      "1223 ---- 1102\n",
      "1224 ---- 1103\n",
      "1225 ---- 1104\n",
      "1226 ---- 1105\n",
      "1227 ---- 1106\n",
      "1228 ---- 1107\n",
      "1229 ---- 1108\n",
      "1230 ---- 1109\n",
      "1231 ---- 1110\n",
      "1232 ---- 1111\n",
      "1233 ---- 1112\n",
      "1234 ---- 1113\n",
      "1235 ---- 1114\n",
      "1236 ---- 1115\n",
      "1237 ---- 1116\n",
      "1238 ---- 1117\n",
      "1239 ---- 1118\n",
      "1240 ---- 1119\n",
      "1241 ---- 1120\n",
      "1242 ---- 1121\n",
      "1243 ---- 1122\n",
      "1244 ---- 1123\n",
      "1245 ---- 1124\n",
      "1246 ---- 1125\n",
      "1247 ---- 1126\n",
      "1248 ---- 1127\n",
      "1249 ---- 1128\n",
      "1250 ---- 1129\n",
      "1251 ---- 1130\n",
      "1252 ---- 1131\n",
      "1253 ---- 1132\n",
      "1254 ---- 1133\n",
      "1255 ---- 1134\n",
      "1256 ---- 1135\n",
      "1257 ---- 1136\n",
      "1258 ---- 1137\n",
      "1259 ---- 1138\n",
      "1260 ---- 1139\n",
      "1261 ---- 1140\n",
      "1262 ---- 1141\n",
      "1263 ---- 1142\n",
      "1264 ---- 1143\n",
      "1265 ---- 1144\n",
      "1266 ---- 1145\n",
      "1267 ---- 1146\n",
      "1268 ---- 1147\n",
      "1269 ---- 1148\n",
      "1270 ---- 1149\n",
      "1271 ---- 1150\n",
      "1272 ---- 1151\n",
      "1273 ---- 1152\n",
      "1274 ---- 1153\n",
      "1275 ---- 1154\n",
      "1276 ---- 1155\n",
      "1277 ---- 1156\n",
      "1278 ---- 1157\n",
      "1279 ---- 1158\n",
      "1280 ---- 1159\n",
      "1281 ---- 1160\n",
      "1282 ---- 1161\n",
      "1283 ---- 1162\n",
      "1284 ---- 1163\n",
      "1285 ---- 1164\n",
      "1286 ---- 1165\n",
      "1287 ---- 1166\n",
      "1288 ---- 1167\n",
      "1289 ---- 1168\n",
      "1290 ---- 1169\n",
      "1291 ---- 1170\n",
      "1292 ---- 1171\n",
      "1293 ---- 1172\n",
      "1294 ---- 1173\n",
      "1295 ---- 1174\n",
      "1296 ---- 1175\n",
      "1297 ---- 1176\n",
      "1298 ---- 1177\n",
      "1299 ---- 1178\n",
      "1300 ---- 1179\n",
      "1301 ---- 1180\n",
      "1302 ---- 1181\n",
      "1303 ---- 1182\n",
      "1304 ---- 1183\n",
      "1305 ---- 1184\n",
      "1306 ---- 1185\n",
      "1307 ---- 1186\n",
      "1308 ---- 1187\n",
      "1309 ---- 1188\n",
      "1310 ---- 1189\n",
      "1311 ---- 1190\n",
      "1312 ---- 1191\n",
      "1313 ---- 1192\n",
      "1314 ---- 1193\n",
      "1315 ---- 1194\n",
      "1316 ---- 1195\n",
      "1317 ---- 1196\n",
      "1318 ---- 1197\n",
      "1319 ---- 1198\n",
      "1320 ---- 1199\n",
      "1321 ---- 1200\n",
      "1322 ---- 1201\n",
      "1323 ---- 1202\n",
      "1324 ---- 1203\n",
      "1325 ---- 1204\n",
      "1326 ---- 1205\n",
      "1327 ---- 1206\n",
      "1328 ---- 1207\n",
      "1329 ---- 1208\n",
      "1330 ---- 1209\n",
      "1331 ---- 1210\n",
      "1332 ---- 1211\n",
      "1333 ---- 1212\n",
      "1334 ---- 1213\n",
      "1335 ---- 1214\n",
      "1336 ---- 1215\n",
      "1337 ---- 1216\n",
      "1338 ---- 1217\n",
      "1339 ---- 1218\n",
      "1340 ---- 1219\n",
      "1341 ---- 1220\n",
      "1342 ---- 1221\n",
      "1343 ---- 1222\n",
      "1344 ---- 1223\n",
      "1345 ---- 1224\n",
      "1346 ---- 1225\n",
      "1347 ---- 1226\n",
      "1348 ---- 1227\n",
      "1349 ---- 1228\n",
      "1350 ---- 1229\n",
      "1351 ---- 1230\n",
      "1352 ---- 1231\n",
      "1353 ---- 1232\n",
      "1354 ---- 1233\n",
      "1355 ---- 1234\n",
      "1356 ---- 1235\n",
      "1357 ---- 1236\n",
      "1358 ---- 1237\n",
      "1359 ---- 1238\n",
      "1360 ---- 1239\n",
      "1361 ---- 1240\n",
      "1362 ---- 1241\n",
      "1363 ---- 1242\n",
      "1364 ---- 1243\n",
      "1365 ---- 1244\n",
      "1366 ---- 1245\n",
      "1367 ---- 1246\n",
      "1368 ---- 1247\n",
      "1369 ---- 1248\n",
      "1370 ---- 1249\n",
      "1371 ---- 1250\n",
      "1372 ---- 1251\n",
      "1373 ---- 1252\n",
      "1374 ---- 1253\n",
      "1375 ---- 1254\n",
      "1376 ---- 1255\n",
      "1377 ---- 1256\n",
      "1378 ---- 1257\n",
      "1379 ---- 1258\n",
      "1380 ---- 1259\n",
      "1381 ---- 1260\n",
      "1382 ---- 1261\n",
      "1383 ---- 1262\n",
      "1384 ---- 1263\n",
      "1385 ---- 1264\n",
      "1386 ---- 1265\n",
      "1387 ---- 1266\n",
      "1388 ---- 1267\n",
      "1389 ---- 1268\n",
      "1390 ---- 1269\n",
      "1391 ---- 1270\n",
      "1392 ---- 1271\n",
      "1393 ---- 1272\n",
      "1394 ---- 1273\n",
      "1395 ---- 1274\n",
      "1396 ---- 1275\n",
      "1397 ---- 1276\n",
      "1398 ---- 1277\n",
      "1399 ---- 1278\n",
      "1400 ---- 1279\n",
      "1401 ---- 1280\n",
      "1402 ---- 1281\n",
      "1403 ---- 1282\n",
      "1404 ---- 1283\n",
      "1405 ---- 1284\n",
      "1406 ---- 1285\n",
      "1407 ---- 1286\n",
      "1408 ---- 1287\n",
      "1409 ---- 1288\n",
      "1410 ---- 1289\n",
      "1411 ---- 1290\n",
      "1412 ---- 1291\n",
      "1413 ---- 1292\n",
      "1414 ---- 1293\n",
      "1415 ---- 1294\n",
      "1416 ---- 1295\n",
      "1417 ---- 1296\n",
      "1418 ---- 1297\n",
      "1419 ---- 1298\n",
      "1420 ---- 1299\n",
      "1421 ---- 1300\n",
      "1422 ---- 1301\n",
      "1423 ---- 1302\n",
      "1424 ---- 1303\n",
      "1425 ---- 1304\n",
      "1426 ---- 1305\n",
      "1427 ---- 1306\n",
      "1428 ---- 1307\n",
      "1429 ---- 1308\n",
      "1430 ---- 1309\n",
      "1431 ---- 1310\n",
      "1432 ---- 1311\n",
      "1433 ---- 1312\n",
      "1434 ---- 1313\n",
      "1435 ---- 1314\n",
      "1436 ---- 1315\n",
      "1437 ---- 1316\n",
      "1438 ---- 1317\n",
      "1439 ---- 1318\n",
      "1440 ---- 1319\n",
      "1441 ---- 1320\n",
      "1442 ---- 1321\n",
      "1443 ---- 1322\n",
      "1444 ---- 1323\n",
      "1445 ---- 1324\n",
      "1446 ---- 1325\n",
      "1447 ---- 1326\n",
      "1448 ---- 1327\n",
      "1449 ---- 1328\n",
      "1450 ---- 1329\n",
      "1451 ---- 1330\n",
      "1452 ---- 1331\n",
      "1453 ---- 1332\n",
      "1454 ---- 1333\n",
      "1455 ---- 1334\n",
      "1456 ---- 1335\n",
      "1457 ---- 1336\n",
      "1458 ---- 1337\n",
      "1459 ---- 1338\n",
      "1460 ---- 1339\n",
      "1461 ---- 1340\n",
      "1462 ---- 1341\n",
      "1463 ---- 1342\n",
      "1464 ---- 1343\n",
      "1465 ---- 1344\n",
      "1466 ---- 1345\n",
      "1467 ---- 1346\n",
      "1468 ---- 1347\n",
      "1469 ---- 1348\n",
      "1470 ---- 1349\n",
      "1471 ---- 1350\n",
      "1472 ---- 1351\n",
      "1473 ---- 1352\n",
      "1474 ---- 1353\n",
      "1475 ---- 1354\n",
      "1476 ---- 1355\n",
      "1477 ---- 1356\n",
      "1478 ---- 1357\n",
      "1479 ---- 1358\n",
      "1480 ---- 1359\n",
      "1481 ---- 1360\n",
      "1482 ---- 1361\n",
      "1483 ---- 1362\n",
      "1484 ---- 1363\n",
      "1485 ---- 1364\n",
      "1486 ---- 1365\n",
      "1487 ---- 1366\n",
      "1488 ---- 1367\n",
      "1489 ---- 1368\n",
      "1490 ---- 1369\n",
      "1491 ---- 1370\n",
      "1492 ---- 1371\n",
      "1493 ---- 1372\n",
      "1494 ---- 1373\n",
      "1495 ---- 1374\n",
      "1496 ---- 1375\n",
      "1497 ---- 1376\n",
      "1498 ---- 1377\n",
      "1499 ---- 1378\n",
      "1500 ---- 1379\n",
      "1501 ---- 1380\n",
      "1502 ---- 1381\n",
      "1503 ---- 1382\n",
      "1504 ---- 1383\n",
      "1505 ---- 1384\n",
      "1506 ---- 1385\n",
      "1507 ---- 1386\n",
      "1508 ---- 1387\n",
      "1509 ---- 1388\n",
      "1510 ---- 1389\n",
      "1511 ---- 1390\n",
      "1512 ---- 1391\n",
      "1513 ---- 1392\n",
      "1514 ---- 1393\n",
      "1515 ---- 1394\n",
      "1516 ---- 1395\n",
      "1517 ---- 1396\n",
      "1518 ---- 1397\n",
      "1519 ---- 1398\n",
      "1520 ---- 1399\n",
      "1521 ---- 1400\n",
      "1522 ---- 1401\n",
      "1523 ---- 1402\n",
      "1524 ---- 1403\n",
      "1525 ---- 1404\n",
      "1526 ---- 1405\n",
      "1527 ---- 1406\n",
      "1528 ---- 1407\n",
      "1529 ---- 1408\n",
      "1530 ---- 1409\n",
      "1531 ---- 1410\n",
      "1532 ---- 1411\n",
      "1533 ---- 1412\n",
      "1534 ---- 1413\n",
      "1535 ---- 1414\n",
      "1536 ---- 1415\n",
      "1537 ---- 1416\n",
      "1538 ---- 1417\n",
      "1539 ---- 1418\n",
      "1540 ---- 1419\n",
      "1541 ---- 1420\n",
      "1542 ---- 1421\n",
      "1543 ---- 1422\n",
      "1544 ---- 1423\n",
      "1545 ---- 1424\n",
      "1546 ---- 1425\n",
      "1547 ---- 1426\n",
      "1548 ---- 1427\n",
      "1549 ---- 1428\n",
      "1550 ---- 1429\n",
      "1551 ---- 1430\n",
      "1552 ---- 1431\n",
      "1553 ---- 1432\n",
      "1554 ---- 1433\n",
      "1555 ---- 1434\n",
      "1556 ---- 1435\n",
      "1557 ---- 1436\n",
      "1558 ---- 1437\n",
      "1559 ---- 1438\n",
      "1560 ---- 1439\n",
      "1561 ---- 1440\n",
      "1562 ---- 1441\n",
      "1563 ---- 1442\n",
      "1564 ---- 1443\n",
      "1565 ---- 1444\n",
      "1566 ---- 1445\n",
      "1567 ---- 1446\n",
      "1568 ---- 1447\n",
      "1569 ---- 1448\n",
      "1570 ---- 1449\n",
      "1571 ---- 1450\n",
      "1572 ---- 1451\n",
      "1573 ---- 1452\n",
      "1574 ---- 1453\n",
      "1575 ---- 1454\n",
      "1576 ---- 1455\n",
      "1577 ---- 1456\n",
      "1578 ---- 1457\n",
      "1579 ---- 1458\n",
      "1580 ---- 1459\n",
      "1581 ---- 1460\n",
      "1582 ---- 1461\n",
      "1583 ---- 1462\n",
      "1584 ---- 1463\n",
      "1585 ---- 1464\n",
      "1586 ---- 1465\n",
      "1587 ---- 1466\n",
      "1588 ---- 1467\n",
      "1589 ---- 1468\n",
      "1590 ---- 1469\n",
      "1591 ---- 1470\n",
      "1592 ---- 1471\n",
      "1593 ---- 1472\n",
      "1594 ---- 1473\n",
      "1595 ---- 1474\n",
      "1596 ---- 1475\n",
      "1597 ---- 1476\n",
      "1598 ---- 1477\n",
      "1599 ---- 1478\n",
      "1600 ---- 1479\n",
      "1601 ---- 1480\n",
      "1602 ---- 1481\n",
      "1603 ---- 1482\n",
      "1604 ---- 1483\n",
      "1605 ---- 1484\n",
      "1606 ---- 1485\n",
      "1607 ---- 1486\n",
      "1608 ---- 1487\n",
      "1609 ---- 1488\n",
      "1610 ---- 1489\n",
      "1611 ---- 1490\n",
      "1612 ---- 1491\n",
      "1613 ---- 1492\n",
      "1614 ---- 1493\n",
      "1615 ---- 1494\n",
      "1616 ---- 1495\n",
      "1617 ---- 1496\n",
      "1618 ---- 1497\n",
      "1619 ---- 1498\n",
      "1620 ---- 1499\n",
      "1621 ---- 1500\n",
      "1622 ---- 1501\n",
      "1623 ---- 1502\n",
      "1624 ---- 1503\n",
      "1625 ---- 1504\n",
      "1626 ---- 1505\n",
      "1627 ---- 1506\n",
      "1628 ---- 1507\n",
      "1629 ---- 1508\n",
      "1630 ---- 1509\n",
      "1631 ---- 1510\n",
      "1632 ---- 1511\n",
      "1633 ---- 1512\n",
      "1634 ---- 1513\n",
      "1635 ---- 1514\n",
      "1636 ---- 1515\n",
      "1637 ---- 1516\n",
      "1638 ---- 1517\n",
      "1639 ---- 1518\n",
      "1640 ---- 1519\n",
      "1641 ---- 1520\n",
      "1642 ---- 1521\n",
      "1643 ---- 1522\n",
      "1644 ---- 1523\n",
      "1645 ---- 1524\n",
      "1646 ---- 1525\n",
      "1647 ---- 1526\n",
      "1648 ---- 1527\n",
      "1649 ---- 1528\n",
      "1650 ---- 1529\n",
      "1651 ---- 1530\n",
      "1652 ---- 1531\n",
      "1653 ---- 1532\n",
      "1654 ---- 1533\n",
      "1655 ---- 1534\n",
      "1656 ---- 1535\n",
      "1657 ---- 1536\n",
      "1658 ---- 1537\n",
      "1659 ---- 1538\n",
      "1660 ---- 1539\n",
      "1661 ---- 1540\n",
      "1662 ---- 1541\n",
      "1663 ---- 1542\n",
      "1664 ---- 1543\n",
      "1665 ---- 1544\n",
      "1666 ---- 1545\n",
      "1667 ---- 1546\n",
      "1668 ---- 1547\n",
      "1669 ---- 1548\n",
      "1670 ---- 1549\n",
      "1671 ---- 1550\n",
      "1672 ---- 1551\n",
      "1673 ---- 1552\n",
      "1674 ---- 1553\n",
      "1675 ---- 1554\n",
      "1676 ---- 1555\n",
      "1677 ---- 1556\n",
      "1678 ---- 1557\n",
      "1679 ---- 1558\n",
      "1680 ---- 1559\n",
      "1681 ---- 1560\n",
      "1682 ---- 1561\n",
      "1683 ---- 1562\n",
      "1684 ---- 1563\n",
      "1685 ---- 1564\n",
      "1686 ---- 1565\n",
      "1687 ---- 1566\n",
      "1688 ---- 1567\n",
      "1689 ---- 1568\n",
      "1690 ---- 1569\n",
      "1691 ---- 1570\n",
      "1692 ---- 1571\n",
      "1693 ---- 1572\n",
      "1694 ---- 1573\n",
      "1695 ---- 1574\n",
      "1696 ---- 1575\n",
      "1697 ---- 1576\n",
      "1698 ---- 1577\n",
      "1699 ---- 1578\n",
      "1700 ---- 1579\n",
      "1701 ---- 1580\n",
      "1702 ---- 1581\n",
      "1703 ---- 1582\n",
      "1704 ---- 1583\n",
      "1705 ---- 1584\n",
      "1706 ---- 1585\n",
      "1707 ---- 1586\n",
      "1708 ---- 1587\n",
      "1709 ---- 1588\n",
      "1710 ---- 1589\n",
      "1711 ---- 1590\n",
      "1712 ---- 1591\n",
      "1713 ---- 1592\n",
      "1714 ---- 1593\n",
      "1715 ---- 1594\n",
      "1716 ---- 1595\n",
      "1717 ---- 1596\n",
      "1718 ---- 1597\n",
      "1719 ---- 1598\n",
      "1720 ---- 1599\n",
      "1721 ---- 1600\n",
      "1722 ---- 1601\n",
      "1723 ---- 1602\n",
      "1724 ---- 1603\n",
      "1725 ---- 1604\n",
      "1726 ---- 1605\n",
      "1727 ---- 1606\n",
      "1728 ---- 1607\n",
      "1729 ---- 1608\n",
      "1730 ---- 1609\n",
      "1731 ---- 1610\n",
      "1732 ---- 1611\n",
      "1733 ---- 1612\n",
      "1734 ---- 1613\n",
      "1735 ---- 1614\n",
      "1736 ---- 1615\n",
      "1737 ---- 1616\n",
      "1738 ---- 1617\n",
      "1739 ---- 1618\n",
      "1740 ---- 1619\n",
      "1741 ---- 1620\n",
      "1742 ---- 1621\n",
      "1743 ---- 1622\n",
      "1744 ---- 1623\n",
      "1745 ---- 1624\n",
      "1746 ---- 1625\n",
      "1747 ---- 1626\n",
      "1748 ---- 1627\n",
      "1749 ---- 1628\n",
      "1750 ---- 1629\n",
      "1751 ---- 1630\n",
      "1752 ---- 1631\n",
      "1753 ---- 1632\n",
      "1754 ---- 1633\n",
      "1755 ---- 1634\n",
      "1756 ---- 1635\n",
      "1757 ---- 1636\n",
      "1758 ---- 1637\n",
      "1759 ---- 1638\n",
      "1760 ---- 1639\n",
      "1761 ---- 1640\n",
      "1762 ---- 1641\n",
      "1763 ---- 1642\n",
      "1764 ---- 1643\n",
      "1765 ---- 1644\n",
      "1766 ---- 1645\n",
      "1767 ---- 1646\n",
      "1768 ---- 1647\n",
      "1769 ---- 1648\n",
      "1770 ---- 1649\n",
      "1771 ---- 1650\n",
      "1772 ---- 1651\n",
      "1773 ---- 1652\n",
      "1774 ---- 1653\n",
      "1775 ---- 1654\n",
      "1776 ---- 1655\n",
      "1777 ---- 1656\n",
      "1778 ---- 1657\n",
      "1779 ---- 1658\n",
      "1780 ---- 1659\n",
      "1781 ---- 1660\n",
      "1782 ---- 1661\n",
      "1783 ---- 1662\n",
      "1784 ---- 1663\n",
      "1785 ---- 1664\n",
      "1786 ---- 1665\n",
      "1787 ---- 1666\n",
      "1788 ---- 1667\n",
      "1789 ---- 1668\n",
      "1790 ---- 1669\n",
      "1791 ---- 1670\n",
      "1792 ---- 1671\n",
      "1793 ---- 1672\n",
      "1794 ---- 1673\n",
      "1795 ---- 1674\n",
      "1796 ---- 1675\n",
      "1797 ---- 1676\n",
      "1798 ---- 1677\n",
      "1799 ---- 1678\n",
      "1800 ---- 1679\n",
      "1801 ---- 1680\n",
      "1802 ---- 1681\n",
      "1803 ---- 1682\n",
      "1804 ---- 1683\n",
      "1805 ---- 1684\n",
      "1806 ---- 1685\n",
      "1807 ---- 1686\n",
      "1808 ---- 1687\n",
      "1809 ---- 1688\n",
      "1810 ---- 1689\n",
      "1811 ---- 1690\n",
      "1812 ---- 1691\n",
      "1813 ---- 1692\n",
      "1814 ---- 1693\n",
      "1815 ---- 1694\n",
      "1816 ---- 1695\n",
      "1817 ---- 1696\n",
      "1818 ---- 1697\n",
      "1819 ---- 1698\n",
      "1820 ---- 1699\n",
      "1821 ---- 1700\n",
      "1822 ---- 1701\n",
      "1823 ---- 1702\n",
      "1824 ---- 1703\n",
      "1825 ---- 1704\n",
      "1826 ---- 1705\n",
      "1827 ---- 1706\n",
      "1828 ---- 1707\n",
      "1829 ---- 1708\n",
      "1830 ---- 1709\n",
      "1831 ---- 1710\n",
      "1832 ---- 1711\n",
      "1833 ---- 1712\n",
      "1834 ---- 1713\n",
      "1835 ---- 1714\n",
      "1836 ---- 1715\n",
      "1837 ---- 1716\n",
      "1838 ---- 1717\n",
      "1839 ---- 1718\n",
      "1840 ---- 1719\n",
      "1841 ---- 1720\n",
      "1842 ---- 1721\n",
      "1843 ---- 1722\n",
      "1844 ---- 1723\n",
      "1845 ---- 1724\n",
      "1846 ---- 1725\n",
      "1847 ---- 1726\n",
      "1848 ---- 1727\n",
      "1849 ---- 1728\n",
      "1850 ---- 1729\n",
      "1851 ---- 1730\n",
      "1852 ---- 1731\n",
      "1853 ---- 1732\n",
      "1854 ---- 1733\n",
      "1855 ---- 1734\n",
      "1856 ---- 1735\n",
      "1857 ---- 1736\n",
      "1858 ---- 1737\n",
      "1859 ---- 1738\n",
      "1860 ---- 1739\n",
      "1861 ---- 1740\n",
      "1862 ---- 1741\n",
      "1863 ---- 1742\n",
      "1864 ---- 1743\n",
      "1865 ---- 1744\n",
      "1866 ---- 1745\n",
      "1867 ---- 1746\n",
      "1868 ---- 1747\n",
      "1869 ---- 1748\n",
      "1870 ---- 1749\n",
      "1871 ---- 1750\n",
      "1872 ---- 1751\n",
      "1873 ---- 1752\n",
      "1874 ---- 1753\n",
      "1875 ---- 1754\n",
      "1876 ---- 1755\n",
      "1877 ---- 1756\n",
      "1878 ---- 1757\n",
      "1879 ---- 1758\n",
      "1880 ---- 1759\n",
      "1881 ---- 1760\n",
      "1882 ---- 1761\n",
      "1883 ---- 1762\n",
      "1884 ---- 1763\n",
      "1885 ---- 1764\n",
      "1886 ---- 1765\n",
      "1887 ---- 1766\n",
      "1888 ---- 1767\n",
      "1889 ---- 1768\n",
      "1890 ---- 1769\n",
      "1891 ---- 1770\n",
      "1892 ---- 1771\n",
      "1893 ---- 1772\n",
      "1894 ---- 1773\n",
      "1895 ---- 1774\n",
      "1896 ---- 1775\n",
      "1897 ---- 1776\n",
      "1898 ---- 1777\n",
      "1899 ---- 1778\n",
      "1900 ---- 1779\n",
      "1901 ---- 1780\n",
      "1902 ---- 1781\n",
      "1903 ---- 1782\n",
      "1904 ---- 1783\n",
      "1905 ---- 1784\n",
      "1906 ---- 1785\n",
      "1907 ---- 1786\n",
      "1908 ---- 1787\n",
      "1909 ---- 1788\n",
      "1910 ---- 1789\n",
      "1911 ---- 1790\n",
      "1912 ---- 1791\n",
      "1913 ---- 1792\n",
      "1914 ---- 1793\n",
      "1915 ---- 1794\n",
      "1916 ---- 1795\n",
      "1917 ---- 1796\n",
      "1918 ---- 1797\n",
      "1919 ---- 1798\n",
      "1920 ---- 1799\n",
      "1921 ---- 1800\n",
      "1922 ---- 1801\n",
      "1923 ---- 1802\n",
      "1924 ---- 1803\n",
      "1925 ---- 1804\n",
      "1926 ---- 1805\n",
      "1927 ---- 1806\n",
      "1928 ---- 1807\n",
      "1929 ---- 1808\n",
      "1930 ---- 1809\n",
      "1931 ---- 1810\n",
      "1932 ---- 1811\n",
      "1933 ---- 1812\n",
      "1934 ---- 1813\n",
      "1935 ---- 1814\n",
      "1936 ---- 1815\n",
      "1937 ---- 1816\n",
      "1938 ---- 1817\n",
      "1939 ---- 1818\n",
      "1940 ---- 1819\n",
      "1941 ---- 1820\n",
      "1942 ---- 1821\n",
      "1943 ---- 1822\n",
      "1944 ---- 1823\n",
      "1945 ---- 1824\n",
      "1946 ---- 1825\n",
      "1947 ---- 1826\n",
      "1948 ---- 1827\n",
      "1949 ---- 1828\n",
      "1950 ---- 1829\n",
      "1951 ---- 1830\n",
      "1952 ---- 1831\n",
      "1953 ---- 1832\n",
      "1954 ---- 1833\n",
      "1955 ---- 1834\n",
      "1956 ---- 1835\n",
      "1957 ---- 1836\n",
      "1958 ---- 1837\n",
      "1959 ---- 1838\n",
      "1960 ---- 1839\n",
      "1961 ---- 1840\n",
      "1962 ---- 1841\n",
      "1963 ---- 1842\n",
      "1964 ---- 1843\n",
      "1965 ---- 1844\n",
      "1966 ---- 1845\n",
      "1967 ---- 1846\n",
      "1968 ---- 1847\n",
      "1969 ---- 1848\n",
      "1970 ---- 1849\n",
      "1971 ---- 1850\n",
      "1972 ---- 1851\n",
      "1973 ---- 1852\n",
      "1974 ---- 1853\n",
      "1975 ---- 1854\n",
      "1976 ---- 1855\n",
      "1977 ---- 1856\n",
      "1978 ---- 1857\n",
      "1979 ---- 1858\n",
      "1980 ---- 1859\n",
      "1981 ---- 1860\n",
      "1982 ---- 1861\n",
      "1983 ---- 1862\n",
      "1984 ---- 1863\n",
      "1985 ---- 1864\n",
      "1986 ---- 1865\n",
      "1987 ---- 1866\n",
      "1988 ---- 1867\n",
      "1989 ---- 1868\n",
      "1990 ---- 1869\n",
      "1991 ---- 1870\n",
      "1992 ---- 1871\n",
      "1993 ---- 1872\n",
      "1994 ---- 1873\n",
      "1995 ---- 1874\n",
      "1996 ---- 1875\n",
      "1997 ---- 1876\n",
      "1998 ---- 1877\n",
      "1999 ---- 1878\n",
      "2000 ---- 1879\n",
      "2001 ---- 1880\n",
      "2002 ---- 1881\n",
      "2003 ---- 1882\n",
      "2004 ---- 1883\n",
      "2005 ---- 1884\n",
      "2006 ---- 1885\n",
      "2007 ---- 1886\n",
      "2008 ---- 1887\n",
      "2009 ---- 1888\n",
      "2010 ---- 1889\n",
      "2011 ---- 1890\n",
      "2012 ---- 1891\n",
      "2013 ---- 1892\n",
      "2014 ---- 1893\n",
      "2015 ---- 1894\n",
      "2016 ---- 1895\n",
      "2017 ---- 1896\n",
      "2018 ---- 1897\n",
      "2019 ---- 1898\n",
      "2020 ---- 1899\n",
      "2021 ---- 1900\n",
      "2022 ---- 1901\n",
      "2023 ---- 1902\n",
      "2024 ---- 1903\n",
      "2025 ---- 1904\n",
      "2026 ---- 1905\n",
      "2027 ---- 1906\n",
      "2028 ---- 1907\n",
      "2029 ---- 1908\n",
      "2030 ---- 1909\n",
      "2031 ---- 1910\n",
      "2032 ---- 1911\n",
      "2033 ---- 1912\n",
      "2034 ---- 1913\n",
      "2035 ---- 1914\n",
      "2036 ---- 1915\n",
      "2037 ---- 1916\n",
      "2038 ---- 1917\n",
      "2039 ---- 1918\n",
      "2040 ---- 1919\n",
      "2041 ---- 1920\n",
      "2042 ---- 1921\n",
      "2043 ---- 1922\n",
      "2044 ---- 1923\n",
      "2045 ---- 1924\n",
      "2046 ---- 1925\n",
      "2047 ---- 1926\n",
      "2048 ---- 1927\n",
      "2049 ---- 1928\n",
      "2050 ---- 1929\n",
      "2051 ---- 1930\n",
      "2052 ---- 1931\n",
      "2053 ---- 1932\n",
      "2054 ---- 1933\n",
      "2055 ---- 1934\n",
      "2056 ---- 1935\n",
      "2057 ---- 1936\n",
      "2058 ---- 1937\n",
      "2059 ---- 1938\n",
      "2060 ---- 1939\n",
      "2061 ---- 1940\n",
      "2062 ---- 1941\n",
      "2063 ---- 1942\n",
      "2064 ---- 1943\n",
      "2065 ---- 1944\n",
      "2066 ---- 1945\n",
      "2067 ---- 1946\n",
      "2068 ---- 1947\n",
      "2069 ---- 1948\n",
      "2070 ---- 1949\n",
      "2071 ---- 1950\n",
      "2072 ---- 1951\n",
      "2073 ---- 1952\n",
      "2074 ---- 1953\n",
      "2075 ---- 1954\n",
      "2076 ---- 1955\n",
      "2077 ---- 1956\n",
      "2078 ---- 1957\n",
      "2079 ---- 1958\n",
      "2080 ---- 1959\n",
      "2081 ---- 1960\n",
      "2082 ---- 1961\n",
      "2083 ---- 1962\n",
      "2084 ---- 1963\n",
      "2085 ---- 1964\n",
      "2086 ---- 1965\n",
      "2087 ---- 1966\n",
      "2088 ---- 1967\n",
      "2089 ---- 1968\n",
      "2090 ---- 1969\n",
      "2091 ---- 1970\n",
      "2092 ---- 1971\n",
      "2093 ---- 1972\n",
      "2094 ---- 1973\n",
      "2095 ---- 1974\n",
      "2096 ---- 1975\n",
      "2097 ---- 1976\n",
      "2098 ---- 1977\n",
      "2099 ---- 1978\n",
      "2100 ---- 1979\n",
      "2101 ---- 1980\n",
      "2102 ---- 1981\n",
      "2103 ---- 1982\n",
      "2104 ---- 1983\n",
      "2105 ---- 1984\n",
      "2106 ---- 1985\n",
      "2107 ---- 1986\n",
      "2108 ---- 1987\n",
      "2109 ---- 1988\n",
      "2110 ---- 1989\n",
      "2111 ---- 1990\n",
      "2112 ---- 1991\n",
      "2113 ---- 1992\n",
      "2114 ---- 1993\n",
      "2115 ---- 1994\n",
      "2116 ---- 1995\n",
      "2117 ---- 1996\n",
      "2118 ---- 1997\n",
      "2119 ---- 1998\n",
      "2120 ---- 1999\n",
      "2121 ---- 2000\n",
      "2122 ---- 2001\n",
      "2123 ---- 2002\n",
      "2124 ---- 2003\n",
      "2125 ---- 2004\n",
      "2126 ---- 2005\n",
      "2127 ---- 2006\n",
      "2128 ---- 2007\n",
      "2129 ---- 2008\n",
      "2130 ---- 2009\n",
      "2131 ---- 2010\n",
      "2132 ---- 2011\n",
      "2133 ---- 2012\n",
      "2134 ---- 2013\n",
      "2135 ---- 2014\n",
      "2136 ---- 2015\n",
      "2137 ---- 2016\n",
      "2138 ---- 2017\n",
      "2139 ---- 2018\n",
      "2140 ---- 2019\n",
      "2141 ---- 2020\n",
      "2142 ---- 2021\n",
      "2143 ---- 2022\n",
      "2144 ---- 2023\n",
      "2145 ---- 2024\n",
      "2146 ---- 2025\n",
      "2147 ---- 2026\n",
      "2148 ---- 2027\n",
      "2149 ---- 2028\n",
      "2150 ---- 2029\n",
      "2151 ---- 2030\n",
      "2152 ---- 2031\n",
      "2153 ---- 2032\n",
      "2154 ---- 2033\n",
      "2155 ---- 2034\n",
      "2156 ---- 2035\n",
      "2157 ---- 2036\n",
      "2158 ---- 2037\n",
      "2159 ---- 2038\n",
      "2160 ---- 2039\n",
      "2161 ---- 2040\n",
      "2162 ---- 2041\n",
      "2163 ---- 2042\n",
      "2164 ---- 2043\n",
      "2165 ---- 2044\n",
      "2166 ---- 2045\n",
      "2167 ---- 2046\n",
      "2168 ---- 2047\n",
      "2169 ---- 2048\n",
      "2170 ---- 2049\n",
      "2171 ---- 2050\n",
      "2172 ---- 2051\n",
      "2173 ---- 2052\n",
      "2174 ---- 2053\n",
      "2175 ---- 2054\n",
      "2176 ---- 2055\n",
      "2177 ---- 2056\n",
      "2178 ---- 2057\n",
      "2179 ---- 2058\n",
      "2180 ---- 2059\n",
      "2181 ---- 2060\n",
      "2182 ---- 2061\n",
      "2183 ---- 2062\n",
      "2184 ---- 2063\n",
      "2185 ---- 2064\n",
      "2186 ---- 2065\n",
      "2187 ---- 2066\n",
      "2188 ---- 2067\n",
      "2189 ---- 2068\n",
      "2190 ---- 2069\n",
      "2191 ---- 2070\n",
      "2192 ---- 2071\n",
      "2193 ---- 2072\n",
      "2194 ---- 2073\n",
      "2195 ---- 2074\n",
      "2196 ---- 2075\n",
      "2197 ---- 2076\n",
      "2198 ---- 2077\n",
      "2199 ---- 2078\n",
      "2200 ---- 2079\n",
      "2201 ---- 2080\n",
      "2202 ---- 2081\n",
      "2203 ---- 2082\n",
      "2204 ---- 2083\n",
      "2205 ---- 2084\n",
      "2206 ---- 2085\n",
      "2207 ---- 2086\n",
      "2208 ---- 2087\n",
      "2209 ---- 2088\n",
      "2210 ---- 2089\n",
      "2211 ---- 2090\n",
      "2212 ---- 2091\n",
      "2213 ---- 2092\n",
      "2214 ---- 2093\n",
      "2215 ---- 2094\n",
      "2216 ---- 2095\n",
      "2217 ---- 2096\n",
      "2218 ---- 2097\n",
      "2219 ---- 2098\n",
      "2220 ---- 2099\n",
      "2221 ---- 2100\n",
      "2222 ---- 2101\n",
      "2223 ---- 2102\n",
      "2224 ---- 2103\n",
      "2225 ---- 2104\n",
      "2226 ---- 2105\n",
      "2227 ---- 2106\n",
      "2228 ---- 2107\n",
      "2229 ---- 2108\n",
      "2230 ---- 2109\n",
      "2231 ---- 2110\n",
      "2232 ---- 2111\n",
      "2233 ---- 2112\n",
      "2234 ---- 2113\n",
      "2235 ---- 2114\n",
      "2236 ---- 2115\n",
      "2237 ---- 2116\n",
      "2238 ---- 2117\n",
      "2239 ---- 2118\n",
      "2240 ---- 2119\n",
      "2241 ---- 2120\n",
      "2242 ---- 2121\n",
      "2243 ---- 2122\n",
      "2244 ---- 2123\n",
      "2245 ---- 2124\n",
      "2246 ---- 2125\n",
      "2247 ---- 2126\n",
      "2248 ---- 2127\n",
      "2249 ---- 2128\n",
      "2250 ---- 2129\n",
      "2251 ---- 2130\n",
      "2252 ---- 2131\n",
      "2253 ---- 2132\n",
      "2254 ---- 2133\n",
      "2255 ---- 2134\n",
      "2256 ---- 2135\n",
      "2257 ---- 2136\n",
      "2258 ---- 2137\n",
      "2259 ---- 2138\n",
      "2260 ---- 2139\n",
      "2261 ---- 2140\n",
      "2262 ---- 2141\n",
      "2263 ---- 2142\n",
      "2264 ---- 2143\n",
      "2265 ---- 2144\n",
      "2266 ---- 2145\n",
      "2267 ---- 2146\n",
      "2268 ---- 2147\n",
      "2269 ---- 2148\n",
      "2270 ---- 2149\n",
      "2271 ---- 2150\n",
      "2272 ---- 2151\n",
      "2273 ---- 2152\n",
      "2274 ---- 2153\n",
      "2275 ---- 2154\n",
      "2276 ---- 2155\n",
      "2277 ---- 2156\n",
      "2278 ---- 2157\n",
      "2279 ---- 2158\n",
      "2280 ---- 2159\n",
      "2281 ---- 2160\n",
      "2282 ---- 2161\n",
      "2283 ---- 2162\n",
      "2284 ---- 2163\n",
      "2285 ---- 2164\n",
      "2286 ---- 2165\n",
      "2287 ---- 2166\n",
      "2288 ---- 2167\n",
      "2289 ---- 2168\n",
      "2290 ---- 2169\n",
      "2291 ---- 2170\n",
      "2292 ---- 2171\n",
      "2293 ---- 2172\n",
      "2294 ---- 2173\n",
      "2295 ---- 2174\n",
      "2296 ---- 2175\n",
      "2297 ---- 2176\n",
      "2298 ---- 2177\n",
      "2299 ---- 2178\n",
      "2300 ---- 2179\n",
      "2301 ---- 2180\n",
      "2302 ---- 2181\n",
      "2303 ---- 2182\n",
      "2304 ---- 2183\n",
      "2305 ---- 2184\n",
      "2306 ---- 2185\n",
      "2307 ---- 2186\n",
      "2308 ---- 2187\n",
      "2309 ---- 2188\n",
      "2310 ---- 2189\n",
      "2311 ---- 2190\n",
      "2312 ---- 2191\n",
      "2313 ---- 2192\n",
      "2314 ---- 2193\n",
      "2315 ---- 2194\n",
      "2316 ---- 2195\n",
      "2317 ---- 2196\n",
      "2318 ---- 2197\n",
      "2319 ---- 2198\n",
      "2320 ---- 2199\n",
      "2321 ---- 2200\n",
      "2322 ---- 2201\n",
      "2323 ---- 2202\n",
      "2324 ---- 2203\n",
      "2325 ---- 2204\n",
      "2326 ---- 2205\n",
      "2327 ---- 2206\n",
      "2328 ---- 2207\n",
      "2329 ---- 2208\n",
      "2330 ---- 2209\n",
      "2331 ---- 2210\n",
      "2332 ---- 2211\n",
      "2333 ---- 2212\n",
      "2334 ---- 2213\n",
      "2335 ---- 2214\n",
      "2336 ---- 2215\n",
      "2337 ---- 2216\n",
      "2338 ---- 2217\n",
      "2339 ---- 2218\n",
      "2340 ---- 2219\n",
      "2341 ---- 2220\n",
      "2342 ---- 2221\n",
      "2343 ---- 2222\n",
      "2344 ---- 2223\n",
      "2345 ---- 2224\n",
      "2346 ---- 2225\n",
      "2347 ---- 2226\n",
      "2348 ---- 2227\n",
      "2349 ---- 2228\n",
      "2350 ---- 2229\n",
      "2351 ---- 2230\n",
      "2352 ---- 2231\n",
      "2353 ---- 2232\n",
      "2354 ---- 2233\n",
      "2355 ---- 2234\n",
      "2356 ---- 2235\n",
      "2357 ---- 2236\n",
      "2358 ---- 2237\n",
      "2359 ---- 2238\n",
      "2360 ---- 2239\n",
      "2361 ---- 2240\n",
      "2362 ---- 2241\n",
      "2363 ---- 2242\n",
      "2364 ---- 2243\n",
      "2365 ---- 2244\n",
      "2366 ---- 2245\n",
      "2367 ---- 2246\n",
      "2368 ---- 2247\n",
      "2369 ---- 2248\n",
      "2370 ---- 2249\n",
      "2371 ---- 2250\n",
      "2372 ---- 2251\n",
      "2373 ---- 2252\n",
      "2374 ---- 2253\n",
      "2375 ---- 2254\n",
      "2376 ---- 2255\n",
      "2377 ---- 2256\n",
      "2378 ---- 2257\n",
      "2379 ---- 2258\n",
      "2380 ---- 2259\n",
      "2381 ---- 2260\n",
      "2382 ---- 2261\n",
      "2383 ---- 2262\n",
      "2384 ---- 2263\n",
      "2385 ---- 2264\n",
      "2386 ---- 2265\n",
      "2387 ---- 2266\n",
      "2388 ---- 2267\n",
      "2389 ---- 2268\n",
      "2390 ---- 2269\n",
      "2391 ---- 2270\n",
      "2392 ---- 2271\n",
      "2393 ---- 2272\n",
      "2394 ---- 2273\n",
      "2395 ---- 2274\n",
      "2396 ---- 2275\n",
      "2397 ---- 2276\n",
      "2398 ---- 2277\n",
      "2399 ---- 2278\n",
      "2400 ---- 2279\n",
      "2401 ---- 2280\n",
      "2402 ---- 2281\n",
      "2403 ---- 2282\n",
      "2404 ---- 2283\n",
      "2405 ---- 2284\n",
      "2406 ---- 2285\n",
      "2407 ---- 2286\n",
      "2408 ---- 2287\n",
      "2409 ---- 2288\n",
      "2410 ---- 2289\n",
      "2411 ---- 2290\n",
      "2412 ---- 2291\n",
      "2413 ---- 2292\n",
      "2414 ---- 2293\n",
      "2415 ---- 2294\n",
      "2416 ---- 2295\n",
      "2417 ---- 2296\n",
      "2418 ---- 2297\n",
      "2419 ---- 2298\n",
      "2420 ---- 2299\n",
      "2421 ---- 2300\n",
      "2422 ---- 2301\n",
      "2423 ---- 2302\n",
      "2424 ---- 2303\n",
      "2425 ---- 2304\n",
      "2426 ---- 2305\n",
      "2427 ---- 2306\n",
      "2428 ---- 2307\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(X_train_final.columns)):\n",
    "    print('{} ---- {}'.format(i, X_train_final.columns[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Unigrams, POS Tag Count, Sentiment Polarity, Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_1 = X_train_final.iloc[:,np.r_[10:12,13:21,121:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4233, 2318)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_1 = X_test_final.iloc[:,np.r_[10:12,13:21,121:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 2318)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.408\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9460    0.8083    0.8717       412\n",
      "           1     0.3361    0.6780    0.4494        59\n",
      "\n",
      "    accuracy                         0.7919       471\n",
      "   macro avg     0.6411    0.7431    0.6606       471\n",
      "weighted avg     0.8696    0.7919    0.8188       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_1_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_1_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_1,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_1), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 333, False Positive: 79, False Negative: 19, True Positive: 40\n",
      "--------------------------------------------------------------------------------\n",
      "[[333  79]\n",
      " [ 19  40]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.81      0.87       412\n",
      "           1       0.34      0.68      0.45        59\n",
      "\n",
      "    accuracy                           0.79       471\n",
      "   macro avg       0.64      0.74      0.66       471\n",
      "weighted avg       0.87      0.79      0.82       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_1 = LogisticRegression(random_state=18, \n",
    "                                solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], \n",
    "                                class_weight='balanced').fit(X_train_model_1, y_train)\n",
    "y_lr = lr_model_1.predict(X_test_model_1)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_model_2 = X_train_final.iloc[:,np.r_[3:1113]]\n",
    "X_train_model_2 = X_train_final.iloc[:, np.r_[3:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4233, 2426)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_2 = X_test_final.iloc[:,np.r_[3:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 2426)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.418\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9529    0.7864    0.8617       412\n",
      "           1     0.3282    0.7288    0.4526        59\n",
      "\n",
      "    accuracy                         0.7792       471\n",
      "   macro avg     0.6406    0.7576    0.6572       471\n",
      "weighted avg     0.8747    0.7792    0.8105       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_2_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_2_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_2,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_2), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 324, False Positive: 88, False Negative: 16, True Positive: 43\n",
      "--------------------------------------------------------------------------------\n",
      "[[324  88]\n",
      " [ 16  43]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.79      0.86       412\n",
      "           1       0.33      0.73      0.45        59\n",
      "\n",
      "    accuracy                           0.78       471\n",
      "   macro avg       0.64      0.76      0.66       471\n",
      "weighted avg       0.87      0.78      0.81       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_2 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_2, y_train)\n",
    "y_lr = lr_model_2.predict(X_test_model_2)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Without Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_3 = X_train_final.iloc[:,np.r_[3:121]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4233, 118)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_3 = X_test_final.iloc[:,np.r_[3:121]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 118)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n",
      "Best score: 0.388\n",
      "Best parameters set:\n",
      "\tclf__C: 5\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9503    0.6966    0.8039       412\n",
      "           1     0.2604    0.7458    0.3860        59\n",
      "\n",
      "    accuracy                         0.7028       471\n",
      "   macro avg     0.6053    0.7212    0.5949       471\n",
      "weighted avg     0.8639    0.7028    0.7516       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_3_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.0001, 0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\", \"elasticnet\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_3_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_3,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_3), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 287, False Positive: 125, False Negative: 15, True Positive: 44\n",
      "--------------------------------------------------------------------------------\n",
      "[[287 125]\n",
      " [ 15  44]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.70      0.80       412\n",
      "           1       0.26      0.75      0.39        59\n",
      "\n",
      "    accuracy                           0.70       471\n",
      "   macro avg       0.61      0.72      0.59       471\n",
      "weighted avg       0.86      0.70      0.75       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_3 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_3, y_train)\n",
    "y_lr = lr_model_3.predict(X_test_model_3)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Without Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_4 = X_train_final.iloc[:,np.r_[3:21,121:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4233, 2326)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_4 = X_test_final.iloc[:,np.r_[3:21,121:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 2326)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.401\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9460    0.8083    0.8717       412\n",
      "           1     0.3361    0.6780    0.4494        59\n",
      "\n",
      "    accuracy                         0.7919       471\n",
      "   macro avg     0.6411    0.7431    0.6606       471\n",
      "weighted avg     0.8696    0.7919    0.8188       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_4_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_4_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_4,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_4), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 333, False Positive: 79, False Negative: 19, True Positive: 40\n",
      "--------------------------------------------------------------------------------\n",
      "[[333  79]\n",
      " [ 19  40]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.81      0.87       412\n",
      "           1       0.34      0.68      0.45        59\n",
      "\n",
      "    accuracy                           0.79       471\n",
      "   macro avg       0.64      0.74      0.66       471\n",
      "weighted avg       0.87      0.79      0.82       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_4 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_4, y_train)\n",
    "y_lr = lr_model_4.predict(X_test_model_4)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Without POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_5 = X_train_final.iloc[:,np.r_[3:13,21:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4233, 2418)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_5 = X_test_final.iloc[:,np.r_[3:13,21:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 2418)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.415\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.79      0.86       412\n",
      "           1       0.33      0.71      0.45        59\n",
      "\n",
      "    accuracy                           0.78       471\n",
      "   macro avg       0.64      0.75      0.66       471\n",
      "weighted avg       0.87      0.78      0.81       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_5_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_5_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_5,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_5), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 326, False Positive: 86, False Negative: 17, True Positive: 42\n",
      "--------------------------------------------------------------------------------\n",
      "[[326  86]\n",
      " [ 17  42]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.79      0.86       412\n",
      "           1       0.33      0.71      0.45        59\n",
      "\n",
      "    accuracy                           0.78       471\n",
      "   macro avg       0.64      0.75      0.66       471\n",
      "weighted avg       0.87      0.78      0.81       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_5 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_5, y_train)\n",
    "y_lr = lr_model_5.predict(X_test_model_5)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6: Without STEM Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_6 = X_train_final.iloc[:,np.r_[10:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4233, 2419)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_6 = X_test_final.iloc[:,np.r_[10:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 2419)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.425\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.79      0.87       412\n",
      "           1       0.34      0.75      0.47        59\n",
      "\n",
      "    accuracy                           0.79       471\n",
      "   macro avg       0.65      0.77      0.67       471\n",
      "weighted avg       0.88      0.79      0.82       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_6_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_6_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_6,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_6), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 326, False Positive: 86, False Negative: 15, True Positive: 44\n",
      "--------------------------------------------------------------------------------\n",
      "[[326  86]\n",
      " [ 15  44]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.79      0.87       412\n",
      "           1       0.34      0.75      0.47        59\n",
      "\n",
      "    accuracy                           0.79       471\n",
      "   macro avg       0.65      0.77      0.67       471\n",
      "weighted avg       0.88      0.79      0.82       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_6 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_6, y_train)\n",
    "y_lr = lr_model_6.predict(X_test_model_6)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7: Without Sentiment Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_7 = X_train_final.iloc[:,np.r_[3:10,12:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4233, 2424)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_7 = X_test_final.iloc[:,np.r_[3:10,12:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 2424)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.420\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.79      0.86       412\n",
      "           1       0.33      0.71      0.45        59\n",
      "\n",
      "    accuracy                           0.78       471\n",
      "   macro avg       0.64      0.75      0.66       471\n",
      "weighted avg       0.87      0.78      0.81       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_7_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_7_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_7,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_7), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 326, False Positive: 86, False Negative: 17, True Positive: 42\n",
      "--------------------------------------------------------------------------------\n",
      "[[326  86]\n",
      " [ 17  42]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.79      0.86       412\n",
      "           1       0.33      0.71      0.45        59\n",
      "\n",
      "    accuracy                           0.78       471\n",
      "   macro avg       0.64      0.75      0.66       471\n",
      "weighted avg       0.87      0.78      0.81       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_7 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_7, y_train)\n",
    "y_lr = lr_model_7.predict(X_test_model_7)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 8: Without NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_8 = X_train_final.iloc[:,np.r_[3:12,13:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4233, 2425)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_8 = X_test_final.iloc[:,np.r_[3:12,13:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 2425)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.418\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.79      0.86       412\n",
      "           1       0.33      0.73      0.45        59\n",
      "\n",
      "    accuracy                           0.78       471\n",
      "   macro avg       0.64      0.76      0.66       471\n",
      "weighted avg       0.87      0.78      0.81       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_8_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_8_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_8,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_8), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 324, False Positive: 88, False Negative: 16, True Positive: 43\n",
      "--------------------------------------------------------------------------------\n",
      "[[324  88]\n",
      " [ 16  43]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.79      0.86       412\n",
      "           1       0.33      0.73      0.45        59\n",
      "\n",
      "    accuracy                           0.78       471\n",
      "   macro avg       0.64      0.76      0.66       471\n",
      "weighted avg       0.87      0.78      0.81       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_8 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_8, y_train)\n",
    "y_lr = lr_model_8.predict(X_test_model_8)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Summary\n",
    "| Experiment | Model Number | Features Used                                                | Precision | Recall | Macro F1 |\n",
    "| :--------: | :----------: | :----------------------------------------------------------: | :-------: | :----: | :------: |\n",
    "| Baseline   | 1            | Unigrams, POS Tag Count, Sentiment Polarity and Subjectivity | 0.64      | 0.74   | 0.66     |\n",
    "| Baseline   | 2            | All features (baseline)                                      | 0.64      | 0.76   | 0.66     |\n",
    "| Baseline   | 3            | Without Unigrams                                             | 0.61      | 0.72   | 0.59     |\n",
    "| Baseline   | 4            | Without Embeddings                                           | 0.64      | 0.74   | 0.66     |\n",
    "| Baseline   | 5            | Without POS tag                                              | 0.64      | 0.75   | 0.66     |\n",
    "| Baseline   | 6            | Without STEM similarity (paper baseline)                     | 0.65      | 0.77   | 0.67     |\n",
    "| Baseline   | 7            | Without sentiment features                                   | 0.64      | 0.75   | 0.66     |\n",
    "| Baseline   | 8            | Without NER                                                  | 0.64      | 0.76   | 0.66     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
