{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Familial Logistic Regression Models Using Merged Data Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26fdb8381f874218a61e936ec7ce5051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 07:55:09 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-10-02 07:55:09 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-10-02 07:55:11 INFO: File exists: /Users/gbaldonado/stanza_resources/en/default.zip\n",
      "2024-10-02 07:55:14 INFO: Finished downloading models and saved to /Users/gbaldonado/stanza_resources\n",
      "2024-10-02 07:55:14 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851bb01e64e84c20ac5c663acc79fa4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 07:55:14 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-10-02 07:55:15 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-10-02 07:55:15 INFO: Using device: cpu\n",
      "2024-10-02 07:55:15 INFO: Loading: tokenize\n",
      "2024-10-02 07:55:15 INFO: Loading: mwt\n",
      "2024-10-02 07:55:15 INFO: Loading: pos\n",
      "2024-10-02 07:55:16 INFO: Loading: lemma\n",
      "2024-10-02 07:55:16 INFO: Loading: constituency\n",
      "2024-10-02 07:55:16 INFO: Loading: depparse\n",
      "2024-10-02 07:55:16 INFO: Loading: sentiment\n",
      "2024-10-02 07:55:16 INFO: Loading: ner\n",
      "2024-10-02 07:55:17 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import pickle\n",
    "import warnings\n",
    "import stanza\n",
    "\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, f1_score, r2_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Set random seed\n",
    "random.seed(18)\n",
    "seed = 18\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Initialize lemmatizer, stop words, and stanza\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline('en') # initialize English neural pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data and quick exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_familial_df_batch_1 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/merged_Familial_sentence_level_batch_1_jaccard.csv\", encoding='utf-8')\n",
    "merged_familial_df_batch_2 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/Familial Plus_sentence_level_batch_2_jaccard.csv\", encoding='utf-8')\n",
    "\n",
    "merged_familial_df = pd.concat([merged_familial_df_batch_1, merged_familial_df_batch_2])\n",
    "\n",
    "# Shuffle the merged dataset\n",
    "merged_familial_df = shuffle(merged_familial_df, random_state=seed)\n",
    "\n",
    "# Train-test split \n",
    "training_df, test_df = train_test_split(merged_familial_df, test_size=0.1, random_state=42, stratify=merged_familial_df['label'])\n",
    "\n",
    "training_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am lucky i have the freedom and priveledge to be here.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I have enough family support, geographical base, and school to be able to go to a four year University doing what I want to be doing.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the main reason i chose this class is because it is a ge requirement.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I want to make myself proud and my family. My parents are my motivators, they work so hard to give me everything they can and I want all their efforts to be worth it.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im 'iustus trying et muros hos gradus est ultimum cursum et related scientia semper est accipere prius non sum liber.</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"I'm also here because of decisions made by my ancestors and myself to survive so far.\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im the first in my family to attend college.</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"I'm the first in my family to attend college.\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i want to be able to work together with my peers during lab so that i can learn from them and develop my skills as an engineer.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I am the first person in my family to go to college and I will be the first one to graduate as well.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                          sentence  \\\n",
       "0                                                                         i am lucky i have the freedom and priveledge to be here.   \n",
       "1                                                            the main reason i chose this class is because it is a ge requirement.   \n",
       "2            im 'iustus trying et muros hos gradus est ultimum cursum et related scientia semper est accipere prius non sum liber.   \n",
       "3                                                                                     im the first in my family to attend college.   \n",
       "4  i want to be able to work together with my peers during lab so that i can learn from them and develop my skills as an engineer.   \n",
       "\n",
       "   label  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      0   \n",
       "3      1   \n",
       "4      0   \n",
       "\n",
       "                                                                                                                                                                       phrase  \n",
       "0                                   ['I have enough family support, geographical base, and school to be able to go to a four year University doing what I want to be doing.']  \n",
       "1  ['I want to make myself proud and my family. My parents are my motivators, they work so hard to give me everything they can and I want all their efforts to be worth it.']  \n",
       "2                                                                                   [\"I'm also here because of decisions made by my ancestors and myself to survive so far.\"]  \n",
       "3                                                                                                                           [\"I'm the first in my family to attend college.\"]  \n",
       "4                                                                    ['I am the first person in my family to go to college and I will be the first one to graduate as well.']  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i lived in mexico for almost half my life and i know how bad the situation over there is, you cant really succeed and when the opportunity to move to the usa and have a better life came up it was something i couldnt say no to.</td>\n",
       "      <td>0</td>\n",
       "      <td>['', 'My family has been my main influence. I mean seeing the struggles we all went through is something I wanted to change for my children in the future I wanted to be better.', '']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>overall, i want to make an impact in my community and be seen as a role model and i am here for a better future for my family and i.</td>\n",
       "      <td>1</td>\n",
       "      <td>['I am here because I would be the first in my family to have attended and hope to graduate from college.', 'Overall, I want to make an impact in my community and be seen as a role model and I am here for a better future for my family and I.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i think the saying you learn and you grow is kind of my motto.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I want to be able to help my family, be able to graduate, find a good job in the future, etc. I want to be able to make my parents, family, and friends happy because of what I can do and show them that I can do it.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the only way that im able to create a better future for my community is by getting a phd and create data dedicated in my community and create policy targeted towards my community.</td>\n",
       "      <td>0</td>\n",
       "      <td>['That everything I do with my life will not only help my family but everyone in my community.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>looking back, i would say this pressure to succeed for the family is a big part of why i studied stem in college instead of the regular hobbies i had growing up: literature and music.</td>\n",
       "      <td>0</td>\n",
       "      <td>['There is a certain pressure put upon many first generation children to thrive for the sake of the family.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                             sentence  \\\n",
       "0  i lived in mexico for almost half my life and i know how bad the situation over there is, you cant really succeed and when the opportunity to move to the usa and have a better life came up it was something i couldnt say no to.   \n",
       "1                                                                                                overall, i want to make an impact in my community and be seen as a role model and i am here for a better future for my family and i.   \n",
       "2                                                                                                                                                                      i think the saying you learn and you grow is kind of my motto.   \n",
       "3                                                 the only way that im able to create a better future for my community is by getting a phd and create data dedicated in my community and create policy targeted towards my community.   \n",
       "4                                             looking back, i would say this pressure to succeed for the family is a big part of why i studied stem in college instead of the regular hobbies i had growing up: literature and music.   \n",
       "\n",
       "   label  \\\n",
       "0      0   \n",
       "1      1   \n",
       "2      0   \n",
       "3      0   \n",
       "4      0   \n",
       "\n",
       "                                                                                                                                                                                                                                                phrase  \n",
       "0                                                               ['', 'My family has been my main influence. I mean seeing the struggles we all went through is something I wanted to change for my children in the future I wanted to be better.', '']  \n",
       "1  ['I am here because I would be the first in my family to have attended and hope to graduate from college.', 'Overall, I want to make an impact in my community and be seen as a role model and I am here for a better future for my family and I.']  \n",
       "2                           ['I want to be able to help my family, be able to graduate, find a good job in the future, etc. I want to be able to make my parents, family, and friends happy because of what I can do and show them that I can do it.']  \n",
       "3                                                                                                                                                     ['That everything I do with my life will not only help my family but everyone in my community.']  \n",
       "4                                                                                                                                        ['There is a certain pressure put upon many first generation children to thrive for the sake of the family.']  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (2380, 3) \n",
      "Test dataset shape: (265, 3)\n",
      "Positive labels present in the dataset : 228  out of 2380 or 9.57983193277311%\n",
      "Positive labels present in the test dataset : 25  out of 265 or 9.433962264150944%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset shape: {training_df.shape} \\nTest dataset shape: {test_df.shape}\")\n",
    "pos_labels = len([n for n in training_df['label'] if n==1])\n",
    "print(\"Positive labels present in the dataset : {}  out of {} or {}%\".format(pos_labels, len(training_df['label']), (pos_labels/len(training_df['label']))*100))\n",
    "pos_labels = len([n for n in test_df['label'] if n==1])\n",
    "print(\"Positive labels present in the test dataset : {}  out of {} or {}%\".format(pos_labels, len(test_df['label']), (pos_labels/len(test_df['label']))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the data for the plots\n",
    "training_counts = training_df['label'].value_counts()\n",
    "test_counts = test_df['label'].value_counts()\n",
    "\n",
    "# Set up the subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot for the training set\n",
    "sns.barplot(x=training_counts.index, y=training_counts.values, ax=axes[0])\n",
    "axes[0].set_title('Distribution of labels in training set')\n",
    "axes[0].set_ylabel('Sentences')\n",
    "axes[0].set_xlabel('Label')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot for the test set\n",
    "sns.barplot(x=test_counts.index, y=test_counts.values, ax=axes[1])\n",
    "axes[1].set_title('Distribution of labels in test set')\n",
    "axes[1].set_ylabel('Sentences')\n",
    "axes[1].set_xlabel('Label')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack Obama']\n"
     ]
    }
   ],
   "source": [
    "def get_ner(text):\n",
    "    ner_list = []\n",
    "    # Annotate the text using stanza\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for entity in sentence.ents:\n",
    "            if entity.type == 'PERSON':\n",
    "                ner_list.append(entity.text)\n",
    "\n",
    "    return ner_list\n",
    "\n",
    "# Example usage\n",
    "text = \"Barack Obama was the 44th doctor of the United States.\"\n",
    "print(get_ner(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a named entity is present in the sentence\n",
    "def named_entity_present(sentence):\n",
    "    ner_list = get_ner(sentence)\n",
    "    if len(ner_list) > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Similarity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the similar words and similarity score\n",
    "# The function takes tokens of sentence as input and if its not a stop word, get its similarity with synsets of STEM.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words |= set([\"help\",\"try\", \"work\", \"process\", \"support\", \"job\"] )\n",
    "def word_similarity(tokens, syns, field):    \n",
    "    if field in ['engineering', 'technology']:\n",
    "        score_threshold = 0.5\n",
    "    else:\n",
    "        score_threshold = 0.2\n",
    "    sim_words = 0\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            try:\n",
    "                syns_word = wordnet.synsets(token) \n",
    "                score = syns_word[0].path_similarity(syns[0])\n",
    "                if score >= score_threshold:\n",
    "                    sim_words += 1\n",
    "            except: \n",
    "                score = 0\n",
    "    \n",
    "    return sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to create columns for similarity based on all STEM fields\n",
    "syns_bio = wordnet.synsets(lemmatizer.lemmatize(\"biology\"))\n",
    "syns_maths = wordnet.synsets(lemmatizer.lemmatize(\"mathematics\")) \n",
    "syns_tech = wordnet.synsets(lemmatizer.lemmatize(\"technology\"))\n",
    "syns_eng = wordnet.synsets(lemmatizer.lemmatize(\"engineering\"))\n",
    "syns_chem = wordnet.synsets(lemmatizer.lemmatize(\"chemistry\"))\n",
    "syns_phy = wordnet.synsets(lemmatizer.lemmatize(\"physics\"))\n",
    "syns_sci = wordnet.synsets(lemmatizer.lemmatize(\"science\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Medical Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['genetics', 'care', 'geriatric', 'surgery', 'cardiology', 'calculi', 'surgery', 'neuromuscular', 'infertility', 'biochemical', 'gynecology', 'dermatopathology', 'abuse', 'clinical', 'neurology', 'immunology', 'pulmonology', 'gastroenterology', 'cardiothoracic', 'neuroradiology', 'behavioral', 'military', 'retardation', 'emergency', 'pathology', 'gynecologic', 'anesthesiology', 'anatomical', 'addiction', 'hospice', 'surgical', 'hematology', 'neonatal', 'transplant', 'hematology', 'endocrinology', 'infectious', 'interventional', 'allergy', 'orbit', 'transfusion', 'adolescent', 'neck', 'research', 'genetic', 'critical', 'dermatology', 'reproductive', 'endocrinology', 'pediatric', 'neurourology', 'cytopathology', 'urology', 'psychosomatic', 'radiation', 'and', 'endocrinologists', 'male', 'anterior', 'internal', 'strabismus', 'community', 'banking', 'musculoskeletal', 'pain', 'plastic', 'uveitis', 'diseases', 'chest', 'glaucoma', 'genitourinary', 'palliative', 'pediatric', 'ophthalmology', 'procedural', 'neuro', 'disabilities', 'retina', 'diagnostic', 'nephrology', 'rehabilitation', 'electrophysiology', 'preventive', 'genetic', 'disease', 'gastroenterology', 'administrative', 'pathology', 'cytogenetics', 'neurodevelopmental', 'rheumatology', 'pelvic', 'aerospace', 'molecular', 'toxicology', 'imaging', 'occupational', 'abdominal', 'pulmonary', 'advanced', 'forensic', 'hepatology', 'child', 'injury', 'female', 'urology', 'interventional', 'urologic', 'renal', 'physical', 'family', 'failure', 'ophthalmology', 'neuroradiology', 'medical', 'cardiac', 'rheumatology', 'oncology', 'fetal', 'oncology', 'neuropathology', 'dermatology', 'radiology', 'ophthalmic', 'psychiatric', 'ocular', 'blood', 'pediatrics', 'liaison', 'pediatrics', 'perinatal', 'chemical', 'head', 'consultation', 'transplant', 'heart', 'metabolism', 'immunopathology', 'gastrointestinal', 'breast', 'diabetes', 'neurology', 'sports', 'public', 'cardiovascular', 'endovascular', 'nephrology', 'vascular', 'adolescent', 'maternal', 'obstetrics', 'microbiology', 'reconstructive', 'sleep', 'segment', 'health', 'internal', 'neurophysiology', 'psychiatry', 'critical', 'reconstructive', 'cornea', 'oculoplastics', 'developmental', 'anesthesiology', 'brain', 'psychiatry', 'infectious', 'medicine', 'sports', 'mental', 'nuclear']\n"
     ]
    }
   ],
   "source": [
    "# Load the medical specialization text file and create a list\n",
    "medical_list = []\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/medical_specialities.txt', 'r') as medical_fields:\n",
    "    for line in medical_fields.readlines():\n",
    "        special_field = line.rstrip('\\n')\n",
    "        special_field = re.sub(\"\\W\",\" \", special_field )\n",
    "#         print(special_field)\n",
    "        medical_list += special_field.split()\n",
    "medical_list = list(set(medical_list))  \n",
    "medical_list = [x.lower() for x in medical_list]\n",
    "print(medical_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get medical words\n",
    "def check_medical_words(tokens):\n",
    "    for token in tokens:\n",
    "        if token not in stop_words and token in [x.lower() for x in medical_list]:\n",
    "            return 1\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sentiment Polarity and Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get polarity and subjectivity of the sentence using TexBlob\n",
    "def get_sentiment(sentence):\n",
    "    sentiments =TextBlob(sentence).sentiment\n",
    "    polarity = sentiments.polarity\n",
    "    subjectivity = sentiments.subjectivity\n",
    "    return polarity, subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the count of POS tags of the sentence\n",
    "def count_pos_tags(tokens):\n",
    "    token_pos = pos_tag(tokens)\n",
    "    count = Counter(tag for word,tag in token_pos)\n",
    "    interjections =  count['UH']\n",
    "    nouns = count['NN'] + count['NNS'] + count['NNP'] + count['NNPS']\n",
    "    adverb = count['RB'] + count['RBS'] + count['RBR']\n",
    "    verb = count['VB'] + count['VBD'] + count['VBG'] + count['VBN']\n",
    "    determiner = count['DT']\n",
    "    pronoun = count['PRP']\n",
    "    adjetive = count['JJ'] + count['JJR'] + count['JJS']\n",
    "    preposition = count['IN']\n",
    "    return interjections, nouns, adverb, verb, determiner, pronoun, adjetive,preposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_extraction(dataframe, field, func, column_names):\n",
    "    return pd.concat((\n",
    "        dataframe,\n",
    "        dataframe[field].apply(\n",
    "            lambda cell: pd.Series(func(cell), index=column_names))), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the w2v dict from pickle file\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/pickle/embeddings10022024.pickle', 'rb') as w2v_file:\n",
    "    w2v_dict = pickle.load(w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of word embeddings:  6142\n"
     ]
    }
   ],
   "source": [
    "print(\"length of word embeddings: \", len(w2v_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vectors for the essay\n",
    "def vectorizer(sequence):\n",
    "    vect = []\n",
    "    numw = 0\n",
    "    for w in sequence: \n",
    "        try :\n",
    "            if numw == 0:\n",
    "                vect = w2v_dict[w]\n",
    "            else:\n",
    "                vect = np.add(vect, w2v_dict[w])\n",
    "            numw += 1\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    return vect/ numw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into words\n",
    "def split_into_words(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vectorizer\n",
    "unigram_vect = CountVectorizer(ngram_range=(1, 1), min_df=2, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Putting them all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for feature engineering\n",
    "def feature_engineering(original_dataset):\n",
    "\n",
    "    dataset = original_dataset.copy()\n",
    "    # create a new column with sentence tokens\n",
    "    dataset['tokens'] = dataset['sentence'].apply(word_tokenize)\n",
    "    # 1. Similarity features\n",
    "    # biology\n",
    "    dataset['bio_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_bio,'biology',)) \n",
    "    # chemistry\n",
    "    dataset['chem_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_chem,'chemistry',))\n",
    "    # physics\n",
    "    dataset['phy_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_phy,'physics',))\n",
    "    # mathematics\n",
    "    dataset['math_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_maths,'mathematics',))\n",
    "    # technology\n",
    "    dataset['tech_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_tech,'technology',))\n",
    "    # engineering\n",
    "    dataset['eng_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_eng,'engineering',))\n",
    "    \n",
    "    # medical terms\n",
    "    dataset['medical_terms'] = dataset['tokens'].apply(check_medical_words)\n",
    "    \n",
    "    # polarity and subjectivity\n",
    "    dataset['polarity'], dataset['subjectivity'] = zip(*dataset['sentence'].apply(get_sentiment))\n",
    "    \n",
    "    # named entity recognition\n",
    "    dataset['ner'] = dataset['sentence'].apply(named_entity_present)\n",
    "    \n",
    "    # pos tag count\n",
    "    dataset = pos_tag_extraction(dataset, 'tokens', count_pos_tags, ['interjections', 'nouns', 'adverb', 'verb', 'determiner', 'pronoun', 'adjetive','preposition'])\n",
    "    \n",
    "    # labels\n",
    "    data_labels = dataset['label']\n",
    "    # X\n",
    "    data_x = dataset.drop(columns='label')\n",
    "\n",
    "    \n",
    "    # vectorize all the essays\n",
    "    vect_arr = data_x.tokens.apply(vectorizer)\n",
    "    for index in range(0, len(vect_arr)):\n",
    "        i = 0\n",
    "        for item in vect_arr[index]:\n",
    "            column_name= \"embedding\" + str(i)\n",
    "            data_x.loc[index, column_name] = item\n",
    "            i +=1\n",
    "    \n",
    "    return data_x,data_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = feature_engineering(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2380, 121)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = feature_engineering(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265, 121)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Unigram features for both train and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2380, 121)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265, 121)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2/Familial/saved_features/X_train.csv\", index=False)\n",
    "X_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2/Familial/saved_features/X_test.csv\", index=False)\n",
    "y_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2/Familial/saved_features/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2/Familial/saved_features/y_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the unigram df for train :  (2380, 1603)\n"
     ]
    }
   ],
   "source": [
    "# Unigrams for training set\n",
    "unigram_matrix = unigram_vect.fit_transform(X_train['sentence'])\n",
    "unigrams = pd.DataFrame(unigram_matrix.toarray())\n",
    "print(\"Shape of the unigram df for train : \",unigrams.shape)\n",
    "unigrams = unigrams.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = pd.concat([X_train, unigrams], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final.columns = X_train_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2380, 1724)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test unigram df shape :  (265, 1603)\n"
     ]
    }
   ],
   "source": [
    "unigram_matrix_test = unigram_vect.transform(X_test['sentence'])\n",
    "unigrams_test = pd.DataFrame(unigram_matrix_test.toarray())\n",
    "unigrams_test = unigrams_test.reset_index(drop=True)\n",
    "print(\"Test unigram df shape : \",unigrams_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265, 1724)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final = pd.concat([X_test, unigrams_test], axis = 1)\n",
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final.columns = X_test_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265, 1724)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ---- sentence\n",
      "1 ---- phrase\n",
      "2 ---- tokens\n",
      "3 ---- bio_sim_words\n",
      "4 ---- chem_sim_words\n",
      "5 ---- phy_sim_words\n",
      "6 ---- math_sim_words\n",
      "7 ---- tech_sim_words\n",
      "8 ---- eng_sim_words\n",
      "9 ---- medical_terms\n",
      "10 ---- polarity\n",
      "11 ---- subjectivity\n",
      "12 ---- ner\n",
      "13 ---- interjections\n",
      "14 ---- nouns\n",
      "15 ---- adverb\n",
      "16 ---- verb\n",
      "17 ---- determiner\n",
      "18 ---- pronoun\n",
      "19 ---- adjetive\n",
      "20 ---- preposition\n",
      "21 ---- embedding0\n",
      "22 ---- embedding1\n",
      "23 ---- embedding2\n",
      "24 ---- embedding3\n",
      "25 ---- embedding4\n",
      "26 ---- embedding5\n",
      "27 ---- embedding6\n",
      "28 ---- embedding7\n",
      "29 ---- embedding8\n",
      "30 ---- embedding9\n",
      "31 ---- embedding10\n",
      "32 ---- embedding11\n",
      "33 ---- embedding12\n",
      "34 ---- embedding13\n",
      "35 ---- embedding14\n",
      "36 ---- embedding15\n",
      "37 ---- embedding16\n",
      "38 ---- embedding17\n",
      "39 ---- embedding18\n",
      "40 ---- embedding19\n",
      "41 ---- embedding20\n",
      "42 ---- embedding21\n",
      "43 ---- embedding22\n",
      "44 ---- embedding23\n",
      "45 ---- embedding24\n",
      "46 ---- embedding25\n",
      "47 ---- embedding26\n",
      "48 ---- embedding27\n",
      "49 ---- embedding28\n",
      "50 ---- embedding29\n",
      "51 ---- embedding30\n",
      "52 ---- embedding31\n",
      "53 ---- embedding32\n",
      "54 ---- embedding33\n",
      "55 ---- embedding34\n",
      "56 ---- embedding35\n",
      "57 ---- embedding36\n",
      "58 ---- embedding37\n",
      "59 ---- embedding38\n",
      "60 ---- embedding39\n",
      "61 ---- embedding40\n",
      "62 ---- embedding41\n",
      "63 ---- embedding42\n",
      "64 ---- embedding43\n",
      "65 ---- embedding44\n",
      "66 ---- embedding45\n",
      "67 ---- embedding46\n",
      "68 ---- embedding47\n",
      "69 ---- embedding48\n",
      "70 ---- embedding49\n",
      "71 ---- embedding50\n",
      "72 ---- embedding51\n",
      "73 ---- embedding52\n",
      "74 ---- embedding53\n",
      "75 ---- embedding54\n",
      "76 ---- embedding55\n",
      "77 ---- embedding56\n",
      "78 ---- embedding57\n",
      "79 ---- embedding58\n",
      "80 ---- embedding59\n",
      "81 ---- embedding60\n",
      "82 ---- embedding61\n",
      "83 ---- embedding62\n",
      "84 ---- embedding63\n",
      "85 ---- embedding64\n",
      "86 ---- embedding65\n",
      "87 ---- embedding66\n",
      "88 ---- embedding67\n",
      "89 ---- embedding68\n",
      "90 ---- embedding69\n",
      "91 ---- embedding70\n",
      "92 ---- embedding71\n",
      "93 ---- embedding72\n",
      "94 ---- embedding73\n",
      "95 ---- embedding74\n",
      "96 ---- embedding75\n",
      "97 ---- embedding76\n",
      "98 ---- embedding77\n",
      "99 ---- embedding78\n",
      "100 ---- embedding79\n",
      "101 ---- embedding80\n",
      "102 ---- embedding81\n",
      "103 ---- embedding82\n",
      "104 ---- embedding83\n",
      "105 ---- embedding84\n",
      "106 ---- embedding85\n",
      "107 ---- embedding86\n",
      "108 ---- embedding87\n",
      "109 ---- embedding88\n",
      "110 ---- embedding89\n",
      "111 ---- embedding90\n",
      "112 ---- embedding91\n",
      "113 ---- embedding92\n",
      "114 ---- embedding93\n",
      "115 ---- embedding94\n",
      "116 ---- embedding95\n",
      "117 ---- embedding96\n",
      "118 ---- embedding97\n",
      "119 ---- embedding98\n",
      "120 ---- embedding99\n",
      "121 ---- 0\n",
      "122 ---- 1\n",
      "123 ---- 2\n",
      "124 ---- 3\n",
      "125 ---- 4\n",
      "126 ---- 5\n",
      "127 ---- 6\n",
      "128 ---- 7\n",
      "129 ---- 8\n",
      "130 ---- 9\n",
      "131 ---- 10\n",
      "132 ---- 11\n",
      "133 ---- 12\n",
      "134 ---- 13\n",
      "135 ---- 14\n",
      "136 ---- 15\n",
      "137 ---- 16\n",
      "138 ---- 17\n",
      "139 ---- 18\n",
      "140 ---- 19\n",
      "141 ---- 20\n",
      "142 ---- 21\n",
      "143 ---- 22\n",
      "144 ---- 23\n",
      "145 ---- 24\n",
      "146 ---- 25\n",
      "147 ---- 26\n",
      "148 ---- 27\n",
      "149 ---- 28\n",
      "150 ---- 29\n",
      "151 ---- 30\n",
      "152 ---- 31\n",
      "153 ---- 32\n",
      "154 ---- 33\n",
      "155 ---- 34\n",
      "156 ---- 35\n",
      "157 ---- 36\n",
      "158 ---- 37\n",
      "159 ---- 38\n",
      "160 ---- 39\n",
      "161 ---- 40\n",
      "162 ---- 41\n",
      "163 ---- 42\n",
      "164 ---- 43\n",
      "165 ---- 44\n",
      "166 ---- 45\n",
      "167 ---- 46\n",
      "168 ---- 47\n",
      "169 ---- 48\n",
      "170 ---- 49\n",
      "171 ---- 50\n",
      "172 ---- 51\n",
      "173 ---- 52\n",
      "174 ---- 53\n",
      "175 ---- 54\n",
      "176 ---- 55\n",
      "177 ---- 56\n",
      "178 ---- 57\n",
      "179 ---- 58\n",
      "180 ---- 59\n",
      "181 ---- 60\n",
      "182 ---- 61\n",
      "183 ---- 62\n",
      "184 ---- 63\n",
      "185 ---- 64\n",
      "186 ---- 65\n",
      "187 ---- 66\n",
      "188 ---- 67\n",
      "189 ---- 68\n",
      "190 ---- 69\n",
      "191 ---- 70\n",
      "192 ---- 71\n",
      "193 ---- 72\n",
      "194 ---- 73\n",
      "195 ---- 74\n",
      "196 ---- 75\n",
      "197 ---- 76\n",
      "198 ---- 77\n",
      "199 ---- 78\n",
      "200 ---- 79\n",
      "201 ---- 80\n",
      "202 ---- 81\n",
      "203 ---- 82\n",
      "204 ---- 83\n",
      "205 ---- 84\n",
      "206 ---- 85\n",
      "207 ---- 86\n",
      "208 ---- 87\n",
      "209 ---- 88\n",
      "210 ---- 89\n",
      "211 ---- 90\n",
      "212 ---- 91\n",
      "213 ---- 92\n",
      "214 ---- 93\n",
      "215 ---- 94\n",
      "216 ---- 95\n",
      "217 ---- 96\n",
      "218 ---- 97\n",
      "219 ---- 98\n",
      "220 ---- 99\n",
      "221 ---- 100\n",
      "222 ---- 101\n",
      "223 ---- 102\n",
      "224 ---- 103\n",
      "225 ---- 104\n",
      "226 ---- 105\n",
      "227 ---- 106\n",
      "228 ---- 107\n",
      "229 ---- 108\n",
      "230 ---- 109\n",
      "231 ---- 110\n",
      "232 ---- 111\n",
      "233 ---- 112\n",
      "234 ---- 113\n",
      "235 ---- 114\n",
      "236 ---- 115\n",
      "237 ---- 116\n",
      "238 ---- 117\n",
      "239 ---- 118\n",
      "240 ---- 119\n",
      "241 ---- 120\n",
      "242 ---- 121\n",
      "243 ---- 122\n",
      "244 ---- 123\n",
      "245 ---- 124\n",
      "246 ---- 125\n",
      "247 ---- 126\n",
      "248 ---- 127\n",
      "249 ---- 128\n",
      "250 ---- 129\n",
      "251 ---- 130\n",
      "252 ---- 131\n",
      "253 ---- 132\n",
      "254 ---- 133\n",
      "255 ---- 134\n",
      "256 ---- 135\n",
      "257 ---- 136\n",
      "258 ---- 137\n",
      "259 ---- 138\n",
      "260 ---- 139\n",
      "261 ---- 140\n",
      "262 ---- 141\n",
      "263 ---- 142\n",
      "264 ---- 143\n",
      "265 ---- 144\n",
      "266 ---- 145\n",
      "267 ---- 146\n",
      "268 ---- 147\n",
      "269 ---- 148\n",
      "270 ---- 149\n",
      "271 ---- 150\n",
      "272 ---- 151\n",
      "273 ---- 152\n",
      "274 ---- 153\n",
      "275 ---- 154\n",
      "276 ---- 155\n",
      "277 ---- 156\n",
      "278 ---- 157\n",
      "279 ---- 158\n",
      "280 ---- 159\n",
      "281 ---- 160\n",
      "282 ---- 161\n",
      "283 ---- 162\n",
      "284 ---- 163\n",
      "285 ---- 164\n",
      "286 ---- 165\n",
      "287 ---- 166\n",
      "288 ---- 167\n",
      "289 ---- 168\n",
      "290 ---- 169\n",
      "291 ---- 170\n",
      "292 ---- 171\n",
      "293 ---- 172\n",
      "294 ---- 173\n",
      "295 ---- 174\n",
      "296 ---- 175\n",
      "297 ---- 176\n",
      "298 ---- 177\n",
      "299 ---- 178\n",
      "300 ---- 179\n",
      "301 ---- 180\n",
      "302 ---- 181\n",
      "303 ---- 182\n",
      "304 ---- 183\n",
      "305 ---- 184\n",
      "306 ---- 185\n",
      "307 ---- 186\n",
      "308 ---- 187\n",
      "309 ---- 188\n",
      "310 ---- 189\n",
      "311 ---- 190\n",
      "312 ---- 191\n",
      "313 ---- 192\n",
      "314 ---- 193\n",
      "315 ---- 194\n",
      "316 ---- 195\n",
      "317 ---- 196\n",
      "318 ---- 197\n",
      "319 ---- 198\n",
      "320 ---- 199\n",
      "321 ---- 200\n",
      "322 ---- 201\n",
      "323 ---- 202\n",
      "324 ---- 203\n",
      "325 ---- 204\n",
      "326 ---- 205\n",
      "327 ---- 206\n",
      "328 ---- 207\n",
      "329 ---- 208\n",
      "330 ---- 209\n",
      "331 ---- 210\n",
      "332 ---- 211\n",
      "333 ---- 212\n",
      "334 ---- 213\n",
      "335 ---- 214\n",
      "336 ---- 215\n",
      "337 ---- 216\n",
      "338 ---- 217\n",
      "339 ---- 218\n",
      "340 ---- 219\n",
      "341 ---- 220\n",
      "342 ---- 221\n",
      "343 ---- 222\n",
      "344 ---- 223\n",
      "345 ---- 224\n",
      "346 ---- 225\n",
      "347 ---- 226\n",
      "348 ---- 227\n",
      "349 ---- 228\n",
      "350 ---- 229\n",
      "351 ---- 230\n",
      "352 ---- 231\n",
      "353 ---- 232\n",
      "354 ---- 233\n",
      "355 ---- 234\n",
      "356 ---- 235\n",
      "357 ---- 236\n",
      "358 ---- 237\n",
      "359 ---- 238\n",
      "360 ---- 239\n",
      "361 ---- 240\n",
      "362 ---- 241\n",
      "363 ---- 242\n",
      "364 ---- 243\n",
      "365 ---- 244\n",
      "366 ---- 245\n",
      "367 ---- 246\n",
      "368 ---- 247\n",
      "369 ---- 248\n",
      "370 ---- 249\n",
      "371 ---- 250\n",
      "372 ---- 251\n",
      "373 ---- 252\n",
      "374 ---- 253\n",
      "375 ---- 254\n",
      "376 ---- 255\n",
      "377 ---- 256\n",
      "378 ---- 257\n",
      "379 ---- 258\n",
      "380 ---- 259\n",
      "381 ---- 260\n",
      "382 ---- 261\n",
      "383 ---- 262\n",
      "384 ---- 263\n",
      "385 ---- 264\n",
      "386 ---- 265\n",
      "387 ---- 266\n",
      "388 ---- 267\n",
      "389 ---- 268\n",
      "390 ---- 269\n",
      "391 ---- 270\n",
      "392 ---- 271\n",
      "393 ---- 272\n",
      "394 ---- 273\n",
      "395 ---- 274\n",
      "396 ---- 275\n",
      "397 ---- 276\n",
      "398 ---- 277\n",
      "399 ---- 278\n",
      "400 ---- 279\n",
      "401 ---- 280\n",
      "402 ---- 281\n",
      "403 ---- 282\n",
      "404 ---- 283\n",
      "405 ---- 284\n",
      "406 ---- 285\n",
      "407 ---- 286\n",
      "408 ---- 287\n",
      "409 ---- 288\n",
      "410 ---- 289\n",
      "411 ---- 290\n",
      "412 ---- 291\n",
      "413 ---- 292\n",
      "414 ---- 293\n",
      "415 ---- 294\n",
      "416 ---- 295\n",
      "417 ---- 296\n",
      "418 ---- 297\n",
      "419 ---- 298\n",
      "420 ---- 299\n",
      "421 ---- 300\n",
      "422 ---- 301\n",
      "423 ---- 302\n",
      "424 ---- 303\n",
      "425 ---- 304\n",
      "426 ---- 305\n",
      "427 ---- 306\n",
      "428 ---- 307\n",
      "429 ---- 308\n",
      "430 ---- 309\n",
      "431 ---- 310\n",
      "432 ---- 311\n",
      "433 ---- 312\n",
      "434 ---- 313\n",
      "435 ---- 314\n",
      "436 ---- 315\n",
      "437 ---- 316\n",
      "438 ---- 317\n",
      "439 ---- 318\n",
      "440 ---- 319\n",
      "441 ---- 320\n",
      "442 ---- 321\n",
      "443 ---- 322\n",
      "444 ---- 323\n",
      "445 ---- 324\n",
      "446 ---- 325\n",
      "447 ---- 326\n",
      "448 ---- 327\n",
      "449 ---- 328\n",
      "450 ---- 329\n",
      "451 ---- 330\n",
      "452 ---- 331\n",
      "453 ---- 332\n",
      "454 ---- 333\n",
      "455 ---- 334\n",
      "456 ---- 335\n",
      "457 ---- 336\n",
      "458 ---- 337\n",
      "459 ---- 338\n",
      "460 ---- 339\n",
      "461 ---- 340\n",
      "462 ---- 341\n",
      "463 ---- 342\n",
      "464 ---- 343\n",
      "465 ---- 344\n",
      "466 ---- 345\n",
      "467 ---- 346\n",
      "468 ---- 347\n",
      "469 ---- 348\n",
      "470 ---- 349\n",
      "471 ---- 350\n",
      "472 ---- 351\n",
      "473 ---- 352\n",
      "474 ---- 353\n",
      "475 ---- 354\n",
      "476 ---- 355\n",
      "477 ---- 356\n",
      "478 ---- 357\n",
      "479 ---- 358\n",
      "480 ---- 359\n",
      "481 ---- 360\n",
      "482 ---- 361\n",
      "483 ---- 362\n",
      "484 ---- 363\n",
      "485 ---- 364\n",
      "486 ---- 365\n",
      "487 ---- 366\n",
      "488 ---- 367\n",
      "489 ---- 368\n",
      "490 ---- 369\n",
      "491 ---- 370\n",
      "492 ---- 371\n",
      "493 ---- 372\n",
      "494 ---- 373\n",
      "495 ---- 374\n",
      "496 ---- 375\n",
      "497 ---- 376\n",
      "498 ---- 377\n",
      "499 ---- 378\n",
      "500 ---- 379\n",
      "501 ---- 380\n",
      "502 ---- 381\n",
      "503 ---- 382\n",
      "504 ---- 383\n",
      "505 ---- 384\n",
      "506 ---- 385\n",
      "507 ---- 386\n",
      "508 ---- 387\n",
      "509 ---- 388\n",
      "510 ---- 389\n",
      "511 ---- 390\n",
      "512 ---- 391\n",
      "513 ---- 392\n",
      "514 ---- 393\n",
      "515 ---- 394\n",
      "516 ---- 395\n",
      "517 ---- 396\n",
      "518 ---- 397\n",
      "519 ---- 398\n",
      "520 ---- 399\n",
      "521 ---- 400\n",
      "522 ---- 401\n",
      "523 ---- 402\n",
      "524 ---- 403\n",
      "525 ---- 404\n",
      "526 ---- 405\n",
      "527 ---- 406\n",
      "528 ---- 407\n",
      "529 ---- 408\n",
      "530 ---- 409\n",
      "531 ---- 410\n",
      "532 ---- 411\n",
      "533 ---- 412\n",
      "534 ---- 413\n",
      "535 ---- 414\n",
      "536 ---- 415\n",
      "537 ---- 416\n",
      "538 ---- 417\n",
      "539 ---- 418\n",
      "540 ---- 419\n",
      "541 ---- 420\n",
      "542 ---- 421\n",
      "543 ---- 422\n",
      "544 ---- 423\n",
      "545 ---- 424\n",
      "546 ---- 425\n",
      "547 ---- 426\n",
      "548 ---- 427\n",
      "549 ---- 428\n",
      "550 ---- 429\n",
      "551 ---- 430\n",
      "552 ---- 431\n",
      "553 ---- 432\n",
      "554 ---- 433\n",
      "555 ---- 434\n",
      "556 ---- 435\n",
      "557 ---- 436\n",
      "558 ---- 437\n",
      "559 ---- 438\n",
      "560 ---- 439\n",
      "561 ---- 440\n",
      "562 ---- 441\n",
      "563 ---- 442\n",
      "564 ---- 443\n",
      "565 ---- 444\n",
      "566 ---- 445\n",
      "567 ---- 446\n",
      "568 ---- 447\n",
      "569 ---- 448\n",
      "570 ---- 449\n",
      "571 ---- 450\n",
      "572 ---- 451\n",
      "573 ---- 452\n",
      "574 ---- 453\n",
      "575 ---- 454\n",
      "576 ---- 455\n",
      "577 ---- 456\n",
      "578 ---- 457\n",
      "579 ---- 458\n",
      "580 ---- 459\n",
      "581 ---- 460\n",
      "582 ---- 461\n",
      "583 ---- 462\n",
      "584 ---- 463\n",
      "585 ---- 464\n",
      "586 ---- 465\n",
      "587 ---- 466\n",
      "588 ---- 467\n",
      "589 ---- 468\n",
      "590 ---- 469\n",
      "591 ---- 470\n",
      "592 ---- 471\n",
      "593 ---- 472\n",
      "594 ---- 473\n",
      "595 ---- 474\n",
      "596 ---- 475\n",
      "597 ---- 476\n",
      "598 ---- 477\n",
      "599 ---- 478\n",
      "600 ---- 479\n",
      "601 ---- 480\n",
      "602 ---- 481\n",
      "603 ---- 482\n",
      "604 ---- 483\n",
      "605 ---- 484\n",
      "606 ---- 485\n",
      "607 ---- 486\n",
      "608 ---- 487\n",
      "609 ---- 488\n",
      "610 ---- 489\n",
      "611 ---- 490\n",
      "612 ---- 491\n",
      "613 ---- 492\n",
      "614 ---- 493\n",
      "615 ---- 494\n",
      "616 ---- 495\n",
      "617 ---- 496\n",
      "618 ---- 497\n",
      "619 ---- 498\n",
      "620 ---- 499\n",
      "621 ---- 500\n",
      "622 ---- 501\n",
      "623 ---- 502\n",
      "624 ---- 503\n",
      "625 ---- 504\n",
      "626 ---- 505\n",
      "627 ---- 506\n",
      "628 ---- 507\n",
      "629 ---- 508\n",
      "630 ---- 509\n",
      "631 ---- 510\n",
      "632 ---- 511\n",
      "633 ---- 512\n",
      "634 ---- 513\n",
      "635 ---- 514\n",
      "636 ---- 515\n",
      "637 ---- 516\n",
      "638 ---- 517\n",
      "639 ---- 518\n",
      "640 ---- 519\n",
      "641 ---- 520\n",
      "642 ---- 521\n",
      "643 ---- 522\n",
      "644 ---- 523\n",
      "645 ---- 524\n",
      "646 ---- 525\n",
      "647 ---- 526\n",
      "648 ---- 527\n",
      "649 ---- 528\n",
      "650 ---- 529\n",
      "651 ---- 530\n",
      "652 ---- 531\n",
      "653 ---- 532\n",
      "654 ---- 533\n",
      "655 ---- 534\n",
      "656 ---- 535\n",
      "657 ---- 536\n",
      "658 ---- 537\n",
      "659 ---- 538\n",
      "660 ---- 539\n",
      "661 ---- 540\n",
      "662 ---- 541\n",
      "663 ---- 542\n",
      "664 ---- 543\n",
      "665 ---- 544\n",
      "666 ---- 545\n",
      "667 ---- 546\n",
      "668 ---- 547\n",
      "669 ---- 548\n",
      "670 ---- 549\n",
      "671 ---- 550\n",
      "672 ---- 551\n",
      "673 ---- 552\n",
      "674 ---- 553\n",
      "675 ---- 554\n",
      "676 ---- 555\n",
      "677 ---- 556\n",
      "678 ---- 557\n",
      "679 ---- 558\n",
      "680 ---- 559\n",
      "681 ---- 560\n",
      "682 ---- 561\n",
      "683 ---- 562\n",
      "684 ---- 563\n",
      "685 ---- 564\n",
      "686 ---- 565\n",
      "687 ---- 566\n",
      "688 ---- 567\n",
      "689 ---- 568\n",
      "690 ---- 569\n",
      "691 ---- 570\n",
      "692 ---- 571\n",
      "693 ---- 572\n",
      "694 ---- 573\n",
      "695 ---- 574\n",
      "696 ---- 575\n",
      "697 ---- 576\n",
      "698 ---- 577\n",
      "699 ---- 578\n",
      "700 ---- 579\n",
      "701 ---- 580\n",
      "702 ---- 581\n",
      "703 ---- 582\n",
      "704 ---- 583\n",
      "705 ---- 584\n",
      "706 ---- 585\n",
      "707 ---- 586\n",
      "708 ---- 587\n",
      "709 ---- 588\n",
      "710 ---- 589\n",
      "711 ---- 590\n",
      "712 ---- 591\n",
      "713 ---- 592\n",
      "714 ---- 593\n",
      "715 ---- 594\n",
      "716 ---- 595\n",
      "717 ---- 596\n",
      "718 ---- 597\n",
      "719 ---- 598\n",
      "720 ---- 599\n",
      "721 ---- 600\n",
      "722 ---- 601\n",
      "723 ---- 602\n",
      "724 ---- 603\n",
      "725 ---- 604\n",
      "726 ---- 605\n",
      "727 ---- 606\n",
      "728 ---- 607\n",
      "729 ---- 608\n",
      "730 ---- 609\n",
      "731 ---- 610\n",
      "732 ---- 611\n",
      "733 ---- 612\n",
      "734 ---- 613\n",
      "735 ---- 614\n",
      "736 ---- 615\n",
      "737 ---- 616\n",
      "738 ---- 617\n",
      "739 ---- 618\n",
      "740 ---- 619\n",
      "741 ---- 620\n",
      "742 ---- 621\n",
      "743 ---- 622\n",
      "744 ---- 623\n",
      "745 ---- 624\n",
      "746 ---- 625\n",
      "747 ---- 626\n",
      "748 ---- 627\n",
      "749 ---- 628\n",
      "750 ---- 629\n",
      "751 ---- 630\n",
      "752 ---- 631\n",
      "753 ---- 632\n",
      "754 ---- 633\n",
      "755 ---- 634\n",
      "756 ---- 635\n",
      "757 ---- 636\n",
      "758 ---- 637\n",
      "759 ---- 638\n",
      "760 ---- 639\n",
      "761 ---- 640\n",
      "762 ---- 641\n",
      "763 ---- 642\n",
      "764 ---- 643\n",
      "765 ---- 644\n",
      "766 ---- 645\n",
      "767 ---- 646\n",
      "768 ---- 647\n",
      "769 ---- 648\n",
      "770 ---- 649\n",
      "771 ---- 650\n",
      "772 ---- 651\n",
      "773 ---- 652\n",
      "774 ---- 653\n",
      "775 ---- 654\n",
      "776 ---- 655\n",
      "777 ---- 656\n",
      "778 ---- 657\n",
      "779 ---- 658\n",
      "780 ---- 659\n",
      "781 ---- 660\n",
      "782 ---- 661\n",
      "783 ---- 662\n",
      "784 ---- 663\n",
      "785 ---- 664\n",
      "786 ---- 665\n",
      "787 ---- 666\n",
      "788 ---- 667\n",
      "789 ---- 668\n",
      "790 ---- 669\n",
      "791 ---- 670\n",
      "792 ---- 671\n",
      "793 ---- 672\n",
      "794 ---- 673\n",
      "795 ---- 674\n",
      "796 ---- 675\n",
      "797 ---- 676\n",
      "798 ---- 677\n",
      "799 ---- 678\n",
      "800 ---- 679\n",
      "801 ---- 680\n",
      "802 ---- 681\n",
      "803 ---- 682\n",
      "804 ---- 683\n",
      "805 ---- 684\n",
      "806 ---- 685\n",
      "807 ---- 686\n",
      "808 ---- 687\n",
      "809 ---- 688\n",
      "810 ---- 689\n",
      "811 ---- 690\n",
      "812 ---- 691\n",
      "813 ---- 692\n",
      "814 ---- 693\n",
      "815 ---- 694\n",
      "816 ---- 695\n",
      "817 ---- 696\n",
      "818 ---- 697\n",
      "819 ---- 698\n",
      "820 ---- 699\n",
      "821 ---- 700\n",
      "822 ---- 701\n",
      "823 ---- 702\n",
      "824 ---- 703\n",
      "825 ---- 704\n",
      "826 ---- 705\n",
      "827 ---- 706\n",
      "828 ---- 707\n",
      "829 ---- 708\n",
      "830 ---- 709\n",
      "831 ---- 710\n",
      "832 ---- 711\n",
      "833 ---- 712\n",
      "834 ---- 713\n",
      "835 ---- 714\n",
      "836 ---- 715\n",
      "837 ---- 716\n",
      "838 ---- 717\n",
      "839 ---- 718\n",
      "840 ---- 719\n",
      "841 ---- 720\n",
      "842 ---- 721\n",
      "843 ---- 722\n",
      "844 ---- 723\n",
      "845 ---- 724\n",
      "846 ---- 725\n",
      "847 ---- 726\n",
      "848 ---- 727\n",
      "849 ---- 728\n",
      "850 ---- 729\n",
      "851 ---- 730\n",
      "852 ---- 731\n",
      "853 ---- 732\n",
      "854 ---- 733\n",
      "855 ---- 734\n",
      "856 ---- 735\n",
      "857 ---- 736\n",
      "858 ---- 737\n",
      "859 ---- 738\n",
      "860 ---- 739\n",
      "861 ---- 740\n",
      "862 ---- 741\n",
      "863 ---- 742\n",
      "864 ---- 743\n",
      "865 ---- 744\n",
      "866 ---- 745\n",
      "867 ---- 746\n",
      "868 ---- 747\n",
      "869 ---- 748\n",
      "870 ---- 749\n",
      "871 ---- 750\n",
      "872 ---- 751\n",
      "873 ---- 752\n",
      "874 ---- 753\n",
      "875 ---- 754\n",
      "876 ---- 755\n",
      "877 ---- 756\n",
      "878 ---- 757\n",
      "879 ---- 758\n",
      "880 ---- 759\n",
      "881 ---- 760\n",
      "882 ---- 761\n",
      "883 ---- 762\n",
      "884 ---- 763\n",
      "885 ---- 764\n",
      "886 ---- 765\n",
      "887 ---- 766\n",
      "888 ---- 767\n",
      "889 ---- 768\n",
      "890 ---- 769\n",
      "891 ---- 770\n",
      "892 ---- 771\n",
      "893 ---- 772\n",
      "894 ---- 773\n",
      "895 ---- 774\n",
      "896 ---- 775\n",
      "897 ---- 776\n",
      "898 ---- 777\n",
      "899 ---- 778\n",
      "900 ---- 779\n",
      "901 ---- 780\n",
      "902 ---- 781\n",
      "903 ---- 782\n",
      "904 ---- 783\n",
      "905 ---- 784\n",
      "906 ---- 785\n",
      "907 ---- 786\n",
      "908 ---- 787\n",
      "909 ---- 788\n",
      "910 ---- 789\n",
      "911 ---- 790\n",
      "912 ---- 791\n",
      "913 ---- 792\n",
      "914 ---- 793\n",
      "915 ---- 794\n",
      "916 ---- 795\n",
      "917 ---- 796\n",
      "918 ---- 797\n",
      "919 ---- 798\n",
      "920 ---- 799\n",
      "921 ---- 800\n",
      "922 ---- 801\n",
      "923 ---- 802\n",
      "924 ---- 803\n",
      "925 ---- 804\n",
      "926 ---- 805\n",
      "927 ---- 806\n",
      "928 ---- 807\n",
      "929 ---- 808\n",
      "930 ---- 809\n",
      "931 ---- 810\n",
      "932 ---- 811\n",
      "933 ---- 812\n",
      "934 ---- 813\n",
      "935 ---- 814\n",
      "936 ---- 815\n",
      "937 ---- 816\n",
      "938 ---- 817\n",
      "939 ---- 818\n",
      "940 ---- 819\n",
      "941 ---- 820\n",
      "942 ---- 821\n",
      "943 ---- 822\n",
      "944 ---- 823\n",
      "945 ---- 824\n",
      "946 ---- 825\n",
      "947 ---- 826\n",
      "948 ---- 827\n",
      "949 ---- 828\n",
      "950 ---- 829\n",
      "951 ---- 830\n",
      "952 ---- 831\n",
      "953 ---- 832\n",
      "954 ---- 833\n",
      "955 ---- 834\n",
      "956 ---- 835\n",
      "957 ---- 836\n",
      "958 ---- 837\n",
      "959 ---- 838\n",
      "960 ---- 839\n",
      "961 ---- 840\n",
      "962 ---- 841\n",
      "963 ---- 842\n",
      "964 ---- 843\n",
      "965 ---- 844\n",
      "966 ---- 845\n",
      "967 ---- 846\n",
      "968 ---- 847\n",
      "969 ---- 848\n",
      "970 ---- 849\n",
      "971 ---- 850\n",
      "972 ---- 851\n",
      "973 ---- 852\n",
      "974 ---- 853\n",
      "975 ---- 854\n",
      "976 ---- 855\n",
      "977 ---- 856\n",
      "978 ---- 857\n",
      "979 ---- 858\n",
      "980 ---- 859\n",
      "981 ---- 860\n",
      "982 ---- 861\n",
      "983 ---- 862\n",
      "984 ---- 863\n",
      "985 ---- 864\n",
      "986 ---- 865\n",
      "987 ---- 866\n",
      "988 ---- 867\n",
      "989 ---- 868\n",
      "990 ---- 869\n",
      "991 ---- 870\n",
      "992 ---- 871\n",
      "993 ---- 872\n",
      "994 ---- 873\n",
      "995 ---- 874\n",
      "996 ---- 875\n",
      "997 ---- 876\n",
      "998 ---- 877\n",
      "999 ---- 878\n",
      "1000 ---- 879\n",
      "1001 ---- 880\n",
      "1002 ---- 881\n",
      "1003 ---- 882\n",
      "1004 ---- 883\n",
      "1005 ---- 884\n",
      "1006 ---- 885\n",
      "1007 ---- 886\n",
      "1008 ---- 887\n",
      "1009 ---- 888\n",
      "1010 ---- 889\n",
      "1011 ---- 890\n",
      "1012 ---- 891\n",
      "1013 ---- 892\n",
      "1014 ---- 893\n",
      "1015 ---- 894\n",
      "1016 ---- 895\n",
      "1017 ---- 896\n",
      "1018 ---- 897\n",
      "1019 ---- 898\n",
      "1020 ---- 899\n",
      "1021 ---- 900\n",
      "1022 ---- 901\n",
      "1023 ---- 902\n",
      "1024 ---- 903\n",
      "1025 ---- 904\n",
      "1026 ---- 905\n",
      "1027 ---- 906\n",
      "1028 ---- 907\n",
      "1029 ---- 908\n",
      "1030 ---- 909\n",
      "1031 ---- 910\n",
      "1032 ---- 911\n",
      "1033 ---- 912\n",
      "1034 ---- 913\n",
      "1035 ---- 914\n",
      "1036 ---- 915\n",
      "1037 ---- 916\n",
      "1038 ---- 917\n",
      "1039 ---- 918\n",
      "1040 ---- 919\n",
      "1041 ---- 920\n",
      "1042 ---- 921\n",
      "1043 ---- 922\n",
      "1044 ---- 923\n",
      "1045 ---- 924\n",
      "1046 ---- 925\n",
      "1047 ---- 926\n",
      "1048 ---- 927\n",
      "1049 ---- 928\n",
      "1050 ---- 929\n",
      "1051 ---- 930\n",
      "1052 ---- 931\n",
      "1053 ---- 932\n",
      "1054 ---- 933\n",
      "1055 ---- 934\n",
      "1056 ---- 935\n",
      "1057 ---- 936\n",
      "1058 ---- 937\n",
      "1059 ---- 938\n",
      "1060 ---- 939\n",
      "1061 ---- 940\n",
      "1062 ---- 941\n",
      "1063 ---- 942\n",
      "1064 ---- 943\n",
      "1065 ---- 944\n",
      "1066 ---- 945\n",
      "1067 ---- 946\n",
      "1068 ---- 947\n",
      "1069 ---- 948\n",
      "1070 ---- 949\n",
      "1071 ---- 950\n",
      "1072 ---- 951\n",
      "1073 ---- 952\n",
      "1074 ---- 953\n",
      "1075 ---- 954\n",
      "1076 ---- 955\n",
      "1077 ---- 956\n",
      "1078 ---- 957\n",
      "1079 ---- 958\n",
      "1080 ---- 959\n",
      "1081 ---- 960\n",
      "1082 ---- 961\n",
      "1083 ---- 962\n",
      "1084 ---- 963\n",
      "1085 ---- 964\n",
      "1086 ---- 965\n",
      "1087 ---- 966\n",
      "1088 ---- 967\n",
      "1089 ---- 968\n",
      "1090 ---- 969\n",
      "1091 ---- 970\n",
      "1092 ---- 971\n",
      "1093 ---- 972\n",
      "1094 ---- 973\n",
      "1095 ---- 974\n",
      "1096 ---- 975\n",
      "1097 ---- 976\n",
      "1098 ---- 977\n",
      "1099 ---- 978\n",
      "1100 ---- 979\n",
      "1101 ---- 980\n",
      "1102 ---- 981\n",
      "1103 ---- 982\n",
      "1104 ---- 983\n",
      "1105 ---- 984\n",
      "1106 ---- 985\n",
      "1107 ---- 986\n",
      "1108 ---- 987\n",
      "1109 ---- 988\n",
      "1110 ---- 989\n",
      "1111 ---- 990\n",
      "1112 ---- 991\n",
      "1113 ---- 992\n",
      "1114 ---- 993\n",
      "1115 ---- 994\n",
      "1116 ---- 995\n",
      "1117 ---- 996\n",
      "1118 ---- 997\n",
      "1119 ---- 998\n",
      "1120 ---- 999\n",
      "1121 ---- 1000\n",
      "1122 ---- 1001\n",
      "1123 ---- 1002\n",
      "1124 ---- 1003\n",
      "1125 ---- 1004\n",
      "1126 ---- 1005\n",
      "1127 ---- 1006\n",
      "1128 ---- 1007\n",
      "1129 ---- 1008\n",
      "1130 ---- 1009\n",
      "1131 ---- 1010\n",
      "1132 ---- 1011\n",
      "1133 ---- 1012\n",
      "1134 ---- 1013\n",
      "1135 ---- 1014\n",
      "1136 ---- 1015\n",
      "1137 ---- 1016\n",
      "1138 ---- 1017\n",
      "1139 ---- 1018\n",
      "1140 ---- 1019\n",
      "1141 ---- 1020\n",
      "1142 ---- 1021\n",
      "1143 ---- 1022\n",
      "1144 ---- 1023\n",
      "1145 ---- 1024\n",
      "1146 ---- 1025\n",
      "1147 ---- 1026\n",
      "1148 ---- 1027\n",
      "1149 ---- 1028\n",
      "1150 ---- 1029\n",
      "1151 ---- 1030\n",
      "1152 ---- 1031\n",
      "1153 ---- 1032\n",
      "1154 ---- 1033\n",
      "1155 ---- 1034\n",
      "1156 ---- 1035\n",
      "1157 ---- 1036\n",
      "1158 ---- 1037\n",
      "1159 ---- 1038\n",
      "1160 ---- 1039\n",
      "1161 ---- 1040\n",
      "1162 ---- 1041\n",
      "1163 ---- 1042\n",
      "1164 ---- 1043\n",
      "1165 ---- 1044\n",
      "1166 ---- 1045\n",
      "1167 ---- 1046\n",
      "1168 ---- 1047\n",
      "1169 ---- 1048\n",
      "1170 ---- 1049\n",
      "1171 ---- 1050\n",
      "1172 ---- 1051\n",
      "1173 ---- 1052\n",
      "1174 ---- 1053\n",
      "1175 ---- 1054\n",
      "1176 ---- 1055\n",
      "1177 ---- 1056\n",
      "1178 ---- 1057\n",
      "1179 ---- 1058\n",
      "1180 ---- 1059\n",
      "1181 ---- 1060\n",
      "1182 ---- 1061\n",
      "1183 ---- 1062\n",
      "1184 ---- 1063\n",
      "1185 ---- 1064\n",
      "1186 ---- 1065\n",
      "1187 ---- 1066\n",
      "1188 ---- 1067\n",
      "1189 ---- 1068\n",
      "1190 ---- 1069\n",
      "1191 ---- 1070\n",
      "1192 ---- 1071\n",
      "1193 ---- 1072\n",
      "1194 ---- 1073\n",
      "1195 ---- 1074\n",
      "1196 ---- 1075\n",
      "1197 ---- 1076\n",
      "1198 ---- 1077\n",
      "1199 ---- 1078\n",
      "1200 ---- 1079\n",
      "1201 ---- 1080\n",
      "1202 ---- 1081\n",
      "1203 ---- 1082\n",
      "1204 ---- 1083\n",
      "1205 ---- 1084\n",
      "1206 ---- 1085\n",
      "1207 ---- 1086\n",
      "1208 ---- 1087\n",
      "1209 ---- 1088\n",
      "1210 ---- 1089\n",
      "1211 ---- 1090\n",
      "1212 ---- 1091\n",
      "1213 ---- 1092\n",
      "1214 ---- 1093\n",
      "1215 ---- 1094\n",
      "1216 ---- 1095\n",
      "1217 ---- 1096\n",
      "1218 ---- 1097\n",
      "1219 ---- 1098\n",
      "1220 ---- 1099\n",
      "1221 ---- 1100\n",
      "1222 ---- 1101\n",
      "1223 ---- 1102\n",
      "1224 ---- 1103\n",
      "1225 ---- 1104\n",
      "1226 ---- 1105\n",
      "1227 ---- 1106\n",
      "1228 ---- 1107\n",
      "1229 ---- 1108\n",
      "1230 ---- 1109\n",
      "1231 ---- 1110\n",
      "1232 ---- 1111\n",
      "1233 ---- 1112\n",
      "1234 ---- 1113\n",
      "1235 ---- 1114\n",
      "1236 ---- 1115\n",
      "1237 ---- 1116\n",
      "1238 ---- 1117\n",
      "1239 ---- 1118\n",
      "1240 ---- 1119\n",
      "1241 ---- 1120\n",
      "1242 ---- 1121\n",
      "1243 ---- 1122\n",
      "1244 ---- 1123\n",
      "1245 ---- 1124\n",
      "1246 ---- 1125\n",
      "1247 ---- 1126\n",
      "1248 ---- 1127\n",
      "1249 ---- 1128\n",
      "1250 ---- 1129\n",
      "1251 ---- 1130\n",
      "1252 ---- 1131\n",
      "1253 ---- 1132\n",
      "1254 ---- 1133\n",
      "1255 ---- 1134\n",
      "1256 ---- 1135\n",
      "1257 ---- 1136\n",
      "1258 ---- 1137\n",
      "1259 ---- 1138\n",
      "1260 ---- 1139\n",
      "1261 ---- 1140\n",
      "1262 ---- 1141\n",
      "1263 ---- 1142\n",
      "1264 ---- 1143\n",
      "1265 ---- 1144\n",
      "1266 ---- 1145\n",
      "1267 ---- 1146\n",
      "1268 ---- 1147\n",
      "1269 ---- 1148\n",
      "1270 ---- 1149\n",
      "1271 ---- 1150\n",
      "1272 ---- 1151\n",
      "1273 ---- 1152\n",
      "1274 ---- 1153\n",
      "1275 ---- 1154\n",
      "1276 ---- 1155\n",
      "1277 ---- 1156\n",
      "1278 ---- 1157\n",
      "1279 ---- 1158\n",
      "1280 ---- 1159\n",
      "1281 ---- 1160\n",
      "1282 ---- 1161\n",
      "1283 ---- 1162\n",
      "1284 ---- 1163\n",
      "1285 ---- 1164\n",
      "1286 ---- 1165\n",
      "1287 ---- 1166\n",
      "1288 ---- 1167\n",
      "1289 ---- 1168\n",
      "1290 ---- 1169\n",
      "1291 ---- 1170\n",
      "1292 ---- 1171\n",
      "1293 ---- 1172\n",
      "1294 ---- 1173\n",
      "1295 ---- 1174\n",
      "1296 ---- 1175\n",
      "1297 ---- 1176\n",
      "1298 ---- 1177\n",
      "1299 ---- 1178\n",
      "1300 ---- 1179\n",
      "1301 ---- 1180\n",
      "1302 ---- 1181\n",
      "1303 ---- 1182\n",
      "1304 ---- 1183\n",
      "1305 ---- 1184\n",
      "1306 ---- 1185\n",
      "1307 ---- 1186\n",
      "1308 ---- 1187\n",
      "1309 ---- 1188\n",
      "1310 ---- 1189\n",
      "1311 ---- 1190\n",
      "1312 ---- 1191\n",
      "1313 ---- 1192\n",
      "1314 ---- 1193\n",
      "1315 ---- 1194\n",
      "1316 ---- 1195\n",
      "1317 ---- 1196\n",
      "1318 ---- 1197\n",
      "1319 ---- 1198\n",
      "1320 ---- 1199\n",
      "1321 ---- 1200\n",
      "1322 ---- 1201\n",
      "1323 ---- 1202\n",
      "1324 ---- 1203\n",
      "1325 ---- 1204\n",
      "1326 ---- 1205\n",
      "1327 ---- 1206\n",
      "1328 ---- 1207\n",
      "1329 ---- 1208\n",
      "1330 ---- 1209\n",
      "1331 ---- 1210\n",
      "1332 ---- 1211\n",
      "1333 ---- 1212\n",
      "1334 ---- 1213\n",
      "1335 ---- 1214\n",
      "1336 ---- 1215\n",
      "1337 ---- 1216\n",
      "1338 ---- 1217\n",
      "1339 ---- 1218\n",
      "1340 ---- 1219\n",
      "1341 ---- 1220\n",
      "1342 ---- 1221\n",
      "1343 ---- 1222\n",
      "1344 ---- 1223\n",
      "1345 ---- 1224\n",
      "1346 ---- 1225\n",
      "1347 ---- 1226\n",
      "1348 ---- 1227\n",
      "1349 ---- 1228\n",
      "1350 ---- 1229\n",
      "1351 ---- 1230\n",
      "1352 ---- 1231\n",
      "1353 ---- 1232\n",
      "1354 ---- 1233\n",
      "1355 ---- 1234\n",
      "1356 ---- 1235\n",
      "1357 ---- 1236\n",
      "1358 ---- 1237\n",
      "1359 ---- 1238\n",
      "1360 ---- 1239\n",
      "1361 ---- 1240\n",
      "1362 ---- 1241\n",
      "1363 ---- 1242\n",
      "1364 ---- 1243\n",
      "1365 ---- 1244\n",
      "1366 ---- 1245\n",
      "1367 ---- 1246\n",
      "1368 ---- 1247\n",
      "1369 ---- 1248\n",
      "1370 ---- 1249\n",
      "1371 ---- 1250\n",
      "1372 ---- 1251\n",
      "1373 ---- 1252\n",
      "1374 ---- 1253\n",
      "1375 ---- 1254\n",
      "1376 ---- 1255\n",
      "1377 ---- 1256\n",
      "1378 ---- 1257\n",
      "1379 ---- 1258\n",
      "1380 ---- 1259\n",
      "1381 ---- 1260\n",
      "1382 ---- 1261\n",
      "1383 ---- 1262\n",
      "1384 ---- 1263\n",
      "1385 ---- 1264\n",
      "1386 ---- 1265\n",
      "1387 ---- 1266\n",
      "1388 ---- 1267\n",
      "1389 ---- 1268\n",
      "1390 ---- 1269\n",
      "1391 ---- 1270\n",
      "1392 ---- 1271\n",
      "1393 ---- 1272\n",
      "1394 ---- 1273\n",
      "1395 ---- 1274\n",
      "1396 ---- 1275\n",
      "1397 ---- 1276\n",
      "1398 ---- 1277\n",
      "1399 ---- 1278\n",
      "1400 ---- 1279\n",
      "1401 ---- 1280\n",
      "1402 ---- 1281\n",
      "1403 ---- 1282\n",
      "1404 ---- 1283\n",
      "1405 ---- 1284\n",
      "1406 ---- 1285\n",
      "1407 ---- 1286\n",
      "1408 ---- 1287\n",
      "1409 ---- 1288\n",
      "1410 ---- 1289\n",
      "1411 ---- 1290\n",
      "1412 ---- 1291\n",
      "1413 ---- 1292\n",
      "1414 ---- 1293\n",
      "1415 ---- 1294\n",
      "1416 ---- 1295\n",
      "1417 ---- 1296\n",
      "1418 ---- 1297\n",
      "1419 ---- 1298\n",
      "1420 ---- 1299\n",
      "1421 ---- 1300\n",
      "1422 ---- 1301\n",
      "1423 ---- 1302\n",
      "1424 ---- 1303\n",
      "1425 ---- 1304\n",
      "1426 ---- 1305\n",
      "1427 ---- 1306\n",
      "1428 ---- 1307\n",
      "1429 ---- 1308\n",
      "1430 ---- 1309\n",
      "1431 ---- 1310\n",
      "1432 ---- 1311\n",
      "1433 ---- 1312\n",
      "1434 ---- 1313\n",
      "1435 ---- 1314\n",
      "1436 ---- 1315\n",
      "1437 ---- 1316\n",
      "1438 ---- 1317\n",
      "1439 ---- 1318\n",
      "1440 ---- 1319\n",
      "1441 ---- 1320\n",
      "1442 ---- 1321\n",
      "1443 ---- 1322\n",
      "1444 ---- 1323\n",
      "1445 ---- 1324\n",
      "1446 ---- 1325\n",
      "1447 ---- 1326\n",
      "1448 ---- 1327\n",
      "1449 ---- 1328\n",
      "1450 ---- 1329\n",
      "1451 ---- 1330\n",
      "1452 ---- 1331\n",
      "1453 ---- 1332\n",
      "1454 ---- 1333\n",
      "1455 ---- 1334\n",
      "1456 ---- 1335\n",
      "1457 ---- 1336\n",
      "1458 ---- 1337\n",
      "1459 ---- 1338\n",
      "1460 ---- 1339\n",
      "1461 ---- 1340\n",
      "1462 ---- 1341\n",
      "1463 ---- 1342\n",
      "1464 ---- 1343\n",
      "1465 ---- 1344\n",
      "1466 ---- 1345\n",
      "1467 ---- 1346\n",
      "1468 ---- 1347\n",
      "1469 ---- 1348\n",
      "1470 ---- 1349\n",
      "1471 ---- 1350\n",
      "1472 ---- 1351\n",
      "1473 ---- 1352\n",
      "1474 ---- 1353\n",
      "1475 ---- 1354\n",
      "1476 ---- 1355\n",
      "1477 ---- 1356\n",
      "1478 ---- 1357\n",
      "1479 ---- 1358\n",
      "1480 ---- 1359\n",
      "1481 ---- 1360\n",
      "1482 ---- 1361\n",
      "1483 ---- 1362\n",
      "1484 ---- 1363\n",
      "1485 ---- 1364\n",
      "1486 ---- 1365\n",
      "1487 ---- 1366\n",
      "1488 ---- 1367\n",
      "1489 ---- 1368\n",
      "1490 ---- 1369\n",
      "1491 ---- 1370\n",
      "1492 ---- 1371\n",
      "1493 ---- 1372\n",
      "1494 ---- 1373\n",
      "1495 ---- 1374\n",
      "1496 ---- 1375\n",
      "1497 ---- 1376\n",
      "1498 ---- 1377\n",
      "1499 ---- 1378\n",
      "1500 ---- 1379\n",
      "1501 ---- 1380\n",
      "1502 ---- 1381\n",
      "1503 ---- 1382\n",
      "1504 ---- 1383\n",
      "1505 ---- 1384\n",
      "1506 ---- 1385\n",
      "1507 ---- 1386\n",
      "1508 ---- 1387\n",
      "1509 ---- 1388\n",
      "1510 ---- 1389\n",
      "1511 ---- 1390\n",
      "1512 ---- 1391\n",
      "1513 ---- 1392\n",
      "1514 ---- 1393\n",
      "1515 ---- 1394\n",
      "1516 ---- 1395\n",
      "1517 ---- 1396\n",
      "1518 ---- 1397\n",
      "1519 ---- 1398\n",
      "1520 ---- 1399\n",
      "1521 ---- 1400\n",
      "1522 ---- 1401\n",
      "1523 ---- 1402\n",
      "1524 ---- 1403\n",
      "1525 ---- 1404\n",
      "1526 ---- 1405\n",
      "1527 ---- 1406\n",
      "1528 ---- 1407\n",
      "1529 ---- 1408\n",
      "1530 ---- 1409\n",
      "1531 ---- 1410\n",
      "1532 ---- 1411\n",
      "1533 ---- 1412\n",
      "1534 ---- 1413\n",
      "1535 ---- 1414\n",
      "1536 ---- 1415\n",
      "1537 ---- 1416\n",
      "1538 ---- 1417\n",
      "1539 ---- 1418\n",
      "1540 ---- 1419\n",
      "1541 ---- 1420\n",
      "1542 ---- 1421\n",
      "1543 ---- 1422\n",
      "1544 ---- 1423\n",
      "1545 ---- 1424\n",
      "1546 ---- 1425\n",
      "1547 ---- 1426\n",
      "1548 ---- 1427\n",
      "1549 ---- 1428\n",
      "1550 ---- 1429\n",
      "1551 ---- 1430\n",
      "1552 ---- 1431\n",
      "1553 ---- 1432\n",
      "1554 ---- 1433\n",
      "1555 ---- 1434\n",
      "1556 ---- 1435\n",
      "1557 ---- 1436\n",
      "1558 ---- 1437\n",
      "1559 ---- 1438\n",
      "1560 ---- 1439\n",
      "1561 ---- 1440\n",
      "1562 ---- 1441\n",
      "1563 ---- 1442\n",
      "1564 ---- 1443\n",
      "1565 ---- 1444\n",
      "1566 ---- 1445\n",
      "1567 ---- 1446\n",
      "1568 ---- 1447\n",
      "1569 ---- 1448\n",
      "1570 ---- 1449\n",
      "1571 ---- 1450\n",
      "1572 ---- 1451\n",
      "1573 ---- 1452\n",
      "1574 ---- 1453\n",
      "1575 ---- 1454\n",
      "1576 ---- 1455\n",
      "1577 ---- 1456\n",
      "1578 ---- 1457\n",
      "1579 ---- 1458\n",
      "1580 ---- 1459\n",
      "1581 ---- 1460\n",
      "1582 ---- 1461\n",
      "1583 ---- 1462\n",
      "1584 ---- 1463\n",
      "1585 ---- 1464\n",
      "1586 ---- 1465\n",
      "1587 ---- 1466\n",
      "1588 ---- 1467\n",
      "1589 ---- 1468\n",
      "1590 ---- 1469\n",
      "1591 ---- 1470\n",
      "1592 ---- 1471\n",
      "1593 ---- 1472\n",
      "1594 ---- 1473\n",
      "1595 ---- 1474\n",
      "1596 ---- 1475\n",
      "1597 ---- 1476\n",
      "1598 ---- 1477\n",
      "1599 ---- 1478\n",
      "1600 ---- 1479\n",
      "1601 ---- 1480\n",
      "1602 ---- 1481\n",
      "1603 ---- 1482\n",
      "1604 ---- 1483\n",
      "1605 ---- 1484\n",
      "1606 ---- 1485\n",
      "1607 ---- 1486\n",
      "1608 ---- 1487\n",
      "1609 ---- 1488\n",
      "1610 ---- 1489\n",
      "1611 ---- 1490\n",
      "1612 ---- 1491\n",
      "1613 ---- 1492\n",
      "1614 ---- 1493\n",
      "1615 ---- 1494\n",
      "1616 ---- 1495\n",
      "1617 ---- 1496\n",
      "1618 ---- 1497\n",
      "1619 ---- 1498\n",
      "1620 ---- 1499\n",
      "1621 ---- 1500\n",
      "1622 ---- 1501\n",
      "1623 ---- 1502\n",
      "1624 ---- 1503\n",
      "1625 ---- 1504\n",
      "1626 ---- 1505\n",
      "1627 ---- 1506\n",
      "1628 ---- 1507\n",
      "1629 ---- 1508\n",
      "1630 ---- 1509\n",
      "1631 ---- 1510\n",
      "1632 ---- 1511\n",
      "1633 ---- 1512\n",
      "1634 ---- 1513\n",
      "1635 ---- 1514\n",
      "1636 ---- 1515\n",
      "1637 ---- 1516\n",
      "1638 ---- 1517\n",
      "1639 ---- 1518\n",
      "1640 ---- 1519\n",
      "1641 ---- 1520\n",
      "1642 ---- 1521\n",
      "1643 ---- 1522\n",
      "1644 ---- 1523\n",
      "1645 ---- 1524\n",
      "1646 ---- 1525\n",
      "1647 ---- 1526\n",
      "1648 ---- 1527\n",
      "1649 ---- 1528\n",
      "1650 ---- 1529\n",
      "1651 ---- 1530\n",
      "1652 ---- 1531\n",
      "1653 ---- 1532\n",
      "1654 ---- 1533\n",
      "1655 ---- 1534\n",
      "1656 ---- 1535\n",
      "1657 ---- 1536\n",
      "1658 ---- 1537\n",
      "1659 ---- 1538\n",
      "1660 ---- 1539\n",
      "1661 ---- 1540\n",
      "1662 ---- 1541\n",
      "1663 ---- 1542\n",
      "1664 ---- 1543\n",
      "1665 ---- 1544\n",
      "1666 ---- 1545\n",
      "1667 ---- 1546\n",
      "1668 ---- 1547\n",
      "1669 ---- 1548\n",
      "1670 ---- 1549\n",
      "1671 ---- 1550\n",
      "1672 ---- 1551\n",
      "1673 ---- 1552\n",
      "1674 ---- 1553\n",
      "1675 ---- 1554\n",
      "1676 ---- 1555\n",
      "1677 ---- 1556\n",
      "1678 ---- 1557\n",
      "1679 ---- 1558\n",
      "1680 ---- 1559\n",
      "1681 ---- 1560\n",
      "1682 ---- 1561\n",
      "1683 ---- 1562\n",
      "1684 ---- 1563\n",
      "1685 ---- 1564\n",
      "1686 ---- 1565\n",
      "1687 ---- 1566\n",
      "1688 ---- 1567\n",
      "1689 ---- 1568\n",
      "1690 ---- 1569\n",
      "1691 ---- 1570\n",
      "1692 ---- 1571\n",
      "1693 ---- 1572\n",
      "1694 ---- 1573\n",
      "1695 ---- 1574\n",
      "1696 ---- 1575\n",
      "1697 ---- 1576\n",
      "1698 ---- 1577\n",
      "1699 ---- 1578\n",
      "1700 ---- 1579\n",
      "1701 ---- 1580\n",
      "1702 ---- 1581\n",
      "1703 ---- 1582\n",
      "1704 ---- 1583\n",
      "1705 ---- 1584\n",
      "1706 ---- 1585\n",
      "1707 ---- 1586\n",
      "1708 ---- 1587\n",
      "1709 ---- 1588\n",
      "1710 ---- 1589\n",
      "1711 ---- 1590\n",
      "1712 ---- 1591\n",
      "1713 ---- 1592\n",
      "1714 ---- 1593\n",
      "1715 ---- 1594\n",
      "1716 ---- 1595\n",
      "1717 ---- 1596\n",
      "1718 ---- 1597\n",
      "1719 ---- 1598\n",
      "1720 ---- 1599\n",
      "1721 ---- 1600\n",
      "1722 ---- 1601\n",
      "1723 ---- 1602\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(X_train_final.columns)):\n",
    "    print('{} ---- {}'.format(i, X_train_final.columns[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Unigrams, POS Tag Count, Sentiment Polarity, Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_1 = X_train_final.iloc[:,np.r_[10:12,13:21,121:1723]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2380, 1612)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_1 = X_test_final.iloc[:,np.r_[10:12,13:21,121:1723]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265, 1612)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.490\n",
      "Best parameters set:\n",
      "\tclf__C: 1\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9648    0.9125    0.9379       240\n",
      "           1     0.4474    0.6800    0.5397        25\n",
      "\n",
      "    accuracy                         0.8906       265\n",
      "   macro avg     0.7061    0.7963    0.7388       265\n",
      "weighted avg     0.9159    0.8906    0.9003       265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_1_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_1_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_1,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_1), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 219, False Positive: 21, False Negative: 8, True Positive: 17\n",
      "--------------------------------------------------------------------------------\n",
      "[[219  21]\n",
      " [  8  17]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.94       240\n",
      "           1       0.45      0.68      0.54        25\n",
      "\n",
      "    accuracy                           0.89       265\n",
      "   macro avg       0.71      0.80      0.74       265\n",
      "weighted avg       0.92      0.89      0.90       265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_1 = LogisticRegression(random_state=18, \n",
    "                                solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], \n",
    "                                class_weight='balanced').fit(X_train_model_1, y_train)\n",
    "y_lr = lr_model_1.predict(X_test_model_1)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_model_2 = X_train_final.iloc[:,np.r_[3:1113]]\n",
    "X_train_model_2 = X_train_final.iloc[:, np.r_[3:1723]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2380, 1720)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_2 = X_test_final.iloc[:,np.r_[3:1723]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265, 1720)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.489\n",
      "Best parameters set:\n",
      "\tclf__C: 1\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9605    0.9125    0.9359       240\n",
      "           1     0.4324    0.6400    0.5161        25\n",
      "\n",
      "    accuracy                         0.8868       265\n",
      "   macro avg     0.6965    0.7762    0.7260       265\n",
      "weighted avg     0.9107    0.8868    0.8963       265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_2_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_2_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_2,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_2), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 219, False Positive: 21, False Negative: 9, True Positive: 16\n",
      "--------------------------------------------------------------------------------\n",
      "[[219  21]\n",
      " [  9  16]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.94       240\n",
      "           1       0.43      0.64      0.52        25\n",
      "\n",
      "    accuracy                           0.89       265\n",
      "   macro avg       0.70      0.78      0.73       265\n",
      "weighted avg       0.91      0.89      0.90       265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_2 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_2, y_train)\n",
    "y_lr = lr_model_2.predict(X_test_model_2)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Without Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_3 = X_train_final.iloc[:,np.r_[3:121]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2380, 118)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_3 = X_test_final.iloc[:,np.r_[3:121]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265, 118)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n",
      "Best score: 0.440\n",
      "Best parameters set:\n",
      "\tclf__C: 25\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9848    0.8083    0.8879       240\n",
      "           1     0.3235    0.8800    0.4731        25\n",
      "\n",
      "    accuracy                         0.8151       265\n",
      "   macro avg     0.6542    0.8442    0.6805       265\n",
      "weighted avg     0.9224    0.8151    0.8487       265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_3_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.0001, 0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\", \"elasticnet\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_3_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_3,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_3), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 194, False Positive: 46, False Negative: 3, True Positive: 22\n",
      "--------------------------------------------------------------------------------\n",
      "[[194  46]\n",
      " [  3  22]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.81      0.89       240\n",
      "           1       0.32      0.88      0.47        25\n",
      "\n",
      "    accuracy                           0.82       265\n",
      "   macro avg       0.65      0.84      0.68       265\n",
      "weighted avg       0.92      0.82      0.85       265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_3 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_3, y_train)\n",
    "y_lr = lr_model_3.predict(X_test_model_3)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Without Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_4 = X_train_final.iloc[:,np.r_[3:21,121:1723]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2380, 1620)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_4 = X_test_final.iloc[:,np.r_[3:21,121:1723]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265, 1620)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.492\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9764    0.8625    0.9159       240\n",
      "           1     0.3774    0.8000    0.5128        25\n",
      "\n",
      "    accuracy                         0.8566       265\n",
      "   macro avg     0.6769    0.8313    0.7144       265\n",
      "weighted avg     0.9199    0.8566    0.8779       265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_4_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_4_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_4,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_4), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 207, False Positive: 33, False Negative: 5, True Positive: 20\n",
      "--------------------------------------------------------------------------------\n",
      "[[207  33]\n",
      " [  5  20]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.86      0.92       240\n",
      "           1       0.38      0.80      0.51        25\n",
      "\n",
      "    accuracy                           0.86       265\n",
      "   macro avg       0.68      0.83      0.71       265\n",
      "weighted avg       0.92      0.86      0.88       265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_4 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_4, y_train)\n",
    "y_lr = lr_model_4.predict(X_test_model_4)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Without POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_5 = X_train_final.iloc[:,np.r_[3:13,21:1723]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2380, 1712)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_5 = X_test_final.iloc[:,np.r_[3:13,21:1723]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265, 1712)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.503\n",
      "Best parameters set:\n",
      "\tclf__C: 0.01\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.85      0.90       240\n",
      "           1       0.34      0.76      0.47        25\n",
      "\n",
      "    accuracy                           0.84       265\n",
      "   macro avg       0.66      0.80      0.69       265\n",
      "weighted avg       0.91      0.84      0.86       265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_5_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_5_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_5,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_5), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 203, False Positive: 37, False Negative: 6, True Positive: 19\n",
      "--------------------------------------------------------------------------------\n",
      "[[203  37]\n",
      " [  6  19]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.85      0.90       240\n",
      "           1       0.34      0.76      0.47        25\n",
      "\n",
      "    accuracy                           0.84       265\n",
      "   macro avg       0.66      0.80      0.69       265\n",
      "weighted avg       0.91      0.84      0.86       265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_5 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_5, y_train)\n",
    "y_lr = lr_model_5.predict(X_test_model_5)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6: Without STEM Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_6 = X_train_final.iloc[:,np.r_[10:1723]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2380, 1713)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_6 = X_test_final.iloc[:,np.r_[10:1723]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265, 1713)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.488\n",
      "Best parameters set:\n",
      "\tclf__C: 1\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.94       240\n",
      "           1       0.44      0.68      0.53        25\n",
      "\n",
      "    accuracy                           0.89       265\n",
      "   macro avg       0.70      0.79      0.73       265\n",
      "weighted avg       0.91      0.89      0.90       265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_6_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_6_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_6,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_6), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 218, False Positive: 22, False Negative: 8, True Positive: 17\n",
      "--------------------------------------------------------------------------------\n",
      "[[218  22]\n",
      " [  8  17]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.94       240\n",
      "           1       0.44      0.68      0.53        25\n",
      "\n",
      "    accuracy                           0.89       265\n",
      "   macro avg       0.70      0.79      0.73       265\n",
      "weighted avg       0.91      0.89      0.90       265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_6 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_6, y_train)\n",
    "y_lr = lr_model_6.predict(X_test_model_6)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7: Without Sentiment Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_7 = X_train_final.iloc[:,np.r_[3:10,12:1723]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2380, 1718)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_7 = X_test_final.iloc[:,np.r_[3:10,12:1723]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265, 1718)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.492\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.87      0.92       240\n",
      "           1       0.39      0.80      0.53        25\n",
      "\n",
      "    accuracy                           0.86       265\n",
      "   macro avg       0.68      0.84      0.72       265\n",
      "weighted avg       0.92      0.86      0.88       265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_7_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_7_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_7,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_7), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 209, False Positive: 31, False Negative: 5, True Positive: 20\n",
      "--------------------------------------------------------------------------------\n",
      "[[209  31]\n",
      " [  5  20]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.87      0.92       240\n",
      "           1       0.39      0.80      0.53        25\n",
      "\n",
      "    accuracy                           0.86       265\n",
      "   macro avg       0.68      0.84      0.72       265\n",
      "weighted avg       0.92      0.86      0.88       265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_7 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_7, y_train)\n",
    "y_lr = lr_model_7.predict(X_test_model_7)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 8: Without NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_8 = X_train_final.iloc[:,np.r_[3:12,13:1723]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2380, 1719)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_8 = X_test_final.iloc[:,np.r_[3:12,13:1723]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265, 1719)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.489\n",
      "Best parameters set:\n",
      "\tclf__C: 1\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.94       240\n",
      "           1       0.43      0.64      0.52        25\n",
      "\n",
      "    accuracy                           0.89       265\n",
      "   macro avg       0.70      0.78      0.73       265\n",
      "weighted avg       0.91      0.89      0.90       265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_8_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_8_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_8,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_8), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 219, False Positive: 21, False Negative: 9, True Positive: 16\n",
      "--------------------------------------------------------------------------------\n",
      "[[219  21]\n",
      " [  9  16]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.94       240\n",
      "           1       0.43      0.64      0.52        25\n",
      "\n",
      "    accuracy                           0.89       265\n",
      "   macro avg       0.70      0.78      0.73       265\n",
      "weighted avg       0.91      0.89      0.90       265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_8 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_8, y_train)\n",
    "y_lr = lr_model_8.predict(X_test_model_8)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Summary\n",
    "| Experiment | Model Number | Features Used                                                | Precision | Recall | Macro F1 |\n",
    "| :--------: | :----------: | :----------------------------------------------------------: | :-------: | :----: | :------: |\n",
    "| Baseline   | 1            | Unigrams, POS Tag Count, Sentiment Polarity and Subjectivity | 0.64      | 0.74   | 0.66     |\n",
    "| Baseline   | 2            | All features (baseline)                                      | 0.64      | 0.76   | 0.66     |\n",
    "| Baseline   | 3            | Without Unigrams                                             | 0.61      | 0.72   | 0.59     |\n",
    "| Baseline   | 4            | Without Embeddings                                           | 0.64      | 0.74   | 0.66     |\n",
    "| Baseline   | 5            | Without POS tag                                              | 0.64      | 0.75   | 0.66     |\n",
    "| Baseline   | 6            | Without STEM similarity (paper baseline)                     | 0.65      | 0.77   | 0.67     |\n",
    "| Baseline   | 7            | Without sentiment features                                   | 0.64      | 0.75   | 0.66     |\n",
    "| Baseline   | 8            | Without NER                                                  | 0.64      | 0.76   | 0.66     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
