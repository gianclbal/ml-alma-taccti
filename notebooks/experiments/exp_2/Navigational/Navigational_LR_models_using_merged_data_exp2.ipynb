{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigational Logistic Regression Models Using Merged Data Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3c4e630c874b3680b1d25f730c64e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 08:42:18 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-10-02 08:42:18 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-10-02 08:42:20 INFO: File exists: /Users/gbaldonado/stanza_resources/en/default.zip\n",
      "2024-10-02 08:42:22 INFO: Finished downloading models and saved to /Users/gbaldonado/stanza_resources\n",
      "2024-10-02 08:42:22 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcfb3f5fbd72492f8932c8f29d5bca90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 08:42:24 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-10-02 08:42:26 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-10-02 08:42:26 INFO: Using device: cpu\n",
      "2024-10-02 08:42:26 INFO: Loading: tokenize\n",
      "2024-10-02 08:42:26 INFO: Loading: mwt\n",
      "2024-10-02 08:42:26 INFO: Loading: pos\n",
      "2024-10-02 08:42:26 INFO: Loading: lemma\n",
      "2024-10-02 08:42:26 INFO: Loading: constituency\n",
      "2024-10-02 08:42:27 INFO: Loading: depparse\n",
      "2024-10-02 08:42:27 INFO: Loading: sentiment\n",
      "2024-10-02 08:42:27 INFO: Loading: ner\n",
      "2024-10-02 08:42:28 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import pickle\n",
    "import warnings\n",
    "import stanza\n",
    "\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, f1_score, r2_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Set random seed\n",
    "random.seed(18)\n",
    "seed = 18\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Initialize lemmatizer, stop words, and stanza\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline('en') # initialize English neural pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data and quick exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_navigational_df_batch_1 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/merged_Navigational_sentence_level_batch_1_jaccard.csv\", encoding='utf-8')\n",
    "merged_navigational_df_batch_2 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/Navigational Plus_sentence_level_batch_2_jaccard.csv\", encoding='utf-8')\n",
    "\n",
    "merged_navigational_df = pd.concat([merged_navigational_df_batch_1, merged_navigational_df_batch_2])\n",
    "\n",
    "# Shuffle the merged dataset\n",
    "merged_familial_df = shuffle(merged_navigational_df, random_state=seed)\n",
    "\n",
    "# Train-test split \n",
    "training_df, test_df = train_test_split(merged_familial_df, test_size=0.3, random_state=42, stratify=merged_navigational_df['label'])\n",
    "\n",
    "training_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i honestly don't know how this course will help me for my future goals or careers.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I am currently taking this physics 111112 class because it is a requirement for my major. I am currently a Biology major so I have no choice but to take it.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is why i am here and i am glad that i am.</td>\n",
       "      <td>0</td>\n",
       "      <td>['First, I took AP Physics in high school, so I thought I had a good grasp of things that would go on in class.I thought I had a decent understanding of things each day, before going to class, but I found myself leaving class extremely confused. So, I switched sections and added this supplementary course.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>so to summarize my second point, i want to encourage people to take an interest in stem because it can be rewarding and if you don't do it then there's a chance you will not have a job in the future.</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"The short version of that story is that I grew up kind of poor, joined the military at 18 years old, used my VA benefits to go school after I separated from active duty, went to art school for a few years, took a class where we learned to develop a mobile application by leveraging preexisting GUI's, and that was the moment when I first got exposed to a little bit of STEM. If you would have told high school me that I'd be 2 semesters away from graduating with a BS in electrical engineering, I would never have believed it, mostly because my grades were terrible, but also because I had no idea what STEM was back then.\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i hope that by the end of this semester that i can get a better grade than without the sci classes help, and be able to go into physics 2 with confidence and the knowledge needed to do well there as well.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I am in this class because I am not good at math. I know physics is very math heavy and conceptually confusing, so I knew sci would be perfect for me this semester. I also have to take physics 2 next semester so I would like to have a firm understanding of the basic concepts before they get even more complicated.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i hope our professor stays this cool and doesnt change, he's really cool the way he is.</td>\n",
       "      <td>0</td>\n",
       "      <td>['The reason Im in this class is to fill my requirement. As a engineer I need this class to get in to my classes.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                       sentence  \\\n",
       "0                                                                                                                            i honestly don't know how this course will help me for my future goals or careers.   \n",
       "1                                                                                                                                                                this is why i am here and i am glad that i am.   \n",
       "2       so to summarize my second point, i want to encourage people to take an interest in stem because it can be rewarding and if you don't do it then there's a chance you will not have a job in the future.   \n",
       "3  i hope that by the end of this semester that i can get a better grade than without the sci classes help, and be able to go into physics 2 with confidence and the knowledge needed to do well there as well.   \n",
       "4                                                                                                                       i hope our professor stays this cool and doesnt change, he's really cool the way he is.   \n",
       "\n",
       "   label  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      0   \n",
       "3      0   \n",
       "4      0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               phrase  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ['I am currently taking this physics 111112 class because it is a requirement for my major. I am currently a Biology major so I have no choice but to take it.']  \n",
       "1                                                                                                                                                                                                                                                                                                                                ['First, I took AP Physics in high school, so I thought I had a good grasp of things that would go on in class.I thought I had a decent understanding of things each day, before going to class, but I found myself leaving class extremely confused. So, I switched sections and added this supplementary course.']  \n",
       "2  [\"The short version of that story is that I grew up kind of poor, joined the military at 18 years old, used my VA benefits to go school after I separated from active duty, went to art school for a few years, took a class where we learned to develop a mobile application by leveraging preexisting GUI's, and that was the moment when I first got exposed to a little bit of STEM. If you would have told high school me that I'd be 2 semesters away from graduating with a BS in electrical engineering, I would never have believed it, mostly because my grades were terrible, but also because I had no idea what STEM was back then.\"]  \n",
       "3                                                                                                                                                                                                                                                                                                                      ['I am in this class because I am not good at math. I know physics is very math heavy and conceptually confusing, so I knew sci would be perfect for me this semester. I also have to take physics 2 next semester so I would like to have a firm understanding of the basic concepts before they get even more complicated.']  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ['The reason Im in this class is to fill my requirement. As a engineer I need this class to get in to my classes.']  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>whether it would be biology or chemistry, i knew that i would need extra support when it comes to science.</td>\n",
       "      <td>0</td>\n",
       "      <td>['This is my first time taking a tutoring class and I knew I made the right choice deciding to take this science class I will need the help with chemistry either I took this or not. I knew that I was pretty bad with science as a subject since I was little. Whether it would be biology or chemistry, I knew that I would need extra support when it comes to science.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i am here to make my mother proud and also to be a great example to my siblings and my family in general.</td>\n",
       "      <td>0</td>\n",
       "      <td>['I am here to further expand my education and to get a degree so that I can move onto my life and live a good career. I am here because I have class and because i have a lot of other responsibilities to attend. I am here to study so that I could do well in all my classes and so that I can keep up with any work I have to finish.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>its the most important that, i think my practice ability also have a increase to a degree.</td>\n",
       "      <td>0</td>\n",
       "      <td>['Beside getting the credit and meeting the GE requirement, I have the interest in it.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this will prepare me to be successful in my upper division courses.</td>\n",
       "      <td>1</td>\n",
       "      <td>['This will prepare me to be successful in my upper division courses.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>but i am really excited and nervous to take the class because it is my first physics class ever and i wish it was in person instead of online.</td>\n",
       "      <td>0</td>\n",
       "      <td>['The reason that I am taking general physics is that I need the class as a prerequirement for medical school. At pretty much all the medical schools I am looking into I need one year of general physics, general chemistry, and general biology. But I am really excited and nervous to take the class because it is my first physics class ever and I wish it was in person instead of online. With that being said even if I did not need it for prerequirement for medical school I would have still taken at least a semester of this class during my four years at college.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                         sentence  \\\n",
       "0                                      whether it would be biology or chemistry, i knew that i would need extra support when it comes to science.   \n",
       "1                                       i am here to make my mother proud and also to be a great example to my siblings and my family in general.   \n",
       "2                                                      its the most important that, i think my practice ability also have a increase to a degree.   \n",
       "3                                                                             this will prepare me to be successful in my upper division courses.   \n",
       "4  but i am really excited and nervous to take the class because it is my first physics class ever and i wish it was in person instead of online.   \n",
       "\n",
       "   label  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      0   \n",
       "3      1   \n",
       "4      0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  phrase  \n",
       "0                                                                                                                                                                                                          ['This is my first time taking a tutoring class and I knew I made the right choice deciding to take this science class I will need the help with chemistry either I took this or not. I knew that I was pretty bad with science as a subject since I was little. Whether it would be biology or chemistry, I knew that I would need extra support when it comes to science.']  \n",
       "1                                                                                                                                                                                                                                           ['I am here to further expand my education and to get a degree so that I can move onto my life and live a good career. I am here because I have class and because i have a lot of other responsibilities to attend. I am here to study so that I could do well in all my classes and so that I can keep up with any work I have to finish.']  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ['Beside getting the credit and meeting the GE requirement, I have the interest in it.']  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ['This will prepare me to be successful in my upper division courses.']  \n",
       "4  ['The reason that I am taking general physics is that I need the class as a prerequirement for medical school. At pretty much all the medical schools I am looking into I need one year of general physics, general chemistry, and general biology. But I am really excited and nervous to take the class because it is my first physics class ever and I wish it was in person instead of online. With that being said even if I did not need it for prerequirement for medical school I would have still taken at least a semester of this class during my four years at college.']  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (5741, 3) \n",
      "Test dataset shape: (2461, 3)\n",
      "Positive labels present in the dataset : 334  out of 5741 or 5.817801776693956%\n",
      "Positive labels present in the test dataset : 144  out of 2461 or 5.851279967492888%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset shape: {training_df.shape} \\nTest dataset shape: {test_df.shape}\")\n",
    "pos_labels = len([n for n in training_df['label'] if n==1])\n",
    "print(\"Positive labels present in the dataset : {}  out of {} or {}%\".format(pos_labels, len(training_df['label']), (pos_labels/len(training_df['label']))*100))\n",
    "pos_labels = len([n for n in test_df['label'] if n==1])\n",
    "print(\"Positive labels present in the test dataset : {}  out of {} or {}%\".format(pos_labels, len(test_df['label']), (pos_labels/len(test_df['label']))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the data for the plots\n",
    "training_counts = training_df['label'].value_counts()\n",
    "test_counts = test_df['label'].value_counts()\n",
    "\n",
    "# Set up the subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot for the training set\n",
    "sns.barplot(x=training_counts.index, y=training_counts.values, ax=axes[0])\n",
    "axes[0].set_title('Distribution of labels in training set')\n",
    "axes[0].set_ylabel('Sentences')\n",
    "axes[0].set_xlabel('Label')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot for the test set\n",
    "sns.barplot(x=test_counts.index, y=test_counts.values, ax=axes[1])\n",
    "axes[1].set_title('Distribution of labels in test set')\n",
    "axes[1].set_ylabel('Sentences')\n",
    "axes[1].set_xlabel('Label')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack Obama']\n"
     ]
    }
   ],
   "source": [
    "def get_ner(text):\n",
    "    ner_list = []\n",
    "    # Annotate the text using stanza\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for entity in sentence.ents:\n",
    "            if entity.type == 'PERSON':\n",
    "                ner_list.append(entity.text)\n",
    "\n",
    "    return ner_list\n",
    "\n",
    "# Example usage\n",
    "text = \"Barack Obama was the 44th doctor of the United States.\"\n",
    "print(get_ner(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a named entity is present in the sentence\n",
    "def named_entity_present(sentence):\n",
    "    ner_list = get_ner(sentence)\n",
    "    if len(ner_list) > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Similarity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the similar words and similarity score\n",
    "# The function takes tokens of sentence as input and if its not a stop word, get its similarity with synsets of STEM.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words |= set([\"help\",\"try\", \"work\", \"process\", \"support\", \"job\"] )\n",
    "def word_similarity(tokens, syns, field):    \n",
    "    if field in ['engineering', 'technology']:\n",
    "        score_threshold = 0.5\n",
    "    else:\n",
    "        score_threshold = 0.2\n",
    "    sim_words = 0\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            try:\n",
    "                syns_word = wordnet.synsets(token) \n",
    "                score = syns_word[0].path_similarity(syns[0])\n",
    "                if score >= score_threshold:\n",
    "                    sim_words += 1\n",
    "            except: \n",
    "                score = 0\n",
    "    \n",
    "    return sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to create columns for similarity based on all STEM fields\n",
    "syns_bio = wordnet.synsets(lemmatizer.lemmatize(\"biology\"))\n",
    "syns_maths = wordnet.synsets(lemmatizer.lemmatize(\"mathematics\")) \n",
    "syns_tech = wordnet.synsets(lemmatizer.lemmatize(\"technology\"))\n",
    "syns_eng = wordnet.synsets(lemmatizer.lemmatize(\"engineering\"))\n",
    "syns_chem = wordnet.synsets(lemmatizer.lemmatize(\"chemistry\"))\n",
    "syns_phy = wordnet.synsets(lemmatizer.lemmatize(\"physics\"))\n",
    "syns_sci = wordnet.synsets(lemmatizer.lemmatize(\"science\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Medical Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['urology', 'occupational', 'ophthalmology', 'diseases', 'anterior', 'sports', 'genitourinary', 'neurourology', 'family', 'genetics', 'injury', 'neuroradiology', 'neuroradiology', 'physical', 'infertility', 'gynecologic', 'public', 'pulmonary', 'interventional', 'psychiatric', 'pediatric', 'uveitis', 'radiology', 'critical', 'clinical', 'transplant', 'genetic', 'genetic', 'breast', 'disabilities', 'cornea', 'rheumatology', 'infectious', 'child', 'male', 'cardiology', 'pelvic', 'chemical', 'endocrinology', 'internal', 'anesthesiology', 'neuromuscular', 'administrative', 'cardiovascular', 'abuse', 'hospice', 'gastrointestinal', 'immunology', 'ophthalmic', 'endocrinologists', 'hepatology', 'consultation', 'hematology', 'health', 'orbit', 'neuropathology', 'interventional', 'infectious', 'rheumatology', 'cardiac', 'diabetes', 'urologic', 'gynecology', 'fetal', 'procedural', 'psychiatry', 'ocular', 'reproductive', 'neuro', 'surgery', 'pulmonology', 'advanced', 'sleep', 'musculoskeletal', 'glaucoma', 'emergency', 'head', 'research', 'psychosomatic', 'female', 'microbiology', 'ophthalmology', 'internal', 'reconstructive', 'strabismus', 'geriatric', 'blood', 'adolescent', 'toxicology', 'vascular', 'cytopathology', 'endocrinology', 'psychiatry', 'nephrology', 'surgery', 'dermatopathology', 'disease', 'plastic', 'forensic', 'neonatal', 'dermatology', 'calculi', 'neurology', 'neurophysiology', 'allergy', 'neurodevelopmental', 'addiction', 'pathology', 'and', 'oncology', 'pediatrics', 'heart', 'oculoplastics', 'abdominal', 'cardiothoracic', 'reconstructive', 'gastroenterology', 'imaging', 'retina', 'adolescent', 'pathology', 'developmental', 'metabolism', 'diagnostic', 'anatomical', 'anesthesiology', 'hematology', 'immunopathology', 'community', 'aerospace', 'preventive', 'behavioral', 'renal', 'chest', 'gastroenterology', 'urology', 'transplant', 'banking', 'critical', 'radiation', 'surgical', 'transfusion', 'cytogenetics', 'mental', 'pediatrics', 'neck', 'perinatal', 'medicine', 'retardation', 'pain', 'sports', 'medical', 'obstetrics', 'rehabilitation', 'molecular', 'segment', 'nephrology', 'palliative', 'military', 'maternal', 'neurology', 'biochemical', 'endovascular', 'electrophysiology', 'pediatric', 'care', 'dermatology', 'nuclear', 'oncology', 'failure', 'liaison', 'brain']\n"
     ]
    }
   ],
   "source": [
    "# Load the medical specialization text file and create a list\n",
    "medical_list = []\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/medical_specialities.txt', 'r') as medical_fields:\n",
    "    for line in medical_fields.readlines():\n",
    "        special_field = line.rstrip('\\n')\n",
    "        special_field = re.sub(\"\\W\",\" \", special_field )\n",
    "#         print(special_field)\n",
    "        medical_list += special_field.split()\n",
    "medical_list = list(set(medical_list))  \n",
    "medical_list = [x.lower() for x in medical_list]\n",
    "print(medical_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get medical words\n",
    "def check_medical_words(tokens):\n",
    "    for token in tokens:\n",
    "        if token not in stop_words and token in [x.lower() for x in medical_list]:\n",
    "            return 1\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sentiment Polarity and Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get polarity and subjectivity of the sentence using TexBlob\n",
    "def get_sentiment(sentence):\n",
    "    sentiments =TextBlob(sentence).sentiment\n",
    "    polarity = sentiments.polarity\n",
    "    subjectivity = sentiments.subjectivity\n",
    "    return polarity, subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the count of POS tags of the sentence\n",
    "def count_pos_tags(tokens):\n",
    "    token_pos = pos_tag(tokens)\n",
    "    count = Counter(tag for word,tag in token_pos)\n",
    "    interjections =  count['UH']\n",
    "    nouns = count['NN'] + count['NNS'] + count['NNP'] + count['NNPS']\n",
    "    adverb = count['RB'] + count['RBS'] + count['RBR']\n",
    "    verb = count['VB'] + count['VBD'] + count['VBG'] + count['VBN']\n",
    "    determiner = count['DT']\n",
    "    pronoun = count['PRP']\n",
    "    adjetive = count['JJ'] + count['JJR'] + count['JJS']\n",
    "    preposition = count['IN']\n",
    "    return interjections, nouns, adverb, verb, determiner, pronoun, adjetive,preposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_extraction(dataframe, field, func, column_names):\n",
    "    return pd.concat((\n",
    "        dataframe,\n",
    "        dataframe[field].apply(\n",
    "            lambda cell: pd.Series(func(cell), index=column_names))), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the w2v dict from pickle file\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/pickle/embeddings10022024.pickle', 'rb') as w2v_file:\n",
    "    w2v_dict = pickle.load(w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of word embeddings:  6142\n"
     ]
    }
   ],
   "source": [
    "print(\"length of word embeddings: \", len(w2v_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vectors for the essay\n",
    "def vectorizer(sequence):\n",
    "    vect = []\n",
    "    numw = 0\n",
    "    for w in sequence: \n",
    "        try :\n",
    "            if numw == 0:\n",
    "                vect = w2v_dict[w]\n",
    "            else:\n",
    "                vect = np.add(vect, w2v_dict[w])\n",
    "            numw += 1\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    return vect/ numw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into words\n",
    "def split_into_words(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vectorizer\n",
    "unigram_vect = CountVectorizer(ngram_range=(1, 1), min_df=2, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Putting them all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for feature engineering\n",
    "def feature_engineering(original_dataset):\n",
    "\n",
    "    dataset = original_dataset.copy()\n",
    "    # create a new column with sentence tokens\n",
    "    dataset['tokens'] = dataset['sentence'].apply(word_tokenize)\n",
    "    # 1. Similarity features\n",
    "    # biology\n",
    "    dataset['bio_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_bio,'biology',)) \n",
    "    # chemistry\n",
    "    dataset['chem_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_chem,'chemistry',))\n",
    "    # physics\n",
    "    dataset['phy_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_phy,'physics',))\n",
    "    # mathematics\n",
    "    dataset['math_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_maths,'mathematics',))\n",
    "    # technology\n",
    "    dataset['tech_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_tech,'technology',))\n",
    "    # engineering\n",
    "    dataset['eng_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_eng,'engineering',))\n",
    "    \n",
    "    # medical terms\n",
    "    dataset['medical_terms'] = dataset['tokens'].apply(check_medical_words)\n",
    "    \n",
    "    # polarity and subjectivity\n",
    "    dataset['polarity'], dataset['subjectivity'] = zip(*dataset['sentence'].apply(get_sentiment))\n",
    "    \n",
    "    # named entity recognition\n",
    "    dataset['ner'] = dataset['sentence'].apply(named_entity_present)\n",
    "    \n",
    "    # pos tag count\n",
    "    dataset = pos_tag_extraction(dataset, 'tokens', count_pos_tags, ['interjections', 'nouns', 'adverb', 'verb', 'determiner', 'pronoun', 'adjetive','preposition'])\n",
    "    \n",
    "    # labels\n",
    "    data_labels = dataset['label']\n",
    "    # X\n",
    "    data_x = dataset.drop(columns='label')\n",
    "\n",
    "    \n",
    "    # vectorize all the essays\n",
    "    vect_arr = data_x.tokens.apply(vectorizer)\n",
    "    for index in range(0, len(vect_arr)):\n",
    "        i = 0\n",
    "        for item in vect_arr[index]:\n",
    "            column_name= \"embedding\" + str(i)\n",
    "            data_x.loc[index, column_name] = item\n",
    "            i +=1\n",
    "    \n",
    "    return data_x,data_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = feature_engineering(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4233, 121)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = feature_engineering(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 121)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Unigram features for both train and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4233, 121)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 121)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mX_train\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Aspirational/saved_features/X_train_final.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m X_test\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Aspirational/saved_features/X_test_final.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m y_train\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_1.1/Aspirational/saved_features/y_train.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "X_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2/Navigational/saved_features/X_train.csv\", index=False)\n",
    "X_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2/Navigational/saved_features/X_test.csv\", index=False)\n",
    "y_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2/Navigational/saved_features/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2/Navigational/saved_features/y_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the unigram df for train :  (4233, 2308)\n"
     ]
    }
   ],
   "source": [
    "# Unigrams for training set\n",
    "unigram_matrix = unigram_vect.fit_transform(X_train['sentence'])\n",
    "unigrams = pd.DataFrame(unigram_matrix.toarray())\n",
    "print(\"Shape of the unigram df for train : \",unigrams.shape)\n",
    "unigrams = unigrams.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = pd.concat([X_train, unigrams], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final.columns = X_train_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4233, 2429)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test unigram df shape :  (471, 2308)\n"
     ]
    }
   ],
   "source": [
    "unigram_matrix_test = unigram_vect.transform(X_test['sentence'])\n",
    "unigrams_test = pd.DataFrame(unigram_matrix_test.toarray())\n",
    "unigrams_test = unigrams_test.reset_index(drop=True)\n",
    "print(\"Test unigram df shape : \",unigrams_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 2429)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final = pd.concat([X_test, unigrams_test], axis = 1)\n",
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final.columns = X_test_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 2429)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ---- sentence\n",
      "1 ---- phrase\n",
      "2 ---- tokens\n",
      "3 ---- bio_sim_words\n",
      "4 ---- chem_sim_words\n",
      "5 ---- phy_sim_words\n",
      "6 ---- math_sim_words\n",
      "7 ---- tech_sim_words\n",
      "8 ---- eng_sim_words\n",
      "9 ---- medical_terms\n",
      "10 ---- polarity\n",
      "11 ---- subjectivity\n",
      "12 ---- ner\n",
      "13 ---- interjections\n",
      "14 ---- nouns\n",
      "15 ---- adverb\n",
      "16 ---- verb\n",
      "17 ---- determiner\n",
      "18 ---- pronoun\n",
      "19 ---- adjetive\n",
      "20 ---- preposition\n",
      "21 ---- embedding0\n",
      "22 ---- embedding1\n",
      "23 ---- embedding2\n",
      "24 ---- embedding3\n",
      "25 ---- embedding4\n",
      "26 ---- embedding5\n",
      "27 ---- embedding6\n",
      "28 ---- embedding7\n",
      "29 ---- embedding8\n",
      "30 ---- embedding9\n",
      "31 ---- embedding10\n",
      "32 ---- embedding11\n",
      "33 ---- embedding12\n",
      "34 ---- embedding13\n",
      "35 ---- embedding14\n",
      "36 ---- embedding15\n",
      "37 ---- embedding16\n",
      "38 ---- embedding17\n",
      "39 ---- embedding18\n",
      "40 ---- embedding19\n",
      "41 ---- embedding20\n",
      "42 ---- embedding21\n",
      "43 ---- embedding22\n",
      "44 ---- embedding23\n",
      "45 ---- embedding24\n",
      "46 ---- embedding25\n",
      "47 ---- embedding26\n",
      "48 ---- embedding27\n",
      "49 ---- embedding28\n",
      "50 ---- embedding29\n",
      "51 ---- embedding30\n",
      "52 ---- embedding31\n",
      "53 ---- embedding32\n",
      "54 ---- embedding33\n",
      "55 ---- embedding34\n",
      "56 ---- embedding35\n",
      "57 ---- embedding36\n",
      "58 ---- embedding37\n",
      "59 ---- embedding38\n",
      "60 ---- embedding39\n",
      "61 ---- embedding40\n",
      "62 ---- embedding41\n",
      "63 ---- embedding42\n",
      "64 ---- embedding43\n",
      "65 ---- embedding44\n",
      "66 ---- embedding45\n",
      "67 ---- embedding46\n",
      "68 ---- embedding47\n",
      "69 ---- embedding48\n",
      "70 ---- embedding49\n",
      "71 ---- embedding50\n",
      "72 ---- embedding51\n",
      "73 ---- embedding52\n",
      "74 ---- embedding53\n",
      "75 ---- embedding54\n",
      "76 ---- embedding55\n",
      "77 ---- embedding56\n",
      "78 ---- embedding57\n",
      "79 ---- embedding58\n",
      "80 ---- embedding59\n",
      "81 ---- embedding60\n",
      "82 ---- embedding61\n",
      "83 ---- embedding62\n",
      "84 ---- embedding63\n",
      "85 ---- embedding64\n",
      "86 ---- embedding65\n",
      "87 ---- embedding66\n",
      "88 ---- embedding67\n",
      "89 ---- embedding68\n",
      "90 ---- embedding69\n",
      "91 ---- embedding70\n",
      "92 ---- embedding71\n",
      "93 ---- embedding72\n",
      "94 ---- embedding73\n",
      "95 ---- embedding74\n",
      "96 ---- embedding75\n",
      "97 ---- embedding76\n",
      "98 ---- embedding77\n",
      "99 ---- embedding78\n",
      "100 ---- embedding79\n",
      "101 ---- embedding80\n",
      "102 ---- embedding81\n",
      "103 ---- embedding82\n",
      "104 ---- embedding83\n",
      "105 ---- embedding84\n",
      "106 ---- embedding85\n",
      "107 ---- embedding86\n",
      "108 ---- embedding87\n",
      "109 ---- embedding88\n",
      "110 ---- embedding89\n",
      "111 ---- embedding90\n",
      "112 ---- embedding91\n",
      "113 ---- embedding92\n",
      "114 ---- embedding93\n",
      "115 ---- embedding94\n",
      "116 ---- embedding95\n",
      "117 ---- embedding96\n",
      "118 ---- embedding97\n",
      "119 ---- embedding98\n",
      "120 ---- embedding99\n",
      "121 ---- 0\n",
      "122 ---- 1\n",
      "123 ---- 2\n",
      "124 ---- 3\n",
      "125 ---- 4\n",
      "126 ---- 5\n",
      "127 ---- 6\n",
      "128 ---- 7\n",
      "129 ---- 8\n",
      "130 ---- 9\n",
      "131 ---- 10\n",
      "132 ---- 11\n",
      "133 ---- 12\n",
      "134 ---- 13\n",
      "135 ---- 14\n",
      "136 ---- 15\n",
      "137 ---- 16\n",
      "138 ---- 17\n",
      "139 ---- 18\n",
      "140 ---- 19\n",
      "141 ---- 20\n",
      "142 ---- 21\n",
      "143 ---- 22\n",
      "144 ---- 23\n",
      "145 ---- 24\n",
      "146 ---- 25\n",
      "147 ---- 26\n",
      "148 ---- 27\n",
      "149 ---- 28\n",
      "150 ---- 29\n",
      "151 ---- 30\n",
      "152 ---- 31\n",
      "153 ---- 32\n",
      "154 ---- 33\n",
      "155 ---- 34\n",
      "156 ---- 35\n",
      "157 ---- 36\n",
      "158 ---- 37\n",
      "159 ---- 38\n",
      "160 ---- 39\n",
      "161 ---- 40\n",
      "162 ---- 41\n",
      "163 ---- 42\n",
      "164 ---- 43\n",
      "165 ---- 44\n",
      "166 ---- 45\n",
      "167 ---- 46\n",
      "168 ---- 47\n",
      "169 ---- 48\n",
      "170 ---- 49\n",
      "171 ---- 50\n",
      "172 ---- 51\n",
      "173 ---- 52\n",
      "174 ---- 53\n",
      "175 ---- 54\n",
      "176 ---- 55\n",
      "177 ---- 56\n",
      "178 ---- 57\n",
      "179 ---- 58\n",
      "180 ---- 59\n",
      "181 ---- 60\n",
      "182 ---- 61\n",
      "183 ---- 62\n",
      "184 ---- 63\n",
      "185 ---- 64\n",
      "186 ---- 65\n",
      "187 ---- 66\n",
      "188 ---- 67\n",
      "189 ---- 68\n",
      "190 ---- 69\n",
      "191 ---- 70\n",
      "192 ---- 71\n",
      "193 ---- 72\n",
      "194 ---- 73\n",
      "195 ---- 74\n",
      "196 ---- 75\n",
      "197 ---- 76\n",
      "198 ---- 77\n",
      "199 ---- 78\n",
      "200 ---- 79\n",
      "201 ---- 80\n",
      "202 ---- 81\n",
      "203 ---- 82\n",
      "204 ---- 83\n",
      "205 ---- 84\n",
      "206 ---- 85\n",
      "207 ---- 86\n",
      "208 ---- 87\n",
      "209 ---- 88\n",
      "210 ---- 89\n",
      "211 ---- 90\n",
      "212 ---- 91\n",
      "213 ---- 92\n",
      "214 ---- 93\n",
      "215 ---- 94\n",
      "216 ---- 95\n",
      "217 ---- 96\n",
      "218 ---- 97\n",
      "219 ---- 98\n",
      "220 ---- 99\n",
      "221 ---- 100\n",
      "222 ---- 101\n",
      "223 ---- 102\n",
      "224 ---- 103\n",
      "225 ---- 104\n",
      "226 ---- 105\n",
      "227 ---- 106\n",
      "228 ---- 107\n",
      "229 ---- 108\n",
      "230 ---- 109\n",
      "231 ---- 110\n",
      "232 ---- 111\n",
      "233 ---- 112\n",
      "234 ---- 113\n",
      "235 ---- 114\n",
      "236 ---- 115\n",
      "237 ---- 116\n",
      "238 ---- 117\n",
      "239 ---- 118\n",
      "240 ---- 119\n",
      "241 ---- 120\n",
      "242 ---- 121\n",
      "243 ---- 122\n",
      "244 ---- 123\n",
      "245 ---- 124\n",
      "246 ---- 125\n",
      "247 ---- 126\n",
      "248 ---- 127\n",
      "249 ---- 128\n",
      "250 ---- 129\n",
      "251 ---- 130\n",
      "252 ---- 131\n",
      "253 ---- 132\n",
      "254 ---- 133\n",
      "255 ---- 134\n",
      "256 ---- 135\n",
      "257 ---- 136\n",
      "258 ---- 137\n",
      "259 ---- 138\n",
      "260 ---- 139\n",
      "261 ---- 140\n",
      "262 ---- 141\n",
      "263 ---- 142\n",
      "264 ---- 143\n",
      "265 ---- 144\n",
      "266 ---- 145\n",
      "267 ---- 146\n",
      "268 ---- 147\n",
      "269 ---- 148\n",
      "270 ---- 149\n",
      "271 ---- 150\n",
      "272 ---- 151\n",
      "273 ---- 152\n",
      "274 ---- 153\n",
      "275 ---- 154\n",
      "276 ---- 155\n",
      "277 ---- 156\n",
      "278 ---- 157\n",
      "279 ---- 158\n",
      "280 ---- 159\n",
      "281 ---- 160\n",
      "282 ---- 161\n",
      "283 ---- 162\n",
      "284 ---- 163\n",
      "285 ---- 164\n",
      "286 ---- 165\n",
      "287 ---- 166\n",
      "288 ---- 167\n",
      "289 ---- 168\n",
      "290 ---- 169\n",
      "291 ---- 170\n",
      "292 ---- 171\n",
      "293 ---- 172\n",
      "294 ---- 173\n",
      "295 ---- 174\n",
      "296 ---- 175\n",
      "297 ---- 176\n",
      "298 ---- 177\n",
      "299 ---- 178\n",
      "300 ---- 179\n",
      "301 ---- 180\n",
      "302 ---- 181\n",
      "303 ---- 182\n",
      "304 ---- 183\n",
      "305 ---- 184\n",
      "306 ---- 185\n",
      "307 ---- 186\n",
      "308 ---- 187\n",
      "309 ---- 188\n",
      "310 ---- 189\n",
      "311 ---- 190\n",
      "312 ---- 191\n",
      "313 ---- 192\n",
      "314 ---- 193\n",
      "315 ---- 194\n",
      "316 ---- 195\n",
      "317 ---- 196\n",
      "318 ---- 197\n",
      "319 ---- 198\n",
      "320 ---- 199\n",
      "321 ---- 200\n",
      "322 ---- 201\n",
      "323 ---- 202\n",
      "324 ---- 203\n",
      "325 ---- 204\n",
      "326 ---- 205\n",
      "327 ---- 206\n",
      "328 ---- 207\n",
      "329 ---- 208\n",
      "330 ---- 209\n",
      "331 ---- 210\n",
      "332 ---- 211\n",
      "333 ---- 212\n",
      "334 ---- 213\n",
      "335 ---- 214\n",
      "336 ---- 215\n",
      "337 ---- 216\n",
      "338 ---- 217\n",
      "339 ---- 218\n",
      "340 ---- 219\n",
      "341 ---- 220\n",
      "342 ---- 221\n",
      "343 ---- 222\n",
      "344 ---- 223\n",
      "345 ---- 224\n",
      "346 ---- 225\n",
      "347 ---- 226\n",
      "348 ---- 227\n",
      "349 ---- 228\n",
      "350 ---- 229\n",
      "351 ---- 230\n",
      "352 ---- 231\n",
      "353 ---- 232\n",
      "354 ---- 233\n",
      "355 ---- 234\n",
      "356 ---- 235\n",
      "357 ---- 236\n",
      "358 ---- 237\n",
      "359 ---- 238\n",
      "360 ---- 239\n",
      "361 ---- 240\n",
      "362 ---- 241\n",
      "363 ---- 242\n",
      "364 ---- 243\n",
      "365 ---- 244\n",
      "366 ---- 245\n",
      "367 ---- 246\n",
      "368 ---- 247\n",
      "369 ---- 248\n",
      "370 ---- 249\n",
      "371 ---- 250\n",
      "372 ---- 251\n",
      "373 ---- 252\n",
      "374 ---- 253\n",
      "375 ---- 254\n",
      "376 ---- 255\n",
      "377 ---- 256\n",
      "378 ---- 257\n",
      "379 ---- 258\n",
      "380 ---- 259\n",
      "381 ---- 260\n",
      "382 ---- 261\n",
      "383 ---- 262\n",
      "384 ---- 263\n",
      "385 ---- 264\n",
      "386 ---- 265\n",
      "387 ---- 266\n",
      "388 ---- 267\n",
      "389 ---- 268\n",
      "390 ---- 269\n",
      "391 ---- 270\n",
      "392 ---- 271\n",
      "393 ---- 272\n",
      "394 ---- 273\n",
      "395 ---- 274\n",
      "396 ---- 275\n",
      "397 ---- 276\n",
      "398 ---- 277\n",
      "399 ---- 278\n",
      "400 ---- 279\n",
      "401 ---- 280\n",
      "402 ---- 281\n",
      "403 ---- 282\n",
      "404 ---- 283\n",
      "405 ---- 284\n",
      "406 ---- 285\n",
      "407 ---- 286\n",
      "408 ---- 287\n",
      "409 ---- 288\n",
      "410 ---- 289\n",
      "411 ---- 290\n",
      "412 ---- 291\n",
      "413 ---- 292\n",
      "414 ---- 293\n",
      "415 ---- 294\n",
      "416 ---- 295\n",
      "417 ---- 296\n",
      "418 ---- 297\n",
      "419 ---- 298\n",
      "420 ---- 299\n",
      "421 ---- 300\n",
      "422 ---- 301\n",
      "423 ---- 302\n",
      "424 ---- 303\n",
      "425 ---- 304\n",
      "426 ---- 305\n",
      "427 ---- 306\n",
      "428 ---- 307\n",
      "429 ---- 308\n",
      "430 ---- 309\n",
      "431 ---- 310\n",
      "432 ---- 311\n",
      "433 ---- 312\n",
      "434 ---- 313\n",
      "435 ---- 314\n",
      "436 ---- 315\n",
      "437 ---- 316\n",
      "438 ---- 317\n",
      "439 ---- 318\n",
      "440 ---- 319\n",
      "441 ---- 320\n",
      "442 ---- 321\n",
      "443 ---- 322\n",
      "444 ---- 323\n",
      "445 ---- 324\n",
      "446 ---- 325\n",
      "447 ---- 326\n",
      "448 ---- 327\n",
      "449 ---- 328\n",
      "450 ---- 329\n",
      "451 ---- 330\n",
      "452 ---- 331\n",
      "453 ---- 332\n",
      "454 ---- 333\n",
      "455 ---- 334\n",
      "456 ---- 335\n",
      "457 ---- 336\n",
      "458 ---- 337\n",
      "459 ---- 338\n",
      "460 ---- 339\n",
      "461 ---- 340\n",
      "462 ---- 341\n",
      "463 ---- 342\n",
      "464 ---- 343\n",
      "465 ---- 344\n",
      "466 ---- 345\n",
      "467 ---- 346\n",
      "468 ---- 347\n",
      "469 ---- 348\n",
      "470 ---- 349\n",
      "471 ---- 350\n",
      "472 ---- 351\n",
      "473 ---- 352\n",
      "474 ---- 353\n",
      "475 ---- 354\n",
      "476 ---- 355\n",
      "477 ---- 356\n",
      "478 ---- 357\n",
      "479 ---- 358\n",
      "480 ---- 359\n",
      "481 ---- 360\n",
      "482 ---- 361\n",
      "483 ---- 362\n",
      "484 ---- 363\n",
      "485 ---- 364\n",
      "486 ---- 365\n",
      "487 ---- 366\n",
      "488 ---- 367\n",
      "489 ---- 368\n",
      "490 ---- 369\n",
      "491 ---- 370\n",
      "492 ---- 371\n",
      "493 ---- 372\n",
      "494 ---- 373\n",
      "495 ---- 374\n",
      "496 ---- 375\n",
      "497 ---- 376\n",
      "498 ---- 377\n",
      "499 ---- 378\n",
      "500 ---- 379\n",
      "501 ---- 380\n",
      "502 ---- 381\n",
      "503 ---- 382\n",
      "504 ---- 383\n",
      "505 ---- 384\n",
      "506 ---- 385\n",
      "507 ---- 386\n",
      "508 ---- 387\n",
      "509 ---- 388\n",
      "510 ---- 389\n",
      "511 ---- 390\n",
      "512 ---- 391\n",
      "513 ---- 392\n",
      "514 ---- 393\n",
      "515 ---- 394\n",
      "516 ---- 395\n",
      "517 ---- 396\n",
      "518 ---- 397\n",
      "519 ---- 398\n",
      "520 ---- 399\n",
      "521 ---- 400\n",
      "522 ---- 401\n",
      "523 ---- 402\n",
      "524 ---- 403\n",
      "525 ---- 404\n",
      "526 ---- 405\n",
      "527 ---- 406\n",
      "528 ---- 407\n",
      "529 ---- 408\n",
      "530 ---- 409\n",
      "531 ---- 410\n",
      "532 ---- 411\n",
      "533 ---- 412\n",
      "534 ---- 413\n",
      "535 ---- 414\n",
      "536 ---- 415\n",
      "537 ---- 416\n",
      "538 ---- 417\n",
      "539 ---- 418\n",
      "540 ---- 419\n",
      "541 ---- 420\n",
      "542 ---- 421\n",
      "543 ---- 422\n",
      "544 ---- 423\n",
      "545 ---- 424\n",
      "546 ---- 425\n",
      "547 ---- 426\n",
      "548 ---- 427\n",
      "549 ---- 428\n",
      "550 ---- 429\n",
      "551 ---- 430\n",
      "552 ---- 431\n",
      "553 ---- 432\n",
      "554 ---- 433\n",
      "555 ---- 434\n",
      "556 ---- 435\n",
      "557 ---- 436\n",
      "558 ---- 437\n",
      "559 ---- 438\n",
      "560 ---- 439\n",
      "561 ---- 440\n",
      "562 ---- 441\n",
      "563 ---- 442\n",
      "564 ---- 443\n",
      "565 ---- 444\n",
      "566 ---- 445\n",
      "567 ---- 446\n",
      "568 ---- 447\n",
      "569 ---- 448\n",
      "570 ---- 449\n",
      "571 ---- 450\n",
      "572 ---- 451\n",
      "573 ---- 452\n",
      "574 ---- 453\n",
      "575 ---- 454\n",
      "576 ---- 455\n",
      "577 ---- 456\n",
      "578 ---- 457\n",
      "579 ---- 458\n",
      "580 ---- 459\n",
      "581 ---- 460\n",
      "582 ---- 461\n",
      "583 ---- 462\n",
      "584 ---- 463\n",
      "585 ---- 464\n",
      "586 ---- 465\n",
      "587 ---- 466\n",
      "588 ---- 467\n",
      "589 ---- 468\n",
      "590 ---- 469\n",
      "591 ---- 470\n",
      "592 ---- 471\n",
      "593 ---- 472\n",
      "594 ---- 473\n",
      "595 ---- 474\n",
      "596 ---- 475\n",
      "597 ---- 476\n",
      "598 ---- 477\n",
      "599 ---- 478\n",
      "600 ---- 479\n",
      "601 ---- 480\n",
      "602 ---- 481\n",
      "603 ---- 482\n",
      "604 ---- 483\n",
      "605 ---- 484\n",
      "606 ---- 485\n",
      "607 ---- 486\n",
      "608 ---- 487\n",
      "609 ---- 488\n",
      "610 ---- 489\n",
      "611 ---- 490\n",
      "612 ---- 491\n",
      "613 ---- 492\n",
      "614 ---- 493\n",
      "615 ---- 494\n",
      "616 ---- 495\n",
      "617 ---- 496\n",
      "618 ---- 497\n",
      "619 ---- 498\n",
      "620 ---- 499\n",
      "621 ---- 500\n",
      "622 ---- 501\n",
      "623 ---- 502\n",
      "624 ---- 503\n",
      "625 ---- 504\n",
      "626 ---- 505\n",
      "627 ---- 506\n",
      "628 ---- 507\n",
      "629 ---- 508\n",
      "630 ---- 509\n",
      "631 ---- 510\n",
      "632 ---- 511\n",
      "633 ---- 512\n",
      "634 ---- 513\n",
      "635 ---- 514\n",
      "636 ---- 515\n",
      "637 ---- 516\n",
      "638 ---- 517\n",
      "639 ---- 518\n",
      "640 ---- 519\n",
      "641 ---- 520\n",
      "642 ---- 521\n",
      "643 ---- 522\n",
      "644 ---- 523\n",
      "645 ---- 524\n",
      "646 ---- 525\n",
      "647 ---- 526\n",
      "648 ---- 527\n",
      "649 ---- 528\n",
      "650 ---- 529\n",
      "651 ---- 530\n",
      "652 ---- 531\n",
      "653 ---- 532\n",
      "654 ---- 533\n",
      "655 ---- 534\n",
      "656 ---- 535\n",
      "657 ---- 536\n",
      "658 ---- 537\n",
      "659 ---- 538\n",
      "660 ---- 539\n",
      "661 ---- 540\n",
      "662 ---- 541\n",
      "663 ---- 542\n",
      "664 ---- 543\n",
      "665 ---- 544\n",
      "666 ---- 545\n",
      "667 ---- 546\n",
      "668 ---- 547\n",
      "669 ---- 548\n",
      "670 ---- 549\n",
      "671 ---- 550\n",
      "672 ---- 551\n",
      "673 ---- 552\n",
      "674 ---- 553\n",
      "675 ---- 554\n",
      "676 ---- 555\n",
      "677 ---- 556\n",
      "678 ---- 557\n",
      "679 ---- 558\n",
      "680 ---- 559\n",
      "681 ---- 560\n",
      "682 ---- 561\n",
      "683 ---- 562\n",
      "684 ---- 563\n",
      "685 ---- 564\n",
      "686 ---- 565\n",
      "687 ---- 566\n",
      "688 ---- 567\n",
      "689 ---- 568\n",
      "690 ---- 569\n",
      "691 ---- 570\n",
      "692 ---- 571\n",
      "693 ---- 572\n",
      "694 ---- 573\n",
      "695 ---- 574\n",
      "696 ---- 575\n",
      "697 ---- 576\n",
      "698 ---- 577\n",
      "699 ---- 578\n",
      "700 ---- 579\n",
      "701 ---- 580\n",
      "702 ---- 581\n",
      "703 ---- 582\n",
      "704 ---- 583\n",
      "705 ---- 584\n",
      "706 ---- 585\n",
      "707 ---- 586\n",
      "708 ---- 587\n",
      "709 ---- 588\n",
      "710 ---- 589\n",
      "711 ---- 590\n",
      "712 ---- 591\n",
      "713 ---- 592\n",
      "714 ---- 593\n",
      "715 ---- 594\n",
      "716 ---- 595\n",
      "717 ---- 596\n",
      "718 ---- 597\n",
      "719 ---- 598\n",
      "720 ---- 599\n",
      "721 ---- 600\n",
      "722 ---- 601\n",
      "723 ---- 602\n",
      "724 ---- 603\n",
      "725 ---- 604\n",
      "726 ---- 605\n",
      "727 ---- 606\n",
      "728 ---- 607\n",
      "729 ---- 608\n",
      "730 ---- 609\n",
      "731 ---- 610\n",
      "732 ---- 611\n",
      "733 ---- 612\n",
      "734 ---- 613\n",
      "735 ---- 614\n",
      "736 ---- 615\n",
      "737 ---- 616\n",
      "738 ---- 617\n",
      "739 ---- 618\n",
      "740 ---- 619\n",
      "741 ---- 620\n",
      "742 ---- 621\n",
      "743 ---- 622\n",
      "744 ---- 623\n",
      "745 ---- 624\n",
      "746 ---- 625\n",
      "747 ---- 626\n",
      "748 ---- 627\n",
      "749 ---- 628\n",
      "750 ---- 629\n",
      "751 ---- 630\n",
      "752 ---- 631\n",
      "753 ---- 632\n",
      "754 ---- 633\n",
      "755 ---- 634\n",
      "756 ---- 635\n",
      "757 ---- 636\n",
      "758 ---- 637\n",
      "759 ---- 638\n",
      "760 ---- 639\n",
      "761 ---- 640\n",
      "762 ---- 641\n",
      "763 ---- 642\n",
      "764 ---- 643\n",
      "765 ---- 644\n",
      "766 ---- 645\n",
      "767 ---- 646\n",
      "768 ---- 647\n",
      "769 ---- 648\n",
      "770 ---- 649\n",
      "771 ---- 650\n",
      "772 ---- 651\n",
      "773 ---- 652\n",
      "774 ---- 653\n",
      "775 ---- 654\n",
      "776 ---- 655\n",
      "777 ---- 656\n",
      "778 ---- 657\n",
      "779 ---- 658\n",
      "780 ---- 659\n",
      "781 ---- 660\n",
      "782 ---- 661\n",
      "783 ---- 662\n",
      "784 ---- 663\n",
      "785 ---- 664\n",
      "786 ---- 665\n",
      "787 ---- 666\n",
      "788 ---- 667\n",
      "789 ---- 668\n",
      "790 ---- 669\n",
      "791 ---- 670\n",
      "792 ---- 671\n",
      "793 ---- 672\n",
      "794 ---- 673\n",
      "795 ---- 674\n",
      "796 ---- 675\n",
      "797 ---- 676\n",
      "798 ---- 677\n",
      "799 ---- 678\n",
      "800 ---- 679\n",
      "801 ---- 680\n",
      "802 ---- 681\n",
      "803 ---- 682\n",
      "804 ---- 683\n",
      "805 ---- 684\n",
      "806 ---- 685\n",
      "807 ---- 686\n",
      "808 ---- 687\n",
      "809 ---- 688\n",
      "810 ---- 689\n",
      "811 ---- 690\n",
      "812 ---- 691\n",
      "813 ---- 692\n",
      "814 ---- 693\n",
      "815 ---- 694\n",
      "816 ---- 695\n",
      "817 ---- 696\n",
      "818 ---- 697\n",
      "819 ---- 698\n",
      "820 ---- 699\n",
      "821 ---- 700\n",
      "822 ---- 701\n",
      "823 ---- 702\n",
      "824 ---- 703\n",
      "825 ---- 704\n",
      "826 ---- 705\n",
      "827 ---- 706\n",
      "828 ---- 707\n",
      "829 ---- 708\n",
      "830 ---- 709\n",
      "831 ---- 710\n",
      "832 ---- 711\n",
      "833 ---- 712\n",
      "834 ---- 713\n",
      "835 ---- 714\n",
      "836 ---- 715\n",
      "837 ---- 716\n",
      "838 ---- 717\n",
      "839 ---- 718\n",
      "840 ---- 719\n",
      "841 ---- 720\n",
      "842 ---- 721\n",
      "843 ---- 722\n",
      "844 ---- 723\n",
      "845 ---- 724\n",
      "846 ---- 725\n",
      "847 ---- 726\n",
      "848 ---- 727\n",
      "849 ---- 728\n",
      "850 ---- 729\n",
      "851 ---- 730\n",
      "852 ---- 731\n",
      "853 ---- 732\n",
      "854 ---- 733\n",
      "855 ---- 734\n",
      "856 ---- 735\n",
      "857 ---- 736\n",
      "858 ---- 737\n",
      "859 ---- 738\n",
      "860 ---- 739\n",
      "861 ---- 740\n",
      "862 ---- 741\n",
      "863 ---- 742\n",
      "864 ---- 743\n",
      "865 ---- 744\n",
      "866 ---- 745\n",
      "867 ---- 746\n",
      "868 ---- 747\n",
      "869 ---- 748\n",
      "870 ---- 749\n",
      "871 ---- 750\n",
      "872 ---- 751\n",
      "873 ---- 752\n",
      "874 ---- 753\n",
      "875 ---- 754\n",
      "876 ---- 755\n",
      "877 ---- 756\n",
      "878 ---- 757\n",
      "879 ---- 758\n",
      "880 ---- 759\n",
      "881 ---- 760\n",
      "882 ---- 761\n",
      "883 ---- 762\n",
      "884 ---- 763\n",
      "885 ---- 764\n",
      "886 ---- 765\n",
      "887 ---- 766\n",
      "888 ---- 767\n",
      "889 ---- 768\n",
      "890 ---- 769\n",
      "891 ---- 770\n",
      "892 ---- 771\n",
      "893 ---- 772\n",
      "894 ---- 773\n",
      "895 ---- 774\n",
      "896 ---- 775\n",
      "897 ---- 776\n",
      "898 ---- 777\n",
      "899 ---- 778\n",
      "900 ---- 779\n",
      "901 ---- 780\n",
      "902 ---- 781\n",
      "903 ---- 782\n",
      "904 ---- 783\n",
      "905 ---- 784\n",
      "906 ---- 785\n",
      "907 ---- 786\n",
      "908 ---- 787\n",
      "909 ---- 788\n",
      "910 ---- 789\n",
      "911 ---- 790\n",
      "912 ---- 791\n",
      "913 ---- 792\n",
      "914 ---- 793\n",
      "915 ---- 794\n",
      "916 ---- 795\n",
      "917 ---- 796\n",
      "918 ---- 797\n",
      "919 ---- 798\n",
      "920 ---- 799\n",
      "921 ---- 800\n",
      "922 ---- 801\n",
      "923 ---- 802\n",
      "924 ---- 803\n",
      "925 ---- 804\n",
      "926 ---- 805\n",
      "927 ---- 806\n",
      "928 ---- 807\n",
      "929 ---- 808\n",
      "930 ---- 809\n",
      "931 ---- 810\n",
      "932 ---- 811\n",
      "933 ---- 812\n",
      "934 ---- 813\n",
      "935 ---- 814\n",
      "936 ---- 815\n",
      "937 ---- 816\n",
      "938 ---- 817\n",
      "939 ---- 818\n",
      "940 ---- 819\n",
      "941 ---- 820\n",
      "942 ---- 821\n",
      "943 ---- 822\n",
      "944 ---- 823\n",
      "945 ---- 824\n",
      "946 ---- 825\n",
      "947 ---- 826\n",
      "948 ---- 827\n",
      "949 ---- 828\n",
      "950 ---- 829\n",
      "951 ---- 830\n",
      "952 ---- 831\n",
      "953 ---- 832\n",
      "954 ---- 833\n",
      "955 ---- 834\n",
      "956 ---- 835\n",
      "957 ---- 836\n",
      "958 ---- 837\n",
      "959 ---- 838\n",
      "960 ---- 839\n",
      "961 ---- 840\n",
      "962 ---- 841\n",
      "963 ---- 842\n",
      "964 ---- 843\n",
      "965 ---- 844\n",
      "966 ---- 845\n",
      "967 ---- 846\n",
      "968 ---- 847\n",
      "969 ---- 848\n",
      "970 ---- 849\n",
      "971 ---- 850\n",
      "972 ---- 851\n",
      "973 ---- 852\n",
      "974 ---- 853\n",
      "975 ---- 854\n",
      "976 ---- 855\n",
      "977 ---- 856\n",
      "978 ---- 857\n",
      "979 ---- 858\n",
      "980 ---- 859\n",
      "981 ---- 860\n",
      "982 ---- 861\n",
      "983 ---- 862\n",
      "984 ---- 863\n",
      "985 ---- 864\n",
      "986 ---- 865\n",
      "987 ---- 866\n",
      "988 ---- 867\n",
      "989 ---- 868\n",
      "990 ---- 869\n",
      "991 ---- 870\n",
      "992 ---- 871\n",
      "993 ---- 872\n",
      "994 ---- 873\n",
      "995 ---- 874\n",
      "996 ---- 875\n",
      "997 ---- 876\n",
      "998 ---- 877\n",
      "999 ---- 878\n",
      "1000 ---- 879\n",
      "1001 ---- 880\n",
      "1002 ---- 881\n",
      "1003 ---- 882\n",
      "1004 ---- 883\n",
      "1005 ---- 884\n",
      "1006 ---- 885\n",
      "1007 ---- 886\n",
      "1008 ---- 887\n",
      "1009 ---- 888\n",
      "1010 ---- 889\n",
      "1011 ---- 890\n",
      "1012 ---- 891\n",
      "1013 ---- 892\n",
      "1014 ---- 893\n",
      "1015 ---- 894\n",
      "1016 ---- 895\n",
      "1017 ---- 896\n",
      "1018 ---- 897\n",
      "1019 ---- 898\n",
      "1020 ---- 899\n",
      "1021 ---- 900\n",
      "1022 ---- 901\n",
      "1023 ---- 902\n",
      "1024 ---- 903\n",
      "1025 ---- 904\n",
      "1026 ---- 905\n",
      "1027 ---- 906\n",
      "1028 ---- 907\n",
      "1029 ---- 908\n",
      "1030 ---- 909\n",
      "1031 ---- 910\n",
      "1032 ---- 911\n",
      "1033 ---- 912\n",
      "1034 ---- 913\n",
      "1035 ---- 914\n",
      "1036 ---- 915\n",
      "1037 ---- 916\n",
      "1038 ---- 917\n",
      "1039 ---- 918\n",
      "1040 ---- 919\n",
      "1041 ---- 920\n",
      "1042 ---- 921\n",
      "1043 ---- 922\n",
      "1044 ---- 923\n",
      "1045 ---- 924\n",
      "1046 ---- 925\n",
      "1047 ---- 926\n",
      "1048 ---- 927\n",
      "1049 ---- 928\n",
      "1050 ---- 929\n",
      "1051 ---- 930\n",
      "1052 ---- 931\n",
      "1053 ---- 932\n",
      "1054 ---- 933\n",
      "1055 ---- 934\n",
      "1056 ---- 935\n",
      "1057 ---- 936\n",
      "1058 ---- 937\n",
      "1059 ---- 938\n",
      "1060 ---- 939\n",
      "1061 ---- 940\n",
      "1062 ---- 941\n",
      "1063 ---- 942\n",
      "1064 ---- 943\n",
      "1065 ---- 944\n",
      "1066 ---- 945\n",
      "1067 ---- 946\n",
      "1068 ---- 947\n",
      "1069 ---- 948\n",
      "1070 ---- 949\n",
      "1071 ---- 950\n",
      "1072 ---- 951\n",
      "1073 ---- 952\n",
      "1074 ---- 953\n",
      "1075 ---- 954\n",
      "1076 ---- 955\n",
      "1077 ---- 956\n",
      "1078 ---- 957\n",
      "1079 ---- 958\n",
      "1080 ---- 959\n",
      "1081 ---- 960\n",
      "1082 ---- 961\n",
      "1083 ---- 962\n",
      "1084 ---- 963\n",
      "1085 ---- 964\n",
      "1086 ---- 965\n",
      "1087 ---- 966\n",
      "1088 ---- 967\n",
      "1089 ---- 968\n",
      "1090 ---- 969\n",
      "1091 ---- 970\n",
      "1092 ---- 971\n",
      "1093 ---- 972\n",
      "1094 ---- 973\n",
      "1095 ---- 974\n",
      "1096 ---- 975\n",
      "1097 ---- 976\n",
      "1098 ---- 977\n",
      "1099 ---- 978\n",
      "1100 ---- 979\n",
      "1101 ---- 980\n",
      "1102 ---- 981\n",
      "1103 ---- 982\n",
      "1104 ---- 983\n",
      "1105 ---- 984\n",
      "1106 ---- 985\n",
      "1107 ---- 986\n",
      "1108 ---- 987\n",
      "1109 ---- 988\n",
      "1110 ---- 989\n",
      "1111 ---- 990\n",
      "1112 ---- 991\n",
      "1113 ---- 992\n",
      "1114 ---- 993\n",
      "1115 ---- 994\n",
      "1116 ---- 995\n",
      "1117 ---- 996\n",
      "1118 ---- 997\n",
      "1119 ---- 998\n",
      "1120 ---- 999\n",
      "1121 ---- 1000\n",
      "1122 ---- 1001\n",
      "1123 ---- 1002\n",
      "1124 ---- 1003\n",
      "1125 ---- 1004\n",
      "1126 ---- 1005\n",
      "1127 ---- 1006\n",
      "1128 ---- 1007\n",
      "1129 ---- 1008\n",
      "1130 ---- 1009\n",
      "1131 ---- 1010\n",
      "1132 ---- 1011\n",
      "1133 ---- 1012\n",
      "1134 ---- 1013\n",
      "1135 ---- 1014\n",
      "1136 ---- 1015\n",
      "1137 ---- 1016\n",
      "1138 ---- 1017\n",
      "1139 ---- 1018\n",
      "1140 ---- 1019\n",
      "1141 ---- 1020\n",
      "1142 ---- 1021\n",
      "1143 ---- 1022\n",
      "1144 ---- 1023\n",
      "1145 ---- 1024\n",
      "1146 ---- 1025\n",
      "1147 ---- 1026\n",
      "1148 ---- 1027\n",
      "1149 ---- 1028\n",
      "1150 ---- 1029\n",
      "1151 ---- 1030\n",
      "1152 ---- 1031\n",
      "1153 ---- 1032\n",
      "1154 ---- 1033\n",
      "1155 ---- 1034\n",
      "1156 ---- 1035\n",
      "1157 ---- 1036\n",
      "1158 ---- 1037\n",
      "1159 ---- 1038\n",
      "1160 ---- 1039\n",
      "1161 ---- 1040\n",
      "1162 ---- 1041\n",
      "1163 ---- 1042\n",
      "1164 ---- 1043\n",
      "1165 ---- 1044\n",
      "1166 ---- 1045\n",
      "1167 ---- 1046\n",
      "1168 ---- 1047\n",
      "1169 ---- 1048\n",
      "1170 ---- 1049\n",
      "1171 ---- 1050\n",
      "1172 ---- 1051\n",
      "1173 ---- 1052\n",
      "1174 ---- 1053\n",
      "1175 ---- 1054\n",
      "1176 ---- 1055\n",
      "1177 ---- 1056\n",
      "1178 ---- 1057\n",
      "1179 ---- 1058\n",
      "1180 ---- 1059\n",
      "1181 ---- 1060\n",
      "1182 ---- 1061\n",
      "1183 ---- 1062\n",
      "1184 ---- 1063\n",
      "1185 ---- 1064\n",
      "1186 ---- 1065\n",
      "1187 ---- 1066\n",
      "1188 ---- 1067\n",
      "1189 ---- 1068\n",
      "1190 ---- 1069\n",
      "1191 ---- 1070\n",
      "1192 ---- 1071\n",
      "1193 ---- 1072\n",
      "1194 ---- 1073\n",
      "1195 ---- 1074\n",
      "1196 ---- 1075\n",
      "1197 ---- 1076\n",
      "1198 ---- 1077\n",
      "1199 ---- 1078\n",
      "1200 ---- 1079\n",
      "1201 ---- 1080\n",
      "1202 ---- 1081\n",
      "1203 ---- 1082\n",
      "1204 ---- 1083\n",
      "1205 ---- 1084\n",
      "1206 ---- 1085\n",
      "1207 ---- 1086\n",
      "1208 ---- 1087\n",
      "1209 ---- 1088\n",
      "1210 ---- 1089\n",
      "1211 ---- 1090\n",
      "1212 ---- 1091\n",
      "1213 ---- 1092\n",
      "1214 ---- 1093\n",
      "1215 ---- 1094\n",
      "1216 ---- 1095\n",
      "1217 ---- 1096\n",
      "1218 ---- 1097\n",
      "1219 ---- 1098\n",
      "1220 ---- 1099\n",
      "1221 ---- 1100\n",
      "1222 ---- 1101\n",
      "1223 ---- 1102\n",
      "1224 ---- 1103\n",
      "1225 ---- 1104\n",
      "1226 ---- 1105\n",
      "1227 ---- 1106\n",
      "1228 ---- 1107\n",
      "1229 ---- 1108\n",
      "1230 ---- 1109\n",
      "1231 ---- 1110\n",
      "1232 ---- 1111\n",
      "1233 ---- 1112\n",
      "1234 ---- 1113\n",
      "1235 ---- 1114\n",
      "1236 ---- 1115\n",
      "1237 ---- 1116\n",
      "1238 ---- 1117\n",
      "1239 ---- 1118\n",
      "1240 ---- 1119\n",
      "1241 ---- 1120\n",
      "1242 ---- 1121\n",
      "1243 ---- 1122\n",
      "1244 ---- 1123\n",
      "1245 ---- 1124\n",
      "1246 ---- 1125\n",
      "1247 ---- 1126\n",
      "1248 ---- 1127\n",
      "1249 ---- 1128\n",
      "1250 ---- 1129\n",
      "1251 ---- 1130\n",
      "1252 ---- 1131\n",
      "1253 ---- 1132\n",
      "1254 ---- 1133\n",
      "1255 ---- 1134\n",
      "1256 ---- 1135\n",
      "1257 ---- 1136\n",
      "1258 ---- 1137\n",
      "1259 ---- 1138\n",
      "1260 ---- 1139\n",
      "1261 ---- 1140\n",
      "1262 ---- 1141\n",
      "1263 ---- 1142\n",
      "1264 ---- 1143\n",
      "1265 ---- 1144\n",
      "1266 ---- 1145\n",
      "1267 ---- 1146\n",
      "1268 ---- 1147\n",
      "1269 ---- 1148\n",
      "1270 ---- 1149\n",
      "1271 ---- 1150\n",
      "1272 ---- 1151\n",
      "1273 ---- 1152\n",
      "1274 ---- 1153\n",
      "1275 ---- 1154\n",
      "1276 ---- 1155\n",
      "1277 ---- 1156\n",
      "1278 ---- 1157\n",
      "1279 ---- 1158\n",
      "1280 ---- 1159\n",
      "1281 ---- 1160\n",
      "1282 ---- 1161\n",
      "1283 ---- 1162\n",
      "1284 ---- 1163\n",
      "1285 ---- 1164\n",
      "1286 ---- 1165\n",
      "1287 ---- 1166\n",
      "1288 ---- 1167\n",
      "1289 ---- 1168\n",
      "1290 ---- 1169\n",
      "1291 ---- 1170\n",
      "1292 ---- 1171\n",
      "1293 ---- 1172\n",
      "1294 ---- 1173\n",
      "1295 ---- 1174\n",
      "1296 ---- 1175\n",
      "1297 ---- 1176\n",
      "1298 ---- 1177\n",
      "1299 ---- 1178\n",
      "1300 ---- 1179\n",
      "1301 ---- 1180\n",
      "1302 ---- 1181\n",
      "1303 ---- 1182\n",
      "1304 ---- 1183\n",
      "1305 ---- 1184\n",
      "1306 ---- 1185\n",
      "1307 ---- 1186\n",
      "1308 ---- 1187\n",
      "1309 ---- 1188\n",
      "1310 ---- 1189\n",
      "1311 ---- 1190\n",
      "1312 ---- 1191\n",
      "1313 ---- 1192\n",
      "1314 ---- 1193\n",
      "1315 ---- 1194\n",
      "1316 ---- 1195\n",
      "1317 ---- 1196\n",
      "1318 ---- 1197\n",
      "1319 ---- 1198\n",
      "1320 ---- 1199\n",
      "1321 ---- 1200\n",
      "1322 ---- 1201\n",
      "1323 ---- 1202\n",
      "1324 ---- 1203\n",
      "1325 ---- 1204\n",
      "1326 ---- 1205\n",
      "1327 ---- 1206\n",
      "1328 ---- 1207\n",
      "1329 ---- 1208\n",
      "1330 ---- 1209\n",
      "1331 ---- 1210\n",
      "1332 ---- 1211\n",
      "1333 ---- 1212\n",
      "1334 ---- 1213\n",
      "1335 ---- 1214\n",
      "1336 ---- 1215\n",
      "1337 ---- 1216\n",
      "1338 ---- 1217\n",
      "1339 ---- 1218\n",
      "1340 ---- 1219\n",
      "1341 ---- 1220\n",
      "1342 ---- 1221\n",
      "1343 ---- 1222\n",
      "1344 ---- 1223\n",
      "1345 ---- 1224\n",
      "1346 ---- 1225\n",
      "1347 ---- 1226\n",
      "1348 ---- 1227\n",
      "1349 ---- 1228\n",
      "1350 ---- 1229\n",
      "1351 ---- 1230\n",
      "1352 ---- 1231\n",
      "1353 ---- 1232\n",
      "1354 ---- 1233\n",
      "1355 ---- 1234\n",
      "1356 ---- 1235\n",
      "1357 ---- 1236\n",
      "1358 ---- 1237\n",
      "1359 ---- 1238\n",
      "1360 ---- 1239\n",
      "1361 ---- 1240\n",
      "1362 ---- 1241\n",
      "1363 ---- 1242\n",
      "1364 ---- 1243\n",
      "1365 ---- 1244\n",
      "1366 ---- 1245\n",
      "1367 ---- 1246\n",
      "1368 ---- 1247\n",
      "1369 ---- 1248\n",
      "1370 ---- 1249\n",
      "1371 ---- 1250\n",
      "1372 ---- 1251\n",
      "1373 ---- 1252\n",
      "1374 ---- 1253\n",
      "1375 ---- 1254\n",
      "1376 ---- 1255\n",
      "1377 ---- 1256\n",
      "1378 ---- 1257\n",
      "1379 ---- 1258\n",
      "1380 ---- 1259\n",
      "1381 ---- 1260\n",
      "1382 ---- 1261\n",
      "1383 ---- 1262\n",
      "1384 ---- 1263\n",
      "1385 ---- 1264\n",
      "1386 ---- 1265\n",
      "1387 ---- 1266\n",
      "1388 ---- 1267\n",
      "1389 ---- 1268\n",
      "1390 ---- 1269\n",
      "1391 ---- 1270\n",
      "1392 ---- 1271\n",
      "1393 ---- 1272\n",
      "1394 ---- 1273\n",
      "1395 ---- 1274\n",
      "1396 ---- 1275\n",
      "1397 ---- 1276\n",
      "1398 ---- 1277\n",
      "1399 ---- 1278\n",
      "1400 ---- 1279\n",
      "1401 ---- 1280\n",
      "1402 ---- 1281\n",
      "1403 ---- 1282\n",
      "1404 ---- 1283\n",
      "1405 ---- 1284\n",
      "1406 ---- 1285\n",
      "1407 ---- 1286\n",
      "1408 ---- 1287\n",
      "1409 ---- 1288\n",
      "1410 ---- 1289\n",
      "1411 ---- 1290\n",
      "1412 ---- 1291\n",
      "1413 ---- 1292\n",
      "1414 ---- 1293\n",
      "1415 ---- 1294\n",
      "1416 ---- 1295\n",
      "1417 ---- 1296\n",
      "1418 ---- 1297\n",
      "1419 ---- 1298\n",
      "1420 ---- 1299\n",
      "1421 ---- 1300\n",
      "1422 ---- 1301\n",
      "1423 ---- 1302\n",
      "1424 ---- 1303\n",
      "1425 ---- 1304\n",
      "1426 ---- 1305\n",
      "1427 ---- 1306\n",
      "1428 ---- 1307\n",
      "1429 ---- 1308\n",
      "1430 ---- 1309\n",
      "1431 ---- 1310\n",
      "1432 ---- 1311\n",
      "1433 ---- 1312\n",
      "1434 ---- 1313\n",
      "1435 ---- 1314\n",
      "1436 ---- 1315\n",
      "1437 ---- 1316\n",
      "1438 ---- 1317\n",
      "1439 ---- 1318\n",
      "1440 ---- 1319\n",
      "1441 ---- 1320\n",
      "1442 ---- 1321\n",
      "1443 ---- 1322\n",
      "1444 ---- 1323\n",
      "1445 ---- 1324\n",
      "1446 ---- 1325\n",
      "1447 ---- 1326\n",
      "1448 ---- 1327\n",
      "1449 ---- 1328\n",
      "1450 ---- 1329\n",
      "1451 ---- 1330\n",
      "1452 ---- 1331\n",
      "1453 ---- 1332\n",
      "1454 ---- 1333\n",
      "1455 ---- 1334\n",
      "1456 ---- 1335\n",
      "1457 ---- 1336\n",
      "1458 ---- 1337\n",
      "1459 ---- 1338\n",
      "1460 ---- 1339\n",
      "1461 ---- 1340\n",
      "1462 ---- 1341\n",
      "1463 ---- 1342\n",
      "1464 ---- 1343\n",
      "1465 ---- 1344\n",
      "1466 ---- 1345\n",
      "1467 ---- 1346\n",
      "1468 ---- 1347\n",
      "1469 ---- 1348\n",
      "1470 ---- 1349\n",
      "1471 ---- 1350\n",
      "1472 ---- 1351\n",
      "1473 ---- 1352\n",
      "1474 ---- 1353\n",
      "1475 ---- 1354\n",
      "1476 ---- 1355\n",
      "1477 ---- 1356\n",
      "1478 ---- 1357\n",
      "1479 ---- 1358\n",
      "1480 ---- 1359\n",
      "1481 ---- 1360\n",
      "1482 ---- 1361\n",
      "1483 ---- 1362\n",
      "1484 ---- 1363\n",
      "1485 ---- 1364\n",
      "1486 ---- 1365\n",
      "1487 ---- 1366\n",
      "1488 ---- 1367\n",
      "1489 ---- 1368\n",
      "1490 ---- 1369\n",
      "1491 ---- 1370\n",
      "1492 ---- 1371\n",
      "1493 ---- 1372\n",
      "1494 ---- 1373\n",
      "1495 ---- 1374\n",
      "1496 ---- 1375\n",
      "1497 ---- 1376\n",
      "1498 ---- 1377\n",
      "1499 ---- 1378\n",
      "1500 ---- 1379\n",
      "1501 ---- 1380\n",
      "1502 ---- 1381\n",
      "1503 ---- 1382\n",
      "1504 ---- 1383\n",
      "1505 ---- 1384\n",
      "1506 ---- 1385\n",
      "1507 ---- 1386\n",
      "1508 ---- 1387\n",
      "1509 ---- 1388\n",
      "1510 ---- 1389\n",
      "1511 ---- 1390\n",
      "1512 ---- 1391\n",
      "1513 ---- 1392\n",
      "1514 ---- 1393\n",
      "1515 ---- 1394\n",
      "1516 ---- 1395\n",
      "1517 ---- 1396\n",
      "1518 ---- 1397\n",
      "1519 ---- 1398\n",
      "1520 ---- 1399\n",
      "1521 ---- 1400\n",
      "1522 ---- 1401\n",
      "1523 ---- 1402\n",
      "1524 ---- 1403\n",
      "1525 ---- 1404\n",
      "1526 ---- 1405\n",
      "1527 ---- 1406\n",
      "1528 ---- 1407\n",
      "1529 ---- 1408\n",
      "1530 ---- 1409\n",
      "1531 ---- 1410\n",
      "1532 ---- 1411\n",
      "1533 ---- 1412\n",
      "1534 ---- 1413\n",
      "1535 ---- 1414\n",
      "1536 ---- 1415\n",
      "1537 ---- 1416\n",
      "1538 ---- 1417\n",
      "1539 ---- 1418\n",
      "1540 ---- 1419\n",
      "1541 ---- 1420\n",
      "1542 ---- 1421\n",
      "1543 ---- 1422\n",
      "1544 ---- 1423\n",
      "1545 ---- 1424\n",
      "1546 ---- 1425\n",
      "1547 ---- 1426\n",
      "1548 ---- 1427\n",
      "1549 ---- 1428\n",
      "1550 ---- 1429\n",
      "1551 ---- 1430\n",
      "1552 ---- 1431\n",
      "1553 ---- 1432\n",
      "1554 ---- 1433\n",
      "1555 ---- 1434\n",
      "1556 ---- 1435\n",
      "1557 ---- 1436\n",
      "1558 ---- 1437\n",
      "1559 ---- 1438\n",
      "1560 ---- 1439\n",
      "1561 ---- 1440\n",
      "1562 ---- 1441\n",
      "1563 ---- 1442\n",
      "1564 ---- 1443\n",
      "1565 ---- 1444\n",
      "1566 ---- 1445\n",
      "1567 ---- 1446\n",
      "1568 ---- 1447\n",
      "1569 ---- 1448\n",
      "1570 ---- 1449\n",
      "1571 ---- 1450\n",
      "1572 ---- 1451\n",
      "1573 ---- 1452\n",
      "1574 ---- 1453\n",
      "1575 ---- 1454\n",
      "1576 ---- 1455\n",
      "1577 ---- 1456\n",
      "1578 ---- 1457\n",
      "1579 ---- 1458\n",
      "1580 ---- 1459\n",
      "1581 ---- 1460\n",
      "1582 ---- 1461\n",
      "1583 ---- 1462\n",
      "1584 ---- 1463\n",
      "1585 ---- 1464\n",
      "1586 ---- 1465\n",
      "1587 ---- 1466\n",
      "1588 ---- 1467\n",
      "1589 ---- 1468\n",
      "1590 ---- 1469\n",
      "1591 ---- 1470\n",
      "1592 ---- 1471\n",
      "1593 ---- 1472\n",
      "1594 ---- 1473\n",
      "1595 ---- 1474\n",
      "1596 ---- 1475\n",
      "1597 ---- 1476\n",
      "1598 ---- 1477\n",
      "1599 ---- 1478\n",
      "1600 ---- 1479\n",
      "1601 ---- 1480\n",
      "1602 ---- 1481\n",
      "1603 ---- 1482\n",
      "1604 ---- 1483\n",
      "1605 ---- 1484\n",
      "1606 ---- 1485\n",
      "1607 ---- 1486\n",
      "1608 ---- 1487\n",
      "1609 ---- 1488\n",
      "1610 ---- 1489\n",
      "1611 ---- 1490\n",
      "1612 ---- 1491\n",
      "1613 ---- 1492\n",
      "1614 ---- 1493\n",
      "1615 ---- 1494\n",
      "1616 ---- 1495\n",
      "1617 ---- 1496\n",
      "1618 ---- 1497\n",
      "1619 ---- 1498\n",
      "1620 ---- 1499\n",
      "1621 ---- 1500\n",
      "1622 ---- 1501\n",
      "1623 ---- 1502\n",
      "1624 ---- 1503\n",
      "1625 ---- 1504\n",
      "1626 ---- 1505\n",
      "1627 ---- 1506\n",
      "1628 ---- 1507\n",
      "1629 ---- 1508\n",
      "1630 ---- 1509\n",
      "1631 ---- 1510\n",
      "1632 ---- 1511\n",
      "1633 ---- 1512\n",
      "1634 ---- 1513\n",
      "1635 ---- 1514\n",
      "1636 ---- 1515\n",
      "1637 ---- 1516\n",
      "1638 ---- 1517\n",
      "1639 ---- 1518\n",
      "1640 ---- 1519\n",
      "1641 ---- 1520\n",
      "1642 ---- 1521\n",
      "1643 ---- 1522\n",
      "1644 ---- 1523\n",
      "1645 ---- 1524\n",
      "1646 ---- 1525\n",
      "1647 ---- 1526\n",
      "1648 ---- 1527\n",
      "1649 ---- 1528\n",
      "1650 ---- 1529\n",
      "1651 ---- 1530\n",
      "1652 ---- 1531\n",
      "1653 ---- 1532\n",
      "1654 ---- 1533\n",
      "1655 ---- 1534\n",
      "1656 ---- 1535\n",
      "1657 ---- 1536\n",
      "1658 ---- 1537\n",
      "1659 ---- 1538\n",
      "1660 ---- 1539\n",
      "1661 ---- 1540\n",
      "1662 ---- 1541\n",
      "1663 ---- 1542\n",
      "1664 ---- 1543\n",
      "1665 ---- 1544\n",
      "1666 ---- 1545\n",
      "1667 ---- 1546\n",
      "1668 ---- 1547\n",
      "1669 ---- 1548\n",
      "1670 ---- 1549\n",
      "1671 ---- 1550\n",
      "1672 ---- 1551\n",
      "1673 ---- 1552\n",
      "1674 ---- 1553\n",
      "1675 ---- 1554\n",
      "1676 ---- 1555\n",
      "1677 ---- 1556\n",
      "1678 ---- 1557\n",
      "1679 ---- 1558\n",
      "1680 ---- 1559\n",
      "1681 ---- 1560\n",
      "1682 ---- 1561\n",
      "1683 ---- 1562\n",
      "1684 ---- 1563\n",
      "1685 ---- 1564\n",
      "1686 ---- 1565\n",
      "1687 ---- 1566\n",
      "1688 ---- 1567\n",
      "1689 ---- 1568\n",
      "1690 ---- 1569\n",
      "1691 ---- 1570\n",
      "1692 ---- 1571\n",
      "1693 ---- 1572\n",
      "1694 ---- 1573\n",
      "1695 ---- 1574\n",
      "1696 ---- 1575\n",
      "1697 ---- 1576\n",
      "1698 ---- 1577\n",
      "1699 ---- 1578\n",
      "1700 ---- 1579\n",
      "1701 ---- 1580\n",
      "1702 ---- 1581\n",
      "1703 ---- 1582\n",
      "1704 ---- 1583\n",
      "1705 ---- 1584\n",
      "1706 ---- 1585\n",
      "1707 ---- 1586\n",
      "1708 ---- 1587\n",
      "1709 ---- 1588\n",
      "1710 ---- 1589\n",
      "1711 ---- 1590\n",
      "1712 ---- 1591\n",
      "1713 ---- 1592\n",
      "1714 ---- 1593\n",
      "1715 ---- 1594\n",
      "1716 ---- 1595\n",
      "1717 ---- 1596\n",
      "1718 ---- 1597\n",
      "1719 ---- 1598\n",
      "1720 ---- 1599\n",
      "1721 ---- 1600\n",
      "1722 ---- 1601\n",
      "1723 ---- 1602\n",
      "1724 ---- 1603\n",
      "1725 ---- 1604\n",
      "1726 ---- 1605\n",
      "1727 ---- 1606\n",
      "1728 ---- 1607\n",
      "1729 ---- 1608\n",
      "1730 ---- 1609\n",
      "1731 ---- 1610\n",
      "1732 ---- 1611\n",
      "1733 ---- 1612\n",
      "1734 ---- 1613\n",
      "1735 ---- 1614\n",
      "1736 ---- 1615\n",
      "1737 ---- 1616\n",
      "1738 ---- 1617\n",
      "1739 ---- 1618\n",
      "1740 ---- 1619\n",
      "1741 ---- 1620\n",
      "1742 ---- 1621\n",
      "1743 ---- 1622\n",
      "1744 ---- 1623\n",
      "1745 ---- 1624\n",
      "1746 ---- 1625\n",
      "1747 ---- 1626\n",
      "1748 ---- 1627\n",
      "1749 ---- 1628\n",
      "1750 ---- 1629\n",
      "1751 ---- 1630\n",
      "1752 ---- 1631\n",
      "1753 ---- 1632\n",
      "1754 ---- 1633\n",
      "1755 ---- 1634\n",
      "1756 ---- 1635\n",
      "1757 ---- 1636\n",
      "1758 ---- 1637\n",
      "1759 ---- 1638\n",
      "1760 ---- 1639\n",
      "1761 ---- 1640\n",
      "1762 ---- 1641\n",
      "1763 ---- 1642\n",
      "1764 ---- 1643\n",
      "1765 ---- 1644\n",
      "1766 ---- 1645\n",
      "1767 ---- 1646\n",
      "1768 ---- 1647\n",
      "1769 ---- 1648\n",
      "1770 ---- 1649\n",
      "1771 ---- 1650\n",
      "1772 ---- 1651\n",
      "1773 ---- 1652\n",
      "1774 ---- 1653\n",
      "1775 ---- 1654\n",
      "1776 ---- 1655\n",
      "1777 ---- 1656\n",
      "1778 ---- 1657\n",
      "1779 ---- 1658\n",
      "1780 ---- 1659\n",
      "1781 ---- 1660\n",
      "1782 ---- 1661\n",
      "1783 ---- 1662\n",
      "1784 ---- 1663\n",
      "1785 ---- 1664\n",
      "1786 ---- 1665\n",
      "1787 ---- 1666\n",
      "1788 ---- 1667\n",
      "1789 ---- 1668\n",
      "1790 ---- 1669\n",
      "1791 ---- 1670\n",
      "1792 ---- 1671\n",
      "1793 ---- 1672\n",
      "1794 ---- 1673\n",
      "1795 ---- 1674\n",
      "1796 ---- 1675\n",
      "1797 ---- 1676\n",
      "1798 ---- 1677\n",
      "1799 ---- 1678\n",
      "1800 ---- 1679\n",
      "1801 ---- 1680\n",
      "1802 ---- 1681\n",
      "1803 ---- 1682\n",
      "1804 ---- 1683\n",
      "1805 ---- 1684\n",
      "1806 ---- 1685\n",
      "1807 ---- 1686\n",
      "1808 ---- 1687\n",
      "1809 ---- 1688\n",
      "1810 ---- 1689\n",
      "1811 ---- 1690\n",
      "1812 ---- 1691\n",
      "1813 ---- 1692\n",
      "1814 ---- 1693\n",
      "1815 ---- 1694\n",
      "1816 ---- 1695\n",
      "1817 ---- 1696\n",
      "1818 ---- 1697\n",
      "1819 ---- 1698\n",
      "1820 ---- 1699\n",
      "1821 ---- 1700\n",
      "1822 ---- 1701\n",
      "1823 ---- 1702\n",
      "1824 ---- 1703\n",
      "1825 ---- 1704\n",
      "1826 ---- 1705\n",
      "1827 ---- 1706\n",
      "1828 ---- 1707\n",
      "1829 ---- 1708\n",
      "1830 ---- 1709\n",
      "1831 ---- 1710\n",
      "1832 ---- 1711\n",
      "1833 ---- 1712\n",
      "1834 ---- 1713\n",
      "1835 ---- 1714\n",
      "1836 ---- 1715\n",
      "1837 ---- 1716\n",
      "1838 ---- 1717\n",
      "1839 ---- 1718\n",
      "1840 ---- 1719\n",
      "1841 ---- 1720\n",
      "1842 ---- 1721\n",
      "1843 ---- 1722\n",
      "1844 ---- 1723\n",
      "1845 ---- 1724\n",
      "1846 ---- 1725\n",
      "1847 ---- 1726\n",
      "1848 ---- 1727\n",
      "1849 ---- 1728\n",
      "1850 ---- 1729\n",
      "1851 ---- 1730\n",
      "1852 ---- 1731\n",
      "1853 ---- 1732\n",
      "1854 ---- 1733\n",
      "1855 ---- 1734\n",
      "1856 ---- 1735\n",
      "1857 ---- 1736\n",
      "1858 ---- 1737\n",
      "1859 ---- 1738\n",
      "1860 ---- 1739\n",
      "1861 ---- 1740\n",
      "1862 ---- 1741\n",
      "1863 ---- 1742\n",
      "1864 ---- 1743\n",
      "1865 ---- 1744\n",
      "1866 ---- 1745\n",
      "1867 ---- 1746\n",
      "1868 ---- 1747\n",
      "1869 ---- 1748\n",
      "1870 ---- 1749\n",
      "1871 ---- 1750\n",
      "1872 ---- 1751\n",
      "1873 ---- 1752\n",
      "1874 ---- 1753\n",
      "1875 ---- 1754\n",
      "1876 ---- 1755\n",
      "1877 ---- 1756\n",
      "1878 ---- 1757\n",
      "1879 ---- 1758\n",
      "1880 ---- 1759\n",
      "1881 ---- 1760\n",
      "1882 ---- 1761\n",
      "1883 ---- 1762\n",
      "1884 ---- 1763\n",
      "1885 ---- 1764\n",
      "1886 ---- 1765\n",
      "1887 ---- 1766\n",
      "1888 ---- 1767\n",
      "1889 ---- 1768\n",
      "1890 ---- 1769\n",
      "1891 ---- 1770\n",
      "1892 ---- 1771\n",
      "1893 ---- 1772\n",
      "1894 ---- 1773\n",
      "1895 ---- 1774\n",
      "1896 ---- 1775\n",
      "1897 ---- 1776\n",
      "1898 ---- 1777\n",
      "1899 ---- 1778\n",
      "1900 ---- 1779\n",
      "1901 ---- 1780\n",
      "1902 ---- 1781\n",
      "1903 ---- 1782\n",
      "1904 ---- 1783\n",
      "1905 ---- 1784\n",
      "1906 ---- 1785\n",
      "1907 ---- 1786\n",
      "1908 ---- 1787\n",
      "1909 ---- 1788\n",
      "1910 ---- 1789\n",
      "1911 ---- 1790\n",
      "1912 ---- 1791\n",
      "1913 ---- 1792\n",
      "1914 ---- 1793\n",
      "1915 ---- 1794\n",
      "1916 ---- 1795\n",
      "1917 ---- 1796\n",
      "1918 ---- 1797\n",
      "1919 ---- 1798\n",
      "1920 ---- 1799\n",
      "1921 ---- 1800\n",
      "1922 ---- 1801\n",
      "1923 ---- 1802\n",
      "1924 ---- 1803\n",
      "1925 ---- 1804\n",
      "1926 ---- 1805\n",
      "1927 ---- 1806\n",
      "1928 ---- 1807\n",
      "1929 ---- 1808\n",
      "1930 ---- 1809\n",
      "1931 ---- 1810\n",
      "1932 ---- 1811\n",
      "1933 ---- 1812\n",
      "1934 ---- 1813\n",
      "1935 ---- 1814\n",
      "1936 ---- 1815\n",
      "1937 ---- 1816\n",
      "1938 ---- 1817\n",
      "1939 ---- 1818\n",
      "1940 ---- 1819\n",
      "1941 ---- 1820\n",
      "1942 ---- 1821\n",
      "1943 ---- 1822\n",
      "1944 ---- 1823\n",
      "1945 ---- 1824\n",
      "1946 ---- 1825\n",
      "1947 ---- 1826\n",
      "1948 ---- 1827\n",
      "1949 ---- 1828\n",
      "1950 ---- 1829\n",
      "1951 ---- 1830\n",
      "1952 ---- 1831\n",
      "1953 ---- 1832\n",
      "1954 ---- 1833\n",
      "1955 ---- 1834\n",
      "1956 ---- 1835\n",
      "1957 ---- 1836\n",
      "1958 ---- 1837\n",
      "1959 ---- 1838\n",
      "1960 ---- 1839\n",
      "1961 ---- 1840\n",
      "1962 ---- 1841\n",
      "1963 ---- 1842\n",
      "1964 ---- 1843\n",
      "1965 ---- 1844\n",
      "1966 ---- 1845\n",
      "1967 ---- 1846\n",
      "1968 ---- 1847\n",
      "1969 ---- 1848\n",
      "1970 ---- 1849\n",
      "1971 ---- 1850\n",
      "1972 ---- 1851\n",
      "1973 ---- 1852\n",
      "1974 ---- 1853\n",
      "1975 ---- 1854\n",
      "1976 ---- 1855\n",
      "1977 ---- 1856\n",
      "1978 ---- 1857\n",
      "1979 ---- 1858\n",
      "1980 ---- 1859\n",
      "1981 ---- 1860\n",
      "1982 ---- 1861\n",
      "1983 ---- 1862\n",
      "1984 ---- 1863\n",
      "1985 ---- 1864\n",
      "1986 ---- 1865\n",
      "1987 ---- 1866\n",
      "1988 ---- 1867\n",
      "1989 ---- 1868\n",
      "1990 ---- 1869\n",
      "1991 ---- 1870\n",
      "1992 ---- 1871\n",
      "1993 ---- 1872\n",
      "1994 ---- 1873\n",
      "1995 ---- 1874\n",
      "1996 ---- 1875\n",
      "1997 ---- 1876\n",
      "1998 ---- 1877\n",
      "1999 ---- 1878\n",
      "2000 ---- 1879\n",
      "2001 ---- 1880\n",
      "2002 ---- 1881\n",
      "2003 ---- 1882\n",
      "2004 ---- 1883\n",
      "2005 ---- 1884\n",
      "2006 ---- 1885\n",
      "2007 ---- 1886\n",
      "2008 ---- 1887\n",
      "2009 ---- 1888\n",
      "2010 ---- 1889\n",
      "2011 ---- 1890\n",
      "2012 ---- 1891\n",
      "2013 ---- 1892\n",
      "2014 ---- 1893\n",
      "2015 ---- 1894\n",
      "2016 ---- 1895\n",
      "2017 ---- 1896\n",
      "2018 ---- 1897\n",
      "2019 ---- 1898\n",
      "2020 ---- 1899\n",
      "2021 ---- 1900\n",
      "2022 ---- 1901\n",
      "2023 ---- 1902\n",
      "2024 ---- 1903\n",
      "2025 ---- 1904\n",
      "2026 ---- 1905\n",
      "2027 ---- 1906\n",
      "2028 ---- 1907\n",
      "2029 ---- 1908\n",
      "2030 ---- 1909\n",
      "2031 ---- 1910\n",
      "2032 ---- 1911\n",
      "2033 ---- 1912\n",
      "2034 ---- 1913\n",
      "2035 ---- 1914\n",
      "2036 ---- 1915\n",
      "2037 ---- 1916\n",
      "2038 ---- 1917\n",
      "2039 ---- 1918\n",
      "2040 ---- 1919\n",
      "2041 ---- 1920\n",
      "2042 ---- 1921\n",
      "2043 ---- 1922\n",
      "2044 ---- 1923\n",
      "2045 ---- 1924\n",
      "2046 ---- 1925\n",
      "2047 ---- 1926\n",
      "2048 ---- 1927\n",
      "2049 ---- 1928\n",
      "2050 ---- 1929\n",
      "2051 ---- 1930\n",
      "2052 ---- 1931\n",
      "2053 ---- 1932\n",
      "2054 ---- 1933\n",
      "2055 ---- 1934\n",
      "2056 ---- 1935\n",
      "2057 ---- 1936\n",
      "2058 ---- 1937\n",
      "2059 ---- 1938\n",
      "2060 ---- 1939\n",
      "2061 ---- 1940\n",
      "2062 ---- 1941\n",
      "2063 ---- 1942\n",
      "2064 ---- 1943\n",
      "2065 ---- 1944\n",
      "2066 ---- 1945\n",
      "2067 ---- 1946\n",
      "2068 ---- 1947\n",
      "2069 ---- 1948\n",
      "2070 ---- 1949\n",
      "2071 ---- 1950\n",
      "2072 ---- 1951\n",
      "2073 ---- 1952\n",
      "2074 ---- 1953\n",
      "2075 ---- 1954\n",
      "2076 ---- 1955\n",
      "2077 ---- 1956\n",
      "2078 ---- 1957\n",
      "2079 ---- 1958\n",
      "2080 ---- 1959\n",
      "2081 ---- 1960\n",
      "2082 ---- 1961\n",
      "2083 ---- 1962\n",
      "2084 ---- 1963\n",
      "2085 ---- 1964\n",
      "2086 ---- 1965\n",
      "2087 ---- 1966\n",
      "2088 ---- 1967\n",
      "2089 ---- 1968\n",
      "2090 ---- 1969\n",
      "2091 ---- 1970\n",
      "2092 ---- 1971\n",
      "2093 ---- 1972\n",
      "2094 ---- 1973\n",
      "2095 ---- 1974\n",
      "2096 ---- 1975\n",
      "2097 ---- 1976\n",
      "2098 ---- 1977\n",
      "2099 ---- 1978\n",
      "2100 ---- 1979\n",
      "2101 ---- 1980\n",
      "2102 ---- 1981\n",
      "2103 ---- 1982\n",
      "2104 ---- 1983\n",
      "2105 ---- 1984\n",
      "2106 ---- 1985\n",
      "2107 ---- 1986\n",
      "2108 ---- 1987\n",
      "2109 ---- 1988\n",
      "2110 ---- 1989\n",
      "2111 ---- 1990\n",
      "2112 ---- 1991\n",
      "2113 ---- 1992\n",
      "2114 ---- 1993\n",
      "2115 ---- 1994\n",
      "2116 ---- 1995\n",
      "2117 ---- 1996\n",
      "2118 ---- 1997\n",
      "2119 ---- 1998\n",
      "2120 ---- 1999\n",
      "2121 ---- 2000\n",
      "2122 ---- 2001\n",
      "2123 ---- 2002\n",
      "2124 ---- 2003\n",
      "2125 ---- 2004\n",
      "2126 ---- 2005\n",
      "2127 ---- 2006\n",
      "2128 ---- 2007\n",
      "2129 ---- 2008\n",
      "2130 ---- 2009\n",
      "2131 ---- 2010\n",
      "2132 ---- 2011\n",
      "2133 ---- 2012\n",
      "2134 ---- 2013\n",
      "2135 ---- 2014\n",
      "2136 ---- 2015\n",
      "2137 ---- 2016\n",
      "2138 ---- 2017\n",
      "2139 ---- 2018\n",
      "2140 ---- 2019\n",
      "2141 ---- 2020\n",
      "2142 ---- 2021\n",
      "2143 ---- 2022\n",
      "2144 ---- 2023\n",
      "2145 ---- 2024\n",
      "2146 ---- 2025\n",
      "2147 ---- 2026\n",
      "2148 ---- 2027\n",
      "2149 ---- 2028\n",
      "2150 ---- 2029\n",
      "2151 ---- 2030\n",
      "2152 ---- 2031\n",
      "2153 ---- 2032\n",
      "2154 ---- 2033\n",
      "2155 ---- 2034\n",
      "2156 ---- 2035\n",
      "2157 ---- 2036\n",
      "2158 ---- 2037\n",
      "2159 ---- 2038\n",
      "2160 ---- 2039\n",
      "2161 ---- 2040\n",
      "2162 ---- 2041\n",
      "2163 ---- 2042\n",
      "2164 ---- 2043\n",
      "2165 ---- 2044\n",
      "2166 ---- 2045\n",
      "2167 ---- 2046\n",
      "2168 ---- 2047\n",
      "2169 ---- 2048\n",
      "2170 ---- 2049\n",
      "2171 ---- 2050\n",
      "2172 ---- 2051\n",
      "2173 ---- 2052\n",
      "2174 ---- 2053\n",
      "2175 ---- 2054\n",
      "2176 ---- 2055\n",
      "2177 ---- 2056\n",
      "2178 ---- 2057\n",
      "2179 ---- 2058\n",
      "2180 ---- 2059\n",
      "2181 ---- 2060\n",
      "2182 ---- 2061\n",
      "2183 ---- 2062\n",
      "2184 ---- 2063\n",
      "2185 ---- 2064\n",
      "2186 ---- 2065\n",
      "2187 ---- 2066\n",
      "2188 ---- 2067\n",
      "2189 ---- 2068\n",
      "2190 ---- 2069\n",
      "2191 ---- 2070\n",
      "2192 ---- 2071\n",
      "2193 ---- 2072\n",
      "2194 ---- 2073\n",
      "2195 ---- 2074\n",
      "2196 ---- 2075\n",
      "2197 ---- 2076\n",
      "2198 ---- 2077\n",
      "2199 ---- 2078\n",
      "2200 ---- 2079\n",
      "2201 ---- 2080\n",
      "2202 ---- 2081\n",
      "2203 ---- 2082\n",
      "2204 ---- 2083\n",
      "2205 ---- 2084\n",
      "2206 ---- 2085\n",
      "2207 ---- 2086\n",
      "2208 ---- 2087\n",
      "2209 ---- 2088\n",
      "2210 ---- 2089\n",
      "2211 ---- 2090\n",
      "2212 ---- 2091\n",
      "2213 ---- 2092\n",
      "2214 ---- 2093\n",
      "2215 ---- 2094\n",
      "2216 ---- 2095\n",
      "2217 ---- 2096\n",
      "2218 ---- 2097\n",
      "2219 ---- 2098\n",
      "2220 ---- 2099\n",
      "2221 ---- 2100\n",
      "2222 ---- 2101\n",
      "2223 ---- 2102\n",
      "2224 ---- 2103\n",
      "2225 ---- 2104\n",
      "2226 ---- 2105\n",
      "2227 ---- 2106\n",
      "2228 ---- 2107\n",
      "2229 ---- 2108\n",
      "2230 ---- 2109\n",
      "2231 ---- 2110\n",
      "2232 ---- 2111\n",
      "2233 ---- 2112\n",
      "2234 ---- 2113\n",
      "2235 ---- 2114\n",
      "2236 ---- 2115\n",
      "2237 ---- 2116\n",
      "2238 ---- 2117\n",
      "2239 ---- 2118\n",
      "2240 ---- 2119\n",
      "2241 ---- 2120\n",
      "2242 ---- 2121\n",
      "2243 ---- 2122\n",
      "2244 ---- 2123\n",
      "2245 ---- 2124\n",
      "2246 ---- 2125\n",
      "2247 ---- 2126\n",
      "2248 ---- 2127\n",
      "2249 ---- 2128\n",
      "2250 ---- 2129\n",
      "2251 ---- 2130\n",
      "2252 ---- 2131\n",
      "2253 ---- 2132\n",
      "2254 ---- 2133\n",
      "2255 ---- 2134\n",
      "2256 ---- 2135\n",
      "2257 ---- 2136\n",
      "2258 ---- 2137\n",
      "2259 ---- 2138\n",
      "2260 ---- 2139\n",
      "2261 ---- 2140\n",
      "2262 ---- 2141\n",
      "2263 ---- 2142\n",
      "2264 ---- 2143\n",
      "2265 ---- 2144\n",
      "2266 ---- 2145\n",
      "2267 ---- 2146\n",
      "2268 ---- 2147\n",
      "2269 ---- 2148\n",
      "2270 ---- 2149\n",
      "2271 ---- 2150\n",
      "2272 ---- 2151\n",
      "2273 ---- 2152\n",
      "2274 ---- 2153\n",
      "2275 ---- 2154\n",
      "2276 ---- 2155\n",
      "2277 ---- 2156\n",
      "2278 ---- 2157\n",
      "2279 ---- 2158\n",
      "2280 ---- 2159\n",
      "2281 ---- 2160\n",
      "2282 ---- 2161\n",
      "2283 ---- 2162\n",
      "2284 ---- 2163\n",
      "2285 ---- 2164\n",
      "2286 ---- 2165\n",
      "2287 ---- 2166\n",
      "2288 ---- 2167\n",
      "2289 ---- 2168\n",
      "2290 ---- 2169\n",
      "2291 ---- 2170\n",
      "2292 ---- 2171\n",
      "2293 ---- 2172\n",
      "2294 ---- 2173\n",
      "2295 ---- 2174\n",
      "2296 ---- 2175\n",
      "2297 ---- 2176\n",
      "2298 ---- 2177\n",
      "2299 ---- 2178\n",
      "2300 ---- 2179\n",
      "2301 ---- 2180\n",
      "2302 ---- 2181\n",
      "2303 ---- 2182\n",
      "2304 ---- 2183\n",
      "2305 ---- 2184\n",
      "2306 ---- 2185\n",
      "2307 ---- 2186\n",
      "2308 ---- 2187\n",
      "2309 ---- 2188\n",
      "2310 ---- 2189\n",
      "2311 ---- 2190\n",
      "2312 ---- 2191\n",
      "2313 ---- 2192\n",
      "2314 ---- 2193\n",
      "2315 ---- 2194\n",
      "2316 ---- 2195\n",
      "2317 ---- 2196\n",
      "2318 ---- 2197\n",
      "2319 ---- 2198\n",
      "2320 ---- 2199\n",
      "2321 ---- 2200\n",
      "2322 ---- 2201\n",
      "2323 ---- 2202\n",
      "2324 ---- 2203\n",
      "2325 ---- 2204\n",
      "2326 ---- 2205\n",
      "2327 ---- 2206\n",
      "2328 ---- 2207\n",
      "2329 ---- 2208\n",
      "2330 ---- 2209\n",
      "2331 ---- 2210\n",
      "2332 ---- 2211\n",
      "2333 ---- 2212\n",
      "2334 ---- 2213\n",
      "2335 ---- 2214\n",
      "2336 ---- 2215\n",
      "2337 ---- 2216\n",
      "2338 ---- 2217\n",
      "2339 ---- 2218\n",
      "2340 ---- 2219\n",
      "2341 ---- 2220\n",
      "2342 ---- 2221\n",
      "2343 ---- 2222\n",
      "2344 ---- 2223\n",
      "2345 ---- 2224\n",
      "2346 ---- 2225\n",
      "2347 ---- 2226\n",
      "2348 ---- 2227\n",
      "2349 ---- 2228\n",
      "2350 ---- 2229\n",
      "2351 ---- 2230\n",
      "2352 ---- 2231\n",
      "2353 ---- 2232\n",
      "2354 ---- 2233\n",
      "2355 ---- 2234\n",
      "2356 ---- 2235\n",
      "2357 ---- 2236\n",
      "2358 ---- 2237\n",
      "2359 ---- 2238\n",
      "2360 ---- 2239\n",
      "2361 ---- 2240\n",
      "2362 ---- 2241\n",
      "2363 ---- 2242\n",
      "2364 ---- 2243\n",
      "2365 ---- 2244\n",
      "2366 ---- 2245\n",
      "2367 ---- 2246\n",
      "2368 ---- 2247\n",
      "2369 ---- 2248\n",
      "2370 ---- 2249\n",
      "2371 ---- 2250\n",
      "2372 ---- 2251\n",
      "2373 ---- 2252\n",
      "2374 ---- 2253\n",
      "2375 ---- 2254\n",
      "2376 ---- 2255\n",
      "2377 ---- 2256\n",
      "2378 ---- 2257\n",
      "2379 ---- 2258\n",
      "2380 ---- 2259\n",
      "2381 ---- 2260\n",
      "2382 ---- 2261\n",
      "2383 ---- 2262\n",
      "2384 ---- 2263\n",
      "2385 ---- 2264\n",
      "2386 ---- 2265\n",
      "2387 ---- 2266\n",
      "2388 ---- 2267\n",
      "2389 ---- 2268\n",
      "2390 ---- 2269\n",
      "2391 ---- 2270\n",
      "2392 ---- 2271\n",
      "2393 ---- 2272\n",
      "2394 ---- 2273\n",
      "2395 ---- 2274\n",
      "2396 ---- 2275\n",
      "2397 ---- 2276\n",
      "2398 ---- 2277\n",
      "2399 ---- 2278\n",
      "2400 ---- 2279\n",
      "2401 ---- 2280\n",
      "2402 ---- 2281\n",
      "2403 ---- 2282\n",
      "2404 ---- 2283\n",
      "2405 ---- 2284\n",
      "2406 ---- 2285\n",
      "2407 ---- 2286\n",
      "2408 ---- 2287\n",
      "2409 ---- 2288\n",
      "2410 ---- 2289\n",
      "2411 ---- 2290\n",
      "2412 ---- 2291\n",
      "2413 ---- 2292\n",
      "2414 ---- 2293\n",
      "2415 ---- 2294\n",
      "2416 ---- 2295\n",
      "2417 ---- 2296\n",
      "2418 ---- 2297\n",
      "2419 ---- 2298\n",
      "2420 ---- 2299\n",
      "2421 ---- 2300\n",
      "2422 ---- 2301\n",
      "2423 ---- 2302\n",
      "2424 ---- 2303\n",
      "2425 ---- 2304\n",
      "2426 ---- 2305\n",
      "2427 ---- 2306\n",
      "2428 ---- 2307\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(X_train_final.columns)):\n",
    "    print('{} ---- {}'.format(i, X_train_final.columns[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Unigrams, POS Tag Count, Sentiment Polarity, Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_1 = X_train_final.iloc[:,np.r_[10:12,13:21,121:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4233, 2318)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_1 = X_test_final.iloc[:,np.r_[10:12,13:21,121:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 2318)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.408\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9460    0.8083    0.8717       412\n",
      "           1     0.3361    0.6780    0.4494        59\n",
      "\n",
      "    accuracy                         0.7919       471\n",
      "   macro avg     0.6411    0.7431    0.6606       471\n",
      "weighted avg     0.8696    0.7919    0.8188       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_1_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_1_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_1,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_1), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 333, False Positive: 79, False Negative: 19, True Positive: 40\n",
      "--------------------------------------------------------------------------------\n",
      "[[333  79]\n",
      " [ 19  40]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.81      0.87       412\n",
      "           1       0.34      0.68      0.45        59\n",
      "\n",
      "    accuracy                           0.79       471\n",
      "   macro avg       0.64      0.74      0.66       471\n",
      "weighted avg       0.87      0.79      0.82       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_1 = LogisticRegression(random_state=18, \n",
    "                                solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], \n",
    "                                class_weight='balanced').fit(X_train_model_1, y_train)\n",
    "y_lr = lr_model_1.predict(X_test_model_1)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_model_2 = X_train_final.iloc[:,np.r_[3:1113]]\n",
    "X_train_model_2 = X_train_final.iloc[:, np.r_[3:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4233, 2426)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_2 = X_test_final.iloc[:,np.r_[3:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 2426)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.418\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9529    0.7864    0.8617       412\n",
      "           1     0.3282    0.7288    0.4526        59\n",
      "\n",
      "    accuracy                         0.7792       471\n",
      "   macro avg     0.6406    0.7576    0.6572       471\n",
      "weighted avg     0.8747    0.7792    0.8105       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_2_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_2_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_2,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_2), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 324, False Positive: 88, False Negative: 16, True Positive: 43\n",
      "--------------------------------------------------------------------------------\n",
      "[[324  88]\n",
      " [ 16  43]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.79      0.86       412\n",
      "           1       0.33      0.73      0.45        59\n",
      "\n",
      "    accuracy                           0.78       471\n",
      "   macro avg       0.64      0.76      0.66       471\n",
      "weighted avg       0.87      0.78      0.81       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_2 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_2, y_train)\n",
    "y_lr = lr_model_2.predict(X_test_model_2)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Without Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_3 = X_train_final.iloc[:,np.r_[3:121]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4233, 118)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_3 = X_test_final.iloc[:,np.r_[3:121]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 118)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n",
      "Best score: 0.388\n",
      "Best parameters set:\n",
      "\tclf__C: 5\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9503    0.6966    0.8039       412\n",
      "           1     0.2604    0.7458    0.3860        59\n",
      "\n",
      "    accuracy                         0.7028       471\n",
      "   macro avg     0.6053    0.7212    0.5949       471\n",
      "weighted avg     0.8639    0.7028    0.7516       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_3_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.0001, 0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\", \"elasticnet\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_3_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_3,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_3), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 287, False Positive: 125, False Negative: 15, True Positive: 44\n",
      "--------------------------------------------------------------------------------\n",
      "[[287 125]\n",
      " [ 15  44]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.70      0.80       412\n",
      "           1       0.26      0.75      0.39        59\n",
      "\n",
      "    accuracy                           0.70       471\n",
      "   macro avg       0.61      0.72      0.59       471\n",
      "weighted avg       0.86      0.70      0.75       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_3 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_3, y_train)\n",
    "y_lr = lr_model_3.predict(X_test_model_3)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Without Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_4 = X_train_final.iloc[:,np.r_[3:21,121:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4233, 2326)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_4 = X_test_final.iloc[:,np.r_[3:21,121:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 2326)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.401\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9460    0.8083    0.8717       412\n",
      "           1     0.3361    0.6780    0.4494        59\n",
      "\n",
      "    accuracy                         0.7919       471\n",
      "   macro avg     0.6411    0.7431    0.6606       471\n",
      "weighted avg     0.8696    0.7919    0.8188       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_4_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_4_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_4,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_4), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 333, False Positive: 79, False Negative: 19, True Positive: 40\n",
      "--------------------------------------------------------------------------------\n",
      "[[333  79]\n",
      " [ 19  40]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.81      0.87       412\n",
      "           1       0.34      0.68      0.45        59\n",
      "\n",
      "    accuracy                           0.79       471\n",
      "   macro avg       0.64      0.74      0.66       471\n",
      "weighted avg       0.87      0.79      0.82       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_4 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_4, y_train)\n",
    "y_lr = lr_model_4.predict(X_test_model_4)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Without POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_5 = X_train_final.iloc[:,np.r_[3:13,21:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4233, 2418)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_5 = X_test_final.iloc[:,np.r_[3:13,21:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 2418)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.415\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.79      0.86       412\n",
      "           1       0.33      0.71      0.45        59\n",
      "\n",
      "    accuracy                           0.78       471\n",
      "   macro avg       0.64      0.75      0.66       471\n",
      "weighted avg       0.87      0.78      0.81       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_5_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_5_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_5,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_5), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 326, False Positive: 86, False Negative: 17, True Positive: 42\n",
      "--------------------------------------------------------------------------------\n",
      "[[326  86]\n",
      " [ 17  42]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.79      0.86       412\n",
      "           1       0.33      0.71      0.45        59\n",
      "\n",
      "    accuracy                           0.78       471\n",
      "   macro avg       0.64      0.75      0.66       471\n",
      "weighted avg       0.87      0.78      0.81       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_5 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_5, y_train)\n",
    "y_lr = lr_model_5.predict(X_test_model_5)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6: Without STEM Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_6 = X_train_final.iloc[:,np.r_[10:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4233, 2419)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_6 = X_test_final.iloc[:,np.r_[10:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 2419)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.425\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.79      0.87       412\n",
      "           1       0.34      0.75      0.47        59\n",
      "\n",
      "    accuracy                           0.79       471\n",
      "   macro avg       0.65      0.77      0.67       471\n",
      "weighted avg       0.88      0.79      0.82       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_6_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_6_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_6,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_6), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 326, False Positive: 86, False Negative: 15, True Positive: 44\n",
      "--------------------------------------------------------------------------------\n",
      "[[326  86]\n",
      " [ 15  44]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.79      0.87       412\n",
      "           1       0.34      0.75      0.47        59\n",
      "\n",
      "    accuracy                           0.79       471\n",
      "   macro avg       0.65      0.77      0.67       471\n",
      "weighted avg       0.88      0.79      0.82       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_6 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_6, y_train)\n",
    "y_lr = lr_model_6.predict(X_test_model_6)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7: Without Sentiment Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_7 = X_train_final.iloc[:,np.r_[3:10,12:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4233, 2424)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_7 = X_test_final.iloc[:,np.r_[3:10,12:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 2424)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.420\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.79      0.86       412\n",
      "           1       0.33      0.71      0.45        59\n",
      "\n",
      "    accuracy                           0.78       471\n",
      "   macro avg       0.64      0.75      0.66       471\n",
      "weighted avg       0.87      0.78      0.81       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_7_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_7_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_7,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_7), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 326, False Positive: 86, False Negative: 17, True Positive: 42\n",
      "--------------------------------------------------------------------------------\n",
      "[[326  86]\n",
      " [ 17  42]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.79      0.86       412\n",
      "           1       0.33      0.71      0.45        59\n",
      "\n",
      "    accuracy                           0.78       471\n",
      "   macro avg       0.64      0.75      0.66       471\n",
      "weighted avg       0.87      0.78      0.81       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_7 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_7, y_train)\n",
    "y_lr = lr_model_7.predict(X_test_model_7)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 8: Without NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_8 = X_train_final.iloc[:,np.r_[3:12,13:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4233, 2425)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_8 = X_test_final.iloc[:,np.r_[3:12,13:2429]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 2425)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.418\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.79      0.86       412\n",
      "           1       0.33      0.73      0.45        59\n",
      "\n",
      "    accuracy                           0.78       471\n",
      "   macro avg       0.64      0.76      0.66       471\n",
      "weighted avg       0.87      0.78      0.81       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_8_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_8_pipeline, parameters, scoring=\"f1\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_8,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_8), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 324, False Positive: 88, False Negative: 16, True Positive: 43\n",
      "--------------------------------------------------------------------------------\n",
      "[[324  88]\n",
      " [ 16  43]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.79      0.86       412\n",
      "           1       0.33      0.73      0.45        59\n",
      "\n",
      "    accuracy                           0.78       471\n",
      "   macro avg       0.64      0.76      0.66       471\n",
      "weighted avg       0.87      0.78      0.81       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model_8 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_8, y_train)\n",
    "y_lr = lr_model_8.predict(X_test_model_8)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Summary\n",
    "| Experiment | Model Number | Features Used                                                | Precision | Recall | Macro F1 |\n",
    "| :--------: | :----------: | :----------------------------------------------------------: | :-------: | :----: | :------: |\n",
    "| Baseline   | 1            | Unigrams, POS Tag Count, Sentiment Polarity and Subjectivity | 0.64      | 0.74   | 0.66     |\n",
    "| Baseline   | 2            | All features (baseline)                                      | 0.64      | 0.76   | 0.66     |\n",
    "| Baseline   | 3            | Without Unigrams                                             | 0.61      | 0.72   | 0.59     |\n",
    "| Baseline   | 4            | Without Embeddings                                           | 0.64      | 0.74   | 0.66     |\n",
    "| Baseline   | 5            | Without POS tag                                              | 0.64      | 0.75   | 0.66     |\n",
    "| Baseline   | 6            | Without STEM similarity (paper baseline)                     | 0.65      | 0.77   | 0.67     |\n",
    "| Baseline   | 7            | Without sentiment features                                   | 0.64      | 0.75   | 0.66     |\n",
    "| Baseline   | 8            | Without NER                                                  | 0.64      | 0.76   | 0.66     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
