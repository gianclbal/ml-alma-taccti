{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Exp 2.0 Batch 1 + Batch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78536a14d97e4f1f962bcb10b61aacd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 21:47:17 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-10-24 21:47:17 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-10-24 21:47:18 INFO: File exists: /Users/gbaldonado/stanza_resources/en/default.zip\n",
      "2024-10-24 21:47:21 INFO: Finished downloading models and saved to /Users/gbaldonado/stanza_resources\n",
      "2024-10-24 21:47:21 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1289d72b90d748519cd2e662bfac7979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 21:47:21 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-10-24 21:47:22 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-10-24 21:47:22 INFO: Using device: cpu\n",
      "2024-10-24 21:47:22 INFO: Loading: tokenize\n",
      "2024-10-24 21:47:22 INFO: Loading: mwt\n",
      "2024-10-24 21:47:22 INFO: Loading: pos\n",
      "2024-10-24 21:47:22 INFO: Loading: lemma\n",
      "2024-10-24 21:47:23 INFO: Loading: constituency\n",
      "2024-10-24 21:47:23 INFO: Loading: depparse\n",
      "2024-10-24 21:47:23 INFO: Loading: sentiment\n",
      "2024-10-24 21:47:23 INFO: Loading: ner\n",
      "2024-10-24 21:47:24 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import pickle\n",
    "import warnings\n",
    "import stanza\n",
    "\n",
    "from random import shuffle\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, f1_score, r2_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Set random seed\n",
    "random.seed(18)\n",
    "seed = 18\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Initialize lemmatizer, stop words, and stanza\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline('en') # initialize English neural pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data and quick exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_aspirational_df_batch_1 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/merged_Aspirational_sentence_level_batch_1_jaccard.csv\", encoding='utf-8')\n",
    "merged_aspirational_df_batch_2 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/Aspirational Plus_sentence_level_batch_2_jaccard.csv\", encoding='utf-8')\n",
    "merged_aspirational_df = pd.concat([merged_aspirational_df_batch_1, merged_aspirational_df_batch_2])\n",
    "\n",
    "merged_familial_df_batch_1 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/merged_Familial_sentence_level_batch_1_jaccard.csv\", encoding='utf-8')\n",
    "merged_familial_df_batch_2 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/Familial Plus_sentence_level_batch_2_jaccard.csv\", encoding='utf-8')\n",
    "merged_familial_df = pd.concat([merged_familial_df_batch_1, merged_familial_df_batch_2])\n",
    "\n",
    "merged_navigational_df_batch_1 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/merged_Navigational_sentence_level_batch_1_jaccard.csv\", encoding='utf-8')\n",
    "merged_navigational_df_batch_2 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/Navigational Plus_sentence_level_batch_2_jaccard.csv\", encoding='utf-8')\n",
    "merged_navigational_df = pd.concat([merged_navigational_df_batch_1, merged_navigational_df_batch_2])\n",
    "\n",
    "merged_resistance_df_batch_1 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/merged_Resistance_sentence_level_batch_1_jaccard.csv\", encoding='utf-8')\n",
    "merged_resistance_df_batch_2 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/Resistance Plus_sentence_level_batch_2_jaccard.csv\", encoding='utf-8')\n",
    "merged_resistance_df = pd.concat([merged_resistance_df_batch_1, merged_resistance_df_batch_2])\n",
    "\n",
    "merged_social_df_batch_1 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/merged_Social_sentence_level_batch_1_jaccard.csv\", encoding='utf-8')\n",
    "merged_social_df_batch_2 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/Social Plus_sentence_level_batch_2_jaccard.csv\", encoding='utf-8')\n",
    "merged_social_df = pd.concat([merged_social_df_batch_1, merged_social_df_batch_2])\n",
    "\n",
    "merged_dataset_dict = {\"Aspirational\": merged_aspirational_df,\n",
    "                  \"Familial\": merged_familial_df,\n",
    "                  \"Navigational\": merged_navigational_df,\n",
    "                  \"Resistance\": merged_resistance_df,\n",
    "                  \"Social\": merged_social_df\n",
    "                  }\n",
    "\n",
    "\n",
    "def prepare_data(df, test_size=0.1, resample=True, strategy='undersample', seed=18):\n",
    "    \"\"\"\n",
    "    Prepare training and testing datasets by shuffling, splitting, and resampling if necessary.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The input DataFrame containing the merged dataset.\n",
    "    - test_size: Proportion of the dataset to include in the test split.\n",
    "    - resample: Boolean indicating whether to apply resampling.\n",
    "    - strategy: 'oversample' or 'undersample' if resampling is enabled.\n",
    "    - seed: Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - training_df: DataFrame containing the training set.\n",
    "    - test_df: DataFrame containing the test set.\n",
    "    \"\"\"\n",
    "\n",
    "    # Shuffle the merged dataset\n",
    "    df = shuffle(df, random_state=seed)\n",
    "\n",
    "    # Train-test split\n",
    "    training_df, test_df = train_test_split(\n",
    "        df,\n",
    "        test_size=test_size,\n",
    "        random_state=seed,\n",
    "        stratify=df['label']\n",
    "    )\n",
    "\n",
    "    # Function for undersampling or oversampling with a target ratio for oversampling\n",
    "    def resample_data(X, y, strategy='oversample', random_state=None, minority_ratio=0.3):\n",
    "        \"\"\"\n",
    "        Resample data using oversampling or undersampling.\n",
    "\n",
    "        Parameters:\n",
    "        X: Features\n",
    "        y: Labels\n",
    "        strategy: 'oversample' or 'undersample'\n",
    "        random_state: Seed for random state\n",
    "        minority_ratio: Target ratio of minority class after oversampling (0.2 to 0.3 recommended)\n",
    "\n",
    "        Returns:\n",
    "        X_resampled: Resampled features\n",
    "        y_resampled: Resampled labels\n",
    "        \"\"\"\n",
    "        # Ensure minority_ratio is within the desired range\n",
    "        if not (0.2 <= minority_ratio <= 0.3):\n",
    "            raise ValueError(\"Minority ratio must be between 0.2 and 0.3\")\n",
    "\n",
    "        # Calculate the desired sampling strategy\n",
    "        # For oversampling, we calculate how many samples we want for the minority class\n",
    "        if strategy == 'oversample':\n",
    "            # Minority class will be resampled to be `minority_ratio` of the total\n",
    "            # i.e., minority / (minority + majority) = minority_ratio\n",
    "            majority_class_count = sum(y == max(set(y), key=list(y).count))  # Majority class count\n",
    "            desired_minority_count = int((minority_ratio / (1 - minority_ratio)) * majority_class_count)\n",
    "            sampling_strategy = {min(set(y), key=list(y).count): desired_minority_count}\n",
    "            sampler = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=random_state)\n",
    "\n",
    "        elif strategy == 'undersample':\n",
    "            minority_class_count = sum(y == min(set(y), key=list(y).count))  # Minority class count\n",
    "            desired_majority_count = int(((1 - minority_ratio) / minority_ratio) * minority_class_count)\n",
    "            sampling_strategy = {max(set(y), key=list(y).count): desired_majority_count}\n",
    "            sampler = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=random_state)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Strategy must be 'oversample' or 'undersample'\")\n",
    "\n",
    "        X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "        return X_resampled, y_resampled\n",
    "\n",
    "    # Separate features and labels\n",
    "    X = training_df.drop(columns=['label'])  # Replace 'label' with your target column name\n",
    "    y = training_df['label']\n",
    "\n",
    "    # Toggle resampling\n",
    "    if resample:\n",
    "        # Apply resampling (choose 'oversample' or 'undersample')\n",
    "        X_resampled, y_resampled = resample_data(X, y, strategy=strategy, random_state=seed)\n",
    "\n",
    "        # Combine resampled data into a single DataFrame\n",
    "        training_df = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled, name='label')], axis=1)\n",
    "\n",
    "    # Reset the index of the DataFrames\n",
    "    training_df.reset_index(drop=True, inplace=True)\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(\"Training and test sets loaded.\")\n",
    "\n",
    "    print(f\"Training dataset shape: {training_df.shape} \\nTest dataset shape: {test_df.shape}\")\n",
    "    pos_labels_train = len(training_df[training_df['label'] == 1])\n",
    "    print(f\"Positive labels present in the training dataset: {pos_labels_train} out of {len(training_df)} or {pos_labels_train/len(training_df)*100:.2f}%\")\n",
    "    pos_labels_test = len(test_df[test_df['label'] == 1])\n",
    "    print(f\"Positive labels present in the test dataset: {pos_labels_test} out of {len(test_df)} or {pos_labels_test/len(test_df)*100:.2f}%\")\n",
    "\n",
    "    return training_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy subset for Aspirational created with 28 rows.\n",
      "Dummy subset for Familial created with 28 rows.\n",
      "Dummy subset for Navigational created with 28 rows.\n",
      "Dummy subset for Resistance created with 28 rows.\n",
      "Dummy subset for Social created with 28 rows.\n",
      "                                                                                                                                                                                                                                      sentence  \\\n",
      "2515                                                                                                                                                                                by that i mean i want to educate myself more biochemistry.   \n",
      "2306  i decided when i was younger that i wanted to be able to live a comfortable lifestyle when i am older and so in order to do that i have to get a job that i not only love, but also will be able to support myself and my future family.   \n",
      "4238                                                                                                                                                                                   thus, i am hopeful that i can exercise that hypothesis.   \n",
      "3594                                                                                                                                                                 why am i here?i am here because i wanted a significant change in my life.   \n",
      "1464                                                                                                 im here beside i want to graduate and make an impact on peoples lives with an inspirational movie or an iconic quote that i come up with.   \n",
      "\n",
      "      label  \\\n",
      "2515      0   \n",
      "2306      1   \n",
      "4238      0   \n",
      "3594      1   \n",
      "1464      0   \n",
      "\n",
      "                                                                                                                                                                                                                                                         phrase  \n",
      "2515                                                                     ['I am here because not only is this a required class for my major but I want to attempt to learn and comprehend physics. By that i mean I want to educate myself more biochemistry.']  \n",
      "2306                ['I decided when I was younger that I wanted to be able to live a comfortable lifestyle when I am older and so in order to do that I have to get a job that I not only love, but also will be able to support myself and my future family']  \n",
      "4238  ['Additionally, I am hopeful that learning from classmates that are of more senior standing than myself could give me some insight on some of the teachers to target or avoid, and answer some of the other questions I have about my future endeavors.']  \n",
      "3594                                                                                                                                        ['I am here because I wanted a significant change in my life.', 'To get a better and broader perspective of life.']  \n",
      "1464                                                                                                                   ['I want to get a job in the film industry. Im not really sure what part of it though. I was thinking a director, editor, or producer.']  \n"
     ]
    }
   ],
   "source": [
    "# Create a dummy subset function that ensures at least 2 samples from each class\n",
    "def create_balanced_dummy_subset(df, label_column='label', n_majority=20, n_minority=8):\n",
    "    \"\"\"Create a subset ensuring at least 2 samples from the minority class.\"\"\"\n",
    "    \n",
    "    # Split the dataset by class\n",
    "    majority_class = df[df[label_column] == 0]\n",
    "    minority_class = df[df[label_column] == 1]\n",
    "    \n",
    "    # Check the sizes of the classes to avoid sampling more than available\n",
    "    n_minority = min(n_minority, len(minority_class))\n",
    "    n_majority = min(n_majority, len(majority_class))\n",
    "    \n",
    "    # Sample from both classes\n",
    "    minority_sample = minority_class.sample(n=n_minority, random_state=42)\n",
    "    majority_sample = majority_class.sample(n=n_majority, random_state=42)\n",
    "    \n",
    "    # Combine the samples and shuffle the dataset\n",
    "    balanced_subset = pd.concat([majority_sample, minority_sample]).sample(frac=1, random_state=42)\n",
    "    \n",
    "    return balanced_subset\n",
    "\n",
    "# Initialize a dictionary to hold the dummy subsets\n",
    "dummy_dataset_dict = {}\n",
    "\n",
    "# Loop through each theme and create a balanced dummy subset\n",
    "for theme_name, df in merged_dataset_dict.items():\n",
    "    dummy_dataset_dict[theme_name] = create_balanced_dummy_subset(df, label_column='label', n_majority=20, n_minority=8)\n",
    "    print(f\"Dummy subset for {theme_name} created with {dummy_dataset_dict[theme_name].shape[0]} rows.\")\n",
    "\n",
    "# Optionally display the first few rows of one of the dummy datasets to verify\n",
    "print(dummy_dataset_dict[\"Aspirational\"].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack Obama']\n"
     ]
    }
   ],
   "source": [
    "def get_ner(text):\n",
    "    ner_list = []\n",
    "    # Annotate the text using stanza\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for entity in sentence.ents:\n",
    "            if entity.type == 'PERSON':\n",
    "                ner_list.append(entity.text)\n",
    "\n",
    "    return ner_list\n",
    "\n",
    "# Example usage\n",
    "text = \"Barack Obama was the 44th doctor of the United States.\"\n",
    "print(get_ner(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a named entity is present in the sentence\n",
    "def named_entity_present(sentence):\n",
    "    ner_list = get_ner(sentence)\n",
    "    if len(ner_list) > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Similarity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the similar words and similarity score\n",
    "# The function takes tokens of sentence as input and if its not a stop word, get its similarity with synsets of STEM.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words |= set([\"help\",\"try\", \"work\", \"process\", \"support\", \"job\"] )\n",
    "def word_similarity(tokens, syns, field):    \n",
    "    if field in ['engineering', 'technology']:\n",
    "        score_threshold = 0.5\n",
    "    else:\n",
    "        score_threshold = 0.2\n",
    "    sim_words = 0\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            try:\n",
    "                syns_word = wordnet.synsets(token) \n",
    "                score = syns_word[0].path_similarity(syns[0])\n",
    "                if score >= score_threshold:\n",
    "                    sim_words += 1\n",
    "            except: \n",
    "                score = 0\n",
    "    \n",
    "    return sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to create columns for similarity based on all STEM fields\n",
    "syns_bio = wordnet.synsets(lemmatizer.lemmatize(\"biology\"))\n",
    "syns_maths = wordnet.synsets(lemmatizer.lemmatize(\"mathematics\")) \n",
    "syns_tech = wordnet.synsets(lemmatizer.lemmatize(\"technology\"))\n",
    "syns_eng = wordnet.synsets(lemmatizer.lemmatize(\"engineering\"))\n",
    "syns_chem = wordnet.synsets(lemmatizer.lemmatize(\"chemistry\"))\n",
    "syns_phy = wordnet.synsets(lemmatizer.lemmatize(\"physics\"))\n",
    "syns_sci = wordnet.synsets(lemmatizer.lemmatize(\"science\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Medical Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neuroradiology', 'occupational', 'hospice', 'gastroenterology', 'rheumatology', 'dermatology', 'banking', 'urology', 'forensic', 'retardation', 'calculi', 'neuropathology', 'hematology', 'neurology', 'cornea', 'sleep', 'endocrinology', 'geriatric', 'ophthalmology', 'male', 'psychiatry', 'oncology', 'surgery', 'internal', 'critical', 'microbiology', 'interventional', 'neurophysiology', 'public', 'anterior', 'internal', 'neonatal', 'anesthesiology', 'endovascular', 'uveitis', 'immunopathology', 'heart', 'oculoplastics', 'infectious', 'rheumatology', 'pediatric', 'liaison', 'reproductive', 'glaucoma', 'procedural', 'genetics', 'maternal', 'plastic', 'advanced', 'cardiac', 'neurodevelopmental', 'endocrinology', 'fetal', 'cardiology', 'sports', 'consultation', 'psychosomatic', 'neuromuscular', 'infertility', 'cytopathology', 'physical', 'disabilities', 'imaging', 'musculoskeletal', 'diabetes', 'child', 'ophthalmology', 'care', 'pathology', 'preventive', 'gastroenterology', 'orbit', 'electrophysiology', 'segment', 'pediatric', 'infectious', 'behavioral', 'neck', 'pediatrics', 'blood', 'emergency', 'addiction', 'diagnostic', 'vascular', 'transplant', 'nephrology', 'gastrointestinal', 'perinatal', 'pulmonology', 'anesthesiology', 'administrative', 'chest', 'oncology', 'abdominal', 'pulmonary', 'strabismus', 'obstetrics', 'clinical', 'female', 'endocrinologists', 'diseases', 'pathology', 'molecular', 'neuro', 'cardiovascular', 'surgical', 'gynecologic', 'health', 'family', 'medicine', 'palliative', 'hepatology', 'renal', 'allergy', 'cardiothoracic', 'transplant', 'hematology', 'retina', 'interventional', 'community', 'genetic', 'injury', 'adolescent', 'nuclear', 'critical', 'pediatrics', 'neurology', 'rehabilitation', 'toxicology', 'medical', 'radiation', 'dermatology', 'radiology', 'dermatopathology', 'failure', 'reconstructive', 'abuse', 'pelvic', 'urologic', 'research', 'metabolism', 'urology', 'brain', 'developmental', 'psychiatry', 'nephrology', 'sports', 'ocular', 'breast', 'anatomical', 'transfusion', 'military', 'pain', 'cytogenetics', 'chemical', 'reconstructive', 'disease', 'genetic', 'gynecology', 'psychiatric', 'and', 'aerospace', 'biochemical', 'surgery', 'genitourinary', 'immunology', 'head', 'adolescent', 'mental', 'ophthalmic', 'neurourology', 'neuroradiology']\n"
     ]
    }
   ],
   "source": [
    "# Load the medical specialization text file and create a list\n",
    "medical_list = []\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/medical_specialities.txt', 'r') as medical_fields:\n",
    "    for line in medical_fields.readlines():\n",
    "        special_field = line.rstrip('\\n')\n",
    "        special_field = re.sub(\"\\W\",\" \", special_field )\n",
    "#         print(special_field)\n",
    "        medical_list += special_field.split()\n",
    "medical_list = list(set(medical_list))  \n",
    "medical_list = [x.lower() for x in medical_list]\n",
    "print(medical_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get medical words\n",
    "def check_medical_words(tokens):\n",
    "    for token in tokens:\n",
    "        if token not in stop_words and token in [x.lower() for x in medical_list]:\n",
    "            return 1\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sentiment Polarity and Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get polarity and subjectivity of the sentence using TexBlob\n",
    "def get_sentiment(sentence):\n",
    "    sentiments =TextBlob(sentence).sentiment\n",
    "    polarity = sentiments.polarity\n",
    "    subjectivity = sentiments.subjectivity\n",
    "    return polarity, subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the count of POS tags of the sentence\n",
    "def count_pos_tags(tokens):\n",
    "    token_pos = pos_tag(tokens)\n",
    "    count = Counter(tag for word,tag in token_pos)\n",
    "    interjections =  count['UH']\n",
    "    nouns = count['NN'] + count['NNS'] + count['NNP'] + count['NNPS']\n",
    "    adverb = count['RB'] + count['RBS'] + count['RBR']\n",
    "    verb = count['VB'] + count['VBD'] + count['VBG'] + count['VBN']\n",
    "    determiner = count['DT']\n",
    "    pronoun = count['PRP']\n",
    "    adjetive = count['JJ'] + count['JJR'] + count['JJS']\n",
    "    preposition = count['IN']\n",
    "    return interjections, nouns, adverb, verb, determiner, pronoun, adjetive,preposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_extraction(dataframe, field, func, column_names):\n",
    "    return pd.concat((\n",
    "        dataframe,\n",
    "        dataframe[field].apply(\n",
    "            lambda cell: pd.Series(func(cell), index=column_names))), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the w2v dict from pickle file\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/pickle/embeddings06122024.pickle', 'rb') as w2v_file:\n",
    "    w2v_dict = pickle.load(w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of word embeddings:  4762\n"
     ]
    }
   ],
   "source": [
    "print(\"length of word embeddings: \", len(w2v_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vectors for the essay\n",
    "def vectorizer(sequence):\n",
    "    vect = []\n",
    "    numw = 0\n",
    "    for w in sequence: \n",
    "        try :\n",
    "            if numw == 0:\n",
    "                vect = w2v_dict[w]\n",
    "            else:\n",
    "                vect = np.add(vect, w2v_dict[w])\n",
    "            numw += 1\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    return vect/ numw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into words\n",
    "def split_into_words(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vectorizer\n",
    "unigram_vect = CountVectorizer(ngram_range=(1, 1), min_df=2, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_unigram(X_TRAIN, X_TEST):\n",
    "    # Fit the unigram vectorizer on the training set\n",
    "    unigram_matrix = unigram_vect.fit_transform(X_TRAIN['sentence'])\n",
    "    \n",
    "    # Convert unigram matrix to DataFrame with proper column names\n",
    "    unigrams = pd.DataFrame(unigram_matrix.toarray(), columns=unigram_vect.get_feature_names_out())\n",
    "    unigrams = unigrams.reset_index(drop=True)\n",
    "    \n",
    "    # Append unigrams to the training data\n",
    "    X_train = pd.concat([X_TRAIN.reset_index(drop=True), unigrams], axis=1)\n",
    "    print(\"Shape of the unigram df for train: \", unigrams.shape)\n",
    "    \n",
    "    # Transform the test set with the fitted vectorizer\n",
    "    unigram_matrix_test = unigram_vect.transform(X_TEST['sentence'])\n",
    "    \n",
    "    # Convert test unigram matrix to DataFrame with proper column names\n",
    "    unigrams_test = pd.DataFrame(unigram_matrix_test.toarray(), columns=unigram_vect.get_feature_names_out())\n",
    "    unigrams_test = unigrams_test.reset_index(drop=True)\n",
    "    \n",
    "    # Append unigrams to the test data\n",
    "    X_test = pd.concat([X_TEST.reset_index(drop=True), unigrams_test], axis=1)\n",
    "    print(\"Test unigram df shape: \", unigrams_test.shape)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Putting them all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Optimized wrapper function for feature engineering using list comprehensions\n",
    "def feature_engineering(original_dataset):\n",
    "\n",
    "    dataset = original_dataset.copy()\n",
    "\n",
    "    # 1. Tokenization: Use list comprehension instead of apply\n",
    "    dataset['tokens'] = [word_tokenize(sentence) for sentence in dataset['sentence']]\n",
    "\n",
    "    # 2. Similarity features: Process all similarity features in a single loop using list comprehension\n",
    "    similarity_columns = ['bio_sim_words', 'chem_sim_words', 'phy_sim_words', 'math_sim_words', 'tech_sim_words', 'eng_sim_words']\n",
    "    sim_functions = [(syns_bio, 'biology'), (syns_chem, 'chemistry'), (syns_phy, 'physics'), (syns_maths, 'mathematics'),\n",
    "                     (syns_tech, 'technology'), (syns_eng, 'engineering')]\n",
    "\n",
    "    # Create similarity feature columns using list comprehensions\n",
    "    for col_name, (syns, label) in zip(similarity_columns, sim_functions):\n",
    "        dataset[col_name] = [word_similarity(tokens, syns, label) for tokens in dataset['tokens']]\n",
    "\n",
    "    # 3. Medical terms: Use list comprehension to check medical terms\n",
    "    dataset['medical_terms'] = [check_medical_words(tokens) for tokens in dataset['tokens']]\n",
    "\n",
    "    # 4. Polarity and subjectivity: Use list comprehension for sentiment analysis\n",
    "    sentiment_results = [get_sentiment(sentence) for sentence in dataset['sentence']]\n",
    "    dataset['polarity'] = [result[0] for result in sentiment_results]\n",
    "    dataset['subjectivity'] = [result[1] for result in sentiment_results]\n",
    "\n",
    "    # 5. Named entity recognition: Use list comprehension for NER\n",
    "    dataset['ner'] = [named_entity_present(sentence) for sentence in dataset['sentence']]\n",
    "\n",
    "    # 6. POS tag count: Assuming pos_tag_extraction cannot be vectorized, use it as-is\n",
    "    dataset = pos_tag_extraction(dataset, 'tokens', count_pos_tags, \n",
    "                                 ['interjections', 'nouns', 'adverb', 'verb', 'determiner', 'pronoun', 'adjective', 'preposition'])\n",
    "\n",
    "    # 7. Labels and Features (X and y split)\n",
    "    data_labels = dataset['label']\n",
    "    data_x = dataset.drop(columns='label')\n",
    "\n",
    "    # 8. Vectorization of tokens using list comprehension for efficient vectorization\n",
    "    vect_arr = np.array([vectorizer(tokens) for tokens in data_x['tokens']])\n",
    "\n",
    "    # 9. Create embedding DataFrame using the vectorized embeddings\n",
    "    embedding_df = pd.DataFrame(vect_arr.tolist(), index=data_x.index)\n",
    "    embedding_df.columns = [f'embedding_{i}' for i in range(embedding_df.shape[1])]\n",
    "\n",
    "    # 10. Concatenate embedding features with the original data\n",
    "    data_x = pd.concat([data_x.reset_index(drop=True), embedding_df], axis=1)\n",
    "\n",
    "    return data_x, data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing theme: Aspirational\n",
      "Training and test sets loaded.\n",
      "Training dataset shape: (8856, 3) \n",
      "Test dataset shape: (985, 3)\n",
      "Positive labels present in the training dataset: 805 out of 8856 or 9.09%\n",
      "Positive labels present in the test dataset: 89 out of 985 or 9.04%\n",
      "Training dataset shape: (8856, 3) \n",
      "Test dataset shape: (985, 3)\n",
      "Positive labels in training set: 805 out of 8856 (9.09%)\n",
      "Positive labels in test set: 89 out of 985 (9.04%)\n",
      " - Feature engineering...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Feature engineering\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - Feature engineering...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_engineering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m feature_engineering(test_df)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Ensure that the labels are integers\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 40\u001b[0m, in \u001b[0;36mfeature_engineering\u001b[0;34m(original_dataset)\u001b[0m\n\u001b[1;32m     37\u001b[0m data_x \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# 8. Vectorization of tokens using list comprehension for efficient vectorization\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m vect_arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([vectorizer(tokens) \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m data_x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# 9. Create embedding DataFrame using the vectorized embeddings\u001b[39;00m\n\u001b[1;32m     43\u001b[0m embedding_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(vect_arr\u001b[38;5;241m.\u001b[39mtolist(), index\u001b[38;5;241m=\u001b[39mdata_x\u001b[38;5;241m.\u001b[39mindex)\n",
      "Cell \u001b[0;32mIn[19], line 40\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     37\u001b[0m data_x \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# 8. Vectorization of tokens using list comprehension for efficient vectorization\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m vect_arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mvectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m data_x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# 9. Create embedding DataFrame using the vectorized embeddings\u001b[39;00m\n\u001b[1;32m     43\u001b[0m embedding_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(vect_arr\u001b[38;5;241m.\u001b[39mtolist(), index\u001b[38;5;241m=\u001b[39mdata_x\u001b[38;5;241m.\u001b[39mindex)\n",
      "Cell \u001b[0;32mIn[15], line 15\u001b[0m, in \u001b[0;36mvectorizer\u001b[0;34m(sequence)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvect\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnumw\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "# Initialize an empty dictionary to hold training and test DataFrames for each theme\n",
    "theme_dfs = {}\n",
    "\n",
    "# Define base directory where subfolders for each theme will be saved\n",
    "base_directory = 'theme_features'\n",
    "\n",
    "# List of themes to ensure subfolders exist for each one\n",
    "theme_list = ['aspirational', 'familial', 'navigational', 'resistance', 'social']\n",
    "\n",
    "# Ensure that the base directory exists, create if it doesn't\n",
    "if not os.path.exists(base_directory):\n",
    "    os.makedirs(base_directory)\n",
    "\n",
    "# Loop through each theme and its corresponding merged dataset\n",
    "for theme_name, merged_df in merged_dataset_dict.items():\n",
    "    print(f\"Processing theme: {theme_name}\")\n",
    "\n",
    "    # Ensure subfolder exists for the current theme\n",
    "    theme_folder = os.path.join(base_directory, theme_name)\n",
    "    if not os.path.exists(theme_folder):\n",
    "        os.makedirs(theme_folder)\n",
    "\n",
    "    # Call the prepare_data function and get the training and test DataFrames\n",
    "    training_df, test_df = prepare_data(merged_df, test_size=0.1, resample=False, strategy='oversample', seed=18)\n",
    "\n",
    "    # Print dataset shape information\n",
    "    print(f\"Training dataset shape: {training_df.shape} \\nTest dataset shape: {test_df.shape}\")\n",
    "\n",
    "    # Count and print the number of positive labels in training and test sets\n",
    "    pos_labels_train = training_df['label'].sum()\n",
    "    print(f\"Positive labels in training set: {pos_labels_train} out of {len(training_df)} ({(pos_labels_train / len(training_df)) * 100:.2f}%)\")\n",
    "\n",
    "    pos_labels_test = test_df['label'].sum()\n",
    "    print(f\"Positive labels in test set: {pos_labels_test} out of {len(test_df)} ({(pos_labels_test / len(test_df)) * 100:.2f}%)\")\n",
    "\n",
    "    # Feature engineering\n",
    "    print(\" - Feature engineering...\")\n",
    "    X_train, y_train = feature_engineering(training_df)\n",
    "    X_test, y_test = feature_engineering(test_df)\n",
    "\n",
    "    # Ensure that the labels are integers\n",
    "    y_train = y_train.astype('int')\n",
    "    y_test = y_test.astype('int')\n",
    "\n",
    "\n",
    "    # Append unigrams (or perform other feature modifications)\n",
    "    X_train_final, X_test_final = append_unigram(X_train, X_test)\n",
    "\n",
    "    # Store the X and y train and test DataFrames in the dictionary as a tuple\n",
    "    theme_dfs[theme_name] = (X_train_final, y_train, X_test_final, y_test)\n",
    "\n",
    "    # Save each DataFrame as CSV in the respective theme subfolder\n",
    "    X_train_file = os.path.join(theme_folder, 'X_train.csv')\n",
    "    y_train_file = os.path.join(theme_folder, 'y_train.csv')\n",
    "    X_test_file = os.path.join(theme_folder, 'X_test.csv')\n",
    "    y_test_file = os.path.join(theme_folder, 'y_test.csv')\n",
    "\n",
    "    X_train_final.to_csv(X_train_file, index=False)\n",
    "    y_train.to_csv(y_train_file, index=False)\n",
    "    X_test_final.to_csv(X_test_file, index=False)\n",
    "    y_test.to_csv(y_test_file, index=False)\n",
    "\n",
    "    print(f\" - Saved feature datasets for theme '{theme_name}' to folder '{theme_folder}'.\")\n",
    "\n",
    "# Display the resulting dictionary\n",
    "print(\"X and y train and test DataFrames stored for each theme:\")\n",
    "for theme, dfs in theme_dfs.items():\n",
    "    print(f\"{theme}: X_train shape: {dfs[0].shape}, y_train shape: {dfs[1].shape}, X_test shape: {dfs[2].shape}, y_test shape: {dfs[3].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# Initialize an empty dictionary to hold training and test DataFrames for each theme\n",
    "oversample_theme_dfs = {}\n",
    "\n",
    "# Define base directory where subfolders for each theme will be saved\n",
    "base_directory = 'oversample_theme_features'\n",
    "\n",
    "# List of themes to ensure subfolders exist for each one\n",
    "theme_list = ['aspirational', 'familial', 'navigational', 'resistance', 'social']\n",
    "\n",
    "# Ensure that the base directory exists, create if it doesn't\n",
    "if not os.path.exists(base_directory):\n",
    "    os.makedirs(base_directory)\n",
    "\n",
    "# Loop through each theme and its corresponding merged dataset\n",
    "for theme_name, merged_df in merged_dataset_dict.items():\n",
    "    print(f\"Processing theme: {theme_name}\")\n",
    "\n",
    "    # Ensure subfolder exists for the current theme\n",
    "    theme_folder = os.path.join(base_directory, theme_name)\n",
    "    if not os.path.exists(theme_folder):\n",
    "        os.makedirs(theme_folder)\n",
    "\n",
    "    # Call the prepare_data function and get the training and test DataFrames\n",
    "    training_df, test_df = prepare_data(merged_df, test_size=0.1, resample=True, strategy='oversample', seed=18)\n",
    "\n",
    "    # Print dataset shape information\n",
    "    print(f\"Training dataset shape: {training_df.shape} \\nTest dataset shape: {test_df.shape}\")\n",
    "\n",
    "    # Count and print the number of positive labels in training and test sets\n",
    "    pos_labels_train = training_df['label'].sum()\n",
    "    print(f\"Positive labels in training set: {pos_labels_train} out of {len(training_df)} ({(pos_labels_train / len(training_df)) * 100:.2f}%)\")\n",
    "\n",
    "    pos_labels_test = test_df['label'].sum()\n",
    "    print(f\"Positive labels in test set: {pos_labels_test} out of {len(test_df)} ({(pos_labels_test / len(test_df)) * 100:.2f}%)\")\n",
    "\n",
    "    # Feature engineering\n",
    "    print(\" - Feature engineering...\")\n",
    "    X_train, y_train = feature_engineering(training_df)\n",
    "    X_test, y_test = feature_engineering(test_df)\n",
    "\n",
    "    # Ensure that the labels are integers\n",
    "    y_train = y_train.astype('int')\n",
    "    y_test = y_test.astype('int')\n",
    "\n",
    "\n",
    "    # Append unigrams (or perform other feature modifications)\n",
    "    X_train_final, X_test_final = append_unigram(X_train, X_test)\n",
    "\n",
    "    # Store the X and y train and test DataFrames in the dictionary as a tuple\n",
    "    oversample_theme_dfs[theme_name] = (X_train_final, y_train, X_test_final, y_test)\n",
    "\n",
    "    # Save each DataFrame as CSV in the respective theme subfolder\n",
    "    X_train_file = os.path.join(theme_folder, 'X_train.csv')\n",
    "    y_train_file = os.path.join(theme_folder, 'y_train.csv')\n",
    "    X_test_file = os.path.join(theme_folder, 'X_test.csv')\n",
    "    y_test_file = os.path.join(theme_folder, 'y_test.csv')\n",
    "\n",
    "    X_train_final.to_csv(X_train_file, index=False)\n",
    "    y_train.to_csv(y_train_file, index=False)\n",
    "    X_test_final.to_csv(X_test_file, index=False)\n",
    "    y_test.to_csv(y_test_file, index=False)\n",
    "\n",
    "    print(f\" - Saved feature datasets for theme '{theme_name}' to folder '{theme_folder}'.\")\n",
    "\n",
    "# Display the resulting dictionary\n",
    "print(\"X and y train and test DataFrames stored for each theme:\")\n",
    "for theme, dfs in oversample_theme_dfs.items():\n",
    "    print(f\"{theme}: X_train shape: {dfs[0].shape}, y_train shape: {dfs[1].shape}, X_test shape: {dfs[2].shape}, y_test shape: {dfs[3].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# Initialize an empty dictionary to hold training and test DataFrames for each theme\n",
    "undersample_theme_dfs = {}\n",
    "\n",
    "# Define base directory where subfolders for each theme will be saved\n",
    "base_directory = 'undersample_theme_features'\n",
    "\n",
    "# List of themes to ensure subfolders exist for each one\n",
    "theme_list = ['aspirational', 'familial', 'navigational', 'resistance', 'social']\n",
    "\n",
    "# Ensure that the base directory exists, create if it doesn't\n",
    "if not os.path.exists(base_directory):\n",
    "    os.makedirs(base_directory)\n",
    "\n",
    "# Loop through each theme and its corresponding merged dataset\n",
    "for theme_name, merged_df in merged_dataset_dict.items():\n",
    "    print(f\"Processing theme: {theme_name}\")\n",
    "\n",
    "    # Ensure subfolder exists for the current theme\n",
    "    theme_folder = os.path.join(base_directory, theme_name)\n",
    "    if not os.path.exists(theme_folder):\n",
    "        os.makedirs(theme_folder)\n",
    "\n",
    "    # Call the prepare_data function and get the training and test DataFrames\n",
    "    training_df, test_df = prepare_data(merged_df, test_size=0.1, resample=True, strategy='oversample', seed=18)\n",
    "\n",
    "    # Print dataset shape information\n",
    "    print(f\"Training dataset shape: {training_df.shape} \\nTest dataset shape: {test_df.shape}\")\n",
    "\n",
    "    # Count and print the number of positive labels in training and test sets\n",
    "    pos_labels_train = training_df['label'].sum()\n",
    "    print(f\"Positive labels in training set: {pos_labels_train} out of {len(training_df)} ({(pos_labels_train / len(training_df)) * 100:.2f}%)\")\n",
    "\n",
    "    pos_labels_test = test_df['label'].sum()\n",
    "    print(f\"Positive labels in test set: {pos_labels_test} out of {len(test_df)} ({(pos_labels_test / len(test_df)) * 100:.2f}%)\")\n",
    "\n",
    "    # Feature engineering\n",
    "    print(\" - Feature engineering...\")\n",
    "    X_train, y_train = feature_engineering(training_df)\n",
    "    X_test, y_test = feature_engineering(test_df)\n",
    "\n",
    "    # Ensure that the labels are integers\n",
    "    y_train = y_train.astype('int')\n",
    "    y_test = y_test.astype('int')\n",
    "\n",
    "\n",
    "    # Append unigrams (or perform other feature modifications)\n",
    "    X_train_final, X_test_final = append_unigram(X_train, X_test)\n",
    "\n",
    "    # Store the X and y train and test DataFrames in the dictionary as a tuple\n",
    "    undersample_theme_dfs[theme_name] = (X_train_final, y_train, X_test_final, y_test)\n",
    "\n",
    "    # Save each DataFrame as CSV in the respective theme subfolder\n",
    "    X_train_file = os.path.join(theme_folder, 'X_train.csv')\n",
    "    y_train_file = os.path.join(theme_folder, 'y_train.csv')\n",
    "    X_test_file = os.path.join(theme_folder, 'X_test.csv')\n",
    "    y_test_file = os.path.join(theme_folder, 'y_test.csv')\n",
    "\n",
    "    X_train_final.to_csv(X_train_file, index=False)\n",
    "    y_train.to_csv(y_train_file, index=False)\n",
    "    X_test_final.to_csv(X_test_file, index=False)\n",
    "    y_test.to_csv(y_test_file, index=False)\n",
    "\n",
    "    print(f\" - Saved feature datasets for theme '{theme_name}' to folder '{theme_folder}'.\")\n",
    "\n",
    "# Display the resulting dictionary\n",
    "print(\"X and y train and test DataFrames stored for each theme:\")\n",
    "for theme, dfs in undersample_theme_dfs.items():\n",
    "    print(f\"{theme}: X_train shape: {dfs[0].shape}, y_train shape: {dfs[1].shape}, X_test shape: {dfs[2].shape}, y_test shape: {dfs[3].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data for theme: aspirational\n",
      "Loaded data for theme: familial\n",
      "Loaded data for theme: navigational\n",
      "Loaded data for theme: resistance\n",
      "Loaded data for theme: social\n"
     ]
    }
   ],
   "source": [
    "def load_theme_features(theme_folder_base='theme_features', themes=['aspirational', 'familial', 'navigational', 'resistance', 'social']):\n",
    "    \"\"\"Load the saved CSVs for each theme from the theme_features folder.\n",
    "    \n",
    "    Parameters:\n",
    "    theme_folder_base (str): The base folder containing the theme subfolders.\n",
    "    themes (list): A list of theme names corresponding to the subfolder names.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary where the keys are the theme names, and the values are tuples containing \n",
    "          (X_train, y_train, X_test, y_test) DataFrames for each theme.\n",
    "    \"\"\"\n",
    "    theme_dfs = {}\n",
    "\n",
    "    for theme in themes:\n",
    "        # Define the paths to the CSV files for the current theme\n",
    "        theme_folder = os.path.join(theme_folder_base, theme)\n",
    "        X_train_file = os.path.join(theme_folder, 'X_train.csv')\n",
    "        y_train_file = os.path.join(theme_folder, 'y_train.csv')\n",
    "        X_test_file = os.path.join(theme_folder, 'X_test.csv')\n",
    "        y_test_file = os.path.join(theme_folder, 'y_test.csv')\n",
    "        \n",
    "        # Load the CSV files into DataFrames\n",
    "        if os.path.exists(X_train_file) and os.path.exists(y_train_file) and os.path.exists(X_test_file) and os.path.exists(y_test_file):\n",
    "            X_train = pd.read_csv(X_train_file)\n",
    "            y_train = pd.read_csv(y_train_file)\n",
    "            X_test = pd.read_csv(X_test_file)\n",
    "            y_test = pd.read_csv(y_test_file)\n",
    "            \n",
    "            # Store them in the dictionary\n",
    "            theme_dfs[theme] = (X_train, y_train, X_test, y_test)\n",
    "            print(f\"Loaded data for theme: {theme}\")\n",
    "        else:\n",
    "            print(f\"Missing files for theme: {theme}. Please check the folder structure.\")\n",
    "    \n",
    "    return theme_dfs\n",
    "\n",
    "# Usage Example:\n",
    "theme_datasets = load_theme_features()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>phrase</th>\n",
       "      <th>tokens</th>\n",
       "      <th>bio_sim_words</th>\n",
       "      <th>chem_sim_words</th>\n",
       "      <th>phy_sim_words</th>\n",
       "      <th>math_sim_words</th>\n",
       "      <th>tech_sim_words</th>\n",
       "      <th>eng_sim_words</th>\n",
       "      <th>medical_terms</th>\n",
       "      <th>...</th>\n",
       "      <th>sf</th>\n",
       "      <th>state</th>\n",
       "      <th>study</th>\n",
       "      <th>successful</th>\n",
       "      <th>things</th>\n",
       "      <th>thought</th>\n",
       "      <th>want</th>\n",
       "      <th>wanted</th>\n",
       "      <th>wasnt</th>\n",
       "      <th>work</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>also another reason that i am here in this course is my major used to be math and physic when i was in my country and i did really good at them but as i moved here i change my major to biology to become optometric.after i done with my career my job should be related to math, physics,and biology.</td>\n",
       "      <td>['I moved here I change my major to biology to become optometric.']</td>\n",
       "      <td>['also', 'another', 'reason', 'that', 'i', 'am', 'here', 'in', 'this', 'course', 'is', 'my', 'major', 'used', 'to', 'be', 'math', 'and', 'physic', 'when', 'i', 'was', 'in', 'my', 'country', 'and', 'i', 'did', 'really', 'good', 'at', 'them', 'but', 'as', 'i', 'moved', 'here', 'i', 'change', 'my', 'major', 'to', 'biology', 'to', 'become', 'optometric.after', 'i', 'done', 'with', 'my', 'career', 'my', 'job', 'should', 'be', 'related', 'to', 'math', ',', 'physics', ',', 'and', 'biology', '.']</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i think i am here because i was born.</td>\n",
       "      <td>['Well honestly, I am here because it is part of my pre req requirements to get into dental school.']</td>\n",
       "      <td>['i', 'think', 'i', 'am', 'here', 'because', 'i', 'was', 'born', '.']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>some of my goals are to become a brain surgeon or a researcher.</td>\n",
       "      <td>['I guess I want my life to be exciting and fun. Some of my goals are to become a brain surgeon or a researcher. I want to help people also live their best lives. I want to find cures to these diseases or at least prevent them.']</td>\n",
       "      <td>['some', 'of', 'my', 'goals', 'are', 'to', 'become', 'a', 'brain', 'surgeon', 'or', 'a', 'researcher', '.']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 152 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                   sentence  \\\n",
       "0  also another reason that i am here in this course is my major used to be math and physic when i was in my country and i did really good at them but as i moved here i change my major to biology to become optometric.after i done with my career my job should be related to math, physics,and biology.   \n",
       "1                                                                                                                                                                                                                                                                     i think i am here because i was born.   \n",
       "2                                                                                                                                                                                                                                           some of my goals are to become a brain surgeon or a researcher.   \n",
       "\n",
       "                                                                                                                                                                                                                                  phrase  \\\n",
       "0                                                                                                                                                                    ['I moved here I change my major to biology to become optometric.']   \n",
       "1                                                                                                                                  ['Well honestly, I am here because it is part of my pre req requirements to get into dental school.']   \n",
       "2  ['I guess I want my life to be exciting and fun. Some of my goals are to become a brain surgeon or a researcher. I want to help people also live their best lives. I want to find cures to these diseases or at least prevent them.']   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          tokens  \\\n",
       "0  ['also', 'another', 'reason', 'that', 'i', 'am', 'here', 'in', 'this', 'course', 'is', 'my', 'major', 'used', 'to', 'be', 'math', 'and', 'physic', 'when', 'i', 'was', 'in', 'my', 'country', 'and', 'i', 'did', 'really', 'good', 'at', 'them', 'but', 'as', 'i', 'moved', 'here', 'i', 'change', 'my', 'major', 'to', 'biology', 'to', 'become', 'optometric.after', 'i', 'done', 'with', 'my', 'career', 'my', 'job', 'should', 'be', 'related', 'to', 'math', ',', 'physics', ',', 'and', 'biology', '.']   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                          ['i', 'think', 'i', 'am', 'here', 'because', 'i', 'was', 'born', '.']   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                    ['some', 'of', 'my', 'goals', 'are', 'to', 'become', 'a', 'brain', 'surgeon', 'or', 'a', 'researcher', '.']   \n",
       "\n",
       "   bio_sim_words  chem_sim_words  phy_sim_words  math_sim_words  \\\n",
       "0              5               5              1               5   \n",
       "1              0               0              0               0   \n",
       "2              0               0              0               0   \n",
       "\n",
       "   tech_sim_words  eng_sim_words  medical_terms  ...  sf  state  study  \\\n",
       "0               0              0              0  ...   0      0      0   \n",
       "1               0              0              0  ...   0      0      0   \n",
       "2               0              0              1  ...   0      0      0   \n",
       "\n",
       "   successful  things  thought  want  wanted  wasnt  work  \n",
       "0           0       0        0     0       0      0     0  \n",
       "1           0       0        0     0       0      0     0  \n",
       "2           0       0        0     0       0      0     0  \n",
       "\n",
       "[3 rows x 152 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theme_datasets[\"aspirational\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = feature_engineering(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 121)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Unigram features for both train and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1609, 121)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 121)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2.0/Aspirational/saved_features/X_train_final.csv\", index=False)\n",
    "X_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2.1/Aspirational/saved_features/X_test_final.csv\", index=False)\n",
    "y_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2.1/Aspirational/saved_features/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2.1/Aspirational/saved_features/y_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_unigram(X_TRAIN, X_TEST):\n",
    "    unigram_matrix = unigram_vect.fit_transform(X_TRAIN['sentence'])\n",
    "    unigrams = pd.DataFrame(unigram_matrix.toarray())\n",
    "    unigrams = unigrams.reset_index(drop=True)\n",
    "    print(\"Shape of the unigram df for train : \",unigrams.shape)\n",
    "    X_train = pd.concat([X_TRAIN, unigrams_test], axis = 1)\n",
    "\n",
    "    unigram_matrix_test = unigram_vect.transform(X_TEST['sentence'])\n",
    "    unigrams_test = pd.DataFrame(unigram_matrix_test.toarray())\n",
    "    unigrams_test = unigrams_test.reset_index(drop=True)\n",
    "    print(\"Test unigram df shape : \",unigrams_test.shape)\n",
    "    X_test = pd.concat([X_TEST, unigrams_test], axis = 1)\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the unigram df for train :  (1609, 1188)\n"
     ]
    }
   ],
   "source": [
    "# Unigrams for training set\n",
    "unigram_matrix = unigram_vect.fit_transform(X_train['sentence'])\n",
    "unigrams = pd.DataFrame(unigram_matrix.toarray())\n",
    "print(\"Shape of the unigram df for train : \",unigrams.shape)\n",
    "unigrams = unigrams.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = pd.concat([X_train, unigrams], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final.columns = X_train_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1609, 1309)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test unigram df shape :  (179, 1188)\n"
     ]
    }
   ],
   "source": [
    "unigram_matrix_test = unigram_vect.transform(X_test['sentence'])\n",
    "unigrams_test = pd.DataFrame(unigram_matrix_test.toarray())\n",
    "unigrams_test = unigrams_test.reset_index(drop=True)\n",
    "print(\"Test unigram df shape : \",unigrams_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 1309)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final = pd.concat([X_test, unigrams_test], axis = 1)\n",
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final.columns = X_test_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 1309)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ---- sentence\n",
      "1 ---- phrase\n",
      "2 ---- tokens\n",
      "3 ---- bio_sim_words\n",
      "4 ---- chem_sim_words\n",
      "5 ---- phy_sim_words\n",
      "6 ---- math_sim_words\n",
      "7 ---- tech_sim_words\n",
      "8 ---- eng_sim_words\n",
      "9 ---- medical_terms\n",
      "10 ---- polarity\n",
      "11 ---- subjectivity\n",
      "12 ---- ner\n",
      "13 ---- interjections\n",
      "14 ---- nouns\n",
      "15 ---- adverb\n",
      "16 ---- verb\n",
      "17 ---- determiner\n",
      "18 ---- pronoun\n",
      "19 ---- adjetive\n",
      "20 ---- preposition\n",
      "21 ---- embedding0\n",
      "22 ---- embedding1\n",
      "23 ---- embedding2\n",
      "24 ---- embedding3\n",
      "25 ---- embedding4\n",
      "26 ---- embedding5\n",
      "27 ---- embedding6\n",
      "28 ---- embedding7\n",
      "29 ---- embedding8\n",
      "30 ---- embedding9\n",
      "31 ---- embedding10\n",
      "32 ---- embedding11\n",
      "33 ---- embedding12\n",
      "34 ---- embedding13\n",
      "35 ---- embedding14\n",
      "36 ---- embedding15\n",
      "37 ---- embedding16\n",
      "38 ---- embedding17\n",
      "39 ---- embedding18\n",
      "40 ---- embedding19\n",
      "41 ---- embedding20\n",
      "42 ---- embedding21\n",
      "43 ---- embedding22\n",
      "44 ---- embedding23\n",
      "45 ---- embedding24\n",
      "46 ---- embedding25\n",
      "47 ---- embedding26\n",
      "48 ---- embedding27\n",
      "49 ---- embedding28\n",
      "50 ---- embedding29\n",
      "51 ---- embedding30\n",
      "52 ---- embedding31\n",
      "53 ---- embedding32\n",
      "54 ---- embedding33\n",
      "55 ---- embedding34\n",
      "56 ---- embedding35\n",
      "57 ---- embedding36\n",
      "58 ---- embedding37\n",
      "59 ---- embedding38\n",
      "60 ---- embedding39\n",
      "61 ---- embedding40\n",
      "62 ---- embedding41\n",
      "63 ---- embedding42\n",
      "64 ---- embedding43\n",
      "65 ---- embedding44\n",
      "66 ---- embedding45\n",
      "67 ---- embedding46\n",
      "68 ---- embedding47\n",
      "69 ---- embedding48\n",
      "70 ---- embedding49\n",
      "71 ---- embedding50\n",
      "72 ---- embedding51\n",
      "73 ---- embedding52\n",
      "74 ---- embedding53\n",
      "75 ---- embedding54\n",
      "76 ---- embedding55\n",
      "77 ---- embedding56\n",
      "78 ---- embedding57\n",
      "79 ---- embedding58\n",
      "80 ---- embedding59\n",
      "81 ---- embedding60\n",
      "82 ---- embedding61\n",
      "83 ---- embedding62\n",
      "84 ---- embedding63\n",
      "85 ---- embedding64\n",
      "86 ---- embedding65\n",
      "87 ---- embedding66\n",
      "88 ---- embedding67\n",
      "89 ---- embedding68\n",
      "90 ---- embedding69\n",
      "91 ---- embedding70\n",
      "92 ---- embedding71\n",
      "93 ---- embedding72\n",
      "94 ---- embedding73\n",
      "95 ---- embedding74\n",
      "96 ---- embedding75\n",
      "97 ---- embedding76\n",
      "98 ---- embedding77\n",
      "99 ---- embedding78\n",
      "100 ---- embedding79\n",
      "101 ---- embedding80\n",
      "102 ---- embedding81\n",
      "103 ---- embedding82\n",
      "104 ---- embedding83\n",
      "105 ---- embedding84\n",
      "106 ---- embedding85\n",
      "107 ---- embedding86\n",
      "108 ---- embedding87\n",
      "109 ---- embedding88\n",
      "110 ---- embedding89\n",
      "111 ---- embedding90\n",
      "112 ---- embedding91\n",
      "113 ---- embedding92\n",
      "114 ---- embedding93\n",
      "115 ---- embedding94\n",
      "116 ---- embedding95\n",
      "117 ---- embedding96\n",
      "118 ---- embedding97\n",
      "119 ---- embedding98\n",
      "120 ---- embedding99\n",
      "121 ---- 0\n",
      "122 ---- 1\n",
      "123 ---- 2\n",
      "124 ---- 3\n",
      "125 ---- 4\n",
      "126 ---- 5\n",
      "127 ---- 6\n",
      "128 ---- 7\n",
      "129 ---- 8\n",
      "130 ---- 9\n",
      "131 ---- 10\n",
      "132 ---- 11\n",
      "133 ---- 12\n",
      "134 ---- 13\n",
      "135 ---- 14\n",
      "136 ---- 15\n",
      "137 ---- 16\n",
      "138 ---- 17\n",
      "139 ---- 18\n",
      "140 ---- 19\n",
      "141 ---- 20\n",
      "142 ---- 21\n",
      "143 ---- 22\n",
      "144 ---- 23\n",
      "145 ---- 24\n",
      "146 ---- 25\n",
      "147 ---- 26\n",
      "148 ---- 27\n",
      "149 ---- 28\n",
      "150 ---- 29\n",
      "151 ---- 30\n",
      "152 ---- 31\n",
      "153 ---- 32\n",
      "154 ---- 33\n",
      "155 ---- 34\n",
      "156 ---- 35\n",
      "157 ---- 36\n",
      "158 ---- 37\n",
      "159 ---- 38\n",
      "160 ---- 39\n",
      "161 ---- 40\n",
      "162 ---- 41\n",
      "163 ---- 42\n",
      "164 ---- 43\n",
      "165 ---- 44\n",
      "166 ---- 45\n",
      "167 ---- 46\n",
      "168 ---- 47\n",
      "169 ---- 48\n",
      "170 ---- 49\n",
      "171 ---- 50\n",
      "172 ---- 51\n",
      "173 ---- 52\n",
      "174 ---- 53\n",
      "175 ---- 54\n",
      "176 ---- 55\n",
      "177 ---- 56\n",
      "178 ---- 57\n",
      "179 ---- 58\n",
      "180 ---- 59\n",
      "181 ---- 60\n",
      "182 ---- 61\n",
      "183 ---- 62\n",
      "184 ---- 63\n",
      "185 ---- 64\n",
      "186 ---- 65\n",
      "187 ---- 66\n",
      "188 ---- 67\n",
      "189 ---- 68\n",
      "190 ---- 69\n",
      "191 ---- 70\n",
      "192 ---- 71\n",
      "193 ---- 72\n",
      "194 ---- 73\n",
      "195 ---- 74\n",
      "196 ---- 75\n",
      "197 ---- 76\n",
      "198 ---- 77\n",
      "199 ---- 78\n",
      "200 ---- 79\n",
      "201 ---- 80\n",
      "202 ---- 81\n",
      "203 ---- 82\n",
      "204 ---- 83\n",
      "205 ---- 84\n",
      "206 ---- 85\n",
      "207 ---- 86\n",
      "208 ---- 87\n",
      "209 ---- 88\n",
      "210 ---- 89\n",
      "211 ---- 90\n",
      "212 ---- 91\n",
      "213 ---- 92\n",
      "214 ---- 93\n",
      "215 ---- 94\n",
      "216 ---- 95\n",
      "217 ---- 96\n",
      "218 ---- 97\n",
      "219 ---- 98\n",
      "220 ---- 99\n",
      "221 ---- 100\n",
      "222 ---- 101\n",
      "223 ---- 102\n",
      "224 ---- 103\n",
      "225 ---- 104\n",
      "226 ---- 105\n",
      "227 ---- 106\n",
      "228 ---- 107\n",
      "229 ---- 108\n",
      "230 ---- 109\n",
      "231 ---- 110\n",
      "232 ---- 111\n",
      "233 ---- 112\n",
      "234 ---- 113\n",
      "235 ---- 114\n",
      "236 ---- 115\n",
      "237 ---- 116\n",
      "238 ---- 117\n",
      "239 ---- 118\n",
      "240 ---- 119\n",
      "241 ---- 120\n",
      "242 ---- 121\n",
      "243 ---- 122\n",
      "244 ---- 123\n",
      "245 ---- 124\n",
      "246 ---- 125\n",
      "247 ---- 126\n",
      "248 ---- 127\n",
      "249 ---- 128\n",
      "250 ---- 129\n",
      "251 ---- 130\n",
      "252 ---- 131\n",
      "253 ---- 132\n",
      "254 ---- 133\n",
      "255 ---- 134\n",
      "256 ---- 135\n",
      "257 ---- 136\n",
      "258 ---- 137\n",
      "259 ---- 138\n",
      "260 ---- 139\n",
      "261 ---- 140\n",
      "262 ---- 141\n",
      "263 ---- 142\n",
      "264 ---- 143\n",
      "265 ---- 144\n",
      "266 ---- 145\n",
      "267 ---- 146\n",
      "268 ---- 147\n",
      "269 ---- 148\n",
      "270 ---- 149\n",
      "271 ---- 150\n",
      "272 ---- 151\n",
      "273 ---- 152\n",
      "274 ---- 153\n",
      "275 ---- 154\n",
      "276 ---- 155\n",
      "277 ---- 156\n",
      "278 ---- 157\n",
      "279 ---- 158\n",
      "280 ---- 159\n",
      "281 ---- 160\n",
      "282 ---- 161\n",
      "283 ---- 162\n",
      "284 ---- 163\n",
      "285 ---- 164\n",
      "286 ---- 165\n",
      "287 ---- 166\n",
      "288 ---- 167\n",
      "289 ---- 168\n",
      "290 ---- 169\n",
      "291 ---- 170\n",
      "292 ---- 171\n",
      "293 ---- 172\n",
      "294 ---- 173\n",
      "295 ---- 174\n",
      "296 ---- 175\n",
      "297 ---- 176\n",
      "298 ---- 177\n",
      "299 ---- 178\n",
      "300 ---- 179\n",
      "301 ---- 180\n",
      "302 ---- 181\n",
      "303 ---- 182\n",
      "304 ---- 183\n",
      "305 ---- 184\n",
      "306 ---- 185\n",
      "307 ---- 186\n",
      "308 ---- 187\n",
      "309 ---- 188\n",
      "310 ---- 189\n",
      "311 ---- 190\n",
      "312 ---- 191\n",
      "313 ---- 192\n",
      "314 ---- 193\n",
      "315 ---- 194\n",
      "316 ---- 195\n",
      "317 ---- 196\n",
      "318 ---- 197\n",
      "319 ---- 198\n",
      "320 ---- 199\n",
      "321 ---- 200\n",
      "322 ---- 201\n",
      "323 ---- 202\n",
      "324 ---- 203\n",
      "325 ---- 204\n",
      "326 ---- 205\n",
      "327 ---- 206\n",
      "328 ---- 207\n",
      "329 ---- 208\n",
      "330 ---- 209\n",
      "331 ---- 210\n",
      "332 ---- 211\n",
      "333 ---- 212\n",
      "334 ---- 213\n",
      "335 ---- 214\n",
      "336 ---- 215\n",
      "337 ---- 216\n",
      "338 ---- 217\n",
      "339 ---- 218\n",
      "340 ---- 219\n",
      "341 ---- 220\n",
      "342 ---- 221\n",
      "343 ---- 222\n",
      "344 ---- 223\n",
      "345 ---- 224\n",
      "346 ---- 225\n",
      "347 ---- 226\n",
      "348 ---- 227\n",
      "349 ---- 228\n",
      "350 ---- 229\n",
      "351 ---- 230\n",
      "352 ---- 231\n",
      "353 ---- 232\n",
      "354 ---- 233\n",
      "355 ---- 234\n",
      "356 ---- 235\n",
      "357 ---- 236\n",
      "358 ---- 237\n",
      "359 ---- 238\n",
      "360 ---- 239\n",
      "361 ---- 240\n",
      "362 ---- 241\n",
      "363 ---- 242\n",
      "364 ---- 243\n",
      "365 ---- 244\n",
      "366 ---- 245\n",
      "367 ---- 246\n",
      "368 ---- 247\n",
      "369 ---- 248\n",
      "370 ---- 249\n",
      "371 ---- 250\n",
      "372 ---- 251\n",
      "373 ---- 252\n",
      "374 ---- 253\n",
      "375 ---- 254\n",
      "376 ---- 255\n",
      "377 ---- 256\n",
      "378 ---- 257\n",
      "379 ---- 258\n",
      "380 ---- 259\n",
      "381 ---- 260\n",
      "382 ---- 261\n",
      "383 ---- 262\n",
      "384 ---- 263\n",
      "385 ---- 264\n",
      "386 ---- 265\n",
      "387 ---- 266\n",
      "388 ---- 267\n",
      "389 ---- 268\n",
      "390 ---- 269\n",
      "391 ---- 270\n",
      "392 ---- 271\n",
      "393 ---- 272\n",
      "394 ---- 273\n",
      "395 ---- 274\n",
      "396 ---- 275\n",
      "397 ---- 276\n",
      "398 ---- 277\n",
      "399 ---- 278\n",
      "400 ---- 279\n",
      "401 ---- 280\n",
      "402 ---- 281\n",
      "403 ---- 282\n",
      "404 ---- 283\n",
      "405 ---- 284\n",
      "406 ---- 285\n",
      "407 ---- 286\n",
      "408 ---- 287\n",
      "409 ---- 288\n",
      "410 ---- 289\n",
      "411 ---- 290\n",
      "412 ---- 291\n",
      "413 ---- 292\n",
      "414 ---- 293\n",
      "415 ---- 294\n",
      "416 ---- 295\n",
      "417 ---- 296\n",
      "418 ---- 297\n",
      "419 ---- 298\n",
      "420 ---- 299\n",
      "421 ---- 300\n",
      "422 ---- 301\n",
      "423 ---- 302\n",
      "424 ---- 303\n",
      "425 ---- 304\n",
      "426 ---- 305\n",
      "427 ---- 306\n",
      "428 ---- 307\n",
      "429 ---- 308\n",
      "430 ---- 309\n",
      "431 ---- 310\n",
      "432 ---- 311\n",
      "433 ---- 312\n",
      "434 ---- 313\n",
      "435 ---- 314\n",
      "436 ---- 315\n",
      "437 ---- 316\n",
      "438 ---- 317\n",
      "439 ---- 318\n",
      "440 ---- 319\n",
      "441 ---- 320\n",
      "442 ---- 321\n",
      "443 ---- 322\n",
      "444 ---- 323\n",
      "445 ---- 324\n",
      "446 ---- 325\n",
      "447 ---- 326\n",
      "448 ---- 327\n",
      "449 ---- 328\n",
      "450 ---- 329\n",
      "451 ---- 330\n",
      "452 ---- 331\n",
      "453 ---- 332\n",
      "454 ---- 333\n",
      "455 ---- 334\n",
      "456 ---- 335\n",
      "457 ---- 336\n",
      "458 ---- 337\n",
      "459 ---- 338\n",
      "460 ---- 339\n",
      "461 ---- 340\n",
      "462 ---- 341\n",
      "463 ---- 342\n",
      "464 ---- 343\n",
      "465 ---- 344\n",
      "466 ---- 345\n",
      "467 ---- 346\n",
      "468 ---- 347\n",
      "469 ---- 348\n",
      "470 ---- 349\n",
      "471 ---- 350\n",
      "472 ---- 351\n",
      "473 ---- 352\n",
      "474 ---- 353\n",
      "475 ---- 354\n",
      "476 ---- 355\n",
      "477 ---- 356\n",
      "478 ---- 357\n",
      "479 ---- 358\n",
      "480 ---- 359\n",
      "481 ---- 360\n",
      "482 ---- 361\n",
      "483 ---- 362\n",
      "484 ---- 363\n",
      "485 ---- 364\n",
      "486 ---- 365\n",
      "487 ---- 366\n",
      "488 ---- 367\n",
      "489 ---- 368\n",
      "490 ---- 369\n",
      "491 ---- 370\n",
      "492 ---- 371\n",
      "493 ---- 372\n",
      "494 ---- 373\n",
      "495 ---- 374\n",
      "496 ---- 375\n",
      "497 ---- 376\n",
      "498 ---- 377\n",
      "499 ---- 378\n",
      "500 ---- 379\n",
      "501 ---- 380\n",
      "502 ---- 381\n",
      "503 ---- 382\n",
      "504 ---- 383\n",
      "505 ---- 384\n",
      "506 ---- 385\n",
      "507 ---- 386\n",
      "508 ---- 387\n",
      "509 ---- 388\n",
      "510 ---- 389\n",
      "511 ---- 390\n",
      "512 ---- 391\n",
      "513 ---- 392\n",
      "514 ---- 393\n",
      "515 ---- 394\n",
      "516 ---- 395\n",
      "517 ---- 396\n",
      "518 ---- 397\n",
      "519 ---- 398\n",
      "520 ---- 399\n",
      "521 ---- 400\n",
      "522 ---- 401\n",
      "523 ---- 402\n",
      "524 ---- 403\n",
      "525 ---- 404\n",
      "526 ---- 405\n",
      "527 ---- 406\n",
      "528 ---- 407\n",
      "529 ---- 408\n",
      "530 ---- 409\n",
      "531 ---- 410\n",
      "532 ---- 411\n",
      "533 ---- 412\n",
      "534 ---- 413\n",
      "535 ---- 414\n",
      "536 ---- 415\n",
      "537 ---- 416\n",
      "538 ---- 417\n",
      "539 ---- 418\n",
      "540 ---- 419\n",
      "541 ---- 420\n",
      "542 ---- 421\n",
      "543 ---- 422\n",
      "544 ---- 423\n",
      "545 ---- 424\n",
      "546 ---- 425\n",
      "547 ---- 426\n",
      "548 ---- 427\n",
      "549 ---- 428\n",
      "550 ---- 429\n",
      "551 ---- 430\n",
      "552 ---- 431\n",
      "553 ---- 432\n",
      "554 ---- 433\n",
      "555 ---- 434\n",
      "556 ---- 435\n",
      "557 ---- 436\n",
      "558 ---- 437\n",
      "559 ---- 438\n",
      "560 ---- 439\n",
      "561 ---- 440\n",
      "562 ---- 441\n",
      "563 ---- 442\n",
      "564 ---- 443\n",
      "565 ---- 444\n",
      "566 ---- 445\n",
      "567 ---- 446\n",
      "568 ---- 447\n",
      "569 ---- 448\n",
      "570 ---- 449\n",
      "571 ---- 450\n",
      "572 ---- 451\n",
      "573 ---- 452\n",
      "574 ---- 453\n",
      "575 ---- 454\n",
      "576 ---- 455\n",
      "577 ---- 456\n",
      "578 ---- 457\n",
      "579 ---- 458\n",
      "580 ---- 459\n",
      "581 ---- 460\n",
      "582 ---- 461\n",
      "583 ---- 462\n",
      "584 ---- 463\n",
      "585 ---- 464\n",
      "586 ---- 465\n",
      "587 ---- 466\n",
      "588 ---- 467\n",
      "589 ---- 468\n",
      "590 ---- 469\n",
      "591 ---- 470\n",
      "592 ---- 471\n",
      "593 ---- 472\n",
      "594 ---- 473\n",
      "595 ---- 474\n",
      "596 ---- 475\n",
      "597 ---- 476\n",
      "598 ---- 477\n",
      "599 ---- 478\n",
      "600 ---- 479\n",
      "601 ---- 480\n",
      "602 ---- 481\n",
      "603 ---- 482\n",
      "604 ---- 483\n",
      "605 ---- 484\n",
      "606 ---- 485\n",
      "607 ---- 486\n",
      "608 ---- 487\n",
      "609 ---- 488\n",
      "610 ---- 489\n",
      "611 ---- 490\n",
      "612 ---- 491\n",
      "613 ---- 492\n",
      "614 ---- 493\n",
      "615 ---- 494\n",
      "616 ---- 495\n",
      "617 ---- 496\n",
      "618 ---- 497\n",
      "619 ---- 498\n",
      "620 ---- 499\n",
      "621 ---- 500\n",
      "622 ---- 501\n",
      "623 ---- 502\n",
      "624 ---- 503\n",
      "625 ---- 504\n",
      "626 ---- 505\n",
      "627 ---- 506\n",
      "628 ---- 507\n",
      "629 ---- 508\n",
      "630 ---- 509\n",
      "631 ---- 510\n",
      "632 ---- 511\n",
      "633 ---- 512\n",
      "634 ---- 513\n",
      "635 ---- 514\n",
      "636 ---- 515\n",
      "637 ---- 516\n",
      "638 ---- 517\n",
      "639 ---- 518\n",
      "640 ---- 519\n",
      "641 ---- 520\n",
      "642 ---- 521\n",
      "643 ---- 522\n",
      "644 ---- 523\n",
      "645 ---- 524\n",
      "646 ---- 525\n",
      "647 ---- 526\n",
      "648 ---- 527\n",
      "649 ---- 528\n",
      "650 ---- 529\n",
      "651 ---- 530\n",
      "652 ---- 531\n",
      "653 ---- 532\n",
      "654 ---- 533\n",
      "655 ---- 534\n",
      "656 ---- 535\n",
      "657 ---- 536\n",
      "658 ---- 537\n",
      "659 ---- 538\n",
      "660 ---- 539\n",
      "661 ---- 540\n",
      "662 ---- 541\n",
      "663 ---- 542\n",
      "664 ---- 543\n",
      "665 ---- 544\n",
      "666 ---- 545\n",
      "667 ---- 546\n",
      "668 ---- 547\n",
      "669 ---- 548\n",
      "670 ---- 549\n",
      "671 ---- 550\n",
      "672 ---- 551\n",
      "673 ---- 552\n",
      "674 ---- 553\n",
      "675 ---- 554\n",
      "676 ---- 555\n",
      "677 ---- 556\n",
      "678 ---- 557\n",
      "679 ---- 558\n",
      "680 ---- 559\n",
      "681 ---- 560\n",
      "682 ---- 561\n",
      "683 ---- 562\n",
      "684 ---- 563\n",
      "685 ---- 564\n",
      "686 ---- 565\n",
      "687 ---- 566\n",
      "688 ---- 567\n",
      "689 ---- 568\n",
      "690 ---- 569\n",
      "691 ---- 570\n",
      "692 ---- 571\n",
      "693 ---- 572\n",
      "694 ---- 573\n",
      "695 ---- 574\n",
      "696 ---- 575\n",
      "697 ---- 576\n",
      "698 ---- 577\n",
      "699 ---- 578\n",
      "700 ---- 579\n",
      "701 ---- 580\n",
      "702 ---- 581\n",
      "703 ---- 582\n",
      "704 ---- 583\n",
      "705 ---- 584\n",
      "706 ---- 585\n",
      "707 ---- 586\n",
      "708 ---- 587\n",
      "709 ---- 588\n",
      "710 ---- 589\n",
      "711 ---- 590\n",
      "712 ---- 591\n",
      "713 ---- 592\n",
      "714 ---- 593\n",
      "715 ---- 594\n",
      "716 ---- 595\n",
      "717 ---- 596\n",
      "718 ---- 597\n",
      "719 ---- 598\n",
      "720 ---- 599\n",
      "721 ---- 600\n",
      "722 ---- 601\n",
      "723 ---- 602\n",
      "724 ---- 603\n",
      "725 ---- 604\n",
      "726 ---- 605\n",
      "727 ---- 606\n",
      "728 ---- 607\n",
      "729 ---- 608\n",
      "730 ---- 609\n",
      "731 ---- 610\n",
      "732 ---- 611\n",
      "733 ---- 612\n",
      "734 ---- 613\n",
      "735 ---- 614\n",
      "736 ---- 615\n",
      "737 ---- 616\n",
      "738 ---- 617\n",
      "739 ---- 618\n",
      "740 ---- 619\n",
      "741 ---- 620\n",
      "742 ---- 621\n",
      "743 ---- 622\n",
      "744 ---- 623\n",
      "745 ---- 624\n",
      "746 ---- 625\n",
      "747 ---- 626\n",
      "748 ---- 627\n",
      "749 ---- 628\n",
      "750 ---- 629\n",
      "751 ---- 630\n",
      "752 ---- 631\n",
      "753 ---- 632\n",
      "754 ---- 633\n",
      "755 ---- 634\n",
      "756 ---- 635\n",
      "757 ---- 636\n",
      "758 ---- 637\n",
      "759 ---- 638\n",
      "760 ---- 639\n",
      "761 ---- 640\n",
      "762 ---- 641\n",
      "763 ---- 642\n",
      "764 ---- 643\n",
      "765 ---- 644\n",
      "766 ---- 645\n",
      "767 ---- 646\n",
      "768 ---- 647\n",
      "769 ---- 648\n",
      "770 ---- 649\n",
      "771 ---- 650\n",
      "772 ---- 651\n",
      "773 ---- 652\n",
      "774 ---- 653\n",
      "775 ---- 654\n",
      "776 ---- 655\n",
      "777 ---- 656\n",
      "778 ---- 657\n",
      "779 ---- 658\n",
      "780 ---- 659\n",
      "781 ---- 660\n",
      "782 ---- 661\n",
      "783 ---- 662\n",
      "784 ---- 663\n",
      "785 ---- 664\n",
      "786 ---- 665\n",
      "787 ---- 666\n",
      "788 ---- 667\n",
      "789 ---- 668\n",
      "790 ---- 669\n",
      "791 ---- 670\n",
      "792 ---- 671\n",
      "793 ---- 672\n",
      "794 ---- 673\n",
      "795 ---- 674\n",
      "796 ---- 675\n",
      "797 ---- 676\n",
      "798 ---- 677\n",
      "799 ---- 678\n",
      "800 ---- 679\n",
      "801 ---- 680\n",
      "802 ---- 681\n",
      "803 ---- 682\n",
      "804 ---- 683\n",
      "805 ---- 684\n",
      "806 ---- 685\n",
      "807 ---- 686\n",
      "808 ---- 687\n",
      "809 ---- 688\n",
      "810 ---- 689\n",
      "811 ---- 690\n",
      "812 ---- 691\n",
      "813 ---- 692\n",
      "814 ---- 693\n",
      "815 ---- 694\n",
      "816 ---- 695\n",
      "817 ---- 696\n",
      "818 ---- 697\n",
      "819 ---- 698\n",
      "820 ---- 699\n",
      "821 ---- 700\n",
      "822 ---- 701\n",
      "823 ---- 702\n",
      "824 ---- 703\n",
      "825 ---- 704\n",
      "826 ---- 705\n",
      "827 ---- 706\n",
      "828 ---- 707\n",
      "829 ---- 708\n",
      "830 ---- 709\n",
      "831 ---- 710\n",
      "832 ---- 711\n",
      "833 ---- 712\n",
      "834 ---- 713\n",
      "835 ---- 714\n",
      "836 ---- 715\n",
      "837 ---- 716\n",
      "838 ---- 717\n",
      "839 ---- 718\n",
      "840 ---- 719\n",
      "841 ---- 720\n",
      "842 ---- 721\n",
      "843 ---- 722\n",
      "844 ---- 723\n",
      "845 ---- 724\n",
      "846 ---- 725\n",
      "847 ---- 726\n",
      "848 ---- 727\n",
      "849 ---- 728\n",
      "850 ---- 729\n",
      "851 ---- 730\n",
      "852 ---- 731\n",
      "853 ---- 732\n",
      "854 ---- 733\n",
      "855 ---- 734\n",
      "856 ---- 735\n",
      "857 ---- 736\n",
      "858 ---- 737\n",
      "859 ---- 738\n",
      "860 ---- 739\n",
      "861 ---- 740\n",
      "862 ---- 741\n",
      "863 ---- 742\n",
      "864 ---- 743\n",
      "865 ---- 744\n",
      "866 ---- 745\n",
      "867 ---- 746\n",
      "868 ---- 747\n",
      "869 ---- 748\n",
      "870 ---- 749\n",
      "871 ---- 750\n",
      "872 ---- 751\n",
      "873 ---- 752\n",
      "874 ---- 753\n",
      "875 ---- 754\n",
      "876 ---- 755\n",
      "877 ---- 756\n",
      "878 ---- 757\n",
      "879 ---- 758\n",
      "880 ---- 759\n",
      "881 ---- 760\n",
      "882 ---- 761\n",
      "883 ---- 762\n",
      "884 ---- 763\n",
      "885 ---- 764\n",
      "886 ---- 765\n",
      "887 ---- 766\n",
      "888 ---- 767\n",
      "889 ---- 768\n",
      "890 ---- 769\n",
      "891 ---- 770\n",
      "892 ---- 771\n",
      "893 ---- 772\n",
      "894 ---- 773\n",
      "895 ---- 774\n",
      "896 ---- 775\n",
      "897 ---- 776\n",
      "898 ---- 777\n",
      "899 ---- 778\n",
      "900 ---- 779\n",
      "901 ---- 780\n",
      "902 ---- 781\n",
      "903 ---- 782\n",
      "904 ---- 783\n",
      "905 ---- 784\n",
      "906 ---- 785\n",
      "907 ---- 786\n",
      "908 ---- 787\n",
      "909 ---- 788\n",
      "910 ---- 789\n",
      "911 ---- 790\n",
      "912 ---- 791\n",
      "913 ---- 792\n",
      "914 ---- 793\n",
      "915 ---- 794\n",
      "916 ---- 795\n",
      "917 ---- 796\n",
      "918 ---- 797\n",
      "919 ---- 798\n",
      "920 ---- 799\n",
      "921 ---- 800\n",
      "922 ---- 801\n",
      "923 ---- 802\n",
      "924 ---- 803\n",
      "925 ---- 804\n",
      "926 ---- 805\n",
      "927 ---- 806\n",
      "928 ---- 807\n",
      "929 ---- 808\n",
      "930 ---- 809\n",
      "931 ---- 810\n",
      "932 ---- 811\n",
      "933 ---- 812\n",
      "934 ---- 813\n",
      "935 ---- 814\n",
      "936 ---- 815\n",
      "937 ---- 816\n",
      "938 ---- 817\n",
      "939 ---- 818\n",
      "940 ---- 819\n",
      "941 ---- 820\n",
      "942 ---- 821\n",
      "943 ---- 822\n",
      "944 ---- 823\n",
      "945 ---- 824\n",
      "946 ---- 825\n",
      "947 ---- 826\n",
      "948 ---- 827\n",
      "949 ---- 828\n",
      "950 ---- 829\n",
      "951 ---- 830\n",
      "952 ---- 831\n",
      "953 ---- 832\n",
      "954 ---- 833\n",
      "955 ---- 834\n",
      "956 ---- 835\n",
      "957 ---- 836\n",
      "958 ---- 837\n",
      "959 ---- 838\n",
      "960 ---- 839\n",
      "961 ---- 840\n",
      "962 ---- 841\n",
      "963 ---- 842\n",
      "964 ---- 843\n",
      "965 ---- 844\n",
      "966 ---- 845\n",
      "967 ---- 846\n",
      "968 ---- 847\n",
      "969 ---- 848\n",
      "970 ---- 849\n",
      "971 ---- 850\n",
      "972 ---- 851\n",
      "973 ---- 852\n",
      "974 ---- 853\n",
      "975 ---- 854\n",
      "976 ---- 855\n",
      "977 ---- 856\n",
      "978 ---- 857\n",
      "979 ---- 858\n",
      "980 ---- 859\n",
      "981 ---- 860\n",
      "982 ---- 861\n",
      "983 ---- 862\n",
      "984 ---- 863\n",
      "985 ---- 864\n",
      "986 ---- 865\n",
      "987 ---- 866\n",
      "988 ---- 867\n",
      "989 ---- 868\n",
      "990 ---- 869\n",
      "991 ---- 870\n",
      "992 ---- 871\n",
      "993 ---- 872\n",
      "994 ---- 873\n",
      "995 ---- 874\n",
      "996 ---- 875\n",
      "997 ---- 876\n",
      "998 ---- 877\n",
      "999 ---- 878\n",
      "1000 ---- 879\n",
      "1001 ---- 880\n",
      "1002 ---- 881\n",
      "1003 ---- 882\n",
      "1004 ---- 883\n",
      "1005 ---- 884\n",
      "1006 ---- 885\n",
      "1007 ---- 886\n",
      "1008 ---- 887\n",
      "1009 ---- 888\n",
      "1010 ---- 889\n",
      "1011 ---- 890\n",
      "1012 ---- 891\n",
      "1013 ---- 892\n",
      "1014 ---- 893\n",
      "1015 ---- 894\n",
      "1016 ---- 895\n",
      "1017 ---- 896\n",
      "1018 ---- 897\n",
      "1019 ---- 898\n",
      "1020 ---- 899\n",
      "1021 ---- 900\n",
      "1022 ---- 901\n",
      "1023 ---- 902\n",
      "1024 ---- 903\n",
      "1025 ---- 904\n",
      "1026 ---- 905\n",
      "1027 ---- 906\n",
      "1028 ---- 907\n",
      "1029 ---- 908\n",
      "1030 ---- 909\n",
      "1031 ---- 910\n",
      "1032 ---- 911\n",
      "1033 ---- 912\n",
      "1034 ---- 913\n",
      "1035 ---- 914\n",
      "1036 ---- 915\n",
      "1037 ---- 916\n",
      "1038 ---- 917\n",
      "1039 ---- 918\n",
      "1040 ---- 919\n",
      "1041 ---- 920\n",
      "1042 ---- 921\n",
      "1043 ---- 922\n",
      "1044 ---- 923\n",
      "1045 ---- 924\n",
      "1046 ---- 925\n",
      "1047 ---- 926\n",
      "1048 ---- 927\n",
      "1049 ---- 928\n",
      "1050 ---- 929\n",
      "1051 ---- 930\n",
      "1052 ---- 931\n",
      "1053 ---- 932\n",
      "1054 ---- 933\n",
      "1055 ---- 934\n",
      "1056 ---- 935\n",
      "1057 ---- 936\n",
      "1058 ---- 937\n",
      "1059 ---- 938\n",
      "1060 ---- 939\n",
      "1061 ---- 940\n",
      "1062 ---- 941\n",
      "1063 ---- 942\n",
      "1064 ---- 943\n",
      "1065 ---- 944\n",
      "1066 ---- 945\n",
      "1067 ---- 946\n",
      "1068 ---- 947\n",
      "1069 ---- 948\n",
      "1070 ---- 949\n",
      "1071 ---- 950\n",
      "1072 ---- 951\n",
      "1073 ---- 952\n",
      "1074 ---- 953\n",
      "1075 ---- 954\n",
      "1076 ---- 955\n",
      "1077 ---- 956\n",
      "1078 ---- 957\n",
      "1079 ---- 958\n",
      "1080 ---- 959\n",
      "1081 ---- 960\n",
      "1082 ---- 961\n",
      "1083 ---- 962\n",
      "1084 ---- 963\n",
      "1085 ---- 964\n",
      "1086 ---- 965\n",
      "1087 ---- 966\n",
      "1088 ---- 967\n",
      "1089 ---- 968\n",
      "1090 ---- 969\n",
      "1091 ---- 970\n",
      "1092 ---- 971\n",
      "1093 ---- 972\n",
      "1094 ---- 973\n",
      "1095 ---- 974\n",
      "1096 ---- 975\n",
      "1097 ---- 976\n",
      "1098 ---- 977\n",
      "1099 ---- 978\n",
      "1100 ---- 979\n",
      "1101 ---- 980\n",
      "1102 ---- 981\n",
      "1103 ---- 982\n",
      "1104 ---- 983\n",
      "1105 ---- 984\n",
      "1106 ---- 985\n",
      "1107 ---- 986\n",
      "1108 ---- 987\n",
      "1109 ---- 988\n",
      "1110 ---- 989\n",
      "1111 ---- 990\n",
      "1112 ---- 991\n",
      "1113 ---- 992\n",
      "1114 ---- 993\n",
      "1115 ---- 994\n",
      "1116 ---- 995\n",
      "1117 ---- 996\n",
      "1118 ---- 997\n",
      "1119 ---- 998\n",
      "1120 ---- 999\n",
      "1121 ---- 1000\n",
      "1122 ---- 1001\n",
      "1123 ---- 1002\n",
      "1124 ---- 1003\n",
      "1125 ---- 1004\n",
      "1126 ---- 1005\n",
      "1127 ---- 1006\n",
      "1128 ---- 1007\n",
      "1129 ---- 1008\n",
      "1130 ---- 1009\n",
      "1131 ---- 1010\n",
      "1132 ---- 1011\n",
      "1133 ---- 1012\n",
      "1134 ---- 1013\n",
      "1135 ---- 1014\n",
      "1136 ---- 1015\n",
      "1137 ---- 1016\n",
      "1138 ---- 1017\n",
      "1139 ---- 1018\n",
      "1140 ---- 1019\n",
      "1141 ---- 1020\n",
      "1142 ---- 1021\n",
      "1143 ---- 1022\n",
      "1144 ---- 1023\n",
      "1145 ---- 1024\n",
      "1146 ---- 1025\n",
      "1147 ---- 1026\n",
      "1148 ---- 1027\n",
      "1149 ---- 1028\n",
      "1150 ---- 1029\n",
      "1151 ---- 1030\n",
      "1152 ---- 1031\n",
      "1153 ---- 1032\n",
      "1154 ---- 1033\n",
      "1155 ---- 1034\n",
      "1156 ---- 1035\n",
      "1157 ---- 1036\n",
      "1158 ---- 1037\n",
      "1159 ---- 1038\n",
      "1160 ---- 1039\n",
      "1161 ---- 1040\n",
      "1162 ---- 1041\n",
      "1163 ---- 1042\n",
      "1164 ---- 1043\n",
      "1165 ---- 1044\n",
      "1166 ---- 1045\n",
      "1167 ---- 1046\n",
      "1168 ---- 1047\n",
      "1169 ---- 1048\n",
      "1170 ---- 1049\n",
      "1171 ---- 1050\n",
      "1172 ---- 1051\n",
      "1173 ---- 1052\n",
      "1174 ---- 1053\n",
      "1175 ---- 1054\n",
      "1176 ---- 1055\n",
      "1177 ---- 1056\n",
      "1178 ---- 1057\n",
      "1179 ---- 1058\n",
      "1180 ---- 1059\n",
      "1181 ---- 1060\n",
      "1182 ---- 1061\n",
      "1183 ---- 1062\n",
      "1184 ---- 1063\n",
      "1185 ---- 1064\n",
      "1186 ---- 1065\n",
      "1187 ---- 1066\n",
      "1188 ---- 1067\n",
      "1189 ---- 1068\n",
      "1190 ---- 1069\n",
      "1191 ---- 1070\n",
      "1192 ---- 1071\n",
      "1193 ---- 1072\n",
      "1194 ---- 1073\n",
      "1195 ---- 1074\n",
      "1196 ---- 1075\n",
      "1197 ---- 1076\n",
      "1198 ---- 1077\n",
      "1199 ---- 1078\n",
      "1200 ---- 1079\n",
      "1201 ---- 1080\n",
      "1202 ---- 1081\n",
      "1203 ---- 1082\n",
      "1204 ---- 1083\n",
      "1205 ---- 1084\n",
      "1206 ---- 1085\n",
      "1207 ---- 1086\n",
      "1208 ---- 1087\n",
      "1209 ---- 1088\n",
      "1210 ---- 1089\n",
      "1211 ---- 1090\n",
      "1212 ---- 1091\n",
      "1213 ---- 1092\n",
      "1214 ---- 1093\n",
      "1215 ---- 1094\n",
      "1216 ---- 1095\n",
      "1217 ---- 1096\n",
      "1218 ---- 1097\n",
      "1219 ---- 1098\n",
      "1220 ---- 1099\n",
      "1221 ---- 1100\n",
      "1222 ---- 1101\n",
      "1223 ---- 1102\n",
      "1224 ---- 1103\n",
      "1225 ---- 1104\n",
      "1226 ---- 1105\n",
      "1227 ---- 1106\n",
      "1228 ---- 1107\n",
      "1229 ---- 1108\n",
      "1230 ---- 1109\n",
      "1231 ---- 1110\n",
      "1232 ---- 1111\n",
      "1233 ---- 1112\n",
      "1234 ---- 1113\n",
      "1235 ---- 1114\n",
      "1236 ---- 1115\n",
      "1237 ---- 1116\n",
      "1238 ---- 1117\n",
      "1239 ---- 1118\n",
      "1240 ---- 1119\n",
      "1241 ---- 1120\n",
      "1242 ---- 1121\n",
      "1243 ---- 1122\n",
      "1244 ---- 1123\n",
      "1245 ---- 1124\n",
      "1246 ---- 1125\n",
      "1247 ---- 1126\n",
      "1248 ---- 1127\n",
      "1249 ---- 1128\n",
      "1250 ---- 1129\n",
      "1251 ---- 1130\n",
      "1252 ---- 1131\n",
      "1253 ---- 1132\n",
      "1254 ---- 1133\n",
      "1255 ---- 1134\n",
      "1256 ---- 1135\n",
      "1257 ---- 1136\n",
      "1258 ---- 1137\n",
      "1259 ---- 1138\n",
      "1260 ---- 1139\n",
      "1261 ---- 1140\n",
      "1262 ---- 1141\n",
      "1263 ---- 1142\n",
      "1264 ---- 1143\n",
      "1265 ---- 1144\n",
      "1266 ---- 1145\n",
      "1267 ---- 1146\n",
      "1268 ---- 1147\n",
      "1269 ---- 1148\n",
      "1270 ---- 1149\n",
      "1271 ---- 1150\n",
      "1272 ---- 1151\n",
      "1273 ---- 1152\n",
      "1274 ---- 1153\n",
      "1275 ---- 1154\n",
      "1276 ---- 1155\n",
      "1277 ---- 1156\n",
      "1278 ---- 1157\n",
      "1279 ---- 1158\n",
      "1280 ---- 1159\n",
      "1281 ---- 1160\n",
      "1282 ---- 1161\n",
      "1283 ---- 1162\n",
      "1284 ---- 1163\n",
      "1285 ---- 1164\n",
      "1286 ---- 1165\n",
      "1287 ---- 1166\n",
      "1288 ---- 1167\n",
      "1289 ---- 1168\n",
      "1290 ---- 1169\n",
      "1291 ---- 1170\n",
      "1292 ---- 1171\n",
      "1293 ---- 1172\n",
      "1294 ---- 1173\n",
      "1295 ---- 1174\n",
      "1296 ---- 1175\n",
      "1297 ---- 1176\n",
      "1298 ---- 1177\n",
      "1299 ---- 1178\n",
      "1300 ---- 1179\n",
      "1301 ---- 1180\n",
      "1302 ---- 1181\n",
      "1303 ---- 1182\n",
      "1304 ---- 1183\n",
      "1305 ---- 1184\n",
      "1306 ---- 1185\n",
      "1307 ---- 1186\n",
      "1308 ---- 1187\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(X_train_final.columns)):\n",
    "    print('{} ---- {}'.format(i, X_train_final.columns[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Model 6: Without STEM Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_6 = X_train_final.iloc[:,np.r_[10:1308]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1609, 1298)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_6 = X_test_final.iloc[:,np.r_[10:1308]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 1298)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.741\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.79      0.78        90\n",
      "           1       0.78      0.78      0.78        89\n",
      "\n",
      "    accuracy                           0.78       179\n",
      "   macro avg       0.78      0.78      0.78       179\n",
      "weighted avg       0.78      0.78      0.78       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_6_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_6_pipeline, parameters, scoring=\"average_precision\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_6,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_6), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 71, False Positive: 19, False Negative: 20, True Positive: 69\n",
      "--------------------------------------------------------------------------------\n",
      "[[71 19]\n",
      " [20 69]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.79      0.78        90\n",
      "           1       0.78      0.78      0.78        89\n",
      "\n",
      "    accuracy                           0.78       179\n",
      "   macro avg       0.78      0.78      0.78       179\n",
      "weighted avg       0.78      0.78      0.78       179\n",
      "\n",
      "Average Precision: 0.7196\n"
     ]
    }
   ],
   "source": [
    "lr_model_6 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_6, y_train)\n",
    "y_lr = lr_model_6.predict(X_test_model_6)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))\n",
    "\n",
    "# Calculate and print the average precision score\n",
    "avg_precision = average_precision_score(y_test, y_lr)\n",
    "print(f'Average Precision: {avg_precision:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF Model 4: Without Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [10,20,50,100,200,300]\n",
    "max_depth = [2,5,8,10,15,20]\n",
    "min_samples_split = [2, 5, 10, 15, 20]\n",
    "min_samples_leaf = [1, 2, 5, 10,20]\n",
    "rf_parameters = dict(n_estimators = n_estimators, max_depth = max_depth,  \n",
    "              min_samples_split = min_samples_split, \n",
    "              min_samples_leaf = min_samples_leaf)\n",
    "\n",
    "## reduced parameters\n",
    "# n_estimators = [50, 100, 200]       # Reduced options\n",
    "# max_depth = [5, 10, 15]             # Reduced options\n",
    "# min_samples_split = [2, 10]         # Reduced options\n",
    "# min_samples_leaf = [1, 5]           # Reduced options\n",
    "\n",
    "# rf_parameters = dict(\n",
    "#     n_estimators = n_estimators, \n",
    "#     max_depth = max_depth,  \n",
    "#     min_samples_split = min_samples_split, \n",
    "#     min_samples_leaf = min_samples_leaf\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_4 = X_train_final.iloc[:,np.r_[3:21,121:1308]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1609, 1205)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_4 = X_test_final.iloc[:,np.r_[3:21,121:1308]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 1205)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 900 candidates, totalling 9000 fits\n",
      "Best score: 0.736\n",
      "Best parameters set:\n",
      "\tmax_depth: 20\n",
      "\tmin_samples_leaf: 2\n",
      "\tmin_samples_split: 2\n",
      "\tn_estimators: 200\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.79      0.79        90\n",
      "           1       0.79      0.79      0.79        89\n",
      "\n",
      "    accuracy                           0.79       179\n",
      "   macro avg       0.79      0.79      0.79       179\n",
      "weighted avg       0.79      0.79      0.79       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_model_4 = RandomForestClassifier(random_state=18,class_weight='balanced')\n",
    "grid_search = GridSearchCV(rf_model_4, rf_parameters, scoring=\"average_precision\", cv = 10, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_model_4,y_train)\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(rf_parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_4), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 71, False Positive: 19, False Negative: 19, True Positive: 70\n",
      "--------------------------------------------------------------------------------\n",
      "[[71 19]\n",
      " [19 70]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.79      0.79        90\n",
      "           1       0.79      0.79      0.79        89\n",
      "\n",
      "    accuracy                           0.79       179\n",
      "   macro avg       0.79      0.79      0.79       179\n",
      "weighted avg       0.79      0.79      0.79       179\n",
      "\n",
      "Average Precision: 0.7248\n"
     ]
    }
   ],
   "source": [
    "randomForest_4 = RandomForestClassifier(random_state=18,\n",
    "                                        class_weight=best_parameters['class_weight'],\n",
    "                                        max_depth=best_parameters['max_depth'],\n",
    "                                        min_samples_leaf=best_parameters['min_samples_leaf'],\n",
    "                                        min_samples_split=best_parameters['min_samples_split'],\n",
    "                                        n_estimators=best_parameters['n_estimators']).fit(X_train_model_4, y_train)\n",
    "\n",
    "y_lr = randomForest_4.predict(X_test_model_4)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))\n",
    "\n",
    "# Calculate and print the average precision score\n",
    "avg_precision = average_precision_score(y_test, y_lr)\n",
    "print(f'Average Precision: {avg_precision:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
