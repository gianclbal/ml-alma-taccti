{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resistance Logistic Regression and Random Forest Models Using Merged Data Experiment 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350a3e5a33734292bc55755da4fa6684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 09:13:51 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-10-16 09:13:51 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-10-16 09:13:53 INFO: File exists: /Users/gbaldonado/stanza_resources/en/default.zip\n",
      "2024-10-16 09:13:56 INFO: Finished downloading models and saved to /Users/gbaldonado/stanza_resources\n",
      "2024-10-16 09:13:56 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8a0b939e274a5992c993e0a5d9a9cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 09:13:56 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-10-16 09:13:58 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-10-16 09:13:58 INFO: Using device: cpu\n",
      "2024-10-16 09:13:58 INFO: Loading: tokenize\n",
      "2024-10-16 09:13:59 INFO: Loading: mwt\n",
      "2024-10-16 09:13:59 INFO: Loading: pos\n",
      "2024-10-16 09:13:59 INFO: Loading: lemma\n",
      "2024-10-16 09:13:59 INFO: Loading: constituency\n",
      "2024-10-16 09:13:59 INFO: Loading: depparse\n",
      "2024-10-16 09:14:00 INFO: Loading: sentiment\n",
      "2024-10-16 09:14:00 INFO: Loading: ner\n",
      "2024-10-16 09:14:00 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import pickle\n",
    "import warnings\n",
    "import stanza\n",
    "\n",
    "from random import shuffle\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, f1_score, r2_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Set random seed\n",
    "random.seed(18)\n",
    "seed = 18\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Initialize lemmatizer, stop words, and stanza\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline('en') # initialize English neural pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data and quick exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and test sets loaded.\n"
     ]
    }
   ],
   "source": [
    "merged_resistance_df_batch_1 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/merged_Resistance_sentence_level_batch_1_jaccard.csv\", encoding='utf-8')\n",
    "merged_resistance_df_batch_2 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/Resistance Plus_sentence_level_batch_2_jaccard.csv\", encoding='utf-8')\n",
    "\n",
    "merged_resistance_df = pd.concat([merged_resistance_df_batch_1, merged_resistance_df_batch_2])\n",
    "\n",
    "\n",
    "seed = 18\n",
    "# Shuffle the merged dataset\n",
    "merged_resistance_df = shuffle(merged_resistance_df, random_state=seed)\n",
    "\n",
    "# Function for undersampling or oversampling\n",
    "def resample_data(X, y, strategy='oversample', random_state=seed):\n",
    "    \"\"\"\n",
    "    Resample the data using either undersampling or oversampling.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Features\n",
    "    - y: Labels\n",
    "    - strategy: 'oversample' or 'undersample'\n",
    "    - random_state: Seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    - X_resampled, y_resampled: Resampled data and labels\n",
    "    \"\"\"\n",
    "    if strategy == 'oversample':\n",
    "        sampler = RandomOverSampler(random_state=random_state)\n",
    "    elif strategy == 'undersample':\n",
    "        sampler = RandomUnderSampler(random_state=random_state)\n",
    "    else:\n",
    "        raise ValueError(\"Strategy must be 'oversample' or 'undersample'\")\n",
    "\n",
    "    X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Separate features and labels\n",
    "X = merged_resistance_df.drop(columns=['label'])  # Replace 'label' with your target column name\n",
    "y = merged_resistance_df['label']\n",
    "\n",
    "# Toggle resampling\n",
    "resample = False  # Set this to False to turn off resampling\n",
    "\n",
    "if resample:\n",
    "    # Apply resampling (choose 'oversample' or 'undersample')\n",
    "    X_resampled, y_resampled = resample_data(X, y, strategy='oversample', random_state=seed)\n",
    "\n",
    "    # Combine resampled data into a single DataFrame\n",
    "    resampled_df = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "else:\n",
    "    # No resampling, use original dataset\n",
    "    resampled_df = merged_resistance_df\n",
    "\n",
    "# Train-test split\n",
    "training_df, test_df = train_test_split(resampled_df, test_size=0.1, random_state=18, stratify=resampled_df['label'])\n",
    "\n",
    "\n",
    "# # Train-test split\n",
    "training_df, test_df = train_test_split(resampled_df, test_size=0.1, random_state=18, stratify=resampled_df['label'])\n",
    "\n",
    "training_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Training and test sets loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (1978, 3) \n",
      "Test dataset shape: (220, 3)\n",
      "Positive labels present in the dataset : 118  out of 1978 or 5.96562184024267%\n",
      "Positive labels present in the test dataset : 13  out of 220 or 5.909090909090909%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset shape: {training_df.shape} \\nTest dataset shape: {test_df.shape}\")\n",
    "pos_labels = len([n for n in training_df['label'] if n==1])\n",
    "print(\"Positive labels present in the dataset : {}  out of {} or {}%\".format(pos_labels, len(training_df['label']), (pos_labels/len(training_df['label']))*100))\n",
    "pos_labels = len([n for n in test_df['label'] if n==1])\n",
    "print(\"Positive labels present in the test dataset : {}  out of {} or {}%\".format(pos_labels, len(test_df['label']), (pos_labels/len(test_df['label']))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the data for the plots\n",
    "training_counts = training_df['label'].value_counts()\n",
    "test_counts = test_df['label'].value_counts()\n",
    "\n",
    "# Set up the subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot for the training set\n",
    "sns.barplot(x=training_counts.index, y=training_counts.values, ax=axes[0])\n",
    "axes[0].set_title('Distribution of labels in training set')\n",
    "axes[0].set_ylabel('Sentences')\n",
    "axes[0].set_xlabel('Label')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot for the test set\n",
    "sns.barplot(x=test_counts.index, y=test_counts.values, ax=axes[1])\n",
    "axes[1].set_title('Distribution of labels in test set')\n",
    "axes[1].set_ylabel('Sentences')\n",
    "axes[1].set_xlabel('Label')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack Obama']\n"
     ]
    }
   ],
   "source": [
    "def get_ner(text):\n",
    "    ner_list = []\n",
    "    # Annotate the text using stanza\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for entity in sentence.ents:\n",
    "            if entity.type == 'PERSON':\n",
    "                ner_list.append(entity.text)\n",
    "\n",
    "    return ner_list\n",
    "\n",
    "# Example usage\n",
    "text = \"Barack Obama was the 44th doctor of the United States.\"\n",
    "print(get_ner(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a named entity is present in the sentence\n",
    "def named_entity_present(sentence):\n",
    "    ner_list = get_ner(sentence)\n",
    "    if len(ner_list) > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Similarity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the similar words and similarity score\n",
    "# The function takes tokens of sentence as input and if its not a stop word, get its similarity with synsets of STEM.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words |= set([\"help\",\"try\", \"work\", \"process\", \"support\", \"job\"] )\n",
    "def word_similarity(tokens, syns, field):    \n",
    "    if field in ['engineering', 'technology']:\n",
    "        score_threshold = 0.5\n",
    "    else:\n",
    "        score_threshold = 0.2\n",
    "    sim_words = 0\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            try:\n",
    "                syns_word = wordnet.synsets(token) \n",
    "                score = syns_word[0].path_similarity(syns[0])\n",
    "                if score >= score_threshold:\n",
    "                    sim_words += 1\n",
    "            except: \n",
    "                score = 0\n",
    "    \n",
    "    return sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to create columns for similarity based on all STEM fields\n",
    "syns_bio = wordnet.synsets(lemmatizer.lemmatize(\"biology\"))\n",
    "syns_maths = wordnet.synsets(lemmatizer.lemmatize(\"mathematics\")) \n",
    "syns_tech = wordnet.synsets(lemmatizer.lemmatize(\"technology\"))\n",
    "syns_eng = wordnet.synsets(lemmatizer.lemmatize(\"engineering\"))\n",
    "syns_chem = wordnet.synsets(lemmatizer.lemmatize(\"chemistry\"))\n",
    "syns_phy = wordnet.synsets(lemmatizer.lemmatize(\"physics\"))\n",
    "syns_sci = wordnet.synsets(lemmatizer.lemmatize(\"science\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Medical Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['psychiatry', 'oncology', 'sports', 'endocrinology', 'surgical', 'surgery', 'aerospace', 'physical', 'chemical', 'pediatrics', 'military', 'forensic', 'molecular', 'advanced', 'heart', 'neurourology', 'urologic', 'neurology', 'neuropathology', 'banking', 'transplant', 'procedural', 'neuro', 'radiology', 'disabilities', 'glaucoma', 'consultation', 'pathology', 'interventional', 'reconstructive', 'vascular', 'preventive', 'infectious', 'genitourinary', 'endovascular', 'oculoplastics', 'adolescent', 'medical', 'failure', 'transplant', 'pediatrics', 'gynecologic', 'pulmonology', 'cytogenetics', 'emergency', 'male', 'psychiatric', 'administrative', 'dermatology', 'anesthesiology', 'female', 'genetic', 'cardiology', 'ophthalmic', 'infectious', 'palliative', 'injury', 'family', 'gastroenterology', 'neonatal', 'radiation', 'behavioral', 'cardiothoracic', 'diabetes', 'obstetrics', 'electrophysiology', 'fetal', 'anesthesiology', 'reproductive', 'anatomical', 'developmental', 'plastic', 'cytopathology', 'brain', 'pulmonary', 'anterior', 'reconstructive', 'orbit', 'internal', 'calculi', 'psychiatry', 'strabismus', 'pathology', 'neuroradiology', 'genetic', 'public', 'endocrinologists', 'abuse', 'pelvic', 'rheumatology', 'urology', 'head', 'gastrointestinal', 'maternal', 'health', 'nuclear', 'dermatology', 'transfusion', 'nephrology', 'retina', 'sleep', 'medicine', 'child', 'abdominal', 'uveitis', 'segment', 'geriatric', 'oncology', 'microbiology', 'gynecology', 'critical', 'surgery', 'musculoskeletal', 'pediatric', 'cardiovascular', 'endocrinology', 'neuromuscular', 'diseases', 'neurology', 'ophthalmology', 'addiction', 'perinatal', 'nephrology', 'occupational', 'clinical', 'neurodevelopmental', 'rehabilitation', 'neuroradiology', 'interventional', 'disease', 'neurophysiology', 'renal', 'biochemical', 'sports', 'community', 'retardation', 'hospice', 'pediatric', 'allergy', 'infertility', 'internal', 'rheumatology', 'hepatology', 'immunopathology', 'gastroenterology', 'imaging', 'metabolism', 'dermatopathology', 'urology', 'toxicology', 'diagnostic', 'chest', 'research', 'breast', 'hematology', 'blood', 'hematology', 'genetics', 'liaison', 'and', 'mental', 'psychosomatic', 'pain', 'immunology', 'cornea', 'care', 'ophthalmology', 'critical', 'cardiac', 'ocular', 'neck', 'adolescent']\n"
     ]
    }
   ],
   "source": [
    "# Load the medical specialization text file and create a list\n",
    "medical_list = []\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/medical_specialities.txt', 'r') as medical_fields:\n",
    "    for line in medical_fields.readlines():\n",
    "        special_field = line.rstrip('\\n')\n",
    "        special_field = re.sub(\"\\W\",\" \", special_field )\n",
    "#         print(special_field)\n",
    "        medical_list += special_field.split()\n",
    "medical_list = list(set(medical_list))  \n",
    "medical_list = [x.lower() for x in medical_list]\n",
    "print(medical_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get medical words\n",
    "def check_medical_words(tokens):\n",
    "    for token in tokens:\n",
    "        if token not in stop_words and token in [x.lower() for x in medical_list]:\n",
    "            return 1\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sentiment Polarity and Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get polarity and subjectivity of the sentence using TexBlob\n",
    "def get_sentiment(sentence):\n",
    "    sentiments =TextBlob(sentence).sentiment\n",
    "    polarity = sentiments.polarity\n",
    "    subjectivity = sentiments.subjectivity\n",
    "    return polarity, subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the count of POS tags of the sentence\n",
    "def count_pos_tags(tokens):\n",
    "    token_pos = pos_tag(tokens)\n",
    "    count = Counter(tag for word,tag in token_pos)\n",
    "    interjections =  count['UH']\n",
    "    nouns = count['NN'] + count['NNS'] + count['NNP'] + count['NNPS']\n",
    "    adverb = count['RB'] + count['RBS'] + count['RBR']\n",
    "    verb = count['VB'] + count['VBD'] + count['VBG'] + count['VBN']\n",
    "    determiner = count['DT']\n",
    "    pronoun = count['PRP']\n",
    "    adjetive = count['JJ'] + count['JJR'] + count['JJS']\n",
    "    preposition = count['IN']\n",
    "    return interjections, nouns, adverb, verb, determiner, pronoun, adjetive,preposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_extraction(dataframe, field, func, column_names):\n",
    "    return pd.concat((\n",
    "        dataframe,\n",
    "        dataframe[field].apply(\n",
    "            lambda cell: pd.Series(func(cell), index=column_names))), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the w2v dict from pickle file\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/pickle/embeddings06122024.pickle', 'rb') as w2v_file:\n",
    "    w2v_dict = pickle.load(w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of word embeddings:  4762\n"
     ]
    }
   ],
   "source": [
    "print(\"length of word embeddings: \", len(w2v_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vectors for the essay\n",
    "def vectorizer(sequence):\n",
    "    vect = []\n",
    "    numw = 0\n",
    "    for w in sequence: \n",
    "        try :\n",
    "            if numw == 0:\n",
    "                vect = w2v_dict[w]\n",
    "            else:\n",
    "                vect = np.add(vect, w2v_dict[w])\n",
    "            numw += 1\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    return vect/ numw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into words\n",
    "def split_into_words(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vectorizer\n",
    "unigram_vect = CountVectorizer(ngram_range=(1, 1), min_df=2, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Putting them all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for feature engineering\n",
    "def feature_engineering(original_dataset):\n",
    "\n",
    "    dataset = original_dataset.copy()\n",
    "    # create a new column with sentence tokens\n",
    "    dataset['tokens'] = dataset['sentence'].apply(word_tokenize)\n",
    "    # 1. Similarity features\n",
    "    # biology\n",
    "    dataset['bio_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_bio,'biology',)) \n",
    "    # chemistry\n",
    "    dataset['chem_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_chem,'chemistry',))\n",
    "    # physics\n",
    "    dataset['phy_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_phy,'physics',))\n",
    "    # mathematics\n",
    "    dataset['math_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_maths,'mathematics',))\n",
    "    # technology\n",
    "    dataset['tech_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_tech,'technology',))\n",
    "    # engineering\n",
    "    dataset['eng_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_eng,'engineering',))\n",
    "    \n",
    "    # medical terms\n",
    "    dataset['medical_terms'] = dataset['tokens'].apply(check_medical_words)\n",
    "    \n",
    "    # polarity and subjectivity\n",
    "    dataset['polarity'], dataset['subjectivity'] = zip(*dataset['sentence'].apply(get_sentiment))\n",
    "    \n",
    "    # named entity recognition\n",
    "    dataset['ner'] = dataset['sentence'].apply(named_entity_present)\n",
    "    \n",
    "    # pos tag count\n",
    "    dataset = pos_tag_extraction(dataset, 'tokens', count_pos_tags, ['interjections', 'nouns', 'adverb', 'verb', 'determiner', 'pronoun', 'adjetive','preposition'])\n",
    "    \n",
    "    # labels\n",
    "    data_labels = dataset['label']\n",
    "    # X\n",
    "    data_x = dataset.drop(columns='label')\n",
    "\n",
    "    \n",
    "    # vectorize all the essays\n",
    "    vect_arr = data_x.tokens.apply(vectorizer)\n",
    "    for index in range(0, len(vect_arr)):\n",
    "        i = 0\n",
    "        for item in vect_arr[index]:\n",
    "            column_name= \"embedding\" + str(i)\n",
    "            data_x.loc[index, column_name] = item\n",
    "            i +=1\n",
    "    \n",
    "    return data_x,data_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = feature_engineering(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235, 121)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = feature_engineering(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 121)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Unigram features for both train and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235, 121)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 121)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2.1/Resistance/saved_features/X_train_final.csv\", index=False)\n",
    "X_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2.1/Resistance/saved_features/X_test_final.csv\", index=False)\n",
    "y_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2.1/Resistance/saved_features/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_2.1/Resistance/saved_features/y_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the unigram df for train :  (235, 342)\n"
     ]
    }
   ],
   "source": [
    "# Unigrams for training set\n",
    "unigram_matrix = unigram_vect.fit_transform(X_train['sentence'])\n",
    "unigrams = pd.DataFrame(unigram_matrix.toarray())\n",
    "print(\"Shape of the unigram df for train : \",unigrams.shape)\n",
    "unigrams = unigrams.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = pd.concat([X_train, unigrams], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final.columns = X_train_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235, 463)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test unigram df shape :  (27, 342)\n"
     ]
    }
   ],
   "source": [
    "unigram_matrix_test = unigram_vect.transform(X_test['sentence'])\n",
    "unigrams_test = pd.DataFrame(unigram_matrix_test.toarray())\n",
    "unigrams_test = unigrams_test.reset_index(drop=True)\n",
    "print(\"Test unigram df shape : \",unigrams_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 463)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final = pd.concat([X_test, unigrams_test], axis = 1)\n",
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final.columns = X_test_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 463)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ---- sentence\n",
      "1 ---- phrase\n",
      "2 ---- tokens\n",
      "3 ---- bio_sim_words\n",
      "4 ---- chem_sim_words\n",
      "5 ---- phy_sim_words\n",
      "6 ---- math_sim_words\n",
      "7 ---- tech_sim_words\n",
      "8 ---- eng_sim_words\n",
      "9 ---- medical_terms\n",
      "10 ---- polarity\n",
      "11 ---- subjectivity\n",
      "12 ---- ner\n",
      "13 ---- interjections\n",
      "14 ---- nouns\n",
      "15 ---- adverb\n",
      "16 ---- verb\n",
      "17 ---- determiner\n",
      "18 ---- pronoun\n",
      "19 ---- adjetive\n",
      "20 ---- preposition\n",
      "21 ---- embedding0\n",
      "22 ---- embedding1\n",
      "23 ---- embedding2\n",
      "24 ---- embedding3\n",
      "25 ---- embedding4\n",
      "26 ---- embedding5\n",
      "27 ---- embedding6\n",
      "28 ---- embedding7\n",
      "29 ---- embedding8\n",
      "30 ---- embedding9\n",
      "31 ---- embedding10\n",
      "32 ---- embedding11\n",
      "33 ---- embedding12\n",
      "34 ---- embedding13\n",
      "35 ---- embedding14\n",
      "36 ---- embedding15\n",
      "37 ---- embedding16\n",
      "38 ---- embedding17\n",
      "39 ---- embedding18\n",
      "40 ---- embedding19\n",
      "41 ---- embedding20\n",
      "42 ---- embedding21\n",
      "43 ---- embedding22\n",
      "44 ---- embedding23\n",
      "45 ---- embedding24\n",
      "46 ---- embedding25\n",
      "47 ---- embedding26\n",
      "48 ---- embedding27\n",
      "49 ---- embedding28\n",
      "50 ---- embedding29\n",
      "51 ---- embedding30\n",
      "52 ---- embedding31\n",
      "53 ---- embedding32\n",
      "54 ---- embedding33\n",
      "55 ---- embedding34\n",
      "56 ---- embedding35\n",
      "57 ---- embedding36\n",
      "58 ---- embedding37\n",
      "59 ---- embedding38\n",
      "60 ---- embedding39\n",
      "61 ---- embedding40\n",
      "62 ---- embedding41\n",
      "63 ---- embedding42\n",
      "64 ---- embedding43\n",
      "65 ---- embedding44\n",
      "66 ---- embedding45\n",
      "67 ---- embedding46\n",
      "68 ---- embedding47\n",
      "69 ---- embedding48\n",
      "70 ---- embedding49\n",
      "71 ---- embedding50\n",
      "72 ---- embedding51\n",
      "73 ---- embedding52\n",
      "74 ---- embedding53\n",
      "75 ---- embedding54\n",
      "76 ---- embedding55\n",
      "77 ---- embedding56\n",
      "78 ---- embedding57\n",
      "79 ---- embedding58\n",
      "80 ---- embedding59\n",
      "81 ---- embedding60\n",
      "82 ---- embedding61\n",
      "83 ---- embedding62\n",
      "84 ---- embedding63\n",
      "85 ---- embedding64\n",
      "86 ---- embedding65\n",
      "87 ---- embedding66\n",
      "88 ---- embedding67\n",
      "89 ---- embedding68\n",
      "90 ---- embedding69\n",
      "91 ---- embedding70\n",
      "92 ---- embedding71\n",
      "93 ---- embedding72\n",
      "94 ---- embedding73\n",
      "95 ---- embedding74\n",
      "96 ---- embedding75\n",
      "97 ---- embedding76\n",
      "98 ---- embedding77\n",
      "99 ---- embedding78\n",
      "100 ---- embedding79\n",
      "101 ---- embedding80\n",
      "102 ---- embedding81\n",
      "103 ---- embedding82\n",
      "104 ---- embedding83\n",
      "105 ---- embedding84\n",
      "106 ---- embedding85\n",
      "107 ---- embedding86\n",
      "108 ---- embedding87\n",
      "109 ---- embedding88\n",
      "110 ---- embedding89\n",
      "111 ---- embedding90\n",
      "112 ---- embedding91\n",
      "113 ---- embedding92\n",
      "114 ---- embedding93\n",
      "115 ---- embedding94\n",
      "116 ---- embedding95\n",
      "117 ---- embedding96\n",
      "118 ---- embedding97\n",
      "119 ---- embedding98\n",
      "120 ---- embedding99\n",
      "121 ---- 0\n",
      "122 ---- 1\n",
      "123 ---- 2\n",
      "124 ---- 3\n",
      "125 ---- 4\n",
      "126 ---- 5\n",
      "127 ---- 6\n",
      "128 ---- 7\n",
      "129 ---- 8\n",
      "130 ---- 9\n",
      "131 ---- 10\n",
      "132 ---- 11\n",
      "133 ---- 12\n",
      "134 ---- 13\n",
      "135 ---- 14\n",
      "136 ---- 15\n",
      "137 ---- 16\n",
      "138 ---- 17\n",
      "139 ---- 18\n",
      "140 ---- 19\n",
      "141 ---- 20\n",
      "142 ---- 21\n",
      "143 ---- 22\n",
      "144 ---- 23\n",
      "145 ---- 24\n",
      "146 ---- 25\n",
      "147 ---- 26\n",
      "148 ---- 27\n",
      "149 ---- 28\n",
      "150 ---- 29\n",
      "151 ---- 30\n",
      "152 ---- 31\n",
      "153 ---- 32\n",
      "154 ---- 33\n",
      "155 ---- 34\n",
      "156 ---- 35\n",
      "157 ---- 36\n",
      "158 ---- 37\n",
      "159 ---- 38\n",
      "160 ---- 39\n",
      "161 ---- 40\n",
      "162 ---- 41\n",
      "163 ---- 42\n",
      "164 ---- 43\n",
      "165 ---- 44\n",
      "166 ---- 45\n",
      "167 ---- 46\n",
      "168 ---- 47\n",
      "169 ---- 48\n",
      "170 ---- 49\n",
      "171 ---- 50\n",
      "172 ---- 51\n",
      "173 ---- 52\n",
      "174 ---- 53\n",
      "175 ---- 54\n",
      "176 ---- 55\n",
      "177 ---- 56\n",
      "178 ---- 57\n",
      "179 ---- 58\n",
      "180 ---- 59\n",
      "181 ---- 60\n",
      "182 ---- 61\n",
      "183 ---- 62\n",
      "184 ---- 63\n",
      "185 ---- 64\n",
      "186 ---- 65\n",
      "187 ---- 66\n",
      "188 ---- 67\n",
      "189 ---- 68\n",
      "190 ---- 69\n",
      "191 ---- 70\n",
      "192 ---- 71\n",
      "193 ---- 72\n",
      "194 ---- 73\n",
      "195 ---- 74\n",
      "196 ---- 75\n",
      "197 ---- 76\n",
      "198 ---- 77\n",
      "199 ---- 78\n",
      "200 ---- 79\n",
      "201 ---- 80\n",
      "202 ---- 81\n",
      "203 ---- 82\n",
      "204 ---- 83\n",
      "205 ---- 84\n",
      "206 ---- 85\n",
      "207 ---- 86\n",
      "208 ---- 87\n",
      "209 ---- 88\n",
      "210 ---- 89\n",
      "211 ---- 90\n",
      "212 ---- 91\n",
      "213 ---- 92\n",
      "214 ---- 93\n",
      "215 ---- 94\n",
      "216 ---- 95\n",
      "217 ---- 96\n",
      "218 ---- 97\n",
      "219 ---- 98\n",
      "220 ---- 99\n",
      "221 ---- 100\n",
      "222 ---- 101\n",
      "223 ---- 102\n",
      "224 ---- 103\n",
      "225 ---- 104\n",
      "226 ---- 105\n",
      "227 ---- 106\n",
      "228 ---- 107\n",
      "229 ---- 108\n",
      "230 ---- 109\n",
      "231 ---- 110\n",
      "232 ---- 111\n",
      "233 ---- 112\n",
      "234 ---- 113\n",
      "235 ---- 114\n",
      "236 ---- 115\n",
      "237 ---- 116\n",
      "238 ---- 117\n",
      "239 ---- 118\n",
      "240 ---- 119\n",
      "241 ---- 120\n",
      "242 ---- 121\n",
      "243 ---- 122\n",
      "244 ---- 123\n",
      "245 ---- 124\n",
      "246 ---- 125\n",
      "247 ---- 126\n",
      "248 ---- 127\n",
      "249 ---- 128\n",
      "250 ---- 129\n",
      "251 ---- 130\n",
      "252 ---- 131\n",
      "253 ---- 132\n",
      "254 ---- 133\n",
      "255 ---- 134\n",
      "256 ---- 135\n",
      "257 ---- 136\n",
      "258 ---- 137\n",
      "259 ---- 138\n",
      "260 ---- 139\n",
      "261 ---- 140\n",
      "262 ---- 141\n",
      "263 ---- 142\n",
      "264 ---- 143\n",
      "265 ---- 144\n",
      "266 ---- 145\n",
      "267 ---- 146\n",
      "268 ---- 147\n",
      "269 ---- 148\n",
      "270 ---- 149\n",
      "271 ---- 150\n",
      "272 ---- 151\n",
      "273 ---- 152\n",
      "274 ---- 153\n",
      "275 ---- 154\n",
      "276 ---- 155\n",
      "277 ---- 156\n",
      "278 ---- 157\n",
      "279 ---- 158\n",
      "280 ---- 159\n",
      "281 ---- 160\n",
      "282 ---- 161\n",
      "283 ---- 162\n",
      "284 ---- 163\n",
      "285 ---- 164\n",
      "286 ---- 165\n",
      "287 ---- 166\n",
      "288 ---- 167\n",
      "289 ---- 168\n",
      "290 ---- 169\n",
      "291 ---- 170\n",
      "292 ---- 171\n",
      "293 ---- 172\n",
      "294 ---- 173\n",
      "295 ---- 174\n",
      "296 ---- 175\n",
      "297 ---- 176\n",
      "298 ---- 177\n",
      "299 ---- 178\n",
      "300 ---- 179\n",
      "301 ---- 180\n",
      "302 ---- 181\n",
      "303 ---- 182\n",
      "304 ---- 183\n",
      "305 ---- 184\n",
      "306 ---- 185\n",
      "307 ---- 186\n",
      "308 ---- 187\n",
      "309 ---- 188\n",
      "310 ---- 189\n",
      "311 ---- 190\n",
      "312 ---- 191\n",
      "313 ---- 192\n",
      "314 ---- 193\n",
      "315 ---- 194\n",
      "316 ---- 195\n",
      "317 ---- 196\n",
      "318 ---- 197\n",
      "319 ---- 198\n",
      "320 ---- 199\n",
      "321 ---- 200\n",
      "322 ---- 201\n",
      "323 ---- 202\n",
      "324 ---- 203\n",
      "325 ---- 204\n",
      "326 ---- 205\n",
      "327 ---- 206\n",
      "328 ---- 207\n",
      "329 ---- 208\n",
      "330 ---- 209\n",
      "331 ---- 210\n",
      "332 ---- 211\n",
      "333 ---- 212\n",
      "334 ---- 213\n",
      "335 ---- 214\n",
      "336 ---- 215\n",
      "337 ---- 216\n",
      "338 ---- 217\n",
      "339 ---- 218\n",
      "340 ---- 219\n",
      "341 ---- 220\n",
      "342 ---- 221\n",
      "343 ---- 222\n",
      "344 ---- 223\n",
      "345 ---- 224\n",
      "346 ---- 225\n",
      "347 ---- 226\n",
      "348 ---- 227\n",
      "349 ---- 228\n",
      "350 ---- 229\n",
      "351 ---- 230\n",
      "352 ---- 231\n",
      "353 ---- 232\n",
      "354 ---- 233\n",
      "355 ---- 234\n",
      "356 ---- 235\n",
      "357 ---- 236\n",
      "358 ---- 237\n",
      "359 ---- 238\n",
      "360 ---- 239\n",
      "361 ---- 240\n",
      "362 ---- 241\n",
      "363 ---- 242\n",
      "364 ---- 243\n",
      "365 ---- 244\n",
      "366 ---- 245\n",
      "367 ---- 246\n",
      "368 ---- 247\n",
      "369 ---- 248\n",
      "370 ---- 249\n",
      "371 ---- 250\n",
      "372 ---- 251\n",
      "373 ---- 252\n",
      "374 ---- 253\n",
      "375 ---- 254\n",
      "376 ---- 255\n",
      "377 ---- 256\n",
      "378 ---- 257\n",
      "379 ---- 258\n",
      "380 ---- 259\n",
      "381 ---- 260\n",
      "382 ---- 261\n",
      "383 ---- 262\n",
      "384 ---- 263\n",
      "385 ---- 264\n",
      "386 ---- 265\n",
      "387 ---- 266\n",
      "388 ---- 267\n",
      "389 ---- 268\n",
      "390 ---- 269\n",
      "391 ---- 270\n",
      "392 ---- 271\n",
      "393 ---- 272\n",
      "394 ---- 273\n",
      "395 ---- 274\n",
      "396 ---- 275\n",
      "397 ---- 276\n",
      "398 ---- 277\n",
      "399 ---- 278\n",
      "400 ---- 279\n",
      "401 ---- 280\n",
      "402 ---- 281\n",
      "403 ---- 282\n",
      "404 ---- 283\n",
      "405 ---- 284\n",
      "406 ---- 285\n",
      "407 ---- 286\n",
      "408 ---- 287\n",
      "409 ---- 288\n",
      "410 ---- 289\n",
      "411 ---- 290\n",
      "412 ---- 291\n",
      "413 ---- 292\n",
      "414 ---- 293\n",
      "415 ---- 294\n",
      "416 ---- 295\n",
      "417 ---- 296\n",
      "418 ---- 297\n",
      "419 ---- 298\n",
      "420 ---- 299\n",
      "421 ---- 300\n",
      "422 ---- 301\n",
      "423 ---- 302\n",
      "424 ---- 303\n",
      "425 ---- 304\n",
      "426 ---- 305\n",
      "427 ---- 306\n",
      "428 ---- 307\n",
      "429 ---- 308\n",
      "430 ---- 309\n",
      "431 ---- 310\n",
      "432 ---- 311\n",
      "433 ---- 312\n",
      "434 ---- 313\n",
      "435 ---- 314\n",
      "436 ---- 315\n",
      "437 ---- 316\n",
      "438 ---- 317\n",
      "439 ---- 318\n",
      "440 ---- 319\n",
      "441 ---- 320\n",
      "442 ---- 321\n",
      "443 ---- 322\n",
      "444 ---- 323\n",
      "445 ---- 324\n",
      "446 ---- 325\n",
      "447 ---- 326\n",
      "448 ---- 327\n",
      "449 ---- 328\n",
      "450 ---- 329\n",
      "451 ---- 330\n",
      "452 ---- 331\n",
      "453 ---- 332\n",
      "454 ---- 333\n",
      "455 ---- 334\n",
      "456 ---- 335\n",
      "457 ---- 336\n",
      "458 ---- 337\n",
      "459 ---- 338\n",
      "460 ---- 339\n",
      "461 ---- 340\n",
      "462 ---- 341\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(X_train_final.columns)):\n",
    "    print('{} ---- {}'.format(i, X_train_final.columns[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Model 5: Without POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_5 = X_train_final.iloc[:,np.r_[3:13,21:341]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235, 330)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_5 = X_test_final.iloc[:,np.r_[3:13,21:341]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 330)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.761\n",
      "Best parameters set:\n",
      "\tclf__C: 1\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50        14\n",
      "           1       0.46      0.46      0.46        13\n",
      "\n",
      "    accuracy                           0.48        27\n",
      "   macro avg       0.48      0.48      0.48        27\n",
      "weighted avg       0.48      0.48      0.48        27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_5_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_5_pipeline, parameters, scoring=\"average_precision\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_5,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_5), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 7, False Positive: 7, False Negative: 7, True Positive: 6\n",
      "--------------------------------------------------------------------------------\n",
      "[[7 7]\n",
      " [7 6]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50        14\n",
      "           1       0.46      0.46      0.46        13\n",
      "\n",
      "    accuracy                           0.48        27\n",
      "   macro avg       0.48      0.48      0.48        27\n",
      "weighted avg       0.48      0.48      0.48        27\n",
      "\n",
      "Average Precision: 0.4723\n"
     ]
    }
   ],
   "source": [
    "lr_model_5 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_5, y_train)\n",
    "y_lr = lr_model_5.predict(X_test_model_5)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))\n",
    "\n",
    "# Calculate and print the average precision score\n",
    "avg_precision = average_precision_score(y_test, y_lr)\n",
    "print(f'Average Precision: {avg_precision:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF Model 5: Without POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [10,20,50,100,200,300]\n",
    "max_depth = [2,5,8,10,15,20]\n",
    "min_samples_split = [2, 5, 10, 15, 20]\n",
    "min_samples_leaf = [1, 2, 5, 10,20]\n",
    "rf_parameters = dict(n_estimators = n_estimators, max_depth = max_depth,  \n",
    "              min_samples_split = min_samples_split, \n",
    "              min_samples_leaf = min_samples_leaf)\n",
    "\n",
    "## reduced parameters\n",
    "# n_estimators = [50, 100, 200]       # Reduced options\n",
    "# max_depth = [5, 10, 15]             # Reduced options\n",
    "# min_samples_split = [2, 10]         # Reduced options\n",
    "# min_samples_leaf = [1, 5]           # Reduced options\n",
    "\n",
    "# rf_parameters = dict(\n",
    "#     n_estimators = n_estimators, \n",
    "#     max_depth = max_depth,  \n",
    "#     min_samples_split = min_samples_split, \n",
    "#     min_samples_leaf = min_samples_leaf\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_5 = X_train_final.iloc[:,np.r_[3:13,21:341]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235, 330)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_5 = X_test_final.iloc[:,np.r_[3:13,21:341]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 330)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_5.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Without POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_5 = X_train_final.iloc[:,np.r_[3:13,21:341]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235, 330)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_5 = X_test_final.iloc[:,np.r_[3:13,21:341]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 330)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 900 candidates, totalling 9000 fits\n",
      "Best score: 0.709\n",
      "Best parameters set:\n",
      "\tmax_depth: 8\n",
      "\tmin_samples_leaf: 10\n",
      "\tmin_samples_split: 2\n",
      "\tn_estimators: 20\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.71      0.80        14\n",
      "           1       0.75      0.92      0.83        13\n",
      "\n",
      "    accuracy                           0.81        27\n",
      "   macro avg       0.83      0.82      0.81        27\n",
      "weighted avg       0.83      0.81      0.81        27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_model_5 = RandomForestClassifier(random_state=18,class_weight='balanced')\n",
    "grid_search = GridSearchCV(rf_model_5, rf_parameters, scoring=\"average_precision\", cv = 10, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_model_5,y_train)\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(rf_parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_5), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 10, False Positive: 4, False Negative: 1, True Positive: 12\n",
      "--------------------------------------------------------------------------------\n",
      "[[10  4]\n",
      " [ 1 12]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.71      0.80        14\n",
      "           1       0.75      0.92      0.83        13\n",
      "\n",
      "    accuracy                           0.81        27\n",
      "   macro avg       0.83      0.82      0.81        27\n",
      "weighted avg       0.83      0.81      0.81        27\n",
      "\n",
      "Average Precision: 0.7293\n"
     ]
    }
   ],
   "source": [
    "randomForest_5 = RandomForestClassifier(random_state=18,\n",
    "                                        class_weight=best_parameters['class_weight'],\n",
    "                                        max_depth=best_parameters['max_depth'],\n",
    "                                        min_samples_leaf=best_parameters['min_samples_leaf'],\n",
    "                                        min_samples_split=best_parameters['min_samples_split'],\n",
    "                                        n_estimators=best_parameters['n_estimators']).fit(X_train_model_5, y_train)\n",
    "\n",
    "y_lr = randomForest_5.predict(X_test_model_5)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))\n",
    "\n",
    "# Calculate and print the average precision score\n",
    "avg_precision = average_precision_score(y_test, y_lr)\n",
    "print(f'Average Precision: {avg_precision:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
