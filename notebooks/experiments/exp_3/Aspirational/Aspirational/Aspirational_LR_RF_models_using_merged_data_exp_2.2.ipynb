{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aspirational Logistic Regression and Random Forest Models Using Merged Data Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b26d971aeb43b580ec90a56eea721e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 20:25:20 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-11-05 20:25:20 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-11-05 20:25:21 INFO: File exists: /Users/gbaldonado/stanza_resources/en/default.zip\n",
      "2024-11-05 20:25:24 INFO: Finished downloading models and saved to /Users/gbaldonado/stanza_resources\n",
      "2024-11-05 20:25:24 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d2767dc10742f9b97e9321003dff78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 20:25:24 INFO: Downloaded file to /Users/gbaldonado/stanza_resources/resources.json\n",
      "2024-11-05 20:25:25 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-11-05 20:25:25 INFO: Using device: cpu\n",
      "2024-11-05 20:25:25 INFO: Loading: tokenize\n",
      "2024-11-05 20:25:25 INFO: Loading: mwt\n",
      "2024-11-05 20:25:25 INFO: Loading: pos\n",
      "2024-11-05 20:25:25 INFO: Loading: lemma\n",
      "2024-11-05 20:25:26 INFO: Loading: constituency\n",
      "2024-11-05 20:25:26 INFO: Loading: depparse\n",
      "2024-11-05 20:25:26 INFO: Loading: sentiment\n",
      "2024-11-05 20:25:26 INFO: Loading: ner\n",
      "2024-11-05 20:25:27 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import pickle\n",
    "import warnings\n",
    "import stanza\n",
    "\n",
    "from random import shuffle\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, f1_score, r2_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Set random seed\n",
    "random.seed(18)\n",
    "seed = 18\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Initialize lemmatizer, stop words, and stanza\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline('en') # initialize English neural pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data and quick exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and test sets loaded.\n"
     ]
    }
   ],
   "source": [
    "merged_aspirational_df_batch_1 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/merged_Aspirational_sentence_level_batch_1_jaccard.csv\", encoding='utf-8')\n",
    "merged_aspirational_df_batch_2 = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/processed_for_model/merged_themes_using_jaccard_method/Aspirational Plus_sentence_level_batch_2_jaccard.csv\", encoding='utf-8')\n",
    "\n",
    "merged_aspirational_df = pd.concat([merged_aspirational_df_batch_1, merged_aspirational_df_batch_2])\n",
    "\n",
    "# Shuffle the merged dataset\n",
    "merged_aspirational_df = shuffle(merged_aspirational_df, random_state=seed)\n",
    "\n",
    "\n",
    "# Function for undersampling or oversampling\n",
    "def resample_data(X, y, strategy='oversample', random_state=seed):\n",
    "    \"\"\"\n",
    "    Resample the data using either undersampling or oversampling.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Features\n",
    "    - y: Labels\n",
    "    - strategy: 'oversample' or 'undersample'\n",
    "    - random_state: Seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    - X_resampled, y_resampled: Resampled data and labels\n",
    "    \"\"\"\n",
    "    if strategy == 'oversample':\n",
    "        sampler = RandomOverSampler(random_state=random_state)\n",
    "    elif strategy == 'undersample':\n",
    "        sampler = RandomUnderSampler(random_state=random_state)\n",
    "    else:\n",
    "        raise ValueError(\"Strategy must be 'oversample' or 'undersample'\")\n",
    "\n",
    "    X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Separate features and labels\n",
    "X = merged_aspirational_df.drop(columns=['label'])  # Replace 'label' with your target column name\n",
    "y = merged_aspirational_df['label']\n",
    "\n",
    "# Toggle resampling\n",
    "resample = False  # Set this to False to turn off resampling\n",
    "\n",
    "if resample:\n",
    "    # Apply resampling (choose 'oversample' or 'undersample')\n",
    "    X_resampled, y_resampled = resample_data(X, y, strategy='oversample', random_state=seed)\n",
    "\n",
    "    # Combine resampled data into a single DataFrame\n",
    "    resampled_df = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "else:\n",
    "    # No resampling, use original dataset\n",
    "    resampled_df = merged_aspirational_df\n",
    "\n",
    "# Train-test split\n",
    "training_df, test_df = train_test_split(resampled_df, test_size=0.1, random_state=18, stratify=resampled_df['label'])\n",
    "\n",
    "\n",
    "# # Train-test split\n",
    "training_df, test_df = train_test_split(resampled_df, test_size=0.1, random_state=18, stratify=resampled_df['label'])\n",
    "\n",
    "training_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Training and test sets loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (8856, 3) \n",
      "Test dataset shape: (985, 3)\n",
      "Positive labels present in the dataset : 805  out of 8856 or 9.08988256549232%\n",
      "Positive labels present in the test dataset : 89  out of 985 or 9.035532994923857%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training dataset shape: {training_df.shape} \\nTest dataset shape: {test_df.shape}\")\n",
    "pos_labels = len([n for n in training_df['label'] if n==1])\n",
    "print(\"Positive labels present in the dataset : {}  out of {} or {}%\".format(pos_labels, len(training_df['label']), (pos_labels/len(training_df['label']))*100))\n",
    "pos_labels = len([n for n in test_df['label'] if n==1])\n",
    "print(\"Positive labels present in the test dataset : {}  out of {} or {}%\".format(pos_labels, len(test_df['label']), (pos_labels/len(test_df['label']))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the data for the plots\n",
    "training_counts = training_df['label'].value_counts()\n",
    "test_counts = test_df['label'].value_counts()\n",
    "\n",
    "# Set up the subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot for the training set\n",
    "sns.barplot(x=training_counts.index, y=training_counts.values, ax=axes[0])\n",
    "axes[0].set_title('Distribution of labels in training set')\n",
    "axes[0].set_ylabel('Sentences')\n",
    "axes[0].set_xlabel('Label')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot for the test set\n",
    "sns.barplot(x=test_counts.index, y=test_counts.values, ax=axes[1])\n",
    "axes[1].set_title('Distribution of labels in test set')\n",
    "axes[1].set_ylabel('Sentences')\n",
    "axes[1].set_xlabel('Label')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack Obama']\n"
     ]
    }
   ],
   "source": [
    "def get_ner(text):\n",
    "    ner_list = []\n",
    "    # Annotate the text using stanza\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for entity in sentence.ents:\n",
    "            if entity.type == 'PERSON':\n",
    "                ner_list.append(entity.text)\n",
    "\n",
    "    return ner_list\n",
    "\n",
    "# Example usage\n",
    "text = \"Barack Obama was the 44th doctor of the United States.\"\n",
    "print(get_ner(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a named entity is present in the sentence\n",
    "def named_entity_present(sentence):\n",
    "    ner_list = get_ner(sentence)\n",
    "    if len(ner_list) > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Similarity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the similar words and similarity score\n",
    "# The function takes tokens of sentence as input and if its not a stop word, get its similarity with synsets of STEM.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words |= set([\"help\",\"try\", \"work\", \"process\", \"support\", \"job\"] )\n",
    "def word_similarity(tokens, syns, field):    \n",
    "    if field in ['engineering', 'technology']:\n",
    "        score_threshold = 0.5\n",
    "    else:\n",
    "        score_threshold = 0.2\n",
    "    sim_words = 0\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            try:\n",
    "                syns_word = wordnet.synsets(token) \n",
    "                score = syns_word[0].path_similarity(syns[0])\n",
    "                if score >= score_threshold:\n",
    "                    sim_words += 1\n",
    "            except: \n",
    "                score = 0\n",
    "    \n",
    "    return sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to create columns for similarity based on all STEM fields\n",
    "syns_bio = wordnet.synsets(lemmatizer.lemmatize(\"biology\"))\n",
    "syns_maths = wordnet.synsets(lemmatizer.lemmatize(\"mathematics\")) \n",
    "syns_tech = wordnet.synsets(lemmatizer.lemmatize(\"technology\"))\n",
    "syns_eng = wordnet.synsets(lemmatizer.lemmatize(\"engineering\"))\n",
    "syns_chem = wordnet.synsets(lemmatizer.lemmatize(\"chemistry\"))\n",
    "syns_phy = wordnet.synsets(lemmatizer.lemmatize(\"physics\"))\n",
    "syns_sci = wordnet.synsets(lemmatizer.lemmatize(\"science\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Medical Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['calculi', 'infertility', 'cardiovascular', 'neurology', 'clinical', 'allergy', 'radiology', 'psychiatry', 'reconstructive', 'abuse', 'cardiac', 'disease', 'sports', 'psychosomatic', 'pulmonary', 'procedural', 'reconstructive', 'metabolism', 'blood', 'glaucoma', 'hepatology', 'research', 'cytogenetics', 'diagnostic', 'radiation', 'genetic', 'anesthesiology', 'critical', 'pain', 'transplant', 'heart', 'endocrinology', 'administrative', 'imaging', 'behavioral', 'dermatology', 'endocrinologists', 'ophthalmic', 'abdominal', 'neurourology', 'pathology', 'brain', 'immunology', 'pathology', 'nephrology', 'endovascular', 'neuroradiology', 'medicine', 'retina', 'rehabilitation', 'anterior', 'male', 'and', 'chemical', 'physical', 'sports', 'strabismus', 'adolescent', 'urologic', 'urology', 'palliative', 'surgery', 'segment', 'infectious', 'pediatrics', 'plastic', 'toxicology', 'psychiatry', 'pediatric', 'molecular', 'liaison', 'pediatrics', 'breast', 'sleep', 'musculoskeletal', 'pelvic', 'critical', 'orbit', 'surgery', 'oncology', 'perinatal', 'family', 'hematology', 'gastroenterology', 'fetal', 'public', 'cytopathology', 'vascular', 'infectious', 'gastrointestinal', 'uveitis', 'pediatric', 'banking', 'internal', 'renal', 'transplant', 'child', 'rheumatology', 'community', 'consultation', 'interventional', 'dermatopathology', 'anesthesiology', 'nephrology', 'gynecology', 'hematology', 'diabetes', 'gastroenterology', 'neuro', 'emergency', 'health', 'neurology', 'cornea', 'psychiatric', 'failure', 'anatomical', 'immunopathology', 'neuromuscular', 'transfusion', 'developmental', 'chest', 'oculoplastics', 'neck', 'microbiology', 'endocrinology', 'neurodevelopmental', 'forensic', 'nuclear', 'cardiothoracic', 'hospice', 'retardation', 'head', 'female', 'ophthalmology', 'advanced', 'addiction', 'ophthalmology', 'injury', 'cardiology', 'aerospace', 'obstetrics', 'mental', 'neonatal', 'military', 'disabilities', 'genetics', 'neuroradiology', 'interventional', 'oncology', 'geriatric', 'adolescent', 'care', 'genitourinary', 'urology', 'ocular', 'biochemical', 'preventive', 'diseases', 'reproductive', 'neuropathology', 'neurophysiology', 'dermatology', 'gynecologic', 'internal', 'maternal', 'pulmonology', 'genetic', 'rheumatology', 'medical', 'surgical', 'electrophysiology', 'occupational']\n"
     ]
    }
   ],
   "source": [
    "# Load the medical specialization text file and create a list\n",
    "medical_list = []\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/medical_specialities.txt', 'r') as medical_fields:\n",
    "    for line in medical_fields.readlines():\n",
    "        special_field = line.rstrip('\\n')\n",
    "        special_field = re.sub(\"\\W\",\" \", special_field )\n",
    "#         print(special_field)\n",
    "        medical_list += special_field.split()\n",
    "medical_list = list(set(medical_list))  \n",
    "medical_list = [x.lower() for x in medical_list]\n",
    "print(medical_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get medical words\n",
    "def check_medical_words(tokens):\n",
    "    for token in tokens:\n",
    "        if token not in stop_words and token in [x.lower() for x in medical_list]:\n",
    "            return 1\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sentiment Polarity and Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get polarity and subjectivity of the sentence using TexBlob\n",
    "def get_sentiment(sentence):\n",
    "    sentiments =TextBlob(sentence).sentiment\n",
    "    polarity = sentiments.polarity\n",
    "    subjectivity = sentiments.subjectivity\n",
    "    return polarity, subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. POS Tag Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get the count of POS tags of the sentence\n",
    "def count_pos_tags(tokens):\n",
    "    token_pos = pos_tag(tokens)\n",
    "    count = Counter(tag for word,tag in token_pos)\n",
    "    interjections =  count['UH']\n",
    "    nouns = count['NN'] + count['NNS'] + count['NNP'] + count['NNPS']\n",
    "    adverb = count['RB'] + count['RBS'] + count['RBR']\n",
    "    verb = count['VB'] + count['VBD'] + count['VBG'] + count['VBN']\n",
    "    determiner = count['DT']\n",
    "    pronoun = count['PRP']\n",
    "    adjetive = count['JJ'] + count['JJR'] + count['JJS']\n",
    "    preposition = count['IN']\n",
    "    return interjections, nouns, adverb, verb, determiner, pronoun, adjetive,preposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_extraction(dataframe, field, func, column_names):\n",
    "    return pd.concat((\n",
    "        dataframe,\n",
    "        dataframe[field].apply(\n",
    "            lambda cell: pd.Series(func(cell), index=column_names))), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the w2v dict from pickle file\n",
    "with open('/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/data/features/pickle/embeddings06122024.pickle', 'rb') as w2v_file:\n",
    "    w2v_dict = pickle.load(w2v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of word embeddings:  4762\n"
     ]
    }
   ],
   "source": [
    "print(\"length of word embeddings: \", len(w2v_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(sequence):\n",
    "    vect = None  # Initialize as None to handle the first word\n",
    "    numw = 0  # Counter for valid words in the sequence\n",
    "\n",
    "    for w in sequence:\n",
    "        try:\n",
    "            if w in w2v_dict:  # Check if the word exists in the dictionary\n",
    "                if vect is None:\n",
    "                    vect = np.array(w2v_dict[w])  # Start with the first valid word vector\n",
    "                else:\n",
    "                    vect = np.add(vect, w2v_dict[w])  # Add subsequent word vectors\n",
    "                numw += 1\n",
    "        except Exception as e:\n",
    "            pass  # Silently handle words not in the dictionary\n",
    "\n",
    "    # Safeguard for empty sequences or missing words\n",
    "    if vect is None or numw == 0:\n",
    "        return np.zeros(10)  # Return a zero vector with the expected dimensionality\n",
    "\n",
    "    return vect / numw  # Return the averaged vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into words\n",
    "def split_into_words(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vectorizer\n",
    "unigram_vect = CountVectorizer(ngram_range=(1, 1), min_df=2, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Putting them all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for feature engineering\n",
    "def feature_engineering(dataset):\n",
    "    # create a new column with sentence tokens\n",
    "    dataset['tokens'] = dataset['sentence'].apply(word_tokenize)\n",
    "    # # 1. Similarity features\n",
    "    # biology\n",
    "    dataset['bio_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_bio,'biology',)) \n",
    "    # chemistry\n",
    "    dataset['chem_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_chem,'chemistry',))\n",
    "    # physics\n",
    "    dataset['phy_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_phy,'physics',))\n",
    "    # mathematics\n",
    "    dataset['math_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_maths,'mathematics',))\n",
    "    # technology\n",
    "    dataset['tech_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_tech,'technology',))\n",
    "    # engineering\n",
    "    dataset['eng_sim_words'] = dataset['tokens'].apply(word_similarity, args=(syns_eng,'engineering',))\n",
    "    \n",
    "    # medical terms\n",
    "    dataset['medical_terms'] = dataset['tokens'].apply(check_medical_words)\n",
    "    \n",
    "    # polarity and subjectivity\n",
    "    dataset['polarity'], dataset['subjectivity'] = zip(*dataset['sentence'].apply(get_sentiment))\n",
    "    \n",
    "    # named entity recognition\n",
    "    dataset['ner'] = dataset['sentence'].apply(named_entity_present)\n",
    "    \n",
    "    # pos tag count\n",
    "    dataset = pos_tag_extraction(dataset, 'tokens', count_pos_tags, ['interjections', 'nouns', 'adverb', 'verb', 'determiner', 'pronoun', 'adjetive','preposition'])\n",
    "    \n",
    "    # labels\n",
    "    data_labels = dataset['label']\n",
    "    \n",
    "    # X\n",
    "    data_x = dataset.drop(columns='label')\n",
    "    \n",
    "    # vectorize all the essays\n",
    "    vect_arr = data_x.tokens.apply(vectorizer)\n",
    "    for index in range(0, len(vect_arr)):\n",
    "        i = 0\n",
    "        for item in vect_arr[index]:\n",
    "            column_name= \"embedding\" + str(i)\n",
    "            data_x.loc[index, column_name] = item\n",
    "            i +=1\n",
    "    \n",
    "    \n",
    "    return data_x,data_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = feature_engineering(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = feature_engineering(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(985, 121)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Unigram features for both train and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8856, 121)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>phrase</th>\n",
       "      <th>tokens</th>\n",
       "      <th>bio_sim_words</th>\n",
       "      <th>chem_sim_words</th>\n",
       "      <th>phy_sim_words</th>\n",
       "      <th>math_sim_words</th>\n",
       "      <th>tech_sim_words</th>\n",
       "      <th>eng_sim_words</th>\n",
       "      <th>medical_terms</th>\n",
       "      <th>...</th>\n",
       "      <th>embedding90</th>\n",
       "      <th>embedding91</th>\n",
       "      <th>embedding92</th>\n",
       "      <th>embedding93</th>\n",
       "      <th>embedding94</th>\n",
       "      <th>embedding95</th>\n",
       "      <th>embedding96</th>\n",
       "      <th>embedding97</th>\n",
       "      <th>embedding98</th>\n",
       "      <th>embedding99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>while i am not the best when it come to the realm of physics, i know this class will help me gain better insight on scientific applications and improve my skill set in mathematical computation and analysis.</td>\n",
       "      <td>['As a Kinesiology major, the main reason I am taking this class is that it is a prerequisite for Physical Therapy school.']</td>\n",
       "      <td>[while, i, am, not, the, best, when, it, come, to, the, realm, of, physics, ,, i, know, this, class, will, help, me, gain, better, insight, on, scientific, applications, and, improve, my, skill, set, in, mathematical, computation, and, analysis, .]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.388465</td>\n",
       "      <td>0.175436</td>\n",
       "      <td>-0.016878</td>\n",
       "      <td>0.028132</td>\n",
       "      <td>0.240545</td>\n",
       "      <td>-0.032458</td>\n",
       "      <td>0.301723</td>\n",
       "      <td>-0.092855</td>\n",
       "      <td>-0.012566</td>\n",
       "      <td>0.077127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it is for sure going to help with my critical thinking and problem solving, whether it is for math or solving a real life problem.</td>\n",
       "      <td>['I am hoping that I am able to do well and succeed in this class so I can take the second part of physics.']</td>\n",
       "      <td>[it, is, for, sure, going, to, help, with, my, critical, thinking, and, problem, solving, ,, whether, it, is, for, math, or, solving, a, real, life, problem, .]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430366</td>\n",
       "      <td>0.139534</td>\n",
       "      <td>-0.071063</td>\n",
       "      <td>0.056950</td>\n",
       "      <td>0.236367</td>\n",
       "      <td>-0.062232</td>\n",
       "      <td>0.340420</td>\n",
       "      <td>-0.131404</td>\n",
       "      <td>-0.035167</td>\n",
       "      <td>0.023667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i just hope i can pull it off.</td>\n",
       "      <td>['Why Am I HereI am here to gain the skills I need to pass the Physics portion of the MCAT and enter Medical School. My goal is to become a surgeon, either Cardio or General Surgery. When I do apply for medical school, I also plan to earn an associates degree in surgical technology, so that even if I dont get in, Ill have something to fall back on, and add to my application for when I reapply.']</td>\n",
       "      <td>[i, just, hope, i, can, pull, it, off, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423034</td>\n",
       "      <td>0.177543</td>\n",
       "      <td>-0.039713</td>\n",
       "      <td>0.021027</td>\n",
       "      <td>0.280373</td>\n",
       "      <td>0.006403</td>\n",
       "      <td>0.218204</td>\n",
       "      <td>-0.054102</td>\n",
       "      <td>-0.010557</td>\n",
       "      <td>0.081190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>so a big piece of this is to find independence by gaining an engineering degree and then once i have gained that independence then can work towards addressing these many issues.</td>\n",
       "      <td>['So a big piece of this is to find independence by gaining an engineering degree and then once I have gained that independence then can work towards addressing these many issues. I think that the Engineers without Borders program looks like a really cool avenue to work in once I make it to the level of a practicing engineer.']</td>\n",
       "      <td>[so, a, big, piece, of, this, is, to, find, independence, by, gaining, an, engineering, degree, and, then, once, i, have, gained, that, independence, then, can, work, towards, addressing, these, many, issues, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.398168</td>\n",
       "      <td>0.166028</td>\n",
       "      <td>-0.015805</td>\n",
       "      <td>0.036184</td>\n",
       "      <td>0.227133</td>\n",
       "      <td>-0.051953</td>\n",
       "      <td>0.317481</td>\n",
       "      <td>-0.105692</td>\n",
       "      <td>-0.013951</td>\n",
       "      <td>0.083509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>maybe this class will also help in my life in general, to do work harder and work with a group better.</td>\n",
       "      <td>[\"I'm here because I'm afraid of failing physics and I'm hoping this class will help me succeed in passing it.\"]</td>\n",
       "      <td>[maybe, this, class, will, also, help, in, my, life, in, general, ,, to, do, work, harder, and, work, with, a, group, better, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425861</td>\n",
       "      <td>0.164463</td>\n",
       "      <td>-0.002573</td>\n",
       "      <td>0.018269</td>\n",
       "      <td>0.235089</td>\n",
       "      <td>-0.070152</td>\n",
       "      <td>0.378019</td>\n",
       "      <td>-0.092525</td>\n",
       "      <td>-0.020376</td>\n",
       "      <td>0.100528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>i want to be able to help my family out with paying bills and stuff.</td>\n",
       "      <td>['I want to get my degree get a good paying job and be financially stable for my future.', 'I want to show the world my own ideas and make it into a film. I also am here to be smart and finish my degree.']</td>\n",
       "      <td>[i, want, to, be, able, to, help, my, family, out, with, paying, bills, and, stuff, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.369852</td>\n",
       "      <td>0.143160</td>\n",
       "      <td>0.016275</td>\n",
       "      <td>0.034761</td>\n",
       "      <td>0.154880</td>\n",
       "      <td>-0.105908</td>\n",
       "      <td>0.286506</td>\n",
       "      <td>-0.114393</td>\n",
       "      <td>-0.073470</td>\n",
       "      <td>0.167038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>i would like to learn how to manage my time better, so that i have sufficient time to complete my lab.</td>\n",
       "      <td>['I am here in lab class to get a better understanding of my physics lecture classes.I would like to learn how to manage my time better, so that I have sufficient time to complete my lab. I want to be able to look forward to a lab when I dont understand the lecture. I hope that this lab class will further my knowledge and interest in future physics courses.']</td>\n",
       "      <td>[i, would, like, to, learn, how, to, manage, my, time, better, ,, so, that, i, have, sufficient, time, to, complete, my, lab, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.457951</td>\n",
       "      <td>0.176199</td>\n",
       "      <td>-0.018036</td>\n",
       "      <td>0.024497</td>\n",
       "      <td>0.257529</td>\n",
       "      <td>-0.027664</td>\n",
       "      <td>0.259761</td>\n",
       "      <td>-0.098766</td>\n",
       "      <td>-0.050169</td>\n",
       "      <td>0.110715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>also, ive heard that chem 215 is more challenging so i thought that taking this sci course would be helpful.</td>\n",
       "      <td>['In a broader, more general sense, I am here at SF State because I want to get a degree and get a good job.']</td>\n",
       "      <td>[also, ,, ive, heard, that, chem, 215, is, more, challenging, so, i, thought, that, taking, this, sci, course, would, be, helpful, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.477649</td>\n",
       "      <td>0.157186</td>\n",
       "      <td>-0.064497</td>\n",
       "      <td>0.052879</td>\n",
       "      <td>0.287822</td>\n",
       "      <td>-0.023268</td>\n",
       "      <td>0.315407</td>\n",
       "      <td>-0.015384</td>\n",
       "      <td>0.018530</td>\n",
       "      <td>0.076002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>i want to achieve better grades by receiving additional support in chemistry.</td>\n",
       "      <td>['Ideally, I am here to graduate and pursue a career in the science field. I feel like I want to grow as a college student add to my current understanding of chemistry. I am here to be a productive individual and add to my learning experience at San Francisco State University. Overall, Im here to focus on my future and have a positive influence on society.']</td>\n",
       "      <td>[i, want, to, achieve, better, grades, by, receiving, additional, support, in, chemistry, .]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389860</td>\n",
       "      <td>0.184082</td>\n",
       "      <td>0.018516</td>\n",
       "      <td>0.014736</td>\n",
       "      <td>0.163938</td>\n",
       "      <td>-0.056742</td>\n",
       "      <td>0.281735</td>\n",
       "      <td>-0.113101</td>\n",
       "      <td>-0.075368</td>\n",
       "      <td>0.125713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>i knew that i wouldnt allow myself to end up impoverished, victimized, or addicted.</td>\n",
       "      <td>['As a kid growing up in the East Bay Area, I always aimed for a prestigious career path because I was unsatisfied with my upbringing']</td>\n",
       "      <td>[i, knew, that, i, wouldnt, allow, myself, to, end, up, impoverished, ,, victimized, ,, or, addicted, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.357353</td>\n",
       "      <td>0.167664</td>\n",
       "      <td>-0.011786</td>\n",
       "      <td>0.044021</td>\n",
       "      <td>0.160520</td>\n",
       "      <td>-0.037122</td>\n",
       "      <td>0.162598</td>\n",
       "      <td>-0.090847</td>\n",
       "      <td>-0.029684</td>\n",
       "      <td>0.114204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>985 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                           sentence  \\\n",
       "0    while i am not the best when it come to the realm of physics, i know this class will help me gain better insight on scientific applications and improve my skill set in mathematical computation and analysis.   \n",
       "1                                                                                it is for sure going to help with my critical thinking and problem solving, whether it is for math or solving a real life problem.   \n",
       "2                                                                                                                                                                                    i just hope i can pull it off.   \n",
       "3                                 so a big piece of this is to find independence by gaining an engineering degree and then once i have gained that independence then can work towards addressing these many issues.   \n",
       "4                                                                                                            maybe this class will also help in my life in general, to do work harder and work with a group better.   \n",
       "..                                                                                                                                                                                                              ...   \n",
       "980                                                                                                                                            i want to be able to help my family out with paying bills and stuff.   \n",
       "981                                                                                                          i would like to learn how to manage my time better, so that i have sufficient time to complete my lab.   \n",
       "982                                                                                                    also, ive heard that chem 215 is more challenging so i thought that taking this sci course would be helpful.   \n",
       "983                                                                                                                                   i want to achieve better grades by receiving additional support in chemistry.   \n",
       "984                                                                                                                             i knew that i wouldnt allow myself to end up impoverished, victimized, or addicted.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                             phrase  \\\n",
       "0                                                                                                                                                                                                                                                                                      ['As a Kinesiology major, the main reason I am taking this class is that it is a prerequisite for Physical Therapy school.']   \n",
       "1                                                                                                                                                                                                                                                                                                     ['I am hoping that I am able to do well and succeed in this class so I can take the second part of physics.']   \n",
       "2    ['Why Am I HereI am here to gain the skills I need to pass the Physics portion of the MCAT and enter Medical School. My goal is to become a surgeon, either Cardio or General Surgery. When I do apply for medical school, I also plan to earn an associates degree in surgical technology, so that even if I dont get in, Ill have something to fall back on, and add to my application for when I reapply.']   \n",
       "3                                                                         ['So a big piece of this is to find independence by gaining an engineering degree and then once I have gained that independence then can work towards addressing these many issues. I think that the Engineers without Borders program looks like a really cool avenue to work in once I make it to the level of a practicing engineer.']   \n",
       "4                                                                                                                                                                                                                                                                                                  [\"I'm here because I'm afraid of failing physics and I'm hoping this class will help me succeed in passing it.\"]   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                              ...   \n",
       "980                                                                                                                                                                                                   ['I want to get my degree get a good paying job and be financially stable for my future.', 'I want to show the world my own ideas and make it into a film. I also am here to be smart and finish my degree.']   \n",
       "981                                       ['I am here in lab class to get a better understanding of my physics lecture classes.I would like to learn how to manage my time better, so that I have sufficient time to complete my lab. I want to be able to look forward to a lab when I dont understand the lecture. I hope that this lab class will further my knowledge and interest in future physics courses.']   \n",
       "982                                                                                                                                                                                                                                                                                                  ['In a broader, more general sense, I am here at SF State because I want to get a degree and get a good job.']   \n",
       "983                                        ['Ideally, I am here to graduate and pursue a career in the science field. I feel like I want to grow as a college student add to my current understanding of chemistry. I am here to be a productive individual and add to my learning experience at San Francisco State University. Overall, Im here to focus on my future and have a positive influence on society.']   \n",
       "984                                                                                                                                                                                                                                                                         ['As a kid growing up in the East Bay Area, I always aimed for a prestigious career path because I was unsatisfied with my upbringing']   \n",
       "\n",
       "                                                                                                                                                                                                                                                       tokens  \\\n",
       "0    [while, i, am, not, the, best, when, it, come, to, the, realm, of, physics, ,, i, know, this, class, will, help, me, gain, better, insight, on, scientific, applications, and, improve, my, skill, set, in, mathematical, computation, and, analysis, .]   \n",
       "1                                                                                            [it, is, for, sure, going, to, help, with, my, critical, thinking, and, problem, solving, ,, whether, it, is, for, math, or, solving, a, real, life, problem, .]   \n",
       "2                                                                                                                                                                                                                   [i, just, hope, i, can, pull, it, off, .]   \n",
       "3                                         [so, a, big, piece, of, this, is, to, find, independence, by, gaining, an, engineering, degree, and, then, once, i, have, gained, that, independence, then, can, work, towards, addressing, these, many, issues, .]   \n",
       "4                                                                                                                            [maybe, this, class, will, also, help, in, my, life, in, general, ,, to, do, work, harder, and, work, with, a, group, better, .]   \n",
       "..                                                                                                                                                                                                                                                        ...   \n",
       "980                                                                                                                                                                    [i, want, to, be, able, to, help, my, family, out, with, paying, bills, and, stuff, .]   \n",
       "981                                                                                                                          [i, would, like, to, learn, how, to, manage, my, time, better, ,, so, that, i, have, sufficient, time, to, complete, my, lab, .]   \n",
       "982                                                                                                                     [also, ,, ive, heard, that, chem, 215, is, more, challenging, so, i, thought, that, taking, this, sci, course, would, be, helpful, .]   \n",
       "983                                                                                                                                                              [i, want, to, achieve, better, grades, by, receiving, additional, support, in, chemistry, .]   \n",
       "984                                                                                                                                                  [i, knew, that, i, wouldnt, allow, myself, to, end, up, impoverished, ,, victimized, ,, or, addicted, .]   \n",
       "\n",
       "     bio_sim_words  chem_sim_words  phy_sim_words  math_sim_words  \\\n",
       "0                1               1              0               1   \n",
       "1                1               1              0               1   \n",
       "2                0               0              0               0   \n",
       "3                0               0              0               0   \n",
       "4                0               0              0               0   \n",
       "..             ...             ...            ...             ...   \n",
       "980              0               0              0               0   \n",
       "981              0               0              0               0   \n",
       "982              0               0              0               0   \n",
       "983              1               1              0               1   \n",
       "984              0               0              0               0   \n",
       "\n",
       "     tech_sim_words  eng_sim_words  medical_terms  ...  embedding90  \\\n",
       "0                 1              1              0  ...     0.388465   \n",
       "1                 0              0              1  ...     0.430366   \n",
       "2                 0              0              0  ...     0.423034   \n",
       "3                 1              1              0  ...     0.398168   \n",
       "4                 0              0              0  ...     0.425861   \n",
       "..              ...            ...            ...  ...          ...   \n",
       "980               0              0              1  ...     0.369852   \n",
       "981               0              0              0  ...     0.457951   \n",
       "982               0              0              0  ...     0.477649   \n",
       "983               0              0              0  ...     0.389860   \n",
       "984               0              0              0  ...     0.357353   \n",
       "\n",
       "     embedding91  embedding92  embedding93  embedding94  embedding95  \\\n",
       "0       0.175436    -0.016878     0.028132     0.240545    -0.032458   \n",
       "1       0.139534    -0.071063     0.056950     0.236367    -0.062232   \n",
       "2       0.177543    -0.039713     0.021027     0.280373     0.006403   \n",
       "3       0.166028    -0.015805     0.036184     0.227133    -0.051953   \n",
       "4       0.164463    -0.002573     0.018269     0.235089    -0.070152   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "980     0.143160     0.016275     0.034761     0.154880    -0.105908   \n",
       "981     0.176199    -0.018036     0.024497     0.257529    -0.027664   \n",
       "982     0.157186    -0.064497     0.052879     0.287822    -0.023268   \n",
       "983     0.184082     0.018516     0.014736     0.163938    -0.056742   \n",
       "984     0.167664    -0.011786     0.044021     0.160520    -0.037122   \n",
       "\n",
       "     embedding96  embedding97  embedding98  embedding99  \n",
       "0       0.301723    -0.092855    -0.012566     0.077127  \n",
       "1       0.340420    -0.131404    -0.035167     0.023667  \n",
       "2       0.218204    -0.054102    -0.010557     0.081190  \n",
       "3       0.317481    -0.105692    -0.013951     0.083509  \n",
       "4       0.378019    -0.092525    -0.020376     0.100528  \n",
       "..           ...          ...          ...          ...  \n",
       "980     0.286506    -0.114393    -0.073470     0.167038  \n",
       "981     0.259761    -0.098766    -0.050169     0.110715  \n",
       "982     0.315407    -0.015384     0.018530     0.076002  \n",
       "983     0.281735    -0.113101    -0.075368     0.125713  \n",
       "984     0.162598    -0.090847    -0.029684     0.114204  \n",
       "\n",
       "[985 rows x 121 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_3/Aspirational/Aspirational/saved_features/X_train_final.csv\", index=False)\n",
    "X_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_3/Aspirational/Aspirational/saved_features/X_test_final.csv\", index=False)\n",
    "y_train.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_3/Aspirational/Aspirational/saved_features/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_3/Aspirational/Aspirational/saved_features/y_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_3/Aspirational/Aspirational/saved_features/X_train_final.csv\")\n",
    "X_test = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_3/Aspirational/Aspirational/saved_features/X_test_final.csv\")\n",
    "y_train = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_3/Aspirational/Aspirational/saved_features/y_train.csv\")\n",
    "y_test = pd.read_csv(\"/Users/gbaldonado/Developer/ml-alma-taccti/ml-alma-taccti/notebooks/experiments/exp_3/Aspirational/Aspirational/saved_features/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "985"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the unigram df for train :  (8856, 3315)\n"
     ]
    }
   ],
   "source": [
    "# Unigrams for training set\n",
    "unigram_matrix = unigram_vect.fit_transform(X_train['sentence'])\n",
    "unigrams = pd.DataFrame(unigram_matrix.toarray())\n",
    "print(\"Shape of the unigram df for train : \",unigrams.shape)\n",
    "unigrams = unigrams.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = pd.concat([X_train, unigrams], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final.columns = X_train_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8856, 3436)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test unigram df shape :  (985, 3315)\n"
     ]
    }
   ],
   "source": [
    "unigram_matrix_test = unigram_vect.transform(X_test['sentence'])\n",
    "unigrams_test = pd.DataFrame(unigram_matrix_test.toarray())\n",
    "unigrams_test = unigrams_test.reset_index(drop=True)\n",
    "print(\"Test unigram df shape : \",unigrams_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(985, 3436)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final = pd.concat([X_test, unigrams_test], axis = 1)\n",
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final.columns = X_test_final.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(985, 3436)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NON-FEATURES [0-2]\n",
    "# STEM similarity [3-9]\n",
    "# SENTIMENT POLARITY [10-11]\n",
    "# NER [12]\n",
    "# POS TAG [13-20]\n",
    "# EMBEDDINGS [21-120]\n",
    "# UNIGRAM [121-1308]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ---- sentence\n",
      "1 ---- phrase\n",
      "2 ---- tokens\n",
      "3 ---- bio_sim_words\n",
      "4 ---- chem_sim_words\n",
      "5 ---- phy_sim_words\n",
      "6 ---- math_sim_words\n",
      "7 ---- tech_sim_words\n",
      "8 ---- eng_sim_words\n",
      "9 ---- medical_terms\n",
      "10 ---- polarity\n",
      "11 ---- subjectivity\n",
      "12 ---- ner\n",
      "13 ---- interjections\n",
      "14 ---- nouns\n",
      "15 ---- adverb\n",
      "16 ---- verb\n",
      "17 ---- determiner\n",
      "18 ---- pronoun\n",
      "19 ---- adjetive\n",
      "20 ---- preposition\n",
      "21 ---- embedding0\n",
      "22 ---- embedding1\n",
      "23 ---- embedding2\n",
      "24 ---- embedding3\n",
      "25 ---- embedding4\n",
      "26 ---- embedding5\n",
      "27 ---- embedding6\n",
      "28 ---- embedding7\n",
      "29 ---- embedding8\n",
      "30 ---- embedding9\n",
      "31 ---- embedding10\n",
      "32 ---- embedding11\n",
      "33 ---- embedding12\n",
      "34 ---- embedding13\n",
      "35 ---- embedding14\n",
      "36 ---- embedding15\n",
      "37 ---- embedding16\n",
      "38 ---- embedding17\n",
      "39 ---- embedding18\n",
      "40 ---- embedding19\n",
      "41 ---- embedding20\n",
      "42 ---- embedding21\n",
      "43 ---- embedding22\n",
      "44 ---- embedding23\n",
      "45 ---- embedding24\n",
      "46 ---- embedding25\n",
      "47 ---- embedding26\n",
      "48 ---- embedding27\n",
      "49 ---- embedding28\n",
      "50 ---- embedding29\n",
      "51 ---- embedding30\n",
      "52 ---- embedding31\n",
      "53 ---- embedding32\n",
      "54 ---- embedding33\n",
      "55 ---- embedding34\n",
      "56 ---- embedding35\n",
      "57 ---- embedding36\n",
      "58 ---- embedding37\n",
      "59 ---- embedding38\n",
      "60 ---- embedding39\n",
      "61 ---- embedding40\n",
      "62 ---- embedding41\n",
      "63 ---- embedding42\n",
      "64 ---- embedding43\n",
      "65 ---- embedding44\n",
      "66 ---- embedding45\n",
      "67 ---- embedding46\n",
      "68 ---- embedding47\n",
      "69 ---- embedding48\n",
      "70 ---- embedding49\n",
      "71 ---- embedding50\n",
      "72 ---- embedding51\n",
      "73 ---- embedding52\n",
      "74 ---- embedding53\n",
      "75 ---- embedding54\n",
      "76 ---- embedding55\n",
      "77 ---- embedding56\n",
      "78 ---- embedding57\n",
      "79 ---- embedding58\n",
      "80 ---- embedding59\n",
      "81 ---- embedding60\n",
      "82 ---- embedding61\n",
      "83 ---- embedding62\n",
      "84 ---- embedding63\n",
      "85 ---- embedding64\n",
      "86 ---- embedding65\n",
      "87 ---- embedding66\n",
      "88 ---- embedding67\n",
      "89 ---- embedding68\n",
      "90 ---- embedding69\n",
      "91 ---- embedding70\n",
      "92 ---- embedding71\n",
      "93 ---- embedding72\n",
      "94 ---- embedding73\n",
      "95 ---- embedding74\n",
      "96 ---- embedding75\n",
      "97 ---- embedding76\n",
      "98 ---- embedding77\n",
      "99 ---- embedding78\n",
      "100 ---- embedding79\n",
      "101 ---- embedding80\n",
      "102 ---- embedding81\n",
      "103 ---- embedding82\n",
      "104 ---- embedding83\n",
      "105 ---- embedding84\n",
      "106 ---- embedding85\n",
      "107 ---- embedding86\n",
      "108 ---- embedding87\n",
      "109 ---- embedding88\n",
      "110 ---- embedding89\n",
      "111 ---- embedding90\n",
      "112 ---- embedding91\n",
      "113 ---- embedding92\n",
      "114 ---- embedding93\n",
      "115 ---- embedding94\n",
      "116 ---- embedding95\n",
      "117 ---- embedding96\n",
      "118 ---- embedding97\n",
      "119 ---- embedding98\n",
      "120 ---- embedding99\n",
      "121 ---- 0\n",
      "122 ---- 1\n",
      "123 ---- 2\n",
      "124 ---- 3\n",
      "125 ---- 4\n",
      "126 ---- 5\n",
      "127 ---- 6\n",
      "128 ---- 7\n",
      "129 ---- 8\n",
      "130 ---- 9\n",
      "131 ---- 10\n",
      "132 ---- 11\n",
      "133 ---- 12\n",
      "134 ---- 13\n",
      "135 ---- 14\n",
      "136 ---- 15\n",
      "137 ---- 16\n",
      "138 ---- 17\n",
      "139 ---- 18\n",
      "140 ---- 19\n",
      "141 ---- 20\n",
      "142 ---- 21\n",
      "143 ---- 22\n",
      "144 ---- 23\n",
      "145 ---- 24\n",
      "146 ---- 25\n",
      "147 ---- 26\n",
      "148 ---- 27\n",
      "149 ---- 28\n",
      "150 ---- 29\n",
      "151 ---- 30\n",
      "152 ---- 31\n",
      "153 ---- 32\n",
      "154 ---- 33\n",
      "155 ---- 34\n",
      "156 ---- 35\n",
      "157 ---- 36\n",
      "158 ---- 37\n",
      "159 ---- 38\n",
      "160 ---- 39\n",
      "161 ---- 40\n",
      "162 ---- 41\n",
      "163 ---- 42\n",
      "164 ---- 43\n",
      "165 ---- 44\n",
      "166 ---- 45\n",
      "167 ---- 46\n",
      "168 ---- 47\n",
      "169 ---- 48\n",
      "170 ---- 49\n",
      "171 ---- 50\n",
      "172 ---- 51\n",
      "173 ---- 52\n",
      "174 ---- 53\n",
      "175 ---- 54\n",
      "176 ---- 55\n",
      "177 ---- 56\n",
      "178 ---- 57\n",
      "179 ---- 58\n",
      "180 ---- 59\n",
      "181 ---- 60\n",
      "182 ---- 61\n",
      "183 ---- 62\n",
      "184 ---- 63\n",
      "185 ---- 64\n",
      "186 ---- 65\n",
      "187 ---- 66\n",
      "188 ---- 67\n",
      "189 ---- 68\n",
      "190 ---- 69\n",
      "191 ---- 70\n",
      "192 ---- 71\n",
      "193 ---- 72\n",
      "194 ---- 73\n",
      "195 ---- 74\n",
      "196 ---- 75\n",
      "197 ---- 76\n",
      "198 ---- 77\n",
      "199 ---- 78\n",
      "200 ---- 79\n",
      "201 ---- 80\n",
      "202 ---- 81\n",
      "203 ---- 82\n",
      "204 ---- 83\n",
      "205 ---- 84\n",
      "206 ---- 85\n",
      "207 ---- 86\n",
      "208 ---- 87\n",
      "209 ---- 88\n",
      "210 ---- 89\n",
      "211 ---- 90\n",
      "212 ---- 91\n",
      "213 ---- 92\n",
      "214 ---- 93\n",
      "215 ---- 94\n",
      "216 ---- 95\n",
      "217 ---- 96\n",
      "218 ---- 97\n",
      "219 ---- 98\n",
      "220 ---- 99\n",
      "221 ---- 100\n",
      "222 ---- 101\n",
      "223 ---- 102\n",
      "224 ---- 103\n",
      "225 ---- 104\n",
      "226 ---- 105\n",
      "227 ---- 106\n",
      "228 ---- 107\n",
      "229 ---- 108\n",
      "230 ---- 109\n",
      "231 ---- 110\n",
      "232 ---- 111\n",
      "233 ---- 112\n",
      "234 ---- 113\n",
      "235 ---- 114\n",
      "236 ---- 115\n",
      "237 ---- 116\n",
      "238 ---- 117\n",
      "239 ---- 118\n",
      "240 ---- 119\n",
      "241 ---- 120\n",
      "242 ---- 121\n",
      "243 ---- 122\n",
      "244 ---- 123\n",
      "245 ---- 124\n",
      "246 ---- 125\n",
      "247 ---- 126\n",
      "248 ---- 127\n",
      "249 ---- 128\n",
      "250 ---- 129\n",
      "251 ---- 130\n",
      "252 ---- 131\n",
      "253 ---- 132\n",
      "254 ---- 133\n",
      "255 ---- 134\n",
      "256 ---- 135\n",
      "257 ---- 136\n",
      "258 ---- 137\n",
      "259 ---- 138\n",
      "260 ---- 139\n",
      "261 ---- 140\n",
      "262 ---- 141\n",
      "263 ---- 142\n",
      "264 ---- 143\n",
      "265 ---- 144\n",
      "266 ---- 145\n",
      "267 ---- 146\n",
      "268 ---- 147\n",
      "269 ---- 148\n",
      "270 ---- 149\n",
      "271 ---- 150\n",
      "272 ---- 151\n",
      "273 ---- 152\n",
      "274 ---- 153\n",
      "275 ---- 154\n",
      "276 ---- 155\n",
      "277 ---- 156\n",
      "278 ---- 157\n",
      "279 ---- 158\n",
      "280 ---- 159\n",
      "281 ---- 160\n",
      "282 ---- 161\n",
      "283 ---- 162\n",
      "284 ---- 163\n",
      "285 ---- 164\n",
      "286 ---- 165\n",
      "287 ---- 166\n",
      "288 ---- 167\n",
      "289 ---- 168\n",
      "290 ---- 169\n",
      "291 ---- 170\n",
      "292 ---- 171\n",
      "293 ---- 172\n",
      "294 ---- 173\n",
      "295 ---- 174\n",
      "296 ---- 175\n",
      "297 ---- 176\n",
      "298 ---- 177\n",
      "299 ---- 178\n",
      "300 ---- 179\n",
      "301 ---- 180\n",
      "302 ---- 181\n",
      "303 ---- 182\n",
      "304 ---- 183\n",
      "305 ---- 184\n",
      "306 ---- 185\n",
      "307 ---- 186\n",
      "308 ---- 187\n",
      "309 ---- 188\n",
      "310 ---- 189\n",
      "311 ---- 190\n",
      "312 ---- 191\n",
      "313 ---- 192\n",
      "314 ---- 193\n",
      "315 ---- 194\n",
      "316 ---- 195\n",
      "317 ---- 196\n",
      "318 ---- 197\n",
      "319 ---- 198\n",
      "320 ---- 199\n",
      "321 ---- 200\n",
      "322 ---- 201\n",
      "323 ---- 202\n",
      "324 ---- 203\n",
      "325 ---- 204\n",
      "326 ---- 205\n",
      "327 ---- 206\n",
      "328 ---- 207\n",
      "329 ---- 208\n",
      "330 ---- 209\n",
      "331 ---- 210\n",
      "332 ---- 211\n",
      "333 ---- 212\n",
      "334 ---- 213\n",
      "335 ---- 214\n",
      "336 ---- 215\n",
      "337 ---- 216\n",
      "338 ---- 217\n",
      "339 ---- 218\n",
      "340 ---- 219\n",
      "341 ---- 220\n",
      "342 ---- 221\n",
      "343 ---- 222\n",
      "344 ---- 223\n",
      "345 ---- 224\n",
      "346 ---- 225\n",
      "347 ---- 226\n",
      "348 ---- 227\n",
      "349 ---- 228\n",
      "350 ---- 229\n",
      "351 ---- 230\n",
      "352 ---- 231\n",
      "353 ---- 232\n",
      "354 ---- 233\n",
      "355 ---- 234\n",
      "356 ---- 235\n",
      "357 ---- 236\n",
      "358 ---- 237\n",
      "359 ---- 238\n",
      "360 ---- 239\n",
      "361 ---- 240\n",
      "362 ---- 241\n",
      "363 ---- 242\n",
      "364 ---- 243\n",
      "365 ---- 244\n",
      "366 ---- 245\n",
      "367 ---- 246\n",
      "368 ---- 247\n",
      "369 ---- 248\n",
      "370 ---- 249\n",
      "371 ---- 250\n",
      "372 ---- 251\n",
      "373 ---- 252\n",
      "374 ---- 253\n",
      "375 ---- 254\n",
      "376 ---- 255\n",
      "377 ---- 256\n",
      "378 ---- 257\n",
      "379 ---- 258\n",
      "380 ---- 259\n",
      "381 ---- 260\n",
      "382 ---- 261\n",
      "383 ---- 262\n",
      "384 ---- 263\n",
      "385 ---- 264\n",
      "386 ---- 265\n",
      "387 ---- 266\n",
      "388 ---- 267\n",
      "389 ---- 268\n",
      "390 ---- 269\n",
      "391 ---- 270\n",
      "392 ---- 271\n",
      "393 ---- 272\n",
      "394 ---- 273\n",
      "395 ---- 274\n",
      "396 ---- 275\n",
      "397 ---- 276\n",
      "398 ---- 277\n",
      "399 ---- 278\n",
      "400 ---- 279\n",
      "401 ---- 280\n",
      "402 ---- 281\n",
      "403 ---- 282\n",
      "404 ---- 283\n",
      "405 ---- 284\n",
      "406 ---- 285\n",
      "407 ---- 286\n",
      "408 ---- 287\n",
      "409 ---- 288\n",
      "410 ---- 289\n",
      "411 ---- 290\n",
      "412 ---- 291\n",
      "413 ---- 292\n",
      "414 ---- 293\n",
      "415 ---- 294\n",
      "416 ---- 295\n",
      "417 ---- 296\n",
      "418 ---- 297\n",
      "419 ---- 298\n",
      "420 ---- 299\n",
      "421 ---- 300\n",
      "422 ---- 301\n",
      "423 ---- 302\n",
      "424 ---- 303\n",
      "425 ---- 304\n",
      "426 ---- 305\n",
      "427 ---- 306\n",
      "428 ---- 307\n",
      "429 ---- 308\n",
      "430 ---- 309\n",
      "431 ---- 310\n",
      "432 ---- 311\n",
      "433 ---- 312\n",
      "434 ---- 313\n",
      "435 ---- 314\n",
      "436 ---- 315\n",
      "437 ---- 316\n",
      "438 ---- 317\n",
      "439 ---- 318\n",
      "440 ---- 319\n",
      "441 ---- 320\n",
      "442 ---- 321\n",
      "443 ---- 322\n",
      "444 ---- 323\n",
      "445 ---- 324\n",
      "446 ---- 325\n",
      "447 ---- 326\n",
      "448 ---- 327\n",
      "449 ---- 328\n",
      "450 ---- 329\n",
      "451 ---- 330\n",
      "452 ---- 331\n",
      "453 ---- 332\n",
      "454 ---- 333\n",
      "455 ---- 334\n",
      "456 ---- 335\n",
      "457 ---- 336\n",
      "458 ---- 337\n",
      "459 ---- 338\n",
      "460 ---- 339\n",
      "461 ---- 340\n",
      "462 ---- 341\n",
      "463 ---- 342\n",
      "464 ---- 343\n",
      "465 ---- 344\n",
      "466 ---- 345\n",
      "467 ---- 346\n",
      "468 ---- 347\n",
      "469 ---- 348\n",
      "470 ---- 349\n",
      "471 ---- 350\n",
      "472 ---- 351\n",
      "473 ---- 352\n",
      "474 ---- 353\n",
      "475 ---- 354\n",
      "476 ---- 355\n",
      "477 ---- 356\n",
      "478 ---- 357\n",
      "479 ---- 358\n",
      "480 ---- 359\n",
      "481 ---- 360\n",
      "482 ---- 361\n",
      "483 ---- 362\n",
      "484 ---- 363\n",
      "485 ---- 364\n",
      "486 ---- 365\n",
      "487 ---- 366\n",
      "488 ---- 367\n",
      "489 ---- 368\n",
      "490 ---- 369\n",
      "491 ---- 370\n",
      "492 ---- 371\n",
      "493 ---- 372\n",
      "494 ---- 373\n",
      "495 ---- 374\n",
      "496 ---- 375\n",
      "497 ---- 376\n",
      "498 ---- 377\n",
      "499 ---- 378\n",
      "500 ---- 379\n",
      "501 ---- 380\n",
      "502 ---- 381\n",
      "503 ---- 382\n",
      "504 ---- 383\n",
      "505 ---- 384\n",
      "506 ---- 385\n",
      "507 ---- 386\n",
      "508 ---- 387\n",
      "509 ---- 388\n",
      "510 ---- 389\n",
      "511 ---- 390\n",
      "512 ---- 391\n",
      "513 ---- 392\n",
      "514 ---- 393\n",
      "515 ---- 394\n",
      "516 ---- 395\n",
      "517 ---- 396\n",
      "518 ---- 397\n",
      "519 ---- 398\n",
      "520 ---- 399\n",
      "521 ---- 400\n",
      "522 ---- 401\n",
      "523 ---- 402\n",
      "524 ---- 403\n",
      "525 ---- 404\n",
      "526 ---- 405\n",
      "527 ---- 406\n",
      "528 ---- 407\n",
      "529 ---- 408\n",
      "530 ---- 409\n",
      "531 ---- 410\n",
      "532 ---- 411\n",
      "533 ---- 412\n",
      "534 ---- 413\n",
      "535 ---- 414\n",
      "536 ---- 415\n",
      "537 ---- 416\n",
      "538 ---- 417\n",
      "539 ---- 418\n",
      "540 ---- 419\n",
      "541 ---- 420\n",
      "542 ---- 421\n",
      "543 ---- 422\n",
      "544 ---- 423\n",
      "545 ---- 424\n",
      "546 ---- 425\n",
      "547 ---- 426\n",
      "548 ---- 427\n",
      "549 ---- 428\n",
      "550 ---- 429\n",
      "551 ---- 430\n",
      "552 ---- 431\n",
      "553 ---- 432\n",
      "554 ---- 433\n",
      "555 ---- 434\n",
      "556 ---- 435\n",
      "557 ---- 436\n",
      "558 ---- 437\n",
      "559 ---- 438\n",
      "560 ---- 439\n",
      "561 ---- 440\n",
      "562 ---- 441\n",
      "563 ---- 442\n",
      "564 ---- 443\n",
      "565 ---- 444\n",
      "566 ---- 445\n",
      "567 ---- 446\n",
      "568 ---- 447\n",
      "569 ---- 448\n",
      "570 ---- 449\n",
      "571 ---- 450\n",
      "572 ---- 451\n",
      "573 ---- 452\n",
      "574 ---- 453\n",
      "575 ---- 454\n",
      "576 ---- 455\n",
      "577 ---- 456\n",
      "578 ---- 457\n",
      "579 ---- 458\n",
      "580 ---- 459\n",
      "581 ---- 460\n",
      "582 ---- 461\n",
      "583 ---- 462\n",
      "584 ---- 463\n",
      "585 ---- 464\n",
      "586 ---- 465\n",
      "587 ---- 466\n",
      "588 ---- 467\n",
      "589 ---- 468\n",
      "590 ---- 469\n",
      "591 ---- 470\n",
      "592 ---- 471\n",
      "593 ---- 472\n",
      "594 ---- 473\n",
      "595 ---- 474\n",
      "596 ---- 475\n",
      "597 ---- 476\n",
      "598 ---- 477\n",
      "599 ---- 478\n",
      "600 ---- 479\n",
      "601 ---- 480\n",
      "602 ---- 481\n",
      "603 ---- 482\n",
      "604 ---- 483\n",
      "605 ---- 484\n",
      "606 ---- 485\n",
      "607 ---- 486\n",
      "608 ---- 487\n",
      "609 ---- 488\n",
      "610 ---- 489\n",
      "611 ---- 490\n",
      "612 ---- 491\n",
      "613 ---- 492\n",
      "614 ---- 493\n",
      "615 ---- 494\n",
      "616 ---- 495\n",
      "617 ---- 496\n",
      "618 ---- 497\n",
      "619 ---- 498\n",
      "620 ---- 499\n",
      "621 ---- 500\n",
      "622 ---- 501\n",
      "623 ---- 502\n",
      "624 ---- 503\n",
      "625 ---- 504\n",
      "626 ---- 505\n",
      "627 ---- 506\n",
      "628 ---- 507\n",
      "629 ---- 508\n",
      "630 ---- 509\n",
      "631 ---- 510\n",
      "632 ---- 511\n",
      "633 ---- 512\n",
      "634 ---- 513\n",
      "635 ---- 514\n",
      "636 ---- 515\n",
      "637 ---- 516\n",
      "638 ---- 517\n",
      "639 ---- 518\n",
      "640 ---- 519\n",
      "641 ---- 520\n",
      "642 ---- 521\n",
      "643 ---- 522\n",
      "644 ---- 523\n",
      "645 ---- 524\n",
      "646 ---- 525\n",
      "647 ---- 526\n",
      "648 ---- 527\n",
      "649 ---- 528\n",
      "650 ---- 529\n",
      "651 ---- 530\n",
      "652 ---- 531\n",
      "653 ---- 532\n",
      "654 ---- 533\n",
      "655 ---- 534\n",
      "656 ---- 535\n",
      "657 ---- 536\n",
      "658 ---- 537\n",
      "659 ---- 538\n",
      "660 ---- 539\n",
      "661 ---- 540\n",
      "662 ---- 541\n",
      "663 ---- 542\n",
      "664 ---- 543\n",
      "665 ---- 544\n",
      "666 ---- 545\n",
      "667 ---- 546\n",
      "668 ---- 547\n",
      "669 ---- 548\n",
      "670 ---- 549\n",
      "671 ---- 550\n",
      "672 ---- 551\n",
      "673 ---- 552\n",
      "674 ---- 553\n",
      "675 ---- 554\n",
      "676 ---- 555\n",
      "677 ---- 556\n",
      "678 ---- 557\n",
      "679 ---- 558\n",
      "680 ---- 559\n",
      "681 ---- 560\n",
      "682 ---- 561\n",
      "683 ---- 562\n",
      "684 ---- 563\n",
      "685 ---- 564\n",
      "686 ---- 565\n",
      "687 ---- 566\n",
      "688 ---- 567\n",
      "689 ---- 568\n",
      "690 ---- 569\n",
      "691 ---- 570\n",
      "692 ---- 571\n",
      "693 ---- 572\n",
      "694 ---- 573\n",
      "695 ---- 574\n",
      "696 ---- 575\n",
      "697 ---- 576\n",
      "698 ---- 577\n",
      "699 ---- 578\n",
      "700 ---- 579\n",
      "701 ---- 580\n",
      "702 ---- 581\n",
      "703 ---- 582\n",
      "704 ---- 583\n",
      "705 ---- 584\n",
      "706 ---- 585\n",
      "707 ---- 586\n",
      "708 ---- 587\n",
      "709 ---- 588\n",
      "710 ---- 589\n",
      "711 ---- 590\n",
      "712 ---- 591\n",
      "713 ---- 592\n",
      "714 ---- 593\n",
      "715 ---- 594\n",
      "716 ---- 595\n",
      "717 ---- 596\n",
      "718 ---- 597\n",
      "719 ---- 598\n",
      "720 ---- 599\n",
      "721 ---- 600\n",
      "722 ---- 601\n",
      "723 ---- 602\n",
      "724 ---- 603\n",
      "725 ---- 604\n",
      "726 ---- 605\n",
      "727 ---- 606\n",
      "728 ---- 607\n",
      "729 ---- 608\n",
      "730 ---- 609\n",
      "731 ---- 610\n",
      "732 ---- 611\n",
      "733 ---- 612\n",
      "734 ---- 613\n",
      "735 ---- 614\n",
      "736 ---- 615\n",
      "737 ---- 616\n",
      "738 ---- 617\n",
      "739 ---- 618\n",
      "740 ---- 619\n",
      "741 ---- 620\n",
      "742 ---- 621\n",
      "743 ---- 622\n",
      "744 ---- 623\n",
      "745 ---- 624\n",
      "746 ---- 625\n",
      "747 ---- 626\n",
      "748 ---- 627\n",
      "749 ---- 628\n",
      "750 ---- 629\n",
      "751 ---- 630\n",
      "752 ---- 631\n",
      "753 ---- 632\n",
      "754 ---- 633\n",
      "755 ---- 634\n",
      "756 ---- 635\n",
      "757 ---- 636\n",
      "758 ---- 637\n",
      "759 ---- 638\n",
      "760 ---- 639\n",
      "761 ---- 640\n",
      "762 ---- 641\n",
      "763 ---- 642\n",
      "764 ---- 643\n",
      "765 ---- 644\n",
      "766 ---- 645\n",
      "767 ---- 646\n",
      "768 ---- 647\n",
      "769 ---- 648\n",
      "770 ---- 649\n",
      "771 ---- 650\n",
      "772 ---- 651\n",
      "773 ---- 652\n",
      "774 ---- 653\n",
      "775 ---- 654\n",
      "776 ---- 655\n",
      "777 ---- 656\n",
      "778 ---- 657\n",
      "779 ---- 658\n",
      "780 ---- 659\n",
      "781 ---- 660\n",
      "782 ---- 661\n",
      "783 ---- 662\n",
      "784 ---- 663\n",
      "785 ---- 664\n",
      "786 ---- 665\n",
      "787 ---- 666\n",
      "788 ---- 667\n",
      "789 ---- 668\n",
      "790 ---- 669\n",
      "791 ---- 670\n",
      "792 ---- 671\n",
      "793 ---- 672\n",
      "794 ---- 673\n",
      "795 ---- 674\n",
      "796 ---- 675\n",
      "797 ---- 676\n",
      "798 ---- 677\n",
      "799 ---- 678\n",
      "800 ---- 679\n",
      "801 ---- 680\n",
      "802 ---- 681\n",
      "803 ---- 682\n",
      "804 ---- 683\n",
      "805 ---- 684\n",
      "806 ---- 685\n",
      "807 ---- 686\n",
      "808 ---- 687\n",
      "809 ---- 688\n",
      "810 ---- 689\n",
      "811 ---- 690\n",
      "812 ---- 691\n",
      "813 ---- 692\n",
      "814 ---- 693\n",
      "815 ---- 694\n",
      "816 ---- 695\n",
      "817 ---- 696\n",
      "818 ---- 697\n",
      "819 ---- 698\n",
      "820 ---- 699\n",
      "821 ---- 700\n",
      "822 ---- 701\n",
      "823 ---- 702\n",
      "824 ---- 703\n",
      "825 ---- 704\n",
      "826 ---- 705\n",
      "827 ---- 706\n",
      "828 ---- 707\n",
      "829 ---- 708\n",
      "830 ---- 709\n",
      "831 ---- 710\n",
      "832 ---- 711\n",
      "833 ---- 712\n",
      "834 ---- 713\n",
      "835 ---- 714\n",
      "836 ---- 715\n",
      "837 ---- 716\n",
      "838 ---- 717\n",
      "839 ---- 718\n",
      "840 ---- 719\n",
      "841 ---- 720\n",
      "842 ---- 721\n",
      "843 ---- 722\n",
      "844 ---- 723\n",
      "845 ---- 724\n",
      "846 ---- 725\n",
      "847 ---- 726\n",
      "848 ---- 727\n",
      "849 ---- 728\n",
      "850 ---- 729\n",
      "851 ---- 730\n",
      "852 ---- 731\n",
      "853 ---- 732\n",
      "854 ---- 733\n",
      "855 ---- 734\n",
      "856 ---- 735\n",
      "857 ---- 736\n",
      "858 ---- 737\n",
      "859 ---- 738\n",
      "860 ---- 739\n",
      "861 ---- 740\n",
      "862 ---- 741\n",
      "863 ---- 742\n",
      "864 ---- 743\n",
      "865 ---- 744\n",
      "866 ---- 745\n",
      "867 ---- 746\n",
      "868 ---- 747\n",
      "869 ---- 748\n",
      "870 ---- 749\n",
      "871 ---- 750\n",
      "872 ---- 751\n",
      "873 ---- 752\n",
      "874 ---- 753\n",
      "875 ---- 754\n",
      "876 ---- 755\n",
      "877 ---- 756\n",
      "878 ---- 757\n",
      "879 ---- 758\n",
      "880 ---- 759\n",
      "881 ---- 760\n",
      "882 ---- 761\n",
      "883 ---- 762\n",
      "884 ---- 763\n",
      "885 ---- 764\n",
      "886 ---- 765\n",
      "887 ---- 766\n",
      "888 ---- 767\n",
      "889 ---- 768\n",
      "890 ---- 769\n",
      "891 ---- 770\n",
      "892 ---- 771\n",
      "893 ---- 772\n",
      "894 ---- 773\n",
      "895 ---- 774\n",
      "896 ---- 775\n",
      "897 ---- 776\n",
      "898 ---- 777\n",
      "899 ---- 778\n",
      "900 ---- 779\n",
      "901 ---- 780\n",
      "902 ---- 781\n",
      "903 ---- 782\n",
      "904 ---- 783\n",
      "905 ---- 784\n",
      "906 ---- 785\n",
      "907 ---- 786\n",
      "908 ---- 787\n",
      "909 ---- 788\n",
      "910 ---- 789\n",
      "911 ---- 790\n",
      "912 ---- 791\n",
      "913 ---- 792\n",
      "914 ---- 793\n",
      "915 ---- 794\n",
      "916 ---- 795\n",
      "917 ---- 796\n",
      "918 ---- 797\n",
      "919 ---- 798\n",
      "920 ---- 799\n",
      "921 ---- 800\n",
      "922 ---- 801\n",
      "923 ---- 802\n",
      "924 ---- 803\n",
      "925 ---- 804\n",
      "926 ---- 805\n",
      "927 ---- 806\n",
      "928 ---- 807\n",
      "929 ---- 808\n",
      "930 ---- 809\n",
      "931 ---- 810\n",
      "932 ---- 811\n",
      "933 ---- 812\n",
      "934 ---- 813\n",
      "935 ---- 814\n",
      "936 ---- 815\n",
      "937 ---- 816\n",
      "938 ---- 817\n",
      "939 ---- 818\n",
      "940 ---- 819\n",
      "941 ---- 820\n",
      "942 ---- 821\n",
      "943 ---- 822\n",
      "944 ---- 823\n",
      "945 ---- 824\n",
      "946 ---- 825\n",
      "947 ---- 826\n",
      "948 ---- 827\n",
      "949 ---- 828\n",
      "950 ---- 829\n",
      "951 ---- 830\n",
      "952 ---- 831\n",
      "953 ---- 832\n",
      "954 ---- 833\n",
      "955 ---- 834\n",
      "956 ---- 835\n",
      "957 ---- 836\n",
      "958 ---- 837\n",
      "959 ---- 838\n",
      "960 ---- 839\n",
      "961 ---- 840\n",
      "962 ---- 841\n",
      "963 ---- 842\n",
      "964 ---- 843\n",
      "965 ---- 844\n",
      "966 ---- 845\n",
      "967 ---- 846\n",
      "968 ---- 847\n",
      "969 ---- 848\n",
      "970 ---- 849\n",
      "971 ---- 850\n",
      "972 ---- 851\n",
      "973 ---- 852\n",
      "974 ---- 853\n",
      "975 ---- 854\n",
      "976 ---- 855\n",
      "977 ---- 856\n",
      "978 ---- 857\n",
      "979 ---- 858\n",
      "980 ---- 859\n",
      "981 ---- 860\n",
      "982 ---- 861\n",
      "983 ---- 862\n",
      "984 ---- 863\n",
      "985 ---- 864\n",
      "986 ---- 865\n",
      "987 ---- 866\n",
      "988 ---- 867\n",
      "989 ---- 868\n",
      "990 ---- 869\n",
      "991 ---- 870\n",
      "992 ---- 871\n",
      "993 ---- 872\n",
      "994 ---- 873\n",
      "995 ---- 874\n",
      "996 ---- 875\n",
      "997 ---- 876\n",
      "998 ---- 877\n",
      "999 ---- 878\n",
      "1000 ---- 879\n",
      "1001 ---- 880\n",
      "1002 ---- 881\n",
      "1003 ---- 882\n",
      "1004 ---- 883\n",
      "1005 ---- 884\n",
      "1006 ---- 885\n",
      "1007 ---- 886\n",
      "1008 ---- 887\n",
      "1009 ---- 888\n",
      "1010 ---- 889\n",
      "1011 ---- 890\n",
      "1012 ---- 891\n",
      "1013 ---- 892\n",
      "1014 ---- 893\n",
      "1015 ---- 894\n",
      "1016 ---- 895\n",
      "1017 ---- 896\n",
      "1018 ---- 897\n",
      "1019 ---- 898\n",
      "1020 ---- 899\n",
      "1021 ---- 900\n",
      "1022 ---- 901\n",
      "1023 ---- 902\n",
      "1024 ---- 903\n",
      "1025 ---- 904\n",
      "1026 ---- 905\n",
      "1027 ---- 906\n",
      "1028 ---- 907\n",
      "1029 ---- 908\n",
      "1030 ---- 909\n",
      "1031 ---- 910\n",
      "1032 ---- 911\n",
      "1033 ---- 912\n",
      "1034 ---- 913\n",
      "1035 ---- 914\n",
      "1036 ---- 915\n",
      "1037 ---- 916\n",
      "1038 ---- 917\n",
      "1039 ---- 918\n",
      "1040 ---- 919\n",
      "1041 ---- 920\n",
      "1042 ---- 921\n",
      "1043 ---- 922\n",
      "1044 ---- 923\n",
      "1045 ---- 924\n",
      "1046 ---- 925\n",
      "1047 ---- 926\n",
      "1048 ---- 927\n",
      "1049 ---- 928\n",
      "1050 ---- 929\n",
      "1051 ---- 930\n",
      "1052 ---- 931\n",
      "1053 ---- 932\n",
      "1054 ---- 933\n",
      "1055 ---- 934\n",
      "1056 ---- 935\n",
      "1057 ---- 936\n",
      "1058 ---- 937\n",
      "1059 ---- 938\n",
      "1060 ---- 939\n",
      "1061 ---- 940\n",
      "1062 ---- 941\n",
      "1063 ---- 942\n",
      "1064 ---- 943\n",
      "1065 ---- 944\n",
      "1066 ---- 945\n",
      "1067 ---- 946\n",
      "1068 ---- 947\n",
      "1069 ---- 948\n",
      "1070 ---- 949\n",
      "1071 ---- 950\n",
      "1072 ---- 951\n",
      "1073 ---- 952\n",
      "1074 ---- 953\n",
      "1075 ---- 954\n",
      "1076 ---- 955\n",
      "1077 ---- 956\n",
      "1078 ---- 957\n",
      "1079 ---- 958\n",
      "1080 ---- 959\n",
      "1081 ---- 960\n",
      "1082 ---- 961\n",
      "1083 ---- 962\n",
      "1084 ---- 963\n",
      "1085 ---- 964\n",
      "1086 ---- 965\n",
      "1087 ---- 966\n",
      "1088 ---- 967\n",
      "1089 ---- 968\n",
      "1090 ---- 969\n",
      "1091 ---- 970\n",
      "1092 ---- 971\n",
      "1093 ---- 972\n",
      "1094 ---- 973\n",
      "1095 ---- 974\n",
      "1096 ---- 975\n",
      "1097 ---- 976\n",
      "1098 ---- 977\n",
      "1099 ---- 978\n",
      "1100 ---- 979\n",
      "1101 ---- 980\n",
      "1102 ---- 981\n",
      "1103 ---- 982\n",
      "1104 ---- 983\n",
      "1105 ---- 984\n",
      "1106 ---- 985\n",
      "1107 ---- 986\n",
      "1108 ---- 987\n",
      "1109 ---- 988\n",
      "1110 ---- 989\n",
      "1111 ---- 990\n",
      "1112 ---- 991\n",
      "1113 ---- 992\n",
      "1114 ---- 993\n",
      "1115 ---- 994\n",
      "1116 ---- 995\n",
      "1117 ---- 996\n",
      "1118 ---- 997\n",
      "1119 ---- 998\n",
      "1120 ---- 999\n",
      "1121 ---- 1000\n",
      "1122 ---- 1001\n",
      "1123 ---- 1002\n",
      "1124 ---- 1003\n",
      "1125 ---- 1004\n",
      "1126 ---- 1005\n",
      "1127 ---- 1006\n",
      "1128 ---- 1007\n",
      "1129 ---- 1008\n",
      "1130 ---- 1009\n",
      "1131 ---- 1010\n",
      "1132 ---- 1011\n",
      "1133 ---- 1012\n",
      "1134 ---- 1013\n",
      "1135 ---- 1014\n",
      "1136 ---- 1015\n",
      "1137 ---- 1016\n",
      "1138 ---- 1017\n",
      "1139 ---- 1018\n",
      "1140 ---- 1019\n",
      "1141 ---- 1020\n",
      "1142 ---- 1021\n",
      "1143 ---- 1022\n",
      "1144 ---- 1023\n",
      "1145 ---- 1024\n",
      "1146 ---- 1025\n",
      "1147 ---- 1026\n",
      "1148 ---- 1027\n",
      "1149 ---- 1028\n",
      "1150 ---- 1029\n",
      "1151 ---- 1030\n",
      "1152 ---- 1031\n",
      "1153 ---- 1032\n",
      "1154 ---- 1033\n",
      "1155 ---- 1034\n",
      "1156 ---- 1035\n",
      "1157 ---- 1036\n",
      "1158 ---- 1037\n",
      "1159 ---- 1038\n",
      "1160 ---- 1039\n",
      "1161 ---- 1040\n",
      "1162 ---- 1041\n",
      "1163 ---- 1042\n",
      "1164 ---- 1043\n",
      "1165 ---- 1044\n",
      "1166 ---- 1045\n",
      "1167 ---- 1046\n",
      "1168 ---- 1047\n",
      "1169 ---- 1048\n",
      "1170 ---- 1049\n",
      "1171 ---- 1050\n",
      "1172 ---- 1051\n",
      "1173 ---- 1052\n",
      "1174 ---- 1053\n",
      "1175 ---- 1054\n",
      "1176 ---- 1055\n",
      "1177 ---- 1056\n",
      "1178 ---- 1057\n",
      "1179 ---- 1058\n",
      "1180 ---- 1059\n",
      "1181 ---- 1060\n",
      "1182 ---- 1061\n",
      "1183 ---- 1062\n",
      "1184 ---- 1063\n",
      "1185 ---- 1064\n",
      "1186 ---- 1065\n",
      "1187 ---- 1066\n",
      "1188 ---- 1067\n",
      "1189 ---- 1068\n",
      "1190 ---- 1069\n",
      "1191 ---- 1070\n",
      "1192 ---- 1071\n",
      "1193 ---- 1072\n",
      "1194 ---- 1073\n",
      "1195 ---- 1074\n",
      "1196 ---- 1075\n",
      "1197 ---- 1076\n",
      "1198 ---- 1077\n",
      "1199 ---- 1078\n",
      "1200 ---- 1079\n",
      "1201 ---- 1080\n",
      "1202 ---- 1081\n",
      "1203 ---- 1082\n",
      "1204 ---- 1083\n",
      "1205 ---- 1084\n",
      "1206 ---- 1085\n",
      "1207 ---- 1086\n",
      "1208 ---- 1087\n",
      "1209 ---- 1088\n",
      "1210 ---- 1089\n",
      "1211 ---- 1090\n",
      "1212 ---- 1091\n",
      "1213 ---- 1092\n",
      "1214 ---- 1093\n",
      "1215 ---- 1094\n",
      "1216 ---- 1095\n",
      "1217 ---- 1096\n",
      "1218 ---- 1097\n",
      "1219 ---- 1098\n",
      "1220 ---- 1099\n",
      "1221 ---- 1100\n",
      "1222 ---- 1101\n",
      "1223 ---- 1102\n",
      "1224 ---- 1103\n",
      "1225 ---- 1104\n",
      "1226 ---- 1105\n",
      "1227 ---- 1106\n",
      "1228 ---- 1107\n",
      "1229 ---- 1108\n",
      "1230 ---- 1109\n",
      "1231 ---- 1110\n",
      "1232 ---- 1111\n",
      "1233 ---- 1112\n",
      "1234 ---- 1113\n",
      "1235 ---- 1114\n",
      "1236 ---- 1115\n",
      "1237 ---- 1116\n",
      "1238 ---- 1117\n",
      "1239 ---- 1118\n",
      "1240 ---- 1119\n",
      "1241 ---- 1120\n",
      "1242 ---- 1121\n",
      "1243 ---- 1122\n",
      "1244 ---- 1123\n",
      "1245 ---- 1124\n",
      "1246 ---- 1125\n",
      "1247 ---- 1126\n",
      "1248 ---- 1127\n",
      "1249 ---- 1128\n",
      "1250 ---- 1129\n",
      "1251 ---- 1130\n",
      "1252 ---- 1131\n",
      "1253 ---- 1132\n",
      "1254 ---- 1133\n",
      "1255 ---- 1134\n",
      "1256 ---- 1135\n",
      "1257 ---- 1136\n",
      "1258 ---- 1137\n",
      "1259 ---- 1138\n",
      "1260 ---- 1139\n",
      "1261 ---- 1140\n",
      "1262 ---- 1141\n",
      "1263 ---- 1142\n",
      "1264 ---- 1143\n",
      "1265 ---- 1144\n",
      "1266 ---- 1145\n",
      "1267 ---- 1146\n",
      "1268 ---- 1147\n",
      "1269 ---- 1148\n",
      "1270 ---- 1149\n",
      "1271 ---- 1150\n",
      "1272 ---- 1151\n",
      "1273 ---- 1152\n",
      "1274 ---- 1153\n",
      "1275 ---- 1154\n",
      "1276 ---- 1155\n",
      "1277 ---- 1156\n",
      "1278 ---- 1157\n",
      "1279 ---- 1158\n",
      "1280 ---- 1159\n",
      "1281 ---- 1160\n",
      "1282 ---- 1161\n",
      "1283 ---- 1162\n",
      "1284 ---- 1163\n",
      "1285 ---- 1164\n",
      "1286 ---- 1165\n",
      "1287 ---- 1166\n",
      "1288 ---- 1167\n",
      "1289 ---- 1168\n",
      "1290 ---- 1169\n",
      "1291 ---- 1170\n",
      "1292 ---- 1171\n",
      "1293 ---- 1172\n",
      "1294 ---- 1173\n",
      "1295 ---- 1174\n",
      "1296 ---- 1175\n",
      "1297 ---- 1176\n",
      "1298 ---- 1177\n",
      "1299 ---- 1178\n",
      "1300 ---- 1179\n",
      "1301 ---- 1180\n",
      "1302 ---- 1181\n",
      "1303 ---- 1182\n",
      "1304 ---- 1183\n",
      "1305 ---- 1184\n",
      "1306 ---- 1185\n",
      "1307 ---- 1186\n",
      "1308 ---- 1187\n",
      "1309 ---- 1188\n",
      "1310 ---- 1189\n",
      "1311 ---- 1190\n",
      "1312 ---- 1191\n",
      "1313 ---- 1192\n",
      "1314 ---- 1193\n",
      "1315 ---- 1194\n",
      "1316 ---- 1195\n",
      "1317 ---- 1196\n",
      "1318 ---- 1197\n",
      "1319 ---- 1198\n",
      "1320 ---- 1199\n",
      "1321 ---- 1200\n",
      "1322 ---- 1201\n",
      "1323 ---- 1202\n",
      "1324 ---- 1203\n",
      "1325 ---- 1204\n",
      "1326 ---- 1205\n",
      "1327 ---- 1206\n",
      "1328 ---- 1207\n",
      "1329 ---- 1208\n",
      "1330 ---- 1209\n",
      "1331 ---- 1210\n",
      "1332 ---- 1211\n",
      "1333 ---- 1212\n",
      "1334 ---- 1213\n",
      "1335 ---- 1214\n",
      "1336 ---- 1215\n",
      "1337 ---- 1216\n",
      "1338 ---- 1217\n",
      "1339 ---- 1218\n",
      "1340 ---- 1219\n",
      "1341 ---- 1220\n",
      "1342 ---- 1221\n",
      "1343 ---- 1222\n",
      "1344 ---- 1223\n",
      "1345 ---- 1224\n",
      "1346 ---- 1225\n",
      "1347 ---- 1226\n",
      "1348 ---- 1227\n",
      "1349 ---- 1228\n",
      "1350 ---- 1229\n",
      "1351 ---- 1230\n",
      "1352 ---- 1231\n",
      "1353 ---- 1232\n",
      "1354 ---- 1233\n",
      "1355 ---- 1234\n",
      "1356 ---- 1235\n",
      "1357 ---- 1236\n",
      "1358 ---- 1237\n",
      "1359 ---- 1238\n",
      "1360 ---- 1239\n",
      "1361 ---- 1240\n",
      "1362 ---- 1241\n",
      "1363 ---- 1242\n",
      "1364 ---- 1243\n",
      "1365 ---- 1244\n",
      "1366 ---- 1245\n",
      "1367 ---- 1246\n",
      "1368 ---- 1247\n",
      "1369 ---- 1248\n",
      "1370 ---- 1249\n",
      "1371 ---- 1250\n",
      "1372 ---- 1251\n",
      "1373 ---- 1252\n",
      "1374 ---- 1253\n",
      "1375 ---- 1254\n",
      "1376 ---- 1255\n",
      "1377 ---- 1256\n",
      "1378 ---- 1257\n",
      "1379 ---- 1258\n",
      "1380 ---- 1259\n",
      "1381 ---- 1260\n",
      "1382 ---- 1261\n",
      "1383 ---- 1262\n",
      "1384 ---- 1263\n",
      "1385 ---- 1264\n",
      "1386 ---- 1265\n",
      "1387 ---- 1266\n",
      "1388 ---- 1267\n",
      "1389 ---- 1268\n",
      "1390 ---- 1269\n",
      "1391 ---- 1270\n",
      "1392 ---- 1271\n",
      "1393 ---- 1272\n",
      "1394 ---- 1273\n",
      "1395 ---- 1274\n",
      "1396 ---- 1275\n",
      "1397 ---- 1276\n",
      "1398 ---- 1277\n",
      "1399 ---- 1278\n",
      "1400 ---- 1279\n",
      "1401 ---- 1280\n",
      "1402 ---- 1281\n",
      "1403 ---- 1282\n",
      "1404 ---- 1283\n",
      "1405 ---- 1284\n",
      "1406 ---- 1285\n",
      "1407 ---- 1286\n",
      "1408 ---- 1287\n",
      "1409 ---- 1288\n",
      "1410 ---- 1289\n",
      "1411 ---- 1290\n",
      "1412 ---- 1291\n",
      "1413 ---- 1292\n",
      "1414 ---- 1293\n",
      "1415 ---- 1294\n",
      "1416 ---- 1295\n",
      "1417 ---- 1296\n",
      "1418 ---- 1297\n",
      "1419 ---- 1298\n",
      "1420 ---- 1299\n",
      "1421 ---- 1300\n",
      "1422 ---- 1301\n",
      "1423 ---- 1302\n",
      "1424 ---- 1303\n",
      "1425 ---- 1304\n",
      "1426 ---- 1305\n",
      "1427 ---- 1306\n",
      "1428 ---- 1307\n",
      "1429 ---- 1308\n",
      "1430 ---- 1309\n",
      "1431 ---- 1310\n",
      "1432 ---- 1311\n",
      "1433 ---- 1312\n",
      "1434 ---- 1313\n",
      "1435 ---- 1314\n",
      "1436 ---- 1315\n",
      "1437 ---- 1316\n",
      "1438 ---- 1317\n",
      "1439 ---- 1318\n",
      "1440 ---- 1319\n",
      "1441 ---- 1320\n",
      "1442 ---- 1321\n",
      "1443 ---- 1322\n",
      "1444 ---- 1323\n",
      "1445 ---- 1324\n",
      "1446 ---- 1325\n",
      "1447 ---- 1326\n",
      "1448 ---- 1327\n",
      "1449 ---- 1328\n",
      "1450 ---- 1329\n",
      "1451 ---- 1330\n",
      "1452 ---- 1331\n",
      "1453 ---- 1332\n",
      "1454 ---- 1333\n",
      "1455 ---- 1334\n",
      "1456 ---- 1335\n",
      "1457 ---- 1336\n",
      "1458 ---- 1337\n",
      "1459 ---- 1338\n",
      "1460 ---- 1339\n",
      "1461 ---- 1340\n",
      "1462 ---- 1341\n",
      "1463 ---- 1342\n",
      "1464 ---- 1343\n",
      "1465 ---- 1344\n",
      "1466 ---- 1345\n",
      "1467 ---- 1346\n",
      "1468 ---- 1347\n",
      "1469 ---- 1348\n",
      "1470 ---- 1349\n",
      "1471 ---- 1350\n",
      "1472 ---- 1351\n",
      "1473 ---- 1352\n",
      "1474 ---- 1353\n",
      "1475 ---- 1354\n",
      "1476 ---- 1355\n",
      "1477 ---- 1356\n",
      "1478 ---- 1357\n",
      "1479 ---- 1358\n",
      "1480 ---- 1359\n",
      "1481 ---- 1360\n",
      "1482 ---- 1361\n",
      "1483 ---- 1362\n",
      "1484 ---- 1363\n",
      "1485 ---- 1364\n",
      "1486 ---- 1365\n",
      "1487 ---- 1366\n",
      "1488 ---- 1367\n",
      "1489 ---- 1368\n",
      "1490 ---- 1369\n",
      "1491 ---- 1370\n",
      "1492 ---- 1371\n",
      "1493 ---- 1372\n",
      "1494 ---- 1373\n",
      "1495 ---- 1374\n",
      "1496 ---- 1375\n",
      "1497 ---- 1376\n",
      "1498 ---- 1377\n",
      "1499 ---- 1378\n",
      "1500 ---- 1379\n",
      "1501 ---- 1380\n",
      "1502 ---- 1381\n",
      "1503 ---- 1382\n",
      "1504 ---- 1383\n",
      "1505 ---- 1384\n",
      "1506 ---- 1385\n",
      "1507 ---- 1386\n",
      "1508 ---- 1387\n",
      "1509 ---- 1388\n",
      "1510 ---- 1389\n",
      "1511 ---- 1390\n",
      "1512 ---- 1391\n",
      "1513 ---- 1392\n",
      "1514 ---- 1393\n",
      "1515 ---- 1394\n",
      "1516 ---- 1395\n",
      "1517 ---- 1396\n",
      "1518 ---- 1397\n",
      "1519 ---- 1398\n",
      "1520 ---- 1399\n",
      "1521 ---- 1400\n",
      "1522 ---- 1401\n",
      "1523 ---- 1402\n",
      "1524 ---- 1403\n",
      "1525 ---- 1404\n",
      "1526 ---- 1405\n",
      "1527 ---- 1406\n",
      "1528 ---- 1407\n",
      "1529 ---- 1408\n",
      "1530 ---- 1409\n",
      "1531 ---- 1410\n",
      "1532 ---- 1411\n",
      "1533 ---- 1412\n",
      "1534 ---- 1413\n",
      "1535 ---- 1414\n",
      "1536 ---- 1415\n",
      "1537 ---- 1416\n",
      "1538 ---- 1417\n",
      "1539 ---- 1418\n",
      "1540 ---- 1419\n",
      "1541 ---- 1420\n",
      "1542 ---- 1421\n",
      "1543 ---- 1422\n",
      "1544 ---- 1423\n",
      "1545 ---- 1424\n",
      "1546 ---- 1425\n",
      "1547 ---- 1426\n",
      "1548 ---- 1427\n",
      "1549 ---- 1428\n",
      "1550 ---- 1429\n",
      "1551 ---- 1430\n",
      "1552 ---- 1431\n",
      "1553 ---- 1432\n",
      "1554 ---- 1433\n",
      "1555 ---- 1434\n",
      "1556 ---- 1435\n",
      "1557 ---- 1436\n",
      "1558 ---- 1437\n",
      "1559 ---- 1438\n",
      "1560 ---- 1439\n",
      "1561 ---- 1440\n",
      "1562 ---- 1441\n",
      "1563 ---- 1442\n",
      "1564 ---- 1443\n",
      "1565 ---- 1444\n",
      "1566 ---- 1445\n",
      "1567 ---- 1446\n",
      "1568 ---- 1447\n",
      "1569 ---- 1448\n",
      "1570 ---- 1449\n",
      "1571 ---- 1450\n",
      "1572 ---- 1451\n",
      "1573 ---- 1452\n",
      "1574 ---- 1453\n",
      "1575 ---- 1454\n",
      "1576 ---- 1455\n",
      "1577 ---- 1456\n",
      "1578 ---- 1457\n",
      "1579 ---- 1458\n",
      "1580 ---- 1459\n",
      "1581 ---- 1460\n",
      "1582 ---- 1461\n",
      "1583 ---- 1462\n",
      "1584 ---- 1463\n",
      "1585 ---- 1464\n",
      "1586 ---- 1465\n",
      "1587 ---- 1466\n",
      "1588 ---- 1467\n",
      "1589 ---- 1468\n",
      "1590 ---- 1469\n",
      "1591 ---- 1470\n",
      "1592 ---- 1471\n",
      "1593 ---- 1472\n",
      "1594 ---- 1473\n",
      "1595 ---- 1474\n",
      "1596 ---- 1475\n",
      "1597 ---- 1476\n",
      "1598 ---- 1477\n",
      "1599 ---- 1478\n",
      "1600 ---- 1479\n",
      "1601 ---- 1480\n",
      "1602 ---- 1481\n",
      "1603 ---- 1482\n",
      "1604 ---- 1483\n",
      "1605 ---- 1484\n",
      "1606 ---- 1485\n",
      "1607 ---- 1486\n",
      "1608 ---- 1487\n",
      "1609 ---- 1488\n",
      "1610 ---- 1489\n",
      "1611 ---- 1490\n",
      "1612 ---- 1491\n",
      "1613 ---- 1492\n",
      "1614 ---- 1493\n",
      "1615 ---- 1494\n",
      "1616 ---- 1495\n",
      "1617 ---- 1496\n",
      "1618 ---- 1497\n",
      "1619 ---- 1498\n",
      "1620 ---- 1499\n",
      "1621 ---- 1500\n",
      "1622 ---- 1501\n",
      "1623 ---- 1502\n",
      "1624 ---- 1503\n",
      "1625 ---- 1504\n",
      "1626 ---- 1505\n",
      "1627 ---- 1506\n",
      "1628 ---- 1507\n",
      "1629 ---- 1508\n",
      "1630 ---- 1509\n",
      "1631 ---- 1510\n",
      "1632 ---- 1511\n",
      "1633 ---- 1512\n",
      "1634 ---- 1513\n",
      "1635 ---- 1514\n",
      "1636 ---- 1515\n",
      "1637 ---- 1516\n",
      "1638 ---- 1517\n",
      "1639 ---- 1518\n",
      "1640 ---- 1519\n",
      "1641 ---- 1520\n",
      "1642 ---- 1521\n",
      "1643 ---- 1522\n",
      "1644 ---- 1523\n",
      "1645 ---- 1524\n",
      "1646 ---- 1525\n",
      "1647 ---- 1526\n",
      "1648 ---- 1527\n",
      "1649 ---- 1528\n",
      "1650 ---- 1529\n",
      "1651 ---- 1530\n",
      "1652 ---- 1531\n",
      "1653 ---- 1532\n",
      "1654 ---- 1533\n",
      "1655 ---- 1534\n",
      "1656 ---- 1535\n",
      "1657 ---- 1536\n",
      "1658 ---- 1537\n",
      "1659 ---- 1538\n",
      "1660 ---- 1539\n",
      "1661 ---- 1540\n",
      "1662 ---- 1541\n",
      "1663 ---- 1542\n",
      "1664 ---- 1543\n",
      "1665 ---- 1544\n",
      "1666 ---- 1545\n",
      "1667 ---- 1546\n",
      "1668 ---- 1547\n",
      "1669 ---- 1548\n",
      "1670 ---- 1549\n",
      "1671 ---- 1550\n",
      "1672 ---- 1551\n",
      "1673 ---- 1552\n",
      "1674 ---- 1553\n",
      "1675 ---- 1554\n",
      "1676 ---- 1555\n",
      "1677 ---- 1556\n",
      "1678 ---- 1557\n",
      "1679 ---- 1558\n",
      "1680 ---- 1559\n",
      "1681 ---- 1560\n",
      "1682 ---- 1561\n",
      "1683 ---- 1562\n",
      "1684 ---- 1563\n",
      "1685 ---- 1564\n",
      "1686 ---- 1565\n",
      "1687 ---- 1566\n",
      "1688 ---- 1567\n",
      "1689 ---- 1568\n",
      "1690 ---- 1569\n",
      "1691 ---- 1570\n",
      "1692 ---- 1571\n",
      "1693 ---- 1572\n",
      "1694 ---- 1573\n",
      "1695 ---- 1574\n",
      "1696 ---- 1575\n",
      "1697 ---- 1576\n",
      "1698 ---- 1577\n",
      "1699 ---- 1578\n",
      "1700 ---- 1579\n",
      "1701 ---- 1580\n",
      "1702 ---- 1581\n",
      "1703 ---- 1582\n",
      "1704 ---- 1583\n",
      "1705 ---- 1584\n",
      "1706 ---- 1585\n",
      "1707 ---- 1586\n",
      "1708 ---- 1587\n",
      "1709 ---- 1588\n",
      "1710 ---- 1589\n",
      "1711 ---- 1590\n",
      "1712 ---- 1591\n",
      "1713 ---- 1592\n",
      "1714 ---- 1593\n",
      "1715 ---- 1594\n",
      "1716 ---- 1595\n",
      "1717 ---- 1596\n",
      "1718 ---- 1597\n",
      "1719 ---- 1598\n",
      "1720 ---- 1599\n",
      "1721 ---- 1600\n",
      "1722 ---- 1601\n",
      "1723 ---- 1602\n",
      "1724 ---- 1603\n",
      "1725 ---- 1604\n",
      "1726 ---- 1605\n",
      "1727 ---- 1606\n",
      "1728 ---- 1607\n",
      "1729 ---- 1608\n",
      "1730 ---- 1609\n",
      "1731 ---- 1610\n",
      "1732 ---- 1611\n",
      "1733 ---- 1612\n",
      "1734 ---- 1613\n",
      "1735 ---- 1614\n",
      "1736 ---- 1615\n",
      "1737 ---- 1616\n",
      "1738 ---- 1617\n",
      "1739 ---- 1618\n",
      "1740 ---- 1619\n",
      "1741 ---- 1620\n",
      "1742 ---- 1621\n",
      "1743 ---- 1622\n",
      "1744 ---- 1623\n",
      "1745 ---- 1624\n",
      "1746 ---- 1625\n",
      "1747 ---- 1626\n",
      "1748 ---- 1627\n",
      "1749 ---- 1628\n",
      "1750 ---- 1629\n",
      "1751 ---- 1630\n",
      "1752 ---- 1631\n",
      "1753 ---- 1632\n",
      "1754 ---- 1633\n",
      "1755 ---- 1634\n",
      "1756 ---- 1635\n",
      "1757 ---- 1636\n",
      "1758 ---- 1637\n",
      "1759 ---- 1638\n",
      "1760 ---- 1639\n",
      "1761 ---- 1640\n",
      "1762 ---- 1641\n",
      "1763 ---- 1642\n",
      "1764 ---- 1643\n",
      "1765 ---- 1644\n",
      "1766 ---- 1645\n",
      "1767 ---- 1646\n",
      "1768 ---- 1647\n",
      "1769 ---- 1648\n",
      "1770 ---- 1649\n",
      "1771 ---- 1650\n",
      "1772 ---- 1651\n",
      "1773 ---- 1652\n",
      "1774 ---- 1653\n",
      "1775 ---- 1654\n",
      "1776 ---- 1655\n",
      "1777 ---- 1656\n",
      "1778 ---- 1657\n",
      "1779 ---- 1658\n",
      "1780 ---- 1659\n",
      "1781 ---- 1660\n",
      "1782 ---- 1661\n",
      "1783 ---- 1662\n",
      "1784 ---- 1663\n",
      "1785 ---- 1664\n",
      "1786 ---- 1665\n",
      "1787 ---- 1666\n",
      "1788 ---- 1667\n",
      "1789 ---- 1668\n",
      "1790 ---- 1669\n",
      "1791 ---- 1670\n",
      "1792 ---- 1671\n",
      "1793 ---- 1672\n",
      "1794 ---- 1673\n",
      "1795 ---- 1674\n",
      "1796 ---- 1675\n",
      "1797 ---- 1676\n",
      "1798 ---- 1677\n",
      "1799 ---- 1678\n",
      "1800 ---- 1679\n",
      "1801 ---- 1680\n",
      "1802 ---- 1681\n",
      "1803 ---- 1682\n",
      "1804 ---- 1683\n",
      "1805 ---- 1684\n",
      "1806 ---- 1685\n",
      "1807 ---- 1686\n",
      "1808 ---- 1687\n",
      "1809 ---- 1688\n",
      "1810 ---- 1689\n",
      "1811 ---- 1690\n",
      "1812 ---- 1691\n",
      "1813 ---- 1692\n",
      "1814 ---- 1693\n",
      "1815 ---- 1694\n",
      "1816 ---- 1695\n",
      "1817 ---- 1696\n",
      "1818 ---- 1697\n",
      "1819 ---- 1698\n",
      "1820 ---- 1699\n",
      "1821 ---- 1700\n",
      "1822 ---- 1701\n",
      "1823 ---- 1702\n",
      "1824 ---- 1703\n",
      "1825 ---- 1704\n",
      "1826 ---- 1705\n",
      "1827 ---- 1706\n",
      "1828 ---- 1707\n",
      "1829 ---- 1708\n",
      "1830 ---- 1709\n",
      "1831 ---- 1710\n",
      "1832 ---- 1711\n",
      "1833 ---- 1712\n",
      "1834 ---- 1713\n",
      "1835 ---- 1714\n",
      "1836 ---- 1715\n",
      "1837 ---- 1716\n",
      "1838 ---- 1717\n",
      "1839 ---- 1718\n",
      "1840 ---- 1719\n",
      "1841 ---- 1720\n",
      "1842 ---- 1721\n",
      "1843 ---- 1722\n",
      "1844 ---- 1723\n",
      "1845 ---- 1724\n",
      "1846 ---- 1725\n",
      "1847 ---- 1726\n",
      "1848 ---- 1727\n",
      "1849 ---- 1728\n",
      "1850 ---- 1729\n",
      "1851 ---- 1730\n",
      "1852 ---- 1731\n",
      "1853 ---- 1732\n",
      "1854 ---- 1733\n",
      "1855 ---- 1734\n",
      "1856 ---- 1735\n",
      "1857 ---- 1736\n",
      "1858 ---- 1737\n",
      "1859 ---- 1738\n",
      "1860 ---- 1739\n",
      "1861 ---- 1740\n",
      "1862 ---- 1741\n",
      "1863 ---- 1742\n",
      "1864 ---- 1743\n",
      "1865 ---- 1744\n",
      "1866 ---- 1745\n",
      "1867 ---- 1746\n",
      "1868 ---- 1747\n",
      "1869 ---- 1748\n",
      "1870 ---- 1749\n",
      "1871 ---- 1750\n",
      "1872 ---- 1751\n",
      "1873 ---- 1752\n",
      "1874 ---- 1753\n",
      "1875 ---- 1754\n",
      "1876 ---- 1755\n",
      "1877 ---- 1756\n",
      "1878 ---- 1757\n",
      "1879 ---- 1758\n",
      "1880 ---- 1759\n",
      "1881 ---- 1760\n",
      "1882 ---- 1761\n",
      "1883 ---- 1762\n",
      "1884 ---- 1763\n",
      "1885 ---- 1764\n",
      "1886 ---- 1765\n",
      "1887 ---- 1766\n",
      "1888 ---- 1767\n",
      "1889 ---- 1768\n",
      "1890 ---- 1769\n",
      "1891 ---- 1770\n",
      "1892 ---- 1771\n",
      "1893 ---- 1772\n",
      "1894 ---- 1773\n",
      "1895 ---- 1774\n",
      "1896 ---- 1775\n",
      "1897 ---- 1776\n",
      "1898 ---- 1777\n",
      "1899 ---- 1778\n",
      "1900 ---- 1779\n",
      "1901 ---- 1780\n",
      "1902 ---- 1781\n",
      "1903 ---- 1782\n",
      "1904 ---- 1783\n",
      "1905 ---- 1784\n",
      "1906 ---- 1785\n",
      "1907 ---- 1786\n",
      "1908 ---- 1787\n",
      "1909 ---- 1788\n",
      "1910 ---- 1789\n",
      "1911 ---- 1790\n",
      "1912 ---- 1791\n",
      "1913 ---- 1792\n",
      "1914 ---- 1793\n",
      "1915 ---- 1794\n",
      "1916 ---- 1795\n",
      "1917 ---- 1796\n",
      "1918 ---- 1797\n",
      "1919 ---- 1798\n",
      "1920 ---- 1799\n",
      "1921 ---- 1800\n",
      "1922 ---- 1801\n",
      "1923 ---- 1802\n",
      "1924 ---- 1803\n",
      "1925 ---- 1804\n",
      "1926 ---- 1805\n",
      "1927 ---- 1806\n",
      "1928 ---- 1807\n",
      "1929 ---- 1808\n",
      "1930 ---- 1809\n",
      "1931 ---- 1810\n",
      "1932 ---- 1811\n",
      "1933 ---- 1812\n",
      "1934 ---- 1813\n",
      "1935 ---- 1814\n",
      "1936 ---- 1815\n",
      "1937 ---- 1816\n",
      "1938 ---- 1817\n",
      "1939 ---- 1818\n",
      "1940 ---- 1819\n",
      "1941 ---- 1820\n",
      "1942 ---- 1821\n",
      "1943 ---- 1822\n",
      "1944 ---- 1823\n",
      "1945 ---- 1824\n",
      "1946 ---- 1825\n",
      "1947 ---- 1826\n",
      "1948 ---- 1827\n",
      "1949 ---- 1828\n",
      "1950 ---- 1829\n",
      "1951 ---- 1830\n",
      "1952 ---- 1831\n",
      "1953 ---- 1832\n",
      "1954 ---- 1833\n",
      "1955 ---- 1834\n",
      "1956 ---- 1835\n",
      "1957 ---- 1836\n",
      "1958 ---- 1837\n",
      "1959 ---- 1838\n",
      "1960 ---- 1839\n",
      "1961 ---- 1840\n",
      "1962 ---- 1841\n",
      "1963 ---- 1842\n",
      "1964 ---- 1843\n",
      "1965 ---- 1844\n",
      "1966 ---- 1845\n",
      "1967 ---- 1846\n",
      "1968 ---- 1847\n",
      "1969 ---- 1848\n",
      "1970 ---- 1849\n",
      "1971 ---- 1850\n",
      "1972 ---- 1851\n",
      "1973 ---- 1852\n",
      "1974 ---- 1853\n",
      "1975 ---- 1854\n",
      "1976 ---- 1855\n",
      "1977 ---- 1856\n",
      "1978 ---- 1857\n",
      "1979 ---- 1858\n",
      "1980 ---- 1859\n",
      "1981 ---- 1860\n",
      "1982 ---- 1861\n",
      "1983 ---- 1862\n",
      "1984 ---- 1863\n",
      "1985 ---- 1864\n",
      "1986 ---- 1865\n",
      "1987 ---- 1866\n",
      "1988 ---- 1867\n",
      "1989 ---- 1868\n",
      "1990 ---- 1869\n",
      "1991 ---- 1870\n",
      "1992 ---- 1871\n",
      "1993 ---- 1872\n",
      "1994 ---- 1873\n",
      "1995 ---- 1874\n",
      "1996 ---- 1875\n",
      "1997 ---- 1876\n",
      "1998 ---- 1877\n",
      "1999 ---- 1878\n",
      "2000 ---- 1879\n",
      "2001 ---- 1880\n",
      "2002 ---- 1881\n",
      "2003 ---- 1882\n",
      "2004 ---- 1883\n",
      "2005 ---- 1884\n",
      "2006 ---- 1885\n",
      "2007 ---- 1886\n",
      "2008 ---- 1887\n",
      "2009 ---- 1888\n",
      "2010 ---- 1889\n",
      "2011 ---- 1890\n",
      "2012 ---- 1891\n",
      "2013 ---- 1892\n",
      "2014 ---- 1893\n",
      "2015 ---- 1894\n",
      "2016 ---- 1895\n",
      "2017 ---- 1896\n",
      "2018 ---- 1897\n",
      "2019 ---- 1898\n",
      "2020 ---- 1899\n",
      "2021 ---- 1900\n",
      "2022 ---- 1901\n",
      "2023 ---- 1902\n",
      "2024 ---- 1903\n",
      "2025 ---- 1904\n",
      "2026 ---- 1905\n",
      "2027 ---- 1906\n",
      "2028 ---- 1907\n",
      "2029 ---- 1908\n",
      "2030 ---- 1909\n",
      "2031 ---- 1910\n",
      "2032 ---- 1911\n",
      "2033 ---- 1912\n",
      "2034 ---- 1913\n",
      "2035 ---- 1914\n",
      "2036 ---- 1915\n",
      "2037 ---- 1916\n",
      "2038 ---- 1917\n",
      "2039 ---- 1918\n",
      "2040 ---- 1919\n",
      "2041 ---- 1920\n",
      "2042 ---- 1921\n",
      "2043 ---- 1922\n",
      "2044 ---- 1923\n",
      "2045 ---- 1924\n",
      "2046 ---- 1925\n",
      "2047 ---- 1926\n",
      "2048 ---- 1927\n",
      "2049 ---- 1928\n",
      "2050 ---- 1929\n",
      "2051 ---- 1930\n",
      "2052 ---- 1931\n",
      "2053 ---- 1932\n",
      "2054 ---- 1933\n",
      "2055 ---- 1934\n",
      "2056 ---- 1935\n",
      "2057 ---- 1936\n",
      "2058 ---- 1937\n",
      "2059 ---- 1938\n",
      "2060 ---- 1939\n",
      "2061 ---- 1940\n",
      "2062 ---- 1941\n",
      "2063 ---- 1942\n",
      "2064 ---- 1943\n",
      "2065 ---- 1944\n",
      "2066 ---- 1945\n",
      "2067 ---- 1946\n",
      "2068 ---- 1947\n",
      "2069 ---- 1948\n",
      "2070 ---- 1949\n",
      "2071 ---- 1950\n",
      "2072 ---- 1951\n",
      "2073 ---- 1952\n",
      "2074 ---- 1953\n",
      "2075 ---- 1954\n",
      "2076 ---- 1955\n",
      "2077 ---- 1956\n",
      "2078 ---- 1957\n",
      "2079 ---- 1958\n",
      "2080 ---- 1959\n",
      "2081 ---- 1960\n",
      "2082 ---- 1961\n",
      "2083 ---- 1962\n",
      "2084 ---- 1963\n",
      "2085 ---- 1964\n",
      "2086 ---- 1965\n",
      "2087 ---- 1966\n",
      "2088 ---- 1967\n",
      "2089 ---- 1968\n",
      "2090 ---- 1969\n",
      "2091 ---- 1970\n",
      "2092 ---- 1971\n",
      "2093 ---- 1972\n",
      "2094 ---- 1973\n",
      "2095 ---- 1974\n",
      "2096 ---- 1975\n",
      "2097 ---- 1976\n",
      "2098 ---- 1977\n",
      "2099 ---- 1978\n",
      "2100 ---- 1979\n",
      "2101 ---- 1980\n",
      "2102 ---- 1981\n",
      "2103 ---- 1982\n",
      "2104 ---- 1983\n",
      "2105 ---- 1984\n",
      "2106 ---- 1985\n",
      "2107 ---- 1986\n",
      "2108 ---- 1987\n",
      "2109 ---- 1988\n",
      "2110 ---- 1989\n",
      "2111 ---- 1990\n",
      "2112 ---- 1991\n",
      "2113 ---- 1992\n",
      "2114 ---- 1993\n",
      "2115 ---- 1994\n",
      "2116 ---- 1995\n",
      "2117 ---- 1996\n",
      "2118 ---- 1997\n",
      "2119 ---- 1998\n",
      "2120 ---- 1999\n",
      "2121 ---- 2000\n",
      "2122 ---- 2001\n",
      "2123 ---- 2002\n",
      "2124 ---- 2003\n",
      "2125 ---- 2004\n",
      "2126 ---- 2005\n",
      "2127 ---- 2006\n",
      "2128 ---- 2007\n",
      "2129 ---- 2008\n",
      "2130 ---- 2009\n",
      "2131 ---- 2010\n",
      "2132 ---- 2011\n",
      "2133 ---- 2012\n",
      "2134 ---- 2013\n",
      "2135 ---- 2014\n",
      "2136 ---- 2015\n",
      "2137 ---- 2016\n",
      "2138 ---- 2017\n",
      "2139 ---- 2018\n",
      "2140 ---- 2019\n",
      "2141 ---- 2020\n",
      "2142 ---- 2021\n",
      "2143 ---- 2022\n",
      "2144 ---- 2023\n",
      "2145 ---- 2024\n",
      "2146 ---- 2025\n",
      "2147 ---- 2026\n",
      "2148 ---- 2027\n",
      "2149 ---- 2028\n",
      "2150 ---- 2029\n",
      "2151 ---- 2030\n",
      "2152 ---- 2031\n",
      "2153 ---- 2032\n",
      "2154 ---- 2033\n",
      "2155 ---- 2034\n",
      "2156 ---- 2035\n",
      "2157 ---- 2036\n",
      "2158 ---- 2037\n",
      "2159 ---- 2038\n",
      "2160 ---- 2039\n",
      "2161 ---- 2040\n",
      "2162 ---- 2041\n",
      "2163 ---- 2042\n",
      "2164 ---- 2043\n",
      "2165 ---- 2044\n",
      "2166 ---- 2045\n",
      "2167 ---- 2046\n",
      "2168 ---- 2047\n",
      "2169 ---- 2048\n",
      "2170 ---- 2049\n",
      "2171 ---- 2050\n",
      "2172 ---- 2051\n",
      "2173 ---- 2052\n",
      "2174 ---- 2053\n",
      "2175 ---- 2054\n",
      "2176 ---- 2055\n",
      "2177 ---- 2056\n",
      "2178 ---- 2057\n",
      "2179 ---- 2058\n",
      "2180 ---- 2059\n",
      "2181 ---- 2060\n",
      "2182 ---- 2061\n",
      "2183 ---- 2062\n",
      "2184 ---- 2063\n",
      "2185 ---- 2064\n",
      "2186 ---- 2065\n",
      "2187 ---- 2066\n",
      "2188 ---- 2067\n",
      "2189 ---- 2068\n",
      "2190 ---- 2069\n",
      "2191 ---- 2070\n",
      "2192 ---- 2071\n",
      "2193 ---- 2072\n",
      "2194 ---- 2073\n",
      "2195 ---- 2074\n",
      "2196 ---- 2075\n",
      "2197 ---- 2076\n",
      "2198 ---- 2077\n",
      "2199 ---- 2078\n",
      "2200 ---- 2079\n",
      "2201 ---- 2080\n",
      "2202 ---- 2081\n",
      "2203 ---- 2082\n",
      "2204 ---- 2083\n",
      "2205 ---- 2084\n",
      "2206 ---- 2085\n",
      "2207 ---- 2086\n",
      "2208 ---- 2087\n",
      "2209 ---- 2088\n",
      "2210 ---- 2089\n",
      "2211 ---- 2090\n",
      "2212 ---- 2091\n",
      "2213 ---- 2092\n",
      "2214 ---- 2093\n",
      "2215 ---- 2094\n",
      "2216 ---- 2095\n",
      "2217 ---- 2096\n",
      "2218 ---- 2097\n",
      "2219 ---- 2098\n",
      "2220 ---- 2099\n",
      "2221 ---- 2100\n",
      "2222 ---- 2101\n",
      "2223 ---- 2102\n",
      "2224 ---- 2103\n",
      "2225 ---- 2104\n",
      "2226 ---- 2105\n",
      "2227 ---- 2106\n",
      "2228 ---- 2107\n",
      "2229 ---- 2108\n",
      "2230 ---- 2109\n",
      "2231 ---- 2110\n",
      "2232 ---- 2111\n",
      "2233 ---- 2112\n",
      "2234 ---- 2113\n",
      "2235 ---- 2114\n",
      "2236 ---- 2115\n",
      "2237 ---- 2116\n",
      "2238 ---- 2117\n",
      "2239 ---- 2118\n",
      "2240 ---- 2119\n",
      "2241 ---- 2120\n",
      "2242 ---- 2121\n",
      "2243 ---- 2122\n",
      "2244 ---- 2123\n",
      "2245 ---- 2124\n",
      "2246 ---- 2125\n",
      "2247 ---- 2126\n",
      "2248 ---- 2127\n",
      "2249 ---- 2128\n",
      "2250 ---- 2129\n",
      "2251 ---- 2130\n",
      "2252 ---- 2131\n",
      "2253 ---- 2132\n",
      "2254 ---- 2133\n",
      "2255 ---- 2134\n",
      "2256 ---- 2135\n",
      "2257 ---- 2136\n",
      "2258 ---- 2137\n",
      "2259 ---- 2138\n",
      "2260 ---- 2139\n",
      "2261 ---- 2140\n",
      "2262 ---- 2141\n",
      "2263 ---- 2142\n",
      "2264 ---- 2143\n",
      "2265 ---- 2144\n",
      "2266 ---- 2145\n",
      "2267 ---- 2146\n",
      "2268 ---- 2147\n",
      "2269 ---- 2148\n",
      "2270 ---- 2149\n",
      "2271 ---- 2150\n",
      "2272 ---- 2151\n",
      "2273 ---- 2152\n",
      "2274 ---- 2153\n",
      "2275 ---- 2154\n",
      "2276 ---- 2155\n",
      "2277 ---- 2156\n",
      "2278 ---- 2157\n",
      "2279 ---- 2158\n",
      "2280 ---- 2159\n",
      "2281 ---- 2160\n",
      "2282 ---- 2161\n",
      "2283 ---- 2162\n",
      "2284 ---- 2163\n",
      "2285 ---- 2164\n",
      "2286 ---- 2165\n",
      "2287 ---- 2166\n",
      "2288 ---- 2167\n",
      "2289 ---- 2168\n",
      "2290 ---- 2169\n",
      "2291 ---- 2170\n",
      "2292 ---- 2171\n",
      "2293 ---- 2172\n",
      "2294 ---- 2173\n",
      "2295 ---- 2174\n",
      "2296 ---- 2175\n",
      "2297 ---- 2176\n",
      "2298 ---- 2177\n",
      "2299 ---- 2178\n",
      "2300 ---- 2179\n",
      "2301 ---- 2180\n",
      "2302 ---- 2181\n",
      "2303 ---- 2182\n",
      "2304 ---- 2183\n",
      "2305 ---- 2184\n",
      "2306 ---- 2185\n",
      "2307 ---- 2186\n",
      "2308 ---- 2187\n",
      "2309 ---- 2188\n",
      "2310 ---- 2189\n",
      "2311 ---- 2190\n",
      "2312 ---- 2191\n",
      "2313 ---- 2192\n",
      "2314 ---- 2193\n",
      "2315 ---- 2194\n",
      "2316 ---- 2195\n",
      "2317 ---- 2196\n",
      "2318 ---- 2197\n",
      "2319 ---- 2198\n",
      "2320 ---- 2199\n",
      "2321 ---- 2200\n",
      "2322 ---- 2201\n",
      "2323 ---- 2202\n",
      "2324 ---- 2203\n",
      "2325 ---- 2204\n",
      "2326 ---- 2205\n",
      "2327 ---- 2206\n",
      "2328 ---- 2207\n",
      "2329 ---- 2208\n",
      "2330 ---- 2209\n",
      "2331 ---- 2210\n",
      "2332 ---- 2211\n",
      "2333 ---- 2212\n",
      "2334 ---- 2213\n",
      "2335 ---- 2214\n",
      "2336 ---- 2215\n",
      "2337 ---- 2216\n",
      "2338 ---- 2217\n",
      "2339 ---- 2218\n",
      "2340 ---- 2219\n",
      "2341 ---- 2220\n",
      "2342 ---- 2221\n",
      "2343 ---- 2222\n",
      "2344 ---- 2223\n",
      "2345 ---- 2224\n",
      "2346 ---- 2225\n",
      "2347 ---- 2226\n",
      "2348 ---- 2227\n",
      "2349 ---- 2228\n",
      "2350 ---- 2229\n",
      "2351 ---- 2230\n",
      "2352 ---- 2231\n",
      "2353 ---- 2232\n",
      "2354 ---- 2233\n",
      "2355 ---- 2234\n",
      "2356 ---- 2235\n",
      "2357 ---- 2236\n",
      "2358 ---- 2237\n",
      "2359 ---- 2238\n",
      "2360 ---- 2239\n",
      "2361 ---- 2240\n",
      "2362 ---- 2241\n",
      "2363 ---- 2242\n",
      "2364 ---- 2243\n",
      "2365 ---- 2244\n",
      "2366 ---- 2245\n",
      "2367 ---- 2246\n",
      "2368 ---- 2247\n",
      "2369 ---- 2248\n",
      "2370 ---- 2249\n",
      "2371 ---- 2250\n",
      "2372 ---- 2251\n",
      "2373 ---- 2252\n",
      "2374 ---- 2253\n",
      "2375 ---- 2254\n",
      "2376 ---- 2255\n",
      "2377 ---- 2256\n",
      "2378 ---- 2257\n",
      "2379 ---- 2258\n",
      "2380 ---- 2259\n",
      "2381 ---- 2260\n",
      "2382 ---- 2261\n",
      "2383 ---- 2262\n",
      "2384 ---- 2263\n",
      "2385 ---- 2264\n",
      "2386 ---- 2265\n",
      "2387 ---- 2266\n",
      "2388 ---- 2267\n",
      "2389 ---- 2268\n",
      "2390 ---- 2269\n",
      "2391 ---- 2270\n",
      "2392 ---- 2271\n",
      "2393 ---- 2272\n",
      "2394 ---- 2273\n",
      "2395 ---- 2274\n",
      "2396 ---- 2275\n",
      "2397 ---- 2276\n",
      "2398 ---- 2277\n",
      "2399 ---- 2278\n",
      "2400 ---- 2279\n",
      "2401 ---- 2280\n",
      "2402 ---- 2281\n",
      "2403 ---- 2282\n",
      "2404 ---- 2283\n",
      "2405 ---- 2284\n",
      "2406 ---- 2285\n",
      "2407 ---- 2286\n",
      "2408 ---- 2287\n",
      "2409 ---- 2288\n",
      "2410 ---- 2289\n",
      "2411 ---- 2290\n",
      "2412 ---- 2291\n",
      "2413 ---- 2292\n",
      "2414 ---- 2293\n",
      "2415 ---- 2294\n",
      "2416 ---- 2295\n",
      "2417 ---- 2296\n",
      "2418 ---- 2297\n",
      "2419 ---- 2298\n",
      "2420 ---- 2299\n",
      "2421 ---- 2300\n",
      "2422 ---- 2301\n",
      "2423 ---- 2302\n",
      "2424 ---- 2303\n",
      "2425 ---- 2304\n",
      "2426 ---- 2305\n",
      "2427 ---- 2306\n",
      "2428 ---- 2307\n",
      "2429 ---- 2308\n",
      "2430 ---- 2309\n",
      "2431 ---- 2310\n",
      "2432 ---- 2311\n",
      "2433 ---- 2312\n",
      "2434 ---- 2313\n",
      "2435 ---- 2314\n",
      "2436 ---- 2315\n",
      "2437 ---- 2316\n",
      "2438 ---- 2317\n",
      "2439 ---- 2318\n",
      "2440 ---- 2319\n",
      "2441 ---- 2320\n",
      "2442 ---- 2321\n",
      "2443 ---- 2322\n",
      "2444 ---- 2323\n",
      "2445 ---- 2324\n",
      "2446 ---- 2325\n",
      "2447 ---- 2326\n",
      "2448 ---- 2327\n",
      "2449 ---- 2328\n",
      "2450 ---- 2329\n",
      "2451 ---- 2330\n",
      "2452 ---- 2331\n",
      "2453 ---- 2332\n",
      "2454 ---- 2333\n",
      "2455 ---- 2334\n",
      "2456 ---- 2335\n",
      "2457 ---- 2336\n",
      "2458 ---- 2337\n",
      "2459 ---- 2338\n",
      "2460 ---- 2339\n",
      "2461 ---- 2340\n",
      "2462 ---- 2341\n",
      "2463 ---- 2342\n",
      "2464 ---- 2343\n",
      "2465 ---- 2344\n",
      "2466 ---- 2345\n",
      "2467 ---- 2346\n",
      "2468 ---- 2347\n",
      "2469 ---- 2348\n",
      "2470 ---- 2349\n",
      "2471 ---- 2350\n",
      "2472 ---- 2351\n",
      "2473 ---- 2352\n",
      "2474 ---- 2353\n",
      "2475 ---- 2354\n",
      "2476 ---- 2355\n",
      "2477 ---- 2356\n",
      "2478 ---- 2357\n",
      "2479 ---- 2358\n",
      "2480 ---- 2359\n",
      "2481 ---- 2360\n",
      "2482 ---- 2361\n",
      "2483 ---- 2362\n",
      "2484 ---- 2363\n",
      "2485 ---- 2364\n",
      "2486 ---- 2365\n",
      "2487 ---- 2366\n",
      "2488 ---- 2367\n",
      "2489 ---- 2368\n",
      "2490 ---- 2369\n",
      "2491 ---- 2370\n",
      "2492 ---- 2371\n",
      "2493 ---- 2372\n",
      "2494 ---- 2373\n",
      "2495 ---- 2374\n",
      "2496 ---- 2375\n",
      "2497 ---- 2376\n",
      "2498 ---- 2377\n",
      "2499 ---- 2378\n",
      "2500 ---- 2379\n",
      "2501 ---- 2380\n",
      "2502 ---- 2381\n",
      "2503 ---- 2382\n",
      "2504 ---- 2383\n",
      "2505 ---- 2384\n",
      "2506 ---- 2385\n",
      "2507 ---- 2386\n",
      "2508 ---- 2387\n",
      "2509 ---- 2388\n",
      "2510 ---- 2389\n",
      "2511 ---- 2390\n",
      "2512 ---- 2391\n",
      "2513 ---- 2392\n",
      "2514 ---- 2393\n",
      "2515 ---- 2394\n",
      "2516 ---- 2395\n",
      "2517 ---- 2396\n",
      "2518 ---- 2397\n",
      "2519 ---- 2398\n",
      "2520 ---- 2399\n",
      "2521 ---- 2400\n",
      "2522 ---- 2401\n",
      "2523 ---- 2402\n",
      "2524 ---- 2403\n",
      "2525 ---- 2404\n",
      "2526 ---- 2405\n",
      "2527 ---- 2406\n",
      "2528 ---- 2407\n",
      "2529 ---- 2408\n",
      "2530 ---- 2409\n",
      "2531 ---- 2410\n",
      "2532 ---- 2411\n",
      "2533 ---- 2412\n",
      "2534 ---- 2413\n",
      "2535 ---- 2414\n",
      "2536 ---- 2415\n",
      "2537 ---- 2416\n",
      "2538 ---- 2417\n",
      "2539 ---- 2418\n",
      "2540 ---- 2419\n",
      "2541 ---- 2420\n",
      "2542 ---- 2421\n",
      "2543 ---- 2422\n",
      "2544 ---- 2423\n",
      "2545 ---- 2424\n",
      "2546 ---- 2425\n",
      "2547 ---- 2426\n",
      "2548 ---- 2427\n",
      "2549 ---- 2428\n",
      "2550 ---- 2429\n",
      "2551 ---- 2430\n",
      "2552 ---- 2431\n",
      "2553 ---- 2432\n",
      "2554 ---- 2433\n",
      "2555 ---- 2434\n",
      "2556 ---- 2435\n",
      "2557 ---- 2436\n",
      "2558 ---- 2437\n",
      "2559 ---- 2438\n",
      "2560 ---- 2439\n",
      "2561 ---- 2440\n",
      "2562 ---- 2441\n",
      "2563 ---- 2442\n",
      "2564 ---- 2443\n",
      "2565 ---- 2444\n",
      "2566 ---- 2445\n",
      "2567 ---- 2446\n",
      "2568 ---- 2447\n",
      "2569 ---- 2448\n",
      "2570 ---- 2449\n",
      "2571 ---- 2450\n",
      "2572 ---- 2451\n",
      "2573 ---- 2452\n",
      "2574 ---- 2453\n",
      "2575 ---- 2454\n",
      "2576 ---- 2455\n",
      "2577 ---- 2456\n",
      "2578 ---- 2457\n",
      "2579 ---- 2458\n",
      "2580 ---- 2459\n",
      "2581 ---- 2460\n",
      "2582 ---- 2461\n",
      "2583 ---- 2462\n",
      "2584 ---- 2463\n",
      "2585 ---- 2464\n",
      "2586 ---- 2465\n",
      "2587 ---- 2466\n",
      "2588 ---- 2467\n",
      "2589 ---- 2468\n",
      "2590 ---- 2469\n",
      "2591 ---- 2470\n",
      "2592 ---- 2471\n",
      "2593 ---- 2472\n",
      "2594 ---- 2473\n",
      "2595 ---- 2474\n",
      "2596 ---- 2475\n",
      "2597 ---- 2476\n",
      "2598 ---- 2477\n",
      "2599 ---- 2478\n",
      "2600 ---- 2479\n",
      "2601 ---- 2480\n",
      "2602 ---- 2481\n",
      "2603 ---- 2482\n",
      "2604 ---- 2483\n",
      "2605 ---- 2484\n",
      "2606 ---- 2485\n",
      "2607 ---- 2486\n",
      "2608 ---- 2487\n",
      "2609 ---- 2488\n",
      "2610 ---- 2489\n",
      "2611 ---- 2490\n",
      "2612 ---- 2491\n",
      "2613 ---- 2492\n",
      "2614 ---- 2493\n",
      "2615 ---- 2494\n",
      "2616 ---- 2495\n",
      "2617 ---- 2496\n",
      "2618 ---- 2497\n",
      "2619 ---- 2498\n",
      "2620 ---- 2499\n",
      "2621 ---- 2500\n",
      "2622 ---- 2501\n",
      "2623 ---- 2502\n",
      "2624 ---- 2503\n",
      "2625 ---- 2504\n",
      "2626 ---- 2505\n",
      "2627 ---- 2506\n",
      "2628 ---- 2507\n",
      "2629 ---- 2508\n",
      "2630 ---- 2509\n",
      "2631 ---- 2510\n",
      "2632 ---- 2511\n",
      "2633 ---- 2512\n",
      "2634 ---- 2513\n",
      "2635 ---- 2514\n",
      "2636 ---- 2515\n",
      "2637 ---- 2516\n",
      "2638 ---- 2517\n",
      "2639 ---- 2518\n",
      "2640 ---- 2519\n",
      "2641 ---- 2520\n",
      "2642 ---- 2521\n",
      "2643 ---- 2522\n",
      "2644 ---- 2523\n",
      "2645 ---- 2524\n",
      "2646 ---- 2525\n",
      "2647 ---- 2526\n",
      "2648 ---- 2527\n",
      "2649 ---- 2528\n",
      "2650 ---- 2529\n",
      "2651 ---- 2530\n",
      "2652 ---- 2531\n",
      "2653 ---- 2532\n",
      "2654 ---- 2533\n",
      "2655 ---- 2534\n",
      "2656 ---- 2535\n",
      "2657 ---- 2536\n",
      "2658 ---- 2537\n",
      "2659 ---- 2538\n",
      "2660 ---- 2539\n",
      "2661 ---- 2540\n",
      "2662 ---- 2541\n",
      "2663 ---- 2542\n",
      "2664 ---- 2543\n",
      "2665 ---- 2544\n",
      "2666 ---- 2545\n",
      "2667 ---- 2546\n",
      "2668 ---- 2547\n",
      "2669 ---- 2548\n",
      "2670 ---- 2549\n",
      "2671 ---- 2550\n",
      "2672 ---- 2551\n",
      "2673 ---- 2552\n",
      "2674 ---- 2553\n",
      "2675 ---- 2554\n",
      "2676 ---- 2555\n",
      "2677 ---- 2556\n",
      "2678 ---- 2557\n",
      "2679 ---- 2558\n",
      "2680 ---- 2559\n",
      "2681 ---- 2560\n",
      "2682 ---- 2561\n",
      "2683 ---- 2562\n",
      "2684 ---- 2563\n",
      "2685 ---- 2564\n",
      "2686 ---- 2565\n",
      "2687 ---- 2566\n",
      "2688 ---- 2567\n",
      "2689 ---- 2568\n",
      "2690 ---- 2569\n",
      "2691 ---- 2570\n",
      "2692 ---- 2571\n",
      "2693 ---- 2572\n",
      "2694 ---- 2573\n",
      "2695 ---- 2574\n",
      "2696 ---- 2575\n",
      "2697 ---- 2576\n",
      "2698 ---- 2577\n",
      "2699 ---- 2578\n",
      "2700 ---- 2579\n",
      "2701 ---- 2580\n",
      "2702 ---- 2581\n",
      "2703 ---- 2582\n",
      "2704 ---- 2583\n",
      "2705 ---- 2584\n",
      "2706 ---- 2585\n",
      "2707 ---- 2586\n",
      "2708 ---- 2587\n",
      "2709 ---- 2588\n",
      "2710 ---- 2589\n",
      "2711 ---- 2590\n",
      "2712 ---- 2591\n",
      "2713 ---- 2592\n",
      "2714 ---- 2593\n",
      "2715 ---- 2594\n",
      "2716 ---- 2595\n",
      "2717 ---- 2596\n",
      "2718 ---- 2597\n",
      "2719 ---- 2598\n",
      "2720 ---- 2599\n",
      "2721 ---- 2600\n",
      "2722 ---- 2601\n",
      "2723 ---- 2602\n",
      "2724 ---- 2603\n",
      "2725 ---- 2604\n",
      "2726 ---- 2605\n",
      "2727 ---- 2606\n",
      "2728 ---- 2607\n",
      "2729 ---- 2608\n",
      "2730 ---- 2609\n",
      "2731 ---- 2610\n",
      "2732 ---- 2611\n",
      "2733 ---- 2612\n",
      "2734 ---- 2613\n",
      "2735 ---- 2614\n",
      "2736 ---- 2615\n",
      "2737 ---- 2616\n",
      "2738 ---- 2617\n",
      "2739 ---- 2618\n",
      "2740 ---- 2619\n",
      "2741 ---- 2620\n",
      "2742 ---- 2621\n",
      "2743 ---- 2622\n",
      "2744 ---- 2623\n",
      "2745 ---- 2624\n",
      "2746 ---- 2625\n",
      "2747 ---- 2626\n",
      "2748 ---- 2627\n",
      "2749 ---- 2628\n",
      "2750 ---- 2629\n",
      "2751 ---- 2630\n",
      "2752 ---- 2631\n",
      "2753 ---- 2632\n",
      "2754 ---- 2633\n",
      "2755 ---- 2634\n",
      "2756 ---- 2635\n",
      "2757 ---- 2636\n",
      "2758 ---- 2637\n",
      "2759 ---- 2638\n",
      "2760 ---- 2639\n",
      "2761 ---- 2640\n",
      "2762 ---- 2641\n",
      "2763 ---- 2642\n",
      "2764 ---- 2643\n",
      "2765 ---- 2644\n",
      "2766 ---- 2645\n",
      "2767 ---- 2646\n",
      "2768 ---- 2647\n",
      "2769 ---- 2648\n",
      "2770 ---- 2649\n",
      "2771 ---- 2650\n",
      "2772 ---- 2651\n",
      "2773 ---- 2652\n",
      "2774 ---- 2653\n",
      "2775 ---- 2654\n",
      "2776 ---- 2655\n",
      "2777 ---- 2656\n",
      "2778 ---- 2657\n",
      "2779 ---- 2658\n",
      "2780 ---- 2659\n",
      "2781 ---- 2660\n",
      "2782 ---- 2661\n",
      "2783 ---- 2662\n",
      "2784 ---- 2663\n",
      "2785 ---- 2664\n",
      "2786 ---- 2665\n",
      "2787 ---- 2666\n",
      "2788 ---- 2667\n",
      "2789 ---- 2668\n",
      "2790 ---- 2669\n",
      "2791 ---- 2670\n",
      "2792 ---- 2671\n",
      "2793 ---- 2672\n",
      "2794 ---- 2673\n",
      "2795 ---- 2674\n",
      "2796 ---- 2675\n",
      "2797 ---- 2676\n",
      "2798 ---- 2677\n",
      "2799 ---- 2678\n",
      "2800 ---- 2679\n",
      "2801 ---- 2680\n",
      "2802 ---- 2681\n",
      "2803 ---- 2682\n",
      "2804 ---- 2683\n",
      "2805 ---- 2684\n",
      "2806 ---- 2685\n",
      "2807 ---- 2686\n",
      "2808 ---- 2687\n",
      "2809 ---- 2688\n",
      "2810 ---- 2689\n",
      "2811 ---- 2690\n",
      "2812 ---- 2691\n",
      "2813 ---- 2692\n",
      "2814 ---- 2693\n",
      "2815 ---- 2694\n",
      "2816 ---- 2695\n",
      "2817 ---- 2696\n",
      "2818 ---- 2697\n",
      "2819 ---- 2698\n",
      "2820 ---- 2699\n",
      "2821 ---- 2700\n",
      "2822 ---- 2701\n",
      "2823 ---- 2702\n",
      "2824 ---- 2703\n",
      "2825 ---- 2704\n",
      "2826 ---- 2705\n",
      "2827 ---- 2706\n",
      "2828 ---- 2707\n",
      "2829 ---- 2708\n",
      "2830 ---- 2709\n",
      "2831 ---- 2710\n",
      "2832 ---- 2711\n",
      "2833 ---- 2712\n",
      "2834 ---- 2713\n",
      "2835 ---- 2714\n",
      "2836 ---- 2715\n",
      "2837 ---- 2716\n",
      "2838 ---- 2717\n",
      "2839 ---- 2718\n",
      "2840 ---- 2719\n",
      "2841 ---- 2720\n",
      "2842 ---- 2721\n",
      "2843 ---- 2722\n",
      "2844 ---- 2723\n",
      "2845 ---- 2724\n",
      "2846 ---- 2725\n",
      "2847 ---- 2726\n",
      "2848 ---- 2727\n",
      "2849 ---- 2728\n",
      "2850 ---- 2729\n",
      "2851 ---- 2730\n",
      "2852 ---- 2731\n",
      "2853 ---- 2732\n",
      "2854 ---- 2733\n",
      "2855 ---- 2734\n",
      "2856 ---- 2735\n",
      "2857 ---- 2736\n",
      "2858 ---- 2737\n",
      "2859 ---- 2738\n",
      "2860 ---- 2739\n",
      "2861 ---- 2740\n",
      "2862 ---- 2741\n",
      "2863 ---- 2742\n",
      "2864 ---- 2743\n",
      "2865 ---- 2744\n",
      "2866 ---- 2745\n",
      "2867 ---- 2746\n",
      "2868 ---- 2747\n",
      "2869 ---- 2748\n",
      "2870 ---- 2749\n",
      "2871 ---- 2750\n",
      "2872 ---- 2751\n",
      "2873 ---- 2752\n",
      "2874 ---- 2753\n",
      "2875 ---- 2754\n",
      "2876 ---- 2755\n",
      "2877 ---- 2756\n",
      "2878 ---- 2757\n",
      "2879 ---- 2758\n",
      "2880 ---- 2759\n",
      "2881 ---- 2760\n",
      "2882 ---- 2761\n",
      "2883 ---- 2762\n",
      "2884 ---- 2763\n",
      "2885 ---- 2764\n",
      "2886 ---- 2765\n",
      "2887 ---- 2766\n",
      "2888 ---- 2767\n",
      "2889 ---- 2768\n",
      "2890 ---- 2769\n",
      "2891 ---- 2770\n",
      "2892 ---- 2771\n",
      "2893 ---- 2772\n",
      "2894 ---- 2773\n",
      "2895 ---- 2774\n",
      "2896 ---- 2775\n",
      "2897 ---- 2776\n",
      "2898 ---- 2777\n",
      "2899 ---- 2778\n",
      "2900 ---- 2779\n",
      "2901 ---- 2780\n",
      "2902 ---- 2781\n",
      "2903 ---- 2782\n",
      "2904 ---- 2783\n",
      "2905 ---- 2784\n",
      "2906 ---- 2785\n",
      "2907 ---- 2786\n",
      "2908 ---- 2787\n",
      "2909 ---- 2788\n",
      "2910 ---- 2789\n",
      "2911 ---- 2790\n",
      "2912 ---- 2791\n",
      "2913 ---- 2792\n",
      "2914 ---- 2793\n",
      "2915 ---- 2794\n",
      "2916 ---- 2795\n",
      "2917 ---- 2796\n",
      "2918 ---- 2797\n",
      "2919 ---- 2798\n",
      "2920 ---- 2799\n",
      "2921 ---- 2800\n",
      "2922 ---- 2801\n",
      "2923 ---- 2802\n",
      "2924 ---- 2803\n",
      "2925 ---- 2804\n",
      "2926 ---- 2805\n",
      "2927 ---- 2806\n",
      "2928 ---- 2807\n",
      "2929 ---- 2808\n",
      "2930 ---- 2809\n",
      "2931 ---- 2810\n",
      "2932 ---- 2811\n",
      "2933 ---- 2812\n",
      "2934 ---- 2813\n",
      "2935 ---- 2814\n",
      "2936 ---- 2815\n",
      "2937 ---- 2816\n",
      "2938 ---- 2817\n",
      "2939 ---- 2818\n",
      "2940 ---- 2819\n",
      "2941 ---- 2820\n",
      "2942 ---- 2821\n",
      "2943 ---- 2822\n",
      "2944 ---- 2823\n",
      "2945 ---- 2824\n",
      "2946 ---- 2825\n",
      "2947 ---- 2826\n",
      "2948 ---- 2827\n",
      "2949 ---- 2828\n",
      "2950 ---- 2829\n",
      "2951 ---- 2830\n",
      "2952 ---- 2831\n",
      "2953 ---- 2832\n",
      "2954 ---- 2833\n",
      "2955 ---- 2834\n",
      "2956 ---- 2835\n",
      "2957 ---- 2836\n",
      "2958 ---- 2837\n",
      "2959 ---- 2838\n",
      "2960 ---- 2839\n",
      "2961 ---- 2840\n",
      "2962 ---- 2841\n",
      "2963 ---- 2842\n",
      "2964 ---- 2843\n",
      "2965 ---- 2844\n",
      "2966 ---- 2845\n",
      "2967 ---- 2846\n",
      "2968 ---- 2847\n",
      "2969 ---- 2848\n",
      "2970 ---- 2849\n",
      "2971 ---- 2850\n",
      "2972 ---- 2851\n",
      "2973 ---- 2852\n",
      "2974 ---- 2853\n",
      "2975 ---- 2854\n",
      "2976 ---- 2855\n",
      "2977 ---- 2856\n",
      "2978 ---- 2857\n",
      "2979 ---- 2858\n",
      "2980 ---- 2859\n",
      "2981 ---- 2860\n",
      "2982 ---- 2861\n",
      "2983 ---- 2862\n",
      "2984 ---- 2863\n",
      "2985 ---- 2864\n",
      "2986 ---- 2865\n",
      "2987 ---- 2866\n",
      "2988 ---- 2867\n",
      "2989 ---- 2868\n",
      "2990 ---- 2869\n",
      "2991 ---- 2870\n",
      "2992 ---- 2871\n",
      "2993 ---- 2872\n",
      "2994 ---- 2873\n",
      "2995 ---- 2874\n",
      "2996 ---- 2875\n",
      "2997 ---- 2876\n",
      "2998 ---- 2877\n",
      "2999 ---- 2878\n",
      "3000 ---- 2879\n",
      "3001 ---- 2880\n",
      "3002 ---- 2881\n",
      "3003 ---- 2882\n",
      "3004 ---- 2883\n",
      "3005 ---- 2884\n",
      "3006 ---- 2885\n",
      "3007 ---- 2886\n",
      "3008 ---- 2887\n",
      "3009 ---- 2888\n",
      "3010 ---- 2889\n",
      "3011 ---- 2890\n",
      "3012 ---- 2891\n",
      "3013 ---- 2892\n",
      "3014 ---- 2893\n",
      "3015 ---- 2894\n",
      "3016 ---- 2895\n",
      "3017 ---- 2896\n",
      "3018 ---- 2897\n",
      "3019 ---- 2898\n",
      "3020 ---- 2899\n",
      "3021 ---- 2900\n",
      "3022 ---- 2901\n",
      "3023 ---- 2902\n",
      "3024 ---- 2903\n",
      "3025 ---- 2904\n",
      "3026 ---- 2905\n",
      "3027 ---- 2906\n",
      "3028 ---- 2907\n",
      "3029 ---- 2908\n",
      "3030 ---- 2909\n",
      "3031 ---- 2910\n",
      "3032 ---- 2911\n",
      "3033 ---- 2912\n",
      "3034 ---- 2913\n",
      "3035 ---- 2914\n",
      "3036 ---- 2915\n",
      "3037 ---- 2916\n",
      "3038 ---- 2917\n",
      "3039 ---- 2918\n",
      "3040 ---- 2919\n",
      "3041 ---- 2920\n",
      "3042 ---- 2921\n",
      "3043 ---- 2922\n",
      "3044 ---- 2923\n",
      "3045 ---- 2924\n",
      "3046 ---- 2925\n",
      "3047 ---- 2926\n",
      "3048 ---- 2927\n",
      "3049 ---- 2928\n",
      "3050 ---- 2929\n",
      "3051 ---- 2930\n",
      "3052 ---- 2931\n",
      "3053 ---- 2932\n",
      "3054 ---- 2933\n",
      "3055 ---- 2934\n",
      "3056 ---- 2935\n",
      "3057 ---- 2936\n",
      "3058 ---- 2937\n",
      "3059 ---- 2938\n",
      "3060 ---- 2939\n",
      "3061 ---- 2940\n",
      "3062 ---- 2941\n",
      "3063 ---- 2942\n",
      "3064 ---- 2943\n",
      "3065 ---- 2944\n",
      "3066 ---- 2945\n",
      "3067 ---- 2946\n",
      "3068 ---- 2947\n",
      "3069 ---- 2948\n",
      "3070 ---- 2949\n",
      "3071 ---- 2950\n",
      "3072 ---- 2951\n",
      "3073 ---- 2952\n",
      "3074 ---- 2953\n",
      "3075 ---- 2954\n",
      "3076 ---- 2955\n",
      "3077 ---- 2956\n",
      "3078 ---- 2957\n",
      "3079 ---- 2958\n",
      "3080 ---- 2959\n",
      "3081 ---- 2960\n",
      "3082 ---- 2961\n",
      "3083 ---- 2962\n",
      "3084 ---- 2963\n",
      "3085 ---- 2964\n",
      "3086 ---- 2965\n",
      "3087 ---- 2966\n",
      "3088 ---- 2967\n",
      "3089 ---- 2968\n",
      "3090 ---- 2969\n",
      "3091 ---- 2970\n",
      "3092 ---- 2971\n",
      "3093 ---- 2972\n",
      "3094 ---- 2973\n",
      "3095 ---- 2974\n",
      "3096 ---- 2975\n",
      "3097 ---- 2976\n",
      "3098 ---- 2977\n",
      "3099 ---- 2978\n",
      "3100 ---- 2979\n",
      "3101 ---- 2980\n",
      "3102 ---- 2981\n",
      "3103 ---- 2982\n",
      "3104 ---- 2983\n",
      "3105 ---- 2984\n",
      "3106 ---- 2985\n",
      "3107 ---- 2986\n",
      "3108 ---- 2987\n",
      "3109 ---- 2988\n",
      "3110 ---- 2989\n",
      "3111 ---- 2990\n",
      "3112 ---- 2991\n",
      "3113 ---- 2992\n",
      "3114 ---- 2993\n",
      "3115 ---- 2994\n",
      "3116 ---- 2995\n",
      "3117 ---- 2996\n",
      "3118 ---- 2997\n",
      "3119 ---- 2998\n",
      "3120 ---- 2999\n",
      "3121 ---- 3000\n",
      "3122 ---- 3001\n",
      "3123 ---- 3002\n",
      "3124 ---- 3003\n",
      "3125 ---- 3004\n",
      "3126 ---- 3005\n",
      "3127 ---- 3006\n",
      "3128 ---- 3007\n",
      "3129 ---- 3008\n",
      "3130 ---- 3009\n",
      "3131 ---- 3010\n",
      "3132 ---- 3011\n",
      "3133 ---- 3012\n",
      "3134 ---- 3013\n",
      "3135 ---- 3014\n",
      "3136 ---- 3015\n",
      "3137 ---- 3016\n",
      "3138 ---- 3017\n",
      "3139 ---- 3018\n",
      "3140 ---- 3019\n",
      "3141 ---- 3020\n",
      "3142 ---- 3021\n",
      "3143 ---- 3022\n",
      "3144 ---- 3023\n",
      "3145 ---- 3024\n",
      "3146 ---- 3025\n",
      "3147 ---- 3026\n",
      "3148 ---- 3027\n",
      "3149 ---- 3028\n",
      "3150 ---- 3029\n",
      "3151 ---- 3030\n",
      "3152 ---- 3031\n",
      "3153 ---- 3032\n",
      "3154 ---- 3033\n",
      "3155 ---- 3034\n",
      "3156 ---- 3035\n",
      "3157 ---- 3036\n",
      "3158 ---- 3037\n",
      "3159 ---- 3038\n",
      "3160 ---- 3039\n",
      "3161 ---- 3040\n",
      "3162 ---- 3041\n",
      "3163 ---- 3042\n",
      "3164 ---- 3043\n",
      "3165 ---- 3044\n",
      "3166 ---- 3045\n",
      "3167 ---- 3046\n",
      "3168 ---- 3047\n",
      "3169 ---- 3048\n",
      "3170 ---- 3049\n",
      "3171 ---- 3050\n",
      "3172 ---- 3051\n",
      "3173 ---- 3052\n",
      "3174 ---- 3053\n",
      "3175 ---- 3054\n",
      "3176 ---- 3055\n",
      "3177 ---- 3056\n",
      "3178 ---- 3057\n",
      "3179 ---- 3058\n",
      "3180 ---- 3059\n",
      "3181 ---- 3060\n",
      "3182 ---- 3061\n",
      "3183 ---- 3062\n",
      "3184 ---- 3063\n",
      "3185 ---- 3064\n",
      "3186 ---- 3065\n",
      "3187 ---- 3066\n",
      "3188 ---- 3067\n",
      "3189 ---- 3068\n",
      "3190 ---- 3069\n",
      "3191 ---- 3070\n",
      "3192 ---- 3071\n",
      "3193 ---- 3072\n",
      "3194 ---- 3073\n",
      "3195 ---- 3074\n",
      "3196 ---- 3075\n",
      "3197 ---- 3076\n",
      "3198 ---- 3077\n",
      "3199 ---- 3078\n",
      "3200 ---- 3079\n",
      "3201 ---- 3080\n",
      "3202 ---- 3081\n",
      "3203 ---- 3082\n",
      "3204 ---- 3083\n",
      "3205 ---- 3084\n",
      "3206 ---- 3085\n",
      "3207 ---- 3086\n",
      "3208 ---- 3087\n",
      "3209 ---- 3088\n",
      "3210 ---- 3089\n",
      "3211 ---- 3090\n",
      "3212 ---- 3091\n",
      "3213 ---- 3092\n",
      "3214 ---- 3093\n",
      "3215 ---- 3094\n",
      "3216 ---- 3095\n",
      "3217 ---- 3096\n",
      "3218 ---- 3097\n",
      "3219 ---- 3098\n",
      "3220 ---- 3099\n",
      "3221 ---- 3100\n",
      "3222 ---- 3101\n",
      "3223 ---- 3102\n",
      "3224 ---- 3103\n",
      "3225 ---- 3104\n",
      "3226 ---- 3105\n",
      "3227 ---- 3106\n",
      "3228 ---- 3107\n",
      "3229 ---- 3108\n",
      "3230 ---- 3109\n",
      "3231 ---- 3110\n",
      "3232 ---- 3111\n",
      "3233 ---- 3112\n",
      "3234 ---- 3113\n",
      "3235 ---- 3114\n",
      "3236 ---- 3115\n",
      "3237 ---- 3116\n",
      "3238 ---- 3117\n",
      "3239 ---- 3118\n",
      "3240 ---- 3119\n",
      "3241 ---- 3120\n",
      "3242 ---- 3121\n",
      "3243 ---- 3122\n",
      "3244 ---- 3123\n",
      "3245 ---- 3124\n",
      "3246 ---- 3125\n",
      "3247 ---- 3126\n",
      "3248 ---- 3127\n",
      "3249 ---- 3128\n",
      "3250 ---- 3129\n",
      "3251 ---- 3130\n",
      "3252 ---- 3131\n",
      "3253 ---- 3132\n",
      "3254 ---- 3133\n",
      "3255 ---- 3134\n",
      "3256 ---- 3135\n",
      "3257 ---- 3136\n",
      "3258 ---- 3137\n",
      "3259 ---- 3138\n",
      "3260 ---- 3139\n",
      "3261 ---- 3140\n",
      "3262 ---- 3141\n",
      "3263 ---- 3142\n",
      "3264 ---- 3143\n",
      "3265 ---- 3144\n",
      "3266 ---- 3145\n",
      "3267 ---- 3146\n",
      "3268 ---- 3147\n",
      "3269 ---- 3148\n",
      "3270 ---- 3149\n",
      "3271 ---- 3150\n",
      "3272 ---- 3151\n",
      "3273 ---- 3152\n",
      "3274 ---- 3153\n",
      "3275 ---- 3154\n",
      "3276 ---- 3155\n",
      "3277 ---- 3156\n",
      "3278 ---- 3157\n",
      "3279 ---- 3158\n",
      "3280 ---- 3159\n",
      "3281 ---- 3160\n",
      "3282 ---- 3161\n",
      "3283 ---- 3162\n",
      "3284 ---- 3163\n",
      "3285 ---- 3164\n",
      "3286 ---- 3165\n",
      "3287 ---- 3166\n",
      "3288 ---- 3167\n",
      "3289 ---- 3168\n",
      "3290 ---- 3169\n",
      "3291 ---- 3170\n",
      "3292 ---- 3171\n",
      "3293 ---- 3172\n",
      "3294 ---- 3173\n",
      "3295 ---- 3174\n",
      "3296 ---- 3175\n",
      "3297 ---- 3176\n",
      "3298 ---- 3177\n",
      "3299 ---- 3178\n",
      "3300 ---- 3179\n",
      "3301 ---- 3180\n",
      "3302 ---- 3181\n",
      "3303 ---- 3182\n",
      "3304 ---- 3183\n",
      "3305 ---- 3184\n",
      "3306 ---- 3185\n",
      "3307 ---- 3186\n",
      "3308 ---- 3187\n",
      "3309 ---- 3188\n",
      "3310 ---- 3189\n",
      "3311 ---- 3190\n",
      "3312 ---- 3191\n",
      "3313 ---- 3192\n",
      "3314 ---- 3193\n",
      "3315 ---- 3194\n",
      "3316 ---- 3195\n",
      "3317 ---- 3196\n",
      "3318 ---- 3197\n",
      "3319 ---- 3198\n",
      "3320 ---- 3199\n",
      "3321 ---- 3200\n",
      "3322 ---- 3201\n",
      "3323 ---- 3202\n",
      "3324 ---- 3203\n",
      "3325 ---- 3204\n",
      "3326 ---- 3205\n",
      "3327 ---- 3206\n",
      "3328 ---- 3207\n",
      "3329 ---- 3208\n",
      "3330 ---- 3209\n",
      "3331 ---- 3210\n",
      "3332 ---- 3211\n",
      "3333 ---- 3212\n",
      "3334 ---- 3213\n",
      "3335 ---- 3214\n",
      "3336 ---- 3215\n",
      "3337 ---- 3216\n",
      "3338 ---- 3217\n",
      "3339 ---- 3218\n",
      "3340 ---- 3219\n",
      "3341 ---- 3220\n",
      "3342 ---- 3221\n",
      "3343 ---- 3222\n",
      "3344 ---- 3223\n",
      "3345 ---- 3224\n",
      "3346 ---- 3225\n",
      "3347 ---- 3226\n",
      "3348 ---- 3227\n",
      "3349 ---- 3228\n",
      "3350 ---- 3229\n",
      "3351 ---- 3230\n",
      "3352 ---- 3231\n",
      "3353 ---- 3232\n",
      "3354 ---- 3233\n",
      "3355 ---- 3234\n",
      "3356 ---- 3235\n",
      "3357 ---- 3236\n",
      "3358 ---- 3237\n",
      "3359 ---- 3238\n",
      "3360 ---- 3239\n",
      "3361 ---- 3240\n",
      "3362 ---- 3241\n",
      "3363 ---- 3242\n",
      "3364 ---- 3243\n",
      "3365 ---- 3244\n",
      "3366 ---- 3245\n",
      "3367 ---- 3246\n",
      "3368 ---- 3247\n",
      "3369 ---- 3248\n",
      "3370 ---- 3249\n",
      "3371 ---- 3250\n",
      "3372 ---- 3251\n",
      "3373 ---- 3252\n",
      "3374 ---- 3253\n",
      "3375 ---- 3254\n",
      "3376 ---- 3255\n",
      "3377 ---- 3256\n",
      "3378 ---- 3257\n",
      "3379 ---- 3258\n",
      "3380 ---- 3259\n",
      "3381 ---- 3260\n",
      "3382 ---- 3261\n",
      "3383 ---- 3262\n",
      "3384 ---- 3263\n",
      "3385 ---- 3264\n",
      "3386 ---- 3265\n",
      "3387 ---- 3266\n",
      "3388 ---- 3267\n",
      "3389 ---- 3268\n",
      "3390 ---- 3269\n",
      "3391 ---- 3270\n",
      "3392 ---- 3271\n",
      "3393 ---- 3272\n",
      "3394 ---- 3273\n",
      "3395 ---- 3274\n",
      "3396 ---- 3275\n",
      "3397 ---- 3276\n",
      "3398 ---- 3277\n",
      "3399 ---- 3278\n",
      "3400 ---- 3279\n",
      "3401 ---- 3280\n",
      "3402 ---- 3281\n",
      "3403 ---- 3282\n",
      "3404 ---- 3283\n",
      "3405 ---- 3284\n",
      "3406 ---- 3285\n",
      "3407 ---- 3286\n",
      "3408 ---- 3287\n",
      "3409 ---- 3288\n",
      "3410 ---- 3289\n",
      "3411 ---- 3290\n",
      "3412 ---- 3291\n",
      "3413 ---- 3292\n",
      "3414 ---- 3293\n",
      "3415 ---- 3294\n",
      "3416 ---- 3295\n",
      "3417 ---- 3296\n",
      "3418 ---- 3297\n",
      "3419 ---- 3298\n",
      "3420 ---- 3299\n",
      "3421 ---- 3300\n",
      "3422 ---- 3301\n",
      "3423 ---- 3302\n",
      "3424 ---- 3303\n",
      "3425 ---- 3304\n",
      "3426 ---- 3305\n",
      "3427 ---- 3306\n",
      "3428 ---- 3307\n",
      "3429 ---- 3308\n",
      "3430 ---- 3309\n",
      "3431 ---- 3310\n",
      "3432 ---- 3311\n",
      "3433 ---- 3312\n",
      "3434 ---- 3313\n",
      "3435 ---- 3314\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(X_train_final.columns)):\n",
    "    print('{} ---- {}'.format(i, X_train_final.columns[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR POS+TAG+NAMED ENTITY + EMBEDDING\n",
    "\n",
    "Baseline: 0.79 0.88 0.82 PRESICION RECALL F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_final.drop(index=1130, inplace=True)\n",
    "# X_train_final.reset_index(drop=True, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train.drop(index=1130, inplace=True)\n",
    "# y_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ner</th>\n",
       "      <th>interjections</th>\n",
       "      <th>nouns</th>\n",
       "      <th>adverb</th>\n",
       "      <th>verb</th>\n",
       "      <th>determiner</th>\n",
       "      <th>pronoun</th>\n",
       "      <th>adjetive</th>\n",
       "      <th>preposition</th>\n",
       "      <th>embedding0</th>\n",
       "      <th>...</th>\n",
       "      <th>embedding90</th>\n",
       "      <th>embedding91</th>\n",
       "      <th>embedding92</th>\n",
       "      <th>embedding93</th>\n",
       "      <th>embedding94</th>\n",
       "      <th>embedding95</th>\n",
       "      <th>embedding96</th>\n",
       "      <th>embedding97</th>\n",
       "      <th>embedding98</th>\n",
       "      <th>embedding99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.135122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.398905</td>\n",
       "      <td>0.203060</td>\n",
       "      <td>-0.029306</td>\n",
       "      <td>0.052515</td>\n",
       "      <td>0.202198</td>\n",
       "      <td>-0.057338</td>\n",
       "      <td>0.199040</td>\n",
       "      <td>-0.080122</td>\n",
       "      <td>-0.005783</td>\n",
       "      <td>0.119288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.192171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400934</td>\n",
       "      <td>0.204706</td>\n",
       "      <td>-0.026469</td>\n",
       "      <td>0.006745</td>\n",
       "      <td>0.264669</td>\n",
       "      <td>-0.033317</td>\n",
       "      <td>0.265443</td>\n",
       "      <td>-0.098124</td>\n",
       "      <td>-0.034243</td>\n",
       "      <td>0.090234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.392725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620449</td>\n",
       "      <td>0.376133</td>\n",
       "      <td>-0.047489</td>\n",
       "      <td>-0.000567</td>\n",
       "      <td>0.266267</td>\n",
       "      <td>-0.014218</td>\n",
       "      <td>0.247715</td>\n",
       "      <td>-0.095664</td>\n",
       "      <td>-0.070036</td>\n",
       "      <td>0.091635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.151105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.399699</td>\n",
       "      <td>0.175391</td>\n",
       "      <td>-0.029511</td>\n",
       "      <td>0.037338</td>\n",
       "      <td>0.207215</td>\n",
       "      <td>-0.050405</td>\n",
       "      <td>0.262613</td>\n",
       "      <td>-0.085173</td>\n",
       "      <td>-0.006876</td>\n",
       "      <td>0.107373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.170550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.442020</td>\n",
       "      <td>0.211859</td>\n",
       "      <td>-0.005990</td>\n",
       "      <td>0.028853</td>\n",
       "      <td>0.234871</td>\n",
       "      <td>-0.035093</td>\n",
       "      <td>0.256147</td>\n",
       "      <td>-0.100829</td>\n",
       "      <td>-0.021365</td>\n",
       "      <td>0.111609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8850</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.192962</td>\n",
       "      <td>...</td>\n",
       "      <td>0.468351</td>\n",
       "      <td>0.180379</td>\n",
       "      <td>-0.053934</td>\n",
       "      <td>0.034485</td>\n",
       "      <td>0.305743</td>\n",
       "      <td>-0.031188</td>\n",
       "      <td>0.330835</td>\n",
       "      <td>-0.030458</td>\n",
       "      <td>0.011676</td>\n",
       "      <td>0.099761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8851</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.163229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.479692</td>\n",
       "      <td>0.298419</td>\n",
       "      <td>-0.022670</td>\n",
       "      <td>0.051767</td>\n",
       "      <td>0.214952</td>\n",
       "      <td>-0.024214</td>\n",
       "      <td>0.217348</td>\n",
       "      <td>-0.101953</td>\n",
       "      <td>-0.011261</td>\n",
       "      <td>0.146930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8852</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.171035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396606</td>\n",
       "      <td>0.190240</td>\n",
       "      <td>-0.047478</td>\n",
       "      <td>0.034147</td>\n",
       "      <td>0.260713</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.227230</td>\n",
       "      <td>-0.132668</td>\n",
       "      <td>-0.028913</td>\n",
       "      <td>0.046274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8853</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.219588</td>\n",
       "      <td>...</td>\n",
       "      <td>0.390188</td>\n",
       "      <td>0.125623</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>-0.017495</td>\n",
       "      <td>0.204110</td>\n",
       "      <td>-0.087263</td>\n",
       "      <td>0.278633</td>\n",
       "      <td>-0.141694</td>\n",
       "      <td>-0.110091</td>\n",
       "      <td>0.161391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8854</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.202825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389158</td>\n",
       "      <td>0.225639</td>\n",
       "      <td>-0.084716</td>\n",
       "      <td>-0.058986</td>\n",
       "      <td>0.448494</td>\n",
       "      <td>0.007002</td>\n",
       "      <td>0.280687</td>\n",
       "      <td>-0.156023</td>\n",
       "      <td>-0.054155</td>\n",
       "      <td>-0.005955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8855 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ner  interjections  nouns  adverb  verb  determiner  pronoun  adjetive  \\\n",
       "0       0              0      9       0     4           3        1         1   \n",
       "1       0              0      8       0     2           2        0         2   \n",
       "2       0              0      1       1     0           0        0         0   \n",
       "3       0              0      8       4     7           1        2         1   \n",
       "4       0              0      8       3     4           3        0         1   \n",
       "...   ...            ...    ...     ...   ...         ...      ...       ...   \n",
       "8850    0              0      4       1     5           1        1         2   \n",
       "8851    0              0      5       1     4           2        0         1   \n",
       "8852    0              0      9       2     5           1        3         2   \n",
       "8853    0              0      4       0     1           0        0         0   \n",
       "8854    0              0      2       1     1           3        0         0   \n",
       "\n",
       "      preposition  embedding0  ...  embedding90  embedding91  embedding92  \\\n",
       "0               2   -0.135122  ...     0.398905     0.203060    -0.029306   \n",
       "1               3   -0.192171  ...     0.400934     0.204706    -0.026469   \n",
       "2               0   -0.392725  ...     0.620449     0.376133    -0.047489   \n",
       "3               3   -0.151105  ...     0.399699     0.175391    -0.029511   \n",
       "4               4   -0.170550  ...     0.442020     0.211859    -0.005990   \n",
       "...           ...         ...  ...          ...          ...          ...   \n",
       "8850            0   -0.192962  ...     0.468351     0.180379    -0.053934   \n",
       "8851            2   -0.163229  ...     0.479692     0.298419    -0.022670   \n",
       "8852            4   -0.171035  ...     0.396606     0.190240    -0.047478   \n",
       "8853            0   -0.219588  ...     0.390188     0.125623     0.001212   \n",
       "8854            0   -0.202825  ...     0.389158     0.225639    -0.084716   \n",
       "\n",
       "      embedding93  embedding94  embedding95  embedding96  embedding97  \\\n",
       "0        0.052515     0.202198    -0.057338     0.199040    -0.080122   \n",
       "1        0.006745     0.264669    -0.033317     0.265443    -0.098124   \n",
       "2       -0.000567     0.266267    -0.014218     0.247715    -0.095664   \n",
       "3        0.037338     0.207215    -0.050405     0.262613    -0.085173   \n",
       "4        0.028853     0.234871    -0.035093     0.256147    -0.100829   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "8850     0.034485     0.305743    -0.031188     0.330835    -0.030458   \n",
       "8851     0.051767     0.214952    -0.024214     0.217348    -0.101953   \n",
       "8852     0.034147     0.260713     0.001284     0.227230    -0.132668   \n",
       "8853    -0.017495     0.204110    -0.087263     0.278633    -0.141694   \n",
       "8854    -0.058986     0.448494     0.007002     0.280687    -0.156023   \n",
       "\n",
       "      embedding98  embedding99  \n",
       "0       -0.005783     0.119288  \n",
       "1       -0.034243     0.090234  \n",
       "2       -0.070036     0.091635  \n",
       "3       -0.006876     0.107373  \n",
       "4       -0.021365     0.111609  \n",
       "...           ...          ...  \n",
       "8850     0.011676     0.099761  \n",
       "8851    -0.011261     0.146930  \n",
       "8852    -0.028913     0.046274  \n",
       "8853    -0.110091     0.161391  \n",
       "8854    -0.054155    -0.005955  \n",
       "\n",
       "[8855 rows x 109 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_1 = X_train_final.iloc[:, np.r_[12, 13:21, 21:121]]\n",
    "X_train_model_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8855, 109)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_1 = X_test_final.iloc[:, np.r_[12, 13:21, 21:121]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(985, 109)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.254\n",
      "Best parameters set:\n",
      "\tclf__C: 25\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.70      0.81       896\n",
      "           1       0.19      0.73      0.31        89\n",
      "\n",
      "    accuracy                           0.70       985\n",
      "   macro avg       0.58      0.72      0.56       985\n",
      "weighted avg       0.89      0.70      0.77       985\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_1_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_1_pipeline, parameters, scoring=\"average_precision\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_1,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_1), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 627, False Positive: 269, False Negative: 24, True Positive: 65\n",
      "--------------------------------------------------------------------------------\n",
      "[[627 269]\n",
      " [ 24  65]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.70      0.81       896\n",
      "           1       0.19      0.73      0.31        89\n",
      "\n",
      "    accuracy                           0.70       985\n",
      "   macro avg       0.58      0.72      0.56       985\n",
      "weighted avg       0.89      0.70      0.77       985\n",
      "\n",
      "Average Precision: 0.1665\n"
     ]
    }
   ],
   "source": [
    "lr_model_1 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_1, y_train)\n",
    "y_lr = lr_model_1.predict(X_test_model_1)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))\n",
    "\n",
    "# Calculate and print the average precision score\n",
    "avg_precision = average_precision_score(y_test, y_lr)\n",
    "print(f'Average Precision: {avg_precision:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Model 6: Without STEM Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_6 = X_train_final.iloc[:,np.r_[10:1308]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8855, 1298)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_6 = X_test_final.iloc[:,np.r_[10:1308]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(985, 1298)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Best score: 0.260\n",
      "Best parameters set:\n",
      "\tclf__C: 0.09\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'liblinear'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.74      0.83       896\n",
      "           1       0.21      0.70      0.32        89\n",
      "\n",
      "    accuracy                           0.73       985\n",
      "   macro avg       0.58      0.72      0.58       985\n",
      "weighted avg       0.89      0.73      0.79       985\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_6_pipeline = Pipeline([ \n",
    "                        ('clf', LogisticRegression(class_weight='balanced',random_state=18)),\n",
    "                       ])\n",
    "\n",
    "parameters = {\n",
    "               'clf__C': [0.001,.009,0.01,.09,1,5,10,25],\n",
    "               'clf__penalty' : [\"l2\"],\n",
    "               'clf__solver': ['liblinear']\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model_6_pipeline, parameters, scoring=\"average_precision\", cv = 10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_model_6,y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_6), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 659, False Positive: 237, False Negative: 27, True Positive: 62\n",
      "--------------------------------------------------------------------------------\n",
      "[[659 237]\n",
      " [ 27  62]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.74      0.83       896\n",
      "           1       0.21      0.70      0.32        89\n",
      "\n",
      "    accuracy                           0.73       985\n",
      "   macro avg       0.58      0.72      0.58       985\n",
      "weighted avg       0.89      0.73      0.79       985\n",
      "\n",
      "Average Precision: 0.1719\n"
     ]
    }
   ],
   "source": [
    "lr_model_6 = LogisticRegression(random_state=18, solver=best_parameters['clf__solver'], \n",
    "                                C=best_parameters['clf__C'], \n",
    "                                penalty=best_parameters['clf__penalty'], class_weight='balanced').fit(X_train_model_6, y_train)\n",
    "y_lr = lr_model_6.predict(X_test_model_6)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))\n",
    "\n",
    "# Calculate and print the average precision score\n",
    "avg_precision = average_precision_score(y_test, y_lr)\n",
    "print(f'Average Precision: {avg_precision:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF Model 4: Without Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [10,20,50,100,200,300]\n",
    "max_depth = [2,5,8,10,15,20]\n",
    "min_samples_split = [2, 5, 10, 15, 20]\n",
    "min_samples_leaf = [1, 2, 5, 10,20]\n",
    "rf_parameters = dict(n_estimators = n_estimators, max_depth = max_depth,  \n",
    "              min_samples_split = min_samples_split, \n",
    "              min_samples_leaf = min_samples_leaf)\n",
    "\n",
    "## reduced parameters\n",
    "# n_estimators = [50, 100, 200]       # Reduced options\n",
    "# max_depth = [5, 10, 15]             # Reduced options\n",
    "# min_samples_split = [2, 10]         # Reduced options\n",
    "# min_samples_leaf = [1, 5]           # Reduced options\n",
    "\n",
    "# rf_parameters = dict(\n",
    "#     n_estimators = n_estimators, \n",
    "#     max_depth = max_depth,  \n",
    "#     min_samples_split = min_samples_split, \n",
    "#     min_samples_leaf = min_samples_leaf\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_model_4 = X_train_final.iloc[:, np.r_[12, 13:21, 21:121]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8855, 109)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_4 = X_train_final.iloc[:, np.r_[12, 13:21, 21:121]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8855, 109)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 900 candidates, totalling 9000 fits\n",
      "Best score: 0.255\n",
      "Best parameters set:\n",
      "\tmax_depth: 10\n",
      "\tmin_samples_leaf: 2\n",
      "\tmin_samples_split: 20\n",
      "\tn_estimators: 100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [985, 8855]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[179], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(rf_parameters\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (param_name, best_parameters[param_name]))\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_model_4\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdigits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2545\u001b[0m, in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2410\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   2411\u001b[0m     {\n\u001b[1;32m   2412\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2436\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2437\u001b[0m ):\n\u001b[1;32m   2438\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[1;32m   2439\u001b[0m \n\u001b[1;32m   2440\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2542\u001b[0m \u001b[38;5;124;03m    <BLANKLINE>\u001b[39;00m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2545\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2548\u001b[0m         labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.8/site-packages/sklearn/utils/validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    405\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    410\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [985, 8855]"
     ]
    }
   ],
   "source": [
    "rf_model_4 = RandomForestClassifier(random_state=18,class_weight='balanced')\n",
    "grid_search = GridSearchCV(rf_model_4, rf_parameters, scoring=\"average_precision\", cv = 10, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_model_4,y_train)\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(rf_parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "\n",
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test_model_4), digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Classifier\n",
      "True Negative: 71, False Positive: 19, False Negative: 19, True Positive: 70\n",
      "--------------------------------------------------------------------------------\n",
      "[[71 19]\n",
      " [19 70]]\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.79      0.79        90\n",
      "           1       0.79      0.79      0.79        89\n",
      "\n",
      "    accuracy                           0.79       179\n",
      "   macro avg       0.79      0.79      0.79       179\n",
      "weighted avg       0.79      0.79      0.79       179\n",
      "\n",
      "Average Precision: 0.7248\n"
     ]
    }
   ],
   "source": [
    "randomForest_4 = RandomForestClassifier(random_state=18,\n",
    "                                        class_weight=best_parameters['class_weight'],\n",
    "                                        max_depth=best_parameters['max_depth'],\n",
    "                                        min_samples_leaf=best_parameters['min_samples_leaf'],\n",
    "                                        min_samples_split=best_parameters['min_samples_split'],\n",
    "                                        n_estimators=best_parameters['n_estimators']).fit(X_train_model_4, y_train)\n",
    "\n",
    "y_lr = randomForest_4.predict(X_test_model_4)\n",
    "print('Logistic regression Classifier')\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_lr).ravel()\n",
    "print('True Negative: {}, False Positive: {}, False Negative: {}, True Positive: {}'.format(tn, fp, fn, tp))\n",
    "print('-' * 80)\n",
    "print(confusion_matrix(y_test, y_lr))\n",
    "print('-' * 80)\n",
    "print(classification_report(y_test, y_lr))\n",
    "\n",
    "# Calculate and print the average precision score\n",
    "avg_precision = average_precision_score(y_test, y_lr)\n",
    "print(f'Average Precision: {avg_precision:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
